{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyNDQ4NDU1", "number": 10371, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODo1NzoyNFrOEiQI_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDoxMDo1OVrOEkzxqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzUxNDg3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODo1NzoyNFrOHQBvag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMToyMzo1NVrOHRPTfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2Nzc4Ng==", "bodyText": "We shouldn't change the metric name for backwards compatibility.", "url": "https://github.com/apache/druid/pull/10371#discussion_r486567786", "createdAt": "2020-09-10T18:57:24Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgzODU4OQ==", "bodyText": "Revert back", "url": "https://github.com/apache/druid/pull/10371#discussion_r487838589", "createdAt": "2020-09-14T11:23:55Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2Nzc4Ng=="}, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzUyNTc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTowMDozMlrOHQB2KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMToyNDoyMlrOHRPURw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2OTUxMg==", "bodyText": "Even though this changes the metric name, it seems fine since this metrics has never been emitted because of the metric name mismatch (segmentsWaitCompact and segmentSizeWaitCompact).", "url": "https://github.com/apache/druid/pull/10371#discussion_r486569512", "createdAt": "2020-09-10T19:00:32Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",\n             stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n     stats.forEachDataSourceStat(\n-        \"segmentsWaitCompact\",\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n         (final String dataSource, final long count) -> {\n           emitter.emit(\n               new ServiceMetricEvent.Builder()\n                   .setDimension(DruidMetrics.DATASOURCE, dataSource)\n-                  .build(\"segment/waitCompact/count\", count)\n+                  .build(\"segment/compacted/intervalCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgzODc5MQ==", "bodyText": "Yep \ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10371#discussion_r487838791", "createdAt": "2020-09-14T11:24:22Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",\n             stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n     stats.forEachDataSourceStat(\n-        \"segmentsWaitCompact\",\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n         (final String dataSource, final long count) -> {\n           emitter.emit(\n               new ServiceMetricEvent.Builder()\n                   .setDimension(DruidMetrics.DATASOURCE, dataSource)\n-                  .build(\"segment/waitCompact/count\", count)\n+                  .build(\"segment/compacted/intervalCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2OTUxMg=="}, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzU0NzU0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTowNzoxM1rOHQCDww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMToyNTo0MFrOHRPW0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3Mjk5NQ==", "bodyText": "Why not byteCountProcessed to align with other metrics?", "url": "https://github.com/apache/druid/pull/10371#discussion_r486572995", "createdAt": "2020-09-10T19:07:13Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteAwaitingCompaction;\n+  @JsonProperty\n+  private long byteProcessed;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgzOTQ0MA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r487839440", "createdAt": "2020-09-14T11:25:40Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteAwaitingCompaction;\n+  @JsonProperty\n+  private long byteProcessed;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3Mjk5NQ=="}, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTM0NTE3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMTowODoyMFrOHRs0ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNDo0NToyOVrOHSf3lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ==", "bodyText": "suggest just \"bytes\" instead of \"byteCount\" for the property name", "url": "https://github.com/apache/druid/pull/10371#discussion_r488322171", "createdAt": "2020-09-15T01:08:20Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM5MjQwNQ==", "bodyText": "Initially I had it as \"bytes\" but @jihoonson suggested \"byteCount\".", "url": "https://github.com/apache/druid/pull/10371#discussion_r488392405", "createdAt": "2020-09-15T05:21:51Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ=="}, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM5Mjg5OQ==", "bodyText": "Im fine either way. \"byteCount\" matches it with the other Counts", "url": "https://github.com/apache/druid/pull/10371#discussion_r488392899", "createdAt": "2020-09-15T05:23:39Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ=="}, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODg3ODc0OQ==", "bodyText": "IIRC, it was byte not bytes. bytes sounds better to me too.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488878749", "createdAt": "2020-09-15T18:31:48Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ=="}, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE1ODU0OA==", "bodyText": "Changed to bytesAwaitingCompaction and bytesProcessed", "url": "https://github.com/apache/druid/pull/10371#discussion_r489158548", "createdAt": "2020-09-16T04:45:29Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ=="}, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTc2MDU2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDo1MTowNFrOHRwk6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDo1ODoyNFrOHRwtaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzcyMw==", "bodyText": "I don't think I understand this comment, if all iteration is done (by that do you mean compactibleTimelineObjectHolderCursor.hasNext returns false?), then iterateAllSegments would do nothing.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488383723", "createdAt": "2020-09-15T04:51:04Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -112,27 +114,38 @@\n   }\n \n   @Override\n-  public Object2LongOpenHashMap<String> totalRemainingSegmentsSizeBytes()\n+  public Map<String, CompactionStatistics> totalRemainingStatistics()\n   {\n-    final Object2LongOpenHashMap<String> resultMap = new Object2LongOpenHashMap<>();\n-    resultMap.defaultReturnValue(UNKNOWN_TOTAL_REMAINING_SEGMENTS_SIZE);\n-    for (QueueEntry entry : queue) {\n-      final VersionedIntervalTimeline<String, DataSegment> timeline = dataSources.get(entry.getDataSource());\n-      final Interval interval = new Interval(timeline.first().getInterval().getStart(), entry.interval.getEnd());\n-\n-      final List<TimelineObjectHolder<String, DataSegment>> holders = timeline.lookup(interval);\n-\n-      long size = 0;\n-      for (DataSegment segment : FluentIterable\n-          .from(holders)\n-          .transformAndConcat(TimelineObjectHolder::getObject)\n-          .transform(PartitionChunk::getObject)) {\n-        size += segment.getSize();\n-      }\n+    return remainingSegments;\n+  }\n+\n+  @Override\n+  public Map<String, CompactionStatistics> totalProcessedStatistics()\n+  {\n+    return processedSegments;\n+  }\n \n-      resultMap.put(entry.getDataSource(), size);\n+  @Override\n+  public void flushAllSegments()\n+  {\n+    if (queue.isEmpty()) {\n+      return;\n+    }\n+    QueueEntry entry;\n+    while ((entry = queue.poll()) != null) {\n+      final List<DataSegment> resultSegments = entry.segments;\n+      final String dataSourceName = resultSegments.get(0).getDataSource();\n+      // This entry was in the queue, meaning that it was not processed. Hence, also aggregates it's\n+      // statistic to the remaining segments counts.\n+      collectSegmentStatistics(remainingSegments, dataSourceName, new SegmentsToCompact(entry.segments));\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor = timelineIterators.get(\n+          dataSourceName\n+      );\n+      // WARNING: This iterates the compactibleTimelineObjectHolderCursor.\n+      // Since this method is intended to only be use after all necessary iteration is done on this iterator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4NTg5Nw==", "bodyText": "I mean that this method (flushAllSegments), will iterates the compactibleTimelineObjectHolderCursor in iterateAllSegments method call. This class NewestSegmentFirstIterator when use as a iterator in the next() method will also iterate the compactibleTimelineObjectHolderCursor.\nHence, this iterator (NewestSegmentFirstIterator) cannot be use to iterate after this method (flushAllSegments`) is called.\nBasically, you cannot call flushAllSegments while iterating the NewestSegmentFirstIterator and you cannot call flushAllSegments then go back to iterating the NewestSegmentFirstIterator", "url": "https://github.com/apache/druid/pull/10371#discussion_r488385897", "createdAt": "2020-09-15T04:58:24Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -112,27 +114,38 @@\n   }\n \n   @Override\n-  public Object2LongOpenHashMap<String> totalRemainingSegmentsSizeBytes()\n+  public Map<String, CompactionStatistics> totalRemainingStatistics()\n   {\n-    final Object2LongOpenHashMap<String> resultMap = new Object2LongOpenHashMap<>();\n-    resultMap.defaultReturnValue(UNKNOWN_TOTAL_REMAINING_SEGMENTS_SIZE);\n-    for (QueueEntry entry : queue) {\n-      final VersionedIntervalTimeline<String, DataSegment> timeline = dataSources.get(entry.getDataSource());\n-      final Interval interval = new Interval(timeline.first().getInterval().getStart(), entry.interval.getEnd());\n-\n-      final List<TimelineObjectHolder<String, DataSegment>> holders = timeline.lookup(interval);\n-\n-      long size = 0;\n-      for (DataSegment segment : FluentIterable\n-          .from(holders)\n-          .transformAndConcat(TimelineObjectHolder::getObject)\n-          .transform(PartitionChunk::getObject)) {\n-        size += segment.getSize();\n-      }\n+    return remainingSegments;\n+  }\n+\n+  @Override\n+  public Map<String, CompactionStatistics> totalProcessedStatistics()\n+  {\n+    return processedSegments;\n+  }\n \n-      resultMap.put(entry.getDataSource(), size);\n+  @Override\n+  public void flushAllSegments()\n+  {\n+    if (queue.isEmpty()) {\n+      return;\n+    }\n+    QueueEntry entry;\n+    while ((entry = queue.poll()) != null) {\n+      final List<DataSegment> resultSegments = entry.segments;\n+      final String dataSourceName = resultSegments.get(0).getDataSource();\n+      // This entry was in the queue, meaning that it was not processed. Hence, also aggregates it's\n+      // statistic to the remaining segments counts.\n+      collectSegmentStatistics(remainingSegments, dataSourceName, new SegmentsToCompact(entry.segments));\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor = timelineIterators.get(\n+          dataSourceName\n+      );\n+      // WARNING: This iterates the compactibleTimelineObjectHolderCursor.\n+      // Since this method is intended to only be use after all necessary iteration is done on this iterator", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzcyMw=="}, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTc2MTI1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDo1MToyOFrOHRwlUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNDo1MzozOVrOHSf_Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ==", "bodyText": "Are there any concerns with performance overhead from this?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488383825", "createdAt": "2020-09-15T04:51:28Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM5MDQ0OA==", "bodyText": "I also thought of having two implementation of statistic calculation. We can have the old method which is just a sum of all segments from the earliest to the segment that we iterated up to. We can call this approximation and the downside is that it can overestimate if there are compacted intervals between the  earliest to the segment that we iterated up to. Then we can have this new implementation which is an exact calculation (i.e. we check each and every interval). Each datasource can choose which implementation to use for its stats calculation.\nHowever, I discussed this with @jihoonson and we think that this new implementation is actually ok to use as a default for all datasources. This code is run as part of Coordinator duty hence it is not on critical path and there is no client waiting, etc. While it is going through all segments, it is doing this at a interval level (we do batch of segments for each interval at a time). Thus, the looping is for every interval of every datasources (not every segments). Also, the auto compaction is run every 30 mins by default. This should be plenty of time for this code to finish before the next cycle begins.\nFurthermore, we can move the auto compaction duty to the last of the set of Coordinator duty and change the scheduling of the Coordinator duty from scheduleWithFixedDelay to scheduleWithFixedRate. This should ensures that the other duty still get run at every 30 mins (assuming the auto compaction doesn't go pass 30 mins which I doubt it will).", "url": "https://github.com/apache/druid/pull/10371#discussion_r488390448", "createdAt": "2020-09-15T05:14:51Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ=="}, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM5MjU4MQ==", "bodyText": "Made the change for auto compaction duty to the last of the set of Coordinator duty and change the scheduling of the Coordinator duty from scheduleWithFixedDelay to scheduleWithFixedRate. This should ensures that the other duty still get run at every 30 mins (assuming the auto compaction doesn't go pass 30 mins which I doubt it will).", "url": "https://github.com/apache/druid/pull/10371#discussion_r488392581", "createdAt": "2020-09-15T05:22:30Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ=="}, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4NDYzNg==", "bodyText": "Yeah it could be expensive, but I think it's fine since the compaction period is supposed to be long enough (30 min by default). It should be documented though that compaction period should be long enough to iterate all datasources.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488984636", "createdAt": "2020-09-15T21:31:03Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ=="}, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE2MDUzNQ==", "bodyText": "I'll add it to the Druid docs in the follow up PR. I did also add it as a comment in DruidCoordinator.java", "url": "https://github.com/apache/druid/pull/10371#discussion_r489160535", "createdAt": "2020-09-16T04:53:39Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ=="}, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTI5MzI5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDowOToxM1rOHSSvJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNDo1NTowMFrOHSgApA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0MzM5OA==", "bodyText": "This looks error-prone since all fields in this class are mutable. Suppose that this class was used as a key in a hash set. If you updated a field in this class after adding it to the set, its hash key would be different which will cause an unintended result. Even though it seems that hashCode() and equals() are used only in unit tests, I would suggest to make this class immutable.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488943398", "createdAt": "2020-09-15T20:09:13Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;\n+  @JsonProperty\n+  private long byteCountProcessed;\n+  @JsonProperty\n+  private long segmentCountAwaitingCompaction;\n+  @JsonProperty\n+  private long segmentCountProcessed;\n+  @JsonProperty\n+  private long intervalCountAwaitingCompaction;\n+  @JsonProperty\n+  private long intervalCountProcessed;\n+\n+  @JsonCreator\n+  public AutoCompactionSnapshot(\n+      @JsonProperty @NotNull String dataSource,\n+      @JsonProperty @NotNull AutoCompactionScheduleStatus scheduleStatus\n+  )\n+  {\n+    this.dataSource = dataSource;\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  @NotNull\n+  public String getDataSource()\n+  {\n+    return dataSource;\n+  }\n+\n+  @NotNull\n+  public AutoCompactionScheduleStatus getScheduleStatus()\n+  {\n+    return scheduleStatus;\n+  }\n+\n+  @Nullable\n+  public String getLatestScheduledTaskId()\n+  {\n+    return latestScheduledTaskId;\n+  }\n+\n+  public long getByteCountAwaitingCompaction()\n+  {\n+    return byteCountAwaitingCompaction;\n+  }\n+\n+  public long getByteCountProcessed()\n+  {\n+    return byteCountProcessed;\n+  }\n+\n+  public long getSegmentCountAwaitingCompaction()\n+  {\n+    return segmentCountAwaitingCompaction;\n+  }\n+\n+  public long getSegmentCountProcessed()\n+  {\n+    return segmentCountProcessed;\n+  }\n+\n+  public long getIntervalCountAwaitingCompaction()\n+  {\n+    return intervalCountAwaitingCompaction;\n+  }\n+\n+  public long getIntervalCountProcessed()\n+  {\n+    return intervalCountProcessed;\n+  }\n+\n+  public void setScheduleStatus(AutoCompactionScheduleStatus scheduleStatus)\n+  {\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  public void setLatestScheduledTaskId(String latestScheduledTaskId)\n+  {\n+    this.latestScheduledTaskId = latestScheduledTaskId;\n+  }\n+\n+  public void setByteCountAwaitingCompaction(long byteCountAwaitingCompaction)\n+  {\n+    this.byteCountAwaitingCompaction = byteCountAwaitingCompaction;\n+  }\n+\n+  public void setByteCountProcessed(long byteCountProcessed)\n+  {\n+    this.byteCountProcessed = byteCountProcessed;\n+  }\n+\n+  public void setSegmentCountAwaitingCompaction(long segmentCountAwaitingCompaction)\n+  {\n+    this.segmentCountAwaitingCompaction = segmentCountAwaitingCompaction;\n+  }\n+\n+  public void setSegmentCountProcessed(long segmentCountProcessed)\n+  {\n+    this.segmentCountProcessed = segmentCountProcessed;\n+  }\n+\n+  public void setIntervalCountAwaitingCompaction(long intervalCountAwaitingCompaction)\n+  {\n+    this.intervalCountAwaitingCompaction = intervalCountAwaitingCompaction;\n+  }\n+\n+  public void setIntervalCountProcessed(long intervalCountProcessed)\n+  {\n+    this.intervalCountProcessed = intervalCountProcessed;\n+  }\n+\n+\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    AutoCompactionSnapshot that = (AutoCompactionSnapshot) o;\n+    return byteCountAwaitingCompaction == that.byteCountAwaitingCompaction &&\n+           byteCountProcessed == that.byteCountProcessed &&\n+           segmentCountAwaitingCompaction == that.segmentCountAwaitingCompaction &&\n+           segmentCountProcessed == that.segmentCountProcessed &&\n+           intervalCountAwaitingCompaction == that.intervalCountAwaitingCompaction &&\n+           intervalCountProcessed == that.intervalCountProcessed &&\n+           dataSource.equals(that.dataSource) &&\n+           Objects.equals(scheduleStatus, that.scheduleStatus) &&\n+           Objects.equals(latestScheduledTaskId, that.latestScheduledTaskId);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(\n+        dataSource,\n+        scheduleStatus,\n+        latestScheduledTaskId,\n+        byteCountAwaitingCompaction,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE2MDg2OA==", "bodyText": "Done. Good catch", "url": "https://github.com/apache/druid/pull/10371#discussion_r489160868", "createdAt": "2020-09-16T04:55:00Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;\n+  @JsonProperty\n+  private long byteCountProcessed;\n+  @JsonProperty\n+  private long segmentCountAwaitingCompaction;\n+  @JsonProperty\n+  private long segmentCountProcessed;\n+  @JsonProperty\n+  private long intervalCountAwaitingCompaction;\n+  @JsonProperty\n+  private long intervalCountProcessed;\n+\n+  @JsonCreator\n+  public AutoCompactionSnapshot(\n+      @JsonProperty @NotNull String dataSource,\n+      @JsonProperty @NotNull AutoCompactionScheduleStatus scheduleStatus\n+  )\n+  {\n+    this.dataSource = dataSource;\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  @NotNull\n+  public String getDataSource()\n+  {\n+    return dataSource;\n+  }\n+\n+  @NotNull\n+  public AutoCompactionScheduleStatus getScheduleStatus()\n+  {\n+    return scheduleStatus;\n+  }\n+\n+  @Nullable\n+  public String getLatestScheduledTaskId()\n+  {\n+    return latestScheduledTaskId;\n+  }\n+\n+  public long getByteCountAwaitingCompaction()\n+  {\n+    return byteCountAwaitingCompaction;\n+  }\n+\n+  public long getByteCountProcessed()\n+  {\n+    return byteCountProcessed;\n+  }\n+\n+  public long getSegmentCountAwaitingCompaction()\n+  {\n+    return segmentCountAwaitingCompaction;\n+  }\n+\n+  public long getSegmentCountProcessed()\n+  {\n+    return segmentCountProcessed;\n+  }\n+\n+  public long getIntervalCountAwaitingCompaction()\n+  {\n+    return intervalCountAwaitingCompaction;\n+  }\n+\n+  public long getIntervalCountProcessed()\n+  {\n+    return intervalCountProcessed;\n+  }\n+\n+  public void setScheduleStatus(AutoCompactionScheduleStatus scheduleStatus)\n+  {\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  public void setLatestScheduledTaskId(String latestScheduledTaskId)\n+  {\n+    this.latestScheduledTaskId = latestScheduledTaskId;\n+  }\n+\n+  public void setByteCountAwaitingCompaction(long byteCountAwaitingCompaction)\n+  {\n+    this.byteCountAwaitingCompaction = byteCountAwaitingCompaction;\n+  }\n+\n+  public void setByteCountProcessed(long byteCountProcessed)\n+  {\n+    this.byteCountProcessed = byteCountProcessed;\n+  }\n+\n+  public void setSegmentCountAwaitingCompaction(long segmentCountAwaitingCompaction)\n+  {\n+    this.segmentCountAwaitingCompaction = segmentCountAwaitingCompaction;\n+  }\n+\n+  public void setSegmentCountProcessed(long segmentCountProcessed)\n+  {\n+    this.segmentCountProcessed = segmentCountProcessed;\n+  }\n+\n+  public void setIntervalCountAwaitingCompaction(long intervalCountAwaitingCompaction)\n+  {\n+    this.intervalCountAwaitingCompaction = intervalCountAwaitingCompaction;\n+  }\n+\n+  public void setIntervalCountProcessed(long intervalCountProcessed)\n+  {\n+    this.intervalCountProcessed = intervalCountProcessed;\n+  }\n+\n+\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    AutoCompactionSnapshot that = (AutoCompactionSnapshot) o;\n+    return byteCountAwaitingCompaction == that.byteCountAwaitingCompaction &&\n+           byteCountProcessed == that.byteCountProcessed &&\n+           segmentCountAwaitingCompaction == that.segmentCountAwaitingCompaction &&\n+           segmentCountProcessed == that.segmentCountProcessed &&\n+           intervalCountAwaitingCompaction == that.intervalCountAwaitingCompaction &&\n+           intervalCountProcessed == that.intervalCountProcessed &&\n+           dataSource.equals(that.dataSource) &&\n+           Objects.equals(scheduleStatus, that.scheduleStatus) &&\n+           Objects.equals(latestScheduledTaskId, that.latestScheduledTaskId);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(\n+        dataSource,\n+        scheduleStatus,\n+        latestScheduledTaskId,\n+        byteCountAwaitingCompaction,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0MzM5OA=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTI5OTI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDoxMTowMFrOHSSy4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNDo1Njo1MlrOHSgCzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0NDM1NQ==", "bodyText": "typo: takea -> take a", "url": "https://github.com/apache/druid/pull/10371#discussion_r488944355", "createdAt": "2020-09-15T20:11:00Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -632,8 +643,9 @@ private void stopBeingLeader()\n   {\n     List<CoordinatorDuty> duties = new ArrayList<>();\n     duties.add(new LogUsedSegments());\n-    duties.addAll(makeCompactSegmentsDuty());\n     duties.addAll(indexingServiceDuties);\n+    // CompactSegmentsDuty should be the last duty as it can takea long time", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE2MTQyMQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489161421", "createdAt": "2020-09-16T04:56:52Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -632,8 +643,9 @@ private void stopBeingLeader()\n   {\n     List<CoordinatorDuty> duties = new ArrayList<>();\n     duties.add(new LogUsedSegments());\n-    duties.addAll(makeCompactSegmentsDuty());\n     duties.addAll(indexingServiceDuties);\n+    // CompactSegmentsDuty should be the last duty as it can takea long time", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0NDM1NQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTMxODgyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDoxNjo0N1rOHSS-fA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDoxNjo0N1rOHSS-fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0NzMyNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10371#discussion_r488947324", "createdAt": "2020-09-15T20:16:47Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -569,7 +580,7 @@ private void becomeLeader()\n       }\n \n       for (final Pair<? extends DutiesRunnable, Duration> dutiesRunnable : dutiesRunnables) {\n-        ScheduledExecutors.scheduleWithFixedDelay(\n+        ScheduledExecutors.scheduleAtFixedRate(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTM0OTA0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDoyNToyNlrOHSTQhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNToxNzo1NlrOHSgeiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk1MTk0Mw==", "bodyText": "It was my bad that updating totalSizesOfSegmentsAwaitingCompactionPerDataSource was not thread-safe, but it should be. It applies same to autoCompactionSnapshotPerDataSource. I think an easy way is storing autoCompactionSnapshotPerDataSource in an AtomicReference so that we can atomically update the reference to the hash map whenever we compute new snapshots.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488951943", "createdAt": "2020-09-15T20:25:26Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +70,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private HashMap<String, AutoCompactionSnapshot> autoCompactionSnapshotPerDataSource = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk2ODczMA==", "bodyText": "When you fix this concurrency issue, please add enough description about what concurrency issue exists here and how it is handled. It would be also useful to check out our concurrency checklist.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488968730", "createdAt": "2020-09-15T20:57:55Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +70,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private HashMap<String, AutoCompactionSnapshot> autoCompactionSnapshotPerDataSource = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk1MTk0Mw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE2ODUyMA==", "bodyText": "Changed the autoCompactionSnapshotPerDataSource into a AtomicReference<Map<String, AutoCompactionSnapshot>>.\nWe will build a new autoCompactionSnapshotPerDataSource of snapshots in each compaction run then replace (atomically) the whole map (autoCompactionSnapshotPerDataSource) at the end in makeStats", "url": "https://github.com/apache/druid/pull/10371#discussion_r489168520", "createdAt": "2020-09-16T05:17:56Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +70,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private HashMap<String, AutoCompactionSnapshot> autoCompactionSnapshotPerDataSource = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk1MTk0Mw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTQ0ODMyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMDo1NTozNVrOHSUNlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNjoxOToyOVrOHShvzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk2NzU3Mg==", "bodyText": "Duplicate call to autoCompactionSnapshotPerDataSource.get().", "url": "https://github.com/apache/druid/pull/10371#discussion_r488967572", "createdAt": "2020-09-15T20:55:35Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();\n+    // Statistics of all segments that still need compaction after this run\n+    Map<String, CompactionStatistics> allRemainingStatistics = iterator.totalRemainingStatistics();\n+    // Statistics of all segments either compacted or skipped after this run\n+    Map<String, CompactionStatistics> allProcessedStatistics = iterator.totalProcessedStatistics();\n+\n+    for (Map.Entry<String, AutoCompactionSnapshot> autoCompactionSnapshotEntry : autoCompactionSnapshotPerDataSource.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotEntry.getKey();\n+      CompactionStatistics remainingStatistics = allRemainingStatistics.get(dataSource);\n+      CompactionStatistics processedStatistics = allProcessedStatistics.get(dataSource);\n+\n+      long byteAwaitingCompaction = 0;\n+      long segmentCountAwaitingCompaction = 0;\n+      long intervalCountAwaitingCompaction = 0;\n+      if (remainingStatistics != null) {\n+        // If null means that all segments are either compacted or skipped.\n+        // Hence, we can leave these set to default value of 0. If not null, we set it to the collected statistic.\n+        byteAwaitingCompaction = remainingStatistics.getByteSum();\n+        segmentCountAwaitingCompaction = remainingStatistics.getSegmentNumberCountSum();\n+        intervalCountAwaitingCompaction = remainingStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      long byteProcessed = 0;\n+      long segmentCountProcessed = 0;\n+      long intervalCountProcessed = 0;\n+      if (processedStatistics != null) {\n+        byteProcessed = processedStatistics.getByteSum();\n+        segmentCountProcessed = processedStatistics.getSegmentNumberCountSum();\n+        intervalCountProcessed = processedStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      autoCompactionSnapshotEntry.getValue().setByteCountAwaitingCompaction(byteAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setByteCountProcessed(byteProcessed);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountAwaitingCompaction(segmentCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountProcessed(segmentCountProcessed);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountAwaitingCompaction(intervalCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountProcessed(intervalCountProcessed);\n+\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          byteAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          segmentCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          intervalCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          byteProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          segmentCountProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          intervalCountProcessed\n+      );\n+    }\n+\n     return stats;\n   }\n \n-  @SuppressWarnings(\"deprecation\") // Intentionally using boxing get() to return null if dataSource is unknown\n   @Nullable\n   public Long getTotalSizeOfSegmentsAwaitingCompaction(String dataSource)\n   {\n-    return totalSizesOfSegmentsAwaitingCompactionPerDataSource.get(dataSource);\n+    AutoCompactionSnapshot autoCompactionSnapshot = autoCompactionSnapshotPerDataSource.get(dataSource);\n+    if (autoCompactionSnapshot == null) {\n+      return null;\n+    }\n+    return autoCompactionSnapshotPerDataSource.get(dataSource).getByteCountAwaitingCompaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE4OTMyNg==", "bodyText": "Oops. Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489189326", "createdAt": "2020-09-16T06:19:29Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();\n+    // Statistics of all segments that still need compaction after this run\n+    Map<String, CompactionStatistics> allRemainingStatistics = iterator.totalRemainingStatistics();\n+    // Statistics of all segments either compacted or skipped after this run\n+    Map<String, CompactionStatistics> allProcessedStatistics = iterator.totalProcessedStatistics();\n+\n+    for (Map.Entry<String, AutoCompactionSnapshot> autoCompactionSnapshotEntry : autoCompactionSnapshotPerDataSource.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotEntry.getKey();\n+      CompactionStatistics remainingStatistics = allRemainingStatistics.get(dataSource);\n+      CompactionStatistics processedStatistics = allProcessedStatistics.get(dataSource);\n+\n+      long byteAwaitingCompaction = 0;\n+      long segmentCountAwaitingCompaction = 0;\n+      long intervalCountAwaitingCompaction = 0;\n+      if (remainingStatistics != null) {\n+        // If null means that all segments are either compacted or skipped.\n+        // Hence, we can leave these set to default value of 0. If not null, we set it to the collected statistic.\n+        byteAwaitingCompaction = remainingStatistics.getByteSum();\n+        segmentCountAwaitingCompaction = remainingStatistics.getSegmentNumberCountSum();\n+        intervalCountAwaitingCompaction = remainingStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      long byteProcessed = 0;\n+      long segmentCountProcessed = 0;\n+      long intervalCountProcessed = 0;\n+      if (processedStatistics != null) {\n+        byteProcessed = processedStatistics.getByteSum();\n+        segmentCountProcessed = processedStatistics.getSegmentNumberCountSum();\n+        intervalCountProcessed = processedStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      autoCompactionSnapshotEntry.getValue().setByteCountAwaitingCompaction(byteAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setByteCountProcessed(byteProcessed);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountAwaitingCompaction(segmentCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountProcessed(segmentCountProcessed);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountAwaitingCompaction(intervalCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountProcessed(intervalCountProcessed);\n+\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          byteAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          segmentCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          intervalCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          byteProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          segmentCountProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          intervalCountProcessed\n+      );\n+    }\n+\n     return stats;\n   }\n \n-  @SuppressWarnings(\"deprecation\") // Intentionally using boxing get() to return null if dataSource is unknown\n   @Nullable\n   public Long getTotalSizeOfSegmentsAwaitingCompaction(String dataSource)\n   {\n-    return totalSizesOfSegmentsAwaitingCompactionPerDataSource.get(dataSource);\n+    AutoCompactionSnapshot autoCompactionSnapshot = autoCompactionSnapshotPerDataSource.get(dataSource);\n+    if (autoCompactionSnapshot == null) {\n+      return null;\n+    }\n+    return autoCompactionSnapshotPerDataSource.get(dataSource).getByteCountAwaitingCompaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk2NzU3Mg=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 296}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTQ5NDU1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxMDowMFrOHSUqGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QwOToyODo0NlrOHTZi3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NDg3NQ==", "bodyText": "Thinking about new methods in this interface, I'm not sure if they are good since now every implementation of this interface should track of remaining and processed segments even though the tracking logic will be likely duplicate (even though we have only one implementation yet \ud83d\ude42). How about adding next(Map<String, CompactionStatistics> stats) so that CompactSegments can pass in an appropriate map? Then, it can just iterate over all remaining entries in the iterator without introducing any methods such as flushAllSegments() which seems to have a complicated contract. CompactionSegmentIterator will not extend Iterator anymore in this case.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488974875", "createdAt": "2020-09-15T21:10:00Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "diffHunk": "@@ -19,22 +19,45 @@\n \n package org.apache.druid.server.coordinator.duty;\n \n-import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;\n+import org.apache.druid.server.coordinator.CompactionStatistics;\n import org.apache.druid.timeline.DataSegment;\n \n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  * Segments in the lists which are the elements of this iterator are sorted according to the natural segment order\n  * (see {@link DataSegment#compareTo}).\n  */\n public interface CompactionSegmentIterator extends Iterator<List<DataSegment>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3NTIxNA==", "bodyText": "I don't understand the proposed alternative. What would the next(Map<String, CompactionStatistics> stats) do? What map is the appropriate map that CompactSegments will pass to this method?", "url": "https://github.com/apache/druid/pull/10371#discussion_r489275214", "createdAt": "2020-09-16T08:54:22Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "diffHunk": "@@ -19,22 +19,45 @@\n \n package org.apache.druid.server.coordinator.duty;\n \n-import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;\n+import org.apache.druid.server.coordinator.CompactionStatistics;\n import org.apache.druid.timeline.DataSegment;\n \n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  * Segments in the lists which are the elements of this iterator are sorted according to the natural segment order\n  * (see {@link DataSegment#compareTo}).\n  */\n public interface CompactionSegmentIterator extends Iterator<List<DataSegment>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NDg3NQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDAwMDkwNA==", "bodyText": "we can only know the skipped and compacted segments when we are iterating the segment timeline (doSegmentsNeedCompaction method in NewestSegmentFirstIterator). This code path isn\u2019t called by the next(). It is computed and stored as a QueueEntry into the queue before the next() is called. The next() method simply just poll the QueueEntry from the queue. This means that the skipped and compacted stats must be in the QueueEntry when the next() call polls the queue. Currently, we also do not store an entry into the queue if the List is empty (no segments to compact). This does not fit well with having to try include the skipped and compacted stats into the QueueEntry. For example, we still need to store and aggregates the skipped and compacted stats when the List is an empty List (no more segments for the datasource).\ntbh I think it is best to keep it this way. Also, since this (the current way or modifying the CompactionSegmentIterator) does not change the API, we can come back and modify it later. Also since NewestSegmentFirstIterator is the only CompactionSegmentIterator right now, the cost for changing isn't too high. Maybe a good time to re-visit this is if we are going to add a new iterator and see what will work best when we have multiple implementations of CompactionSegmentIterator", "url": "https://github.com/apache/druid/pull/10371#discussion_r490000904", "createdAt": "2020-09-17T06:30:22Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "diffHunk": "@@ -19,22 +19,45 @@\n \n package org.apache.druid.server.coordinator.duty;\n \n-import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;\n+import org.apache.druid.server.coordinator.CompactionStatistics;\n import org.apache.druid.timeline.DataSegment;\n \n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  * Segments in the lists which are the elements of this iterator are sorted according to the natural segment order\n  * (see {@link DataSegment#compareTo}).\n  */\n public interface CompactionSegmentIterator extends Iterator<List<DataSegment>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NDg3NQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDEwMzUxOA==", "bodyText": "Actually, I have another idea which I really like and I think works well + clean + simple (I hope you like it too).\nRemoved flushAllSegments() and totalRemainingStatistics()  from the CompactionSegmentIterator. Keep totalCompactedStatistics() and totalSkippedStatistics() in the CompactionSegmentIterator. The stats for totalCompactedStatistics() and totalSkippedStatistics()  is maintain and stored in the CompactionSegmentIterator (NewestSegmentFirstIterator). I think this makes perfect sense since the \"already compacted\" and \"skipped segments\" are never returned from the iterator\u2019s next() (in fact, it\u2019s not even put into the queue that backs the iterator). Hence, as we iterate through the already compacted and skipped segments, we should just aggregates the stats within the CompactionSegmentIterator and keep it within the CompactionSegmentIterator. The totalCompactedStatistics() and totalSkippedStatistics()  will simply just returns the aggregates up to the current point of the iterator being iterated. We will then simply just iterate the iterator as before when creating the compaction task and the List that is returned by the iterator is then aggregates into the AutoCompactionSnapshot.Builder's Compacted stats (this is in CompactSegments). Similarly, we will continue to iterate the iterator in makeStats() (after we run out of task slot), then the List that is returned by the iterator is then aggregates into AutoCompactionSnapshot.Builder's Remaining stats (this is in CompactSegments).\nThere are Stats maintained in both CompactSegments  and CompactionSegmentIterator . But I think it\u2019s much clearer since we separate the \u201cskipped/already compacted\u201d from the \u201csegments returned by the iterator\u201d. Also, no complex contracts in the CompactionSegmentIterator anymore . Logic in CompactionSegmentIterator  for totalCompactedStatistics() and totalSkippedStatistics()  is also very simple.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490103518", "createdAt": "2020-09-17T09:28:46Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "diffHunk": "@@ -19,22 +19,45 @@\n \n package org.apache.druid.server.coordinator.duty;\n \n-import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;\n+import org.apache.druid.server.coordinator.CompactionStatistics;\n import org.apache.druid.timeline.DataSegment;\n \n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  * Segments in the lists which are the elements of this iterator are sorted according to the natural segment order\n  * (see {@link DataSegment#compareTo}).\n  */\n public interface CompactionSegmentIterator extends Iterator<List<DataSegment>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NDg3NQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTUwMzc3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxMjo1M1rOHSUvlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNjoyNTo0MlrOHSh5jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjI3Nw==", "bodyText": "I think it should be segmentBytes.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976277", "createdAt": "2020-09-15T21:12:53Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE5MTgyMQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489191821", "createdAt": "2020-09-16T06:25:42Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjI3Nw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTUwNDQ2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxMzowOVrOHSUwBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNjoyNTo0N1rOHSh5tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjM4OQ==", "bodyText": "Same here. I think it should be segmentBytes.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976389", "createdAt": "2020-09-15T21:13:09Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE5MTg2MA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489191860", "createdAt": "2020-09-16T06:25:47Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjM4OQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTUwNjY3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxMzo1NVrOHSUxTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNzoyMToyM1rOHSje_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjcxNw==", "bodyText": "Should we emit metrics for skipped intervals segments as well?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976717", "createdAt": "2020-09-15T21:13:55Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIxNjY2NA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489216664", "createdAt": "2020-09-16T07:19:19Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjcxNw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIxNzc5MQ==", "bodyText": "Added metrics for skipped byte, skipped interval count and skipped segment counts", "url": "https://github.com/apache/druid/pull/10371#discussion_r489217791", "createdAt": "2020-09-16T07:21:23Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjcxNw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTUxNDc3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxNjoxNVrOHSU1-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNzoyMToxNVrOHSjetA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NzkxMg==", "bodyText": "Should we distinguish segments processed and segments skipped? Information about segments skipped will be useful that you can be aware of how many segments (or intervals) that auto compaction has skipped.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488977912", "createdAt": "2020-09-15T21:16:15Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIxNzcxNg==", "bodyText": "I agree. I split the processedSegments into compactedSegments and skippedSegments. Also added new fields in       autoCompactionSnapshot for the skipped byte, skipped interval count and skipped segment counts", "url": "https://github.com/apache/druid/pull/10371#discussion_r489217716", "createdAt": "2020-09-16T07:21:15Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NzkxMg=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTUxNjkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToxNzowMlrOHSU3Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNzowMTo1NVrOHSi5Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3ODI1OA==", "bodyText": "doSegmentsNeedCompaction()?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488978258", "createdAt": "2020-09-15T21:17:02Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+  }\n \n-      if (!candidates.isEmpty()) {\n-        final boolean isCompactibleSize = candidates.getTotalSize() <= inputSegmentSize;\n-        final boolean needsCompaction = needsCompaction(\n-            ClientCompactionTaskQueryTuningConfig.from(config.getTuningConfig(), config.getMaxRowsPerSegment()),\n-            candidates\n-        );\n+  /**\n+   * This method encapsulates the logic for checking if a given {@param candidates} needs compaction or not.\n+   * If {@param logCannotCompactReason} is true then the reason for {@param candidates} not needing compaction is\n+   * logged (for the case that {@param candidates} does not needs compaction).\n+   *\n+   * @return true if the {@param candidates} needs compaction, false if the {@param candidates} does not needs compaction\n+   */\n+  private boolean isSegmentsNeedCompact(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIwODA2Nw==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r489208067", "createdAt": "2020-09-16T07:01:55Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+  }\n \n-      if (!candidates.isEmpty()) {\n-        final boolean isCompactibleSize = candidates.getTotalSize() <= inputSegmentSize;\n-        final boolean needsCompaction = needsCompaction(\n-            ClientCompactionTaskQueryTuningConfig.from(config.getTuningConfig(), config.getMaxRowsPerSegment()),\n-            candidates\n-        );\n+  /**\n+   * This method encapsulates the logic for checking if a given {@param candidates} needs compaction or not.\n+   * If {@param logCannotCompactReason} is true then the reason for {@param candidates} not needing compaction is\n+   * logged (for the case that {@param candidates} does not needs compaction).\n+   *\n+   * @return true if the {@param candidates} needs compaction, false if the {@param candidates} does not needs compaction\n+   */\n+  private boolean isSegmentsNeedCompact(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3ODI1OA=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTU0MTMwOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToyNToyMVrOHSVGDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNzozNDowMVrOHSj7Lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4MjAzMQ==", "bodyText": "Hmm this test looks similar to testRun(). Can they be merged by moving the snapshot verification to assertCompactSegments() (or merging assertCompactSegmentStatistics and assertCompactSegments)?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488982031", "createdAt": "2020-09-15T21:25:21Z", "author": {"login": "jihoonson"}, "path": "server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java", "diffHunk": "@@ -270,12 +275,246 @@ public String get()\n     assertLastSegmentNotCompacted(compactSegments);\n   }\n \n+  @Test\n+  public void testMakeStats()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIyMzQxNA==", "bodyText": "I feel that each test is already doing a lot. Combining them will make the tests hard to understand as it will be testing and verifying many many many things. They are also not duplicated code as they are verifying different things so I think it is fine to separate them.", "url": "https://github.com/apache/druid/pull/10371#discussion_r489223414", "createdAt": "2020-09-16T07:31:07Z", "author": {"login": "maytasm"}, "path": "server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java", "diffHunk": "@@ -270,12 +275,246 @@ public String get()\n     assertLastSegmentNotCompacted(compactSegments);\n   }\n \n+  @Test\n+  public void testMakeStats()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4MjAzMQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIyNTAwNw==", "bodyText": "For example, if you only care about the actual compaction (i.e.the spec in the compaction is not as expected, etc) then you can focus on testRun() and ignore all the stuff in assertCompactSegmentStatistics (along with all the complicated calculation of each stats in each compaction call).\nOn the other hand if you need to debug incorrect calculation in bytesCompacted (but the actual compaction tasks are fine) then you can focus on testMakeStats()", "url": "https://github.com/apache/druid/pull/10371#discussion_r489225007", "createdAt": "2020-09-16T07:34:01Z", "author": {"login": "maytasm"}, "path": "server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java", "diffHunk": "@@ -270,12 +275,246 @@ public String get()\n     assertLastSegmentNotCompacted(compactSegments);\n   }\n \n+  @Test\n+  public void testMakeStats()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4MjAzMQ=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMwNDg1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMzo1ODo1N1rOHT9Pqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozMDowMFrOHT-nMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY4ODQyNw==", "bodyText": "nit: you can use while(iterator.hasNext())", "url": "https://github.com/apache/druid/pull/10371#discussion_r490688427", "createdAt": "2020-09-18T03:58:57Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -234,29 +294,174 @@ private CoordinatorStats doRun(\n     return newContext;\n   }\n \n-  private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIterator iterator)\n+  /**\n+   * This method can be use to atomically update the snapshots in {@code autoCompactionSnapshotPerDataSource} when\n+   * no compaction task is schedule in this run. Currently, this method does not update compaction statistics\n+   * (bytes, interval count, segment count, etc) since we skip iterating through the segments and cannot get an update\n+   * on those statistics. Thus, this method only updates the schedule status and task list (compaction statistics\n+   * remains the same as the previous snapshot).\n+   */\n+  private void updateAutoCompactionSnapshotWhenNoCompactTaskScheduled(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders\n+  )\n+  {\n+    Map<String, AutoCompactionSnapshot> previousSnapshots = autoCompactionSnapshotPerDataSource.get();\n+    for (Map.Entry<String, AutoCompactionSnapshot.Builder> autoCompactionSnapshotBuilderEntry : currentRunAutoCompactionSnapshotBuilders.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotBuilderEntry.getKey();\n+      AutoCompactionSnapshot previousSnapshot = previousSnapshots.get(dataSource);\n+      if (previousSnapshot != null) {\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesAwaitingCompaction(previousSnapshot.getBytesAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesCompacted(previousSnapshot.getBytesCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesSkipped(previousSnapshot.getBytesSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountAwaitingCompaction(previousSnapshot.getSegmentCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountCompacted(previousSnapshot.getSegmentCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountSkipped(previousSnapshot.getSegmentCountSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountAwaitingCompaction(previousSnapshot.getIntervalCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountCompacted(previousSnapshot.getIntervalCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountSkipped(previousSnapshot.getIntervalCountSkipped());\n+      }\n+    }\n+\n+    Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = Maps.transformValues(\n+        currentRunAutoCompactionSnapshotBuilders,\n+        AutoCompactionSnapshot.Builder::build\n+    );\n+    // Atomic update of autoCompactionSnapshotPerDataSource with the latest from this coordinator run\n+    autoCompactionSnapshotPerDataSource.set(currentAutoCompactionSnapshotPerDataSource);\n+  }\n+\n+  private CoordinatorStats makeStats(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders,\n+      int numCompactionTasks,\n+      CompactionSegmentIterator iterator\n+  )\n   {\n+    final Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = new HashMap<>();\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Iterate through all the remaining segments in the iterator.\n+    // As these segments could be compacted but were not compacted due to lack of task slot, we will aggregates\n+    // the statistic to the AwaitingCompaction statistics\n+    for (; iterator.hasNext();) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMDgzNQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490710835", "createdAt": "2020-09-18T05:30:00Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -234,29 +294,174 @@ private CoordinatorStats doRun(\n     return newContext;\n   }\n \n-  private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIterator iterator)\n+  /**\n+   * This method can be use to atomically update the snapshots in {@code autoCompactionSnapshotPerDataSource} when\n+   * no compaction task is schedule in this run. Currently, this method does not update compaction statistics\n+   * (bytes, interval count, segment count, etc) since we skip iterating through the segments and cannot get an update\n+   * on those statistics. Thus, this method only updates the schedule status and task list (compaction statistics\n+   * remains the same as the previous snapshot).\n+   */\n+  private void updateAutoCompactionSnapshotWhenNoCompactTaskScheduled(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders\n+  )\n+  {\n+    Map<String, AutoCompactionSnapshot> previousSnapshots = autoCompactionSnapshotPerDataSource.get();\n+    for (Map.Entry<String, AutoCompactionSnapshot.Builder> autoCompactionSnapshotBuilderEntry : currentRunAutoCompactionSnapshotBuilders.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotBuilderEntry.getKey();\n+      AutoCompactionSnapshot previousSnapshot = previousSnapshots.get(dataSource);\n+      if (previousSnapshot != null) {\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesAwaitingCompaction(previousSnapshot.getBytesAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesCompacted(previousSnapshot.getBytesCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesSkipped(previousSnapshot.getBytesSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountAwaitingCompaction(previousSnapshot.getSegmentCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountCompacted(previousSnapshot.getSegmentCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountSkipped(previousSnapshot.getSegmentCountSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountAwaitingCompaction(previousSnapshot.getIntervalCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountCompacted(previousSnapshot.getIntervalCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountSkipped(previousSnapshot.getIntervalCountSkipped());\n+      }\n+    }\n+\n+    Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = Maps.transformValues(\n+        currentRunAutoCompactionSnapshotBuilders,\n+        AutoCompactionSnapshot.Builder::build\n+    );\n+    // Atomic update of autoCompactionSnapshotPerDataSource with the latest from this coordinator run\n+    autoCompactionSnapshotPerDataSource.set(currentAutoCompactionSnapshotPerDataSource);\n+  }\n+\n+  private CoordinatorStats makeStats(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders,\n+      int numCompactionTasks,\n+      CompactionSegmentIterator iterator\n+  )\n   {\n+    final Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = new HashMap<>();\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Iterate through all the remaining segments in the iterator.\n+    // As these segments could be compacted but were not compacted due to lack of task slot, we will aggregates\n+    // the statistic to the AwaitingCompaction statistics\n+    for (; iterator.hasNext();) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY4ODQyNw=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMxNjk1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDowNjoxMlrOHT9WWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozMToyMFrOHT-opg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDEzNg==", "bodyText": "Could you please add a Javadoc about the concurrent access pattern on autoCompactionSnapshotPerDataSource? I guess it can say \"This variable is updated by the Coordinator thread executing duties and read by HTTP threads processing Coordinator API calls.\"", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690136", "createdAt": "2020-09-18T04:06:12Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +77,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private AtomicReference<Map<String, AutoCompactionSnapshot>> autoCompactionSnapshotPerDataSource = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDE5MA==", "bodyText": "nit: this variable can be final.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690190", "createdAt": "2020-09-18T04:06:23Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +77,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private AtomicReference<Map<String, AutoCompactionSnapshot>> autoCompactionSnapshotPerDataSource = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDEzNg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMTIwNg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490711206", "createdAt": "2020-09-18T05:31:20Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +77,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private AtomicReference<Map<String, AutoCompactionSnapshot>> autoCompactionSnapshotPerDataSource = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDEzNg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMyMjM5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDowOTo0MlrOHT9ZYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozNTo1OVrOHT-uDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg==", "bodyText": "Hmm, should it be interval/waitCompact/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690912", "createdAt": "2020-09-18T04:09:42Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTU3Mg==", "bodyText": "Same question for other metrics for interval.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691572", "createdAt": "2020-09-18T04:12:21Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMTUwMg==", "bodyText": "yep. that make much more sense.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490711502", "createdAt": "2020-09-18T05:32:27Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMjU4OQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490712589", "createdAt": "2020-09-18T05:35:59Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMyMjk1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDowOTo1OFrOHT9Zqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozNToyNFrOHT-tcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDk4Ng==", "bodyText": "Maybe compactTask/maxSlot/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690986", "createdAt": "2020-09-18T04:09:58Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMjQzMg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490712432", "createdAt": "2020-09-18T05:35:24Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDk4Ng=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMyMzU3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDoxMDoxOFrOHT9aAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozNTozM1rOHT-tng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTA3NQ==", "bodyText": "Similarly, maybe compactTask/availableSlot/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691075", "createdAt": "2020-09-18T04:10:18Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMjQ3OA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490712478", "createdAt": "2020-09-18T05:35:33Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTA3NQ=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDMyNDkxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDoxMDo1OVrOHT9axw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNTozNjo1MVrOHT-u_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTI3MQ==", "bodyText": "I think segment/waitCompact/bytes would be enough. The second segment in segmentBytes seems duplicate.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691271", "createdAt": "2020-09-18T04:10:59Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTQ1OA==", "bodyText": "Same comment for other metrics for segment size and count.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691458", "createdAt": "2020-09-18T04:11:54Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTI3MQ=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMjgyOA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/10371#discussion_r490712828", "createdAt": "2020-09-18T05:36:51Z", "author": {"login": "maytasm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTI3MQ=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 24}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3192, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}