{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYwNzE0ODEx", "number": 9155, "title": "Tutorials use new ingestion spec where possible", "bodyText": "There are 2 main changes\n\nUse task type index_parallel instead of index\nRemove the use of parser + firehose in favor of inputFormat + inputSource\n\nindex_parallel is the preferred method starting in 0.17. Setting the job to\nindex_parallel with the default maxNumConcurrentSubTasks(1) is the equivalent\nof an index task\nInstead of using a parserSpec, dimensionSpec and timestampSpec have been\npromoted to the dataSchema. The format is described in the ioConfig as the\ninputFormat.\nThere are a few cases where the new format is not supported\n\nHadoop must use firehoses instead of the inputSource and inputFormat\nThere is no equivalent of a combining firehose as an inputSource\nA Combining firehose does not support index_parallel", "createdAt": "2020-01-09T00:25:39Z", "url": "https://github.com/apache/druid/pull/9155", "merged": true, "mergeCommit": {"oid": "85a3d416b000e2e7378c9cd92c71f1c1815f6fb8"}, "closed": true, "closedAt": "2020-01-15T22:08:30Z", "author": {"login": "suneet-s"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb6ot9EABqjI5NTE2MTc5NjI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb6rKkgAH2gAyMzYwNzE0ODExOjA5ODhkZWMyZDZhY2VjNzVlYzZjZWE4OTFhYzY3YzA2ZDk3OGM5MDk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "author": {"user": {"login": "suneet-s", "name": "Suneet Saldanha"}}, "url": "https://github.com/apache/druid/commit/6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "committedDate": "2020-01-15T17:14:30Z", "message": "Tutorials use new ingestion spec where possible\n\nThere are 2 main changes\n  * Use task type index_parallel instead of index\n  * Remove the use of parser + firehose in favor of inputFormat + inputSource\n\nindex_parallel is the preferred method starting in 0.17. Setting the job to\nindex_parallel with the default maxNumConcurrentSubTasks(1) is the equivalent\nof an index task\n\nInstead of using a parserSpec, dimensionSpec and timestampSpec have been\npromoted to the dataSchema. The format is described in the ioConfig as the\ninputFormat.\n\nThere are a few cases where the new format is not supported\n * Hadoop must use firehoses instead of the inputSource and inputFormat\n * There is no equivalent of a combining firehose as an inputSource\n * A Combining firehose does not support index_parallel"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "author": {"user": {"login": "suneet-s", "name": "Suneet Saldanha"}}, "url": "https://github.com/apache/druid/commit/6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "committedDate": "2020-01-15T17:14:30Z", "message": "Tutorials use new ingestion spec where possible\n\nThere are 2 main changes\n  * Use task type index_parallel instead of index\n  * Remove the use of parser + firehose in favor of inputFormat + inputSource\n\nindex_parallel is the preferred method starting in 0.17. Setting the job to\nindex_parallel with the default maxNumConcurrentSubTasks(1) is the equivalent\nof an index task\n\nInstead of using a parserSpec, dimensionSpec and timestampSpec have been\npromoted to the dataSchema. The format is described in the ioConfig as the\ninputFormat.\n\nThere are a few cases where the new format is not supported\n * Hadoop must use firehoses instead of the inputSource and inputFormat\n * There is no equivalent of a combining firehose as an inputSource\n * A Combining firehose does not support index_parallel"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQzNDgwODg0", "url": "https://github.com/apache/druid/pull/9155#pullrequestreview-343480884", "createdAt": "2020-01-15T19:54:54Z", "commit": {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQxOTo1NDo1NVrOFeEjGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQxOTo1NDo1NVrOFeEjGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NjEyMQ==", "bodyText": "typo: The the", "url": "https://github.com/apache/druid/pull/9155#discussion_r367076121", "createdAt": "2020-01-15T19:54:55Z", "author": {"login": "jihoonson"}, "path": "docs/tutorials/tutorial-ingestion-spec.md", "diffHunk": "@@ -490,44 +421,57 @@ The `dataSchema` is shared across all task types, but each task type has its own\n \n ## Define the input source\n \n-Now let's define our input source, which is specified in an `ioConfig` object. Each task type has its own type of `ioConfig`. The native batch task uses \"firehoses\" to read input data, so let's configure a \"local\" firehose to read the example netflow data we saved earlier:\n+Now let's define our input source, which is specified in an `ioConfig` object. Each task type has its own type of `ioConfig`. To read input data, we need to specify an `inputSource`. The the example netflow data we saved earlier needs to be read from a local file, which is configured below:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce"}, "originalPosition": 325}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0988dec2d6acec75ec6cea891ac67c06d978c909", "author": {"user": {"login": "suneet-s", "name": "Suneet Saldanha"}}, "url": "https://github.com/apache/druid/commit/0988dec2d6acec75ec6cea891ac67c06d978c909", "committedDate": "2020-01-15T20:00:00Z", "message": "fix typo"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3687, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}