{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NjAxNTE2", "number": 9724, "title": "Add integration tests for kafka ingestion", "bodyText": "Add integration tests for kafka ingestion\nDescription\nThis PR adds the same coverage of integration test for Kafka as what we already have for Kinesis.\nJust to remind everyone...the coverage of integration test for Kafka and Kinesis we have now are:\n\nFunctional tests when Druid and Kafka/Kinesis are in stable state\n\n\nlegacy parser\ninputFormat\nGreater than 1 taskCount\n\n\nFunctional tests when Druid is in an unstable state\n\n\nlosing nodes\nStop/start supervisor\n\n\nFunctional tests when Kafka/Kinesis is in an unstable state\n\n\nadding partitions\nremoving partitions (This is only for Kinesis as we cannot decrease partition for kafka)\n\nTo verify ingestion:\n\nKafka/Kinesis lag should be minimal, the consumer should be able to pull off the queue at a comparable rate to the producer.\nRealtime queries works from the indexing tasks\nQueries works reading from historical segments (after handed off)\nQueries return expected count/value/etc.\n\nAdditionally, for Kafka we have the above set of coverage for both when Kafka producer has transaction enabled and transaction disabled. Note that this PR includes all existing Kafka integration tests for Kafka. The original tests and the location of the new test methods after this PR refactoring are:\n\ntransaction enabled + legacy parser is now in ITKafkaIndexingServiceTransactionalParallelizedTest#testKafkaIndexDataWithLegacyParserStableState\ntransaction disabled + legacy parser is now in ITKafkaIndexingServiceNonTransactionalParallelizedTest#testKafkaIndexDataWithLegacyParserStableState\ntransaction enabled + inputFormat parser is now in ITKafkaIndexingServiceTransactionalParallelizedTest#testKafkaIndexDataWithInputFormatStableState\ntransaction disabled + inputFormat parser is now in ITKafkaIndexingServiceNonTransactionalParallelizedTest#testKafkaIndexDataWithInputFormatStableState\n\nOther important change in this PR...\n\nKinesisAdminClient - Create stream, delete stream, etc. for Kafka\nKafkaEventWriter - Wrapper around Kafka producer for generating test data\nAdded useful mvn command flags for integration tests development and debugging\nAdded the framework/functionality for easily enabling certain test classes/packages to be run in parallel (use testng parallel framework by executing multiple test methods concurrently using multiple threads). This is added in this PR as all the Kafka integration tests (new+old) when run serially takes 1 hour to 1 hour 15 minutes. Without the parallel functionality, the kafka-index test groups will be the bottleneck our travis CI. Also, splitting up kafka-index doesn't really make sense as logically they are testing the same functionality. (Note: tests take long time due to lots of idle time such as waiting for supervisor to come up after restart, druid node to come up after restart, ingestion tasks to ingest data after insert into stream, etc.)\n\nUpgraded testng to 6.14.3 due to testng bug for parallel test execution in the version we were using (cbeust/testng#1660)\nAdded a new test tag in integration-tests/src/test/resources/testng.xml for including tests that can be run in parallel.\nRemoved DruidTestRunnerFactory and moved the setup/teardown logic into SuiteListener. The runner is executed for each test tag while the SuiteListener is executed for each suites. We do not want the teardown to happen after the first test tag as the second test tag will fail to run. (we now have two test tag, the serialized tests and the parallelized tests).\n\n\n\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-04-19T04:15:20Z", "url": "https://github.com/apache/druid/pull/9724", "merged": true, "mergeCommit": {"oid": "16f5ae440510951b247920d41833da98804897d0"}, "closed": true, "closedAt": "2020-04-22T17:43:35Z", "author": {"login": "maytasm"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYboSDAH2gAyNDA1NjAxNTE2OmZlOTViNjc5NTRhYTkyMzNhMDEwOGU0OGRjMzNkNDc1YTQxZjM0ZGQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcaLj7rgFqTM5ODQwNTk2OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fe95b67954aa9233a0108e48dc33d475a41f34dd", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/fe95b67954aa9233a0108e48dc33d475a41f34dd", "committedDate": "2020-04-17T06:51:42Z", "message": "add kafka admin and kafka writer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eee34658803c721228147af4138c068c5a4f139e", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/eee34658803c721228147af4138c068c5a4f139e", "committedDate": "2020-04-17T08:52:44Z", "message": "refactor kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3cd2135246cc3946d3f363d4e34efe355b7a1711", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/3cd2135246cc3946d3f363d4e34efe355b7a1711", "committedDate": "2020-04-17T09:20:16Z", "message": "fix typo refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8714cff257fb57f976bca760a5c1eea18ab459a5", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/8714cff257fb57f976bca760a5c1eea18ab459a5", "committedDate": "2020-04-18T07:56:20Z", "message": "parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c97efd81d55b181e69d64f7868fcea5bb046b5e", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2c97efd81d55b181e69d64f7868fcea5bb046b5e", "committedDate": "2020-04-18T11:21:28Z", "message": "parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a737c519f7763bb054ab8d3b9b4d6396c2e9eb4", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/8a737c519f7763bb054ab8d3b9b4d6396c2e9eb4", "committedDate": "2020-04-18T12:18:28Z", "message": "parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e12449d9ca951db4b51b0082bd6032abe394ac3f", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/e12449d9ca951db4b51b0082bd6032abe394ac3f", "committedDate": "2020-04-18T22:36:07Z", "message": "parallel works now"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fae658f2b1eddfd92e3d802118942bcfafae653a", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/fae658f2b1eddfd92e3d802118942bcfafae653a", "committedDate": "2020-04-19T03:47:40Z", "message": "add kafka it"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33837300dee1eb0fb23c70f03303434bdf927765", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/33837300dee1eb0fb23c70f03303434bdf927765", "committedDate": "2020-04-19T04:37:04Z", "message": "add doc to readme"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "29c39cf9f8d97fedd6280a4b3a3f6839aedf5439", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/29c39cf9f8d97fedd6280a4b3a3f6839aedf5439", "committedDate": "2020-04-20T00:25:35Z", "message": "fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f7e05e4ee781a32848df2e568d03c96047926ac", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/3f7e05e4ee781a32848df2e568d03c96047926ac", "committedDate": "2020-04-20T04:10:22Z", "message": "fix failing test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0a8c1a30b8d88eb0176d00f84534a741018969f", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/d0a8c1a30b8d88eb0176d00f84534a741018969f", "committedDate": "2020-04-20T05:40:28Z", "message": "test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9febc083410b4967dc1e0b919a6690846281ed81", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/9febc083410b4967dc1e0b919a6690846281ed81", "committedDate": "2020-04-20T09:27:53Z", "message": "test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6db038ec35c92af13fe5ebb947091973bb50eb47", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/6db038ec35c92af13fe5ebb947091973bb50eb47", "committedDate": "2020-04-20T10:18:43Z", "message": "test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473", "committedDate": "2020-04-20T17:39:16Z", "message": "test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2OTcyOTc4", "url": "https://github.com/apache/druid/pull/9724#pullrequestreview-396972978", "createdAt": "2020-04-21T04:09:15Z", "commit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOToxNVrOGIxjkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDoyMjoyMlrOGIxyuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MzcxMg==", "bodyText": "dependecy -> dependency", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853712", "createdAt": "2020-04-21T04:09:15Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1Mzc1Mg==", "bodyText": "straving -> starving", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853752", "createdAt": "2020-04-21T04:09:26Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,\n+test does not use excessive memory straving other concurent task, test does not modify and/or use other task, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MzgzMQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853831", "createdAt": "2020-04-21T04:09:42Z", "author": {"login": "jon-wei"}, "path": "integration-tests/pom.xml", "diffHunk": "@@ -385,6 +367,7 @@\n                                 <configuration>\n                                     <environmentVariables>\n                                     <DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>${start.hadoop.docker}</DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>\n+                                    <DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>${skip.start.docker}</DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NTgwMA==", "bodyText": "Suggest adding a brief comment here noting that Kinesis doesn't immediately drop the old shards after the resharding which is why the counts are summed", "url": "https://github.com/apache/druid/pull/9724#discussion_r411855800", "createdAt": "2020-04-21T04:16:14Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KinesisAdminClient.java", "diffHunk": "@@ -129,18 +132,28 @@ public void updateShardCount(String streamName, int newShardCount, boolean block\n     }\n   }\n \n+  @Override\n   public boolean isStreamActive(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return verifyStreamStatus(streamDescription, StreamStatus.ACTIVE);\n   }\n \n+  @Override\n   public int getStreamShardCount(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return getStreamShardCount(streamDescription);\n   }\n \n+  @Override\n+  public boolean verfiyShardCountUpdated(String streamName, int oldShardCount, int newShardCount)\n+  {\n+    int actualShardCount = getStreamShardCount(streamName);\n+    return actualShardCount == oldShardCount + newShardCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NzU5NQ==", "bodyText": "Hmm, it's a bit strange that the stream indexing test extends something called \"batch index test\", maybe the base \"batch index\" class is generic enough that it should be renamed to something else, or some other refactoring", "url": "https://github.com/apache/druid/pull/9724#discussion_r411857595", "createdAt": "2020-04-21T04:22:22Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88", "committedDate": "2020-04-21T06:44:56Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NjIxNTM3", "url": "https://github.com/apache/druid/pull/9724#pullrequestreview-397621537", "createdAt": "2020-04-21T19:46:05Z", "commit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODI5OTIx", "url": "https://github.com/apache/druid/pull/9724#pullrequestreview-397829921", "createdAt": "2020-04-22T04:10:38Z", "commit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "state": "COMMENTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoxMDozOFrOGJidCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo1MDo1OVrOGJkguQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NDg1Nw==", "bodyText": "Can you add javadoc for this class?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412654857", "createdAt": "2020-04-22T04:10:38Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();\n \n-  void flush();\n+  void flush() throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NTYxNw==", "bodyText": "This method is not being called, but seems like it should be called in AbstractStreamIndexingTest.doMethodTeardown().", "url": "https://github.com/apache/druid/pull/9724#discussion_r412655617", "createdAt": "2020-04-22T04:12:52Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjM4NA==", "bodyText": "nit: probably better to name createStreamAdminClient() since it creates an instance rather than returning an existing one.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412656384", "createdAt": "2020-04-22T04:15:20Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjU3OA==", "bodyText": "nit: can be private.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412656578", "createdAt": "2020-04-22T04:15:55Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1ODY2MA==", "bodyText": "Does AllParallelizedTests parallelize tests in a single class? Or can it parallelize across classes? Probably worth mentioning here.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412658660", "createdAt": "2020-04-22T04:22:09Z", "author": {"login": "jihoonson"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MDQ3Ng==", "bodyText": "Can you add javadoc? It would be nice to explain where this class is used, how it is used, what is the contracts for implementations and so on.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412660476", "createdAt": "2020-04-22T04:27:49Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTE4NA==", "bodyText": "SeekableStream series use the word \"partition\" instead of \"shard\". It would be nice to use a consistent name.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412661184", "createdAt": "2020-04-22T04:30:08Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient\n+{\n+  void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception;\n+\n+  void deleteStream(String streamName) throws Exception;\n+\n+  void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTczOA==", "bodyText": "nit: can be private final.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412661738", "createdAt": "2020-04-22T04:31:43Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MjY0Ng==", "bodyText": "nit: Intellij recommends using setProperty() instead of put(). I guess it's because the parameter type is more strict.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412662646", "createdAt": "2020-04-22T04:34:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDI1MQ==", "bodyText": "What does \"active stream\" mean for Kafka?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412664251", "createdAt": "2020-04-22T04:39:41Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);\n+    adminClient = AdminClient.create(config);\n+  }\n+\n+  @Override\n+  public void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception\n+  {\n+    final short replicationFactor = 1;\n+    final NewTopic newTopic = new NewTopic(streamName, partitionCount, replicationFactor);\n+    final CreateTopicsResult createTopicsResult = adminClient.createTopics(Collections.singleton(newTopic));\n+    // Wait for create topic to compelte\n+    createTopicsResult.values().get(streamName).get();\n+  }\n+\n+  @Override\n+  public void deleteStream(String streamName) throws Exception\n+  {\n+    DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(ImmutableList.of(streamName));\n+    deleteTopicsResult.values().get(streamName).get();\n+  }\n+\n+  /**\n+   * This method can only increase the partition count of {@param streamName} to have a final partition\n+   * count of {@param newPartitionCount}\n+   * If {@param blocksUntilStarted} is set to true, then this method will blocks until the partitioning\n+   * started (but not nessesary finished), otherwise, the method will returns right after issue the reshard command\n+   */\n+  @Override\n+  public void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception\n+  {\n+    Map<String, NewPartitions> counts = new HashMap<>();\n+    counts.put(streamName, NewPartitions.increaseTo(newPartitionCount));\n+    CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(counts);\n+    if (blocksUntilStarted) {\n+      createPartitionsResult.values().get(streamName).get();\n+\n+    }\n+  }\n+\n+  @Override\n+  public boolean isStreamActive(String streamName)\n+  {\n+    return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDc4NQ==", "bodyText": "These 3 private variables can be final.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412664785", "createdAt": "2020-04-22T04:41:21Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaEventWriter.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import org.apache.druid.indexer.TaskIdUtils;\n+import org.apache.druid.testing.IntegrationTestingConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.Future;\n+\n+public class KafkaEventWriter implements StreamEventWriter\n+{\n+  private static final String TEST_PROPERTY_PREFIX = \"kafka.test.property.\";\n+  private KafkaProducer<String, String> producer;\n+  private boolean txnEnabled;\n+  private List<Future<RecordMetadata>> pendingWriteRecords = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTEwMw==", "bodyText": "nit: it would be better to check the compaction config is set by calling the compaction config API.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669103", "createdAt": "2020-04-22T04:55:11Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -251,6 +251,9 @@ private void submitCompactionConfig(Integer maxRowsPerSegment, Period skipOffset\n                                                                                  null);\n     compactionResource.submitCompactionConfig(compactionConfig);\n \n+    // Wait for compaction config to persist\n+    Thread.sleep(2000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTQ4Mw==", "bodyText": "Why not injecting it in Kafka/Kinesis tests?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669483", "createdAt": "2020-04-22T04:56:20Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractIndexerTest.java", "diffHunk": "@@ -57,7 +57,7 @@\n   protected TestQueryHelper queryHelper;\n \n   @Inject\n-  private IntegrationTestingConfig config;\n+  public IntegrationTestingConfig config;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTY4Mg==", "bodyText": "nit: can be protected.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669682", "createdAt": "2020-04-22T04:56:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractKafkaIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.indexing.kafka.KafkaConsumerConfigs;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.testing.utils.KafkaAdminClient;\n+import org.apache.druid.testing.utils.KafkaEventWriter;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Function;\n+\n+public abstract class AbstractKafkaIndexingServiceTest extends AbstractStreamIndexingTest\n+{\n+  public abstract boolean isKafkaWriterTransactionalEnabled();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3MjU1Mg==", "bodyText": "\"may have already gone\"?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412672552", "createdAt": "2020-04-22T05:05:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void testIndexWithStreamReshardHelper(int newShardCount) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Reshard the supervisor by split from STREAM_SHARD_COUNT to newShardCount and waits until the resharding starts\n+      streamAdminClient.updateShardCount(generatedTestConfig.getStreamName(), newShardCount, true);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.isStreamActive(generatedTestConfig.getStreamName()),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.verfiyShardCountUpdated(generatedTestConfig.getStreamName(), STREAM_SHARD_COUNT, newShardCount),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void verifyIngestedData(GeneratedTestConfig generatedTestConfig) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for stream indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = generatedTestConfig.getStreamQueryPropsTransform().apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(generatedTestConfig.getSupervisorId());\n+    // wait for all indexing tasks to finish\n+    LOG.info(\"Waiting for all indexing tasks to finish\");\n+    ITRetryUtil.retryUntilTrue(\n+        () -> (indexer.getUncompletedTasksForDataSource(generatedTestConfig.getFullDatasourceName()).size() == 0),\n+        \"Waiting for Tasks Completion\"\n+    );\n+    // wait for segments to be handed off\n+    ITRetryUtil.retryUntil(\n+        () -> coordinator.areSegmentsLoaded(generatedTestConfig.getFullDatasourceName()),\n+        true,\n+        10000,\n+        30,\n+        \"Real-time generated segments loaded\"\n+    );\n+\n+    // this query will be answered by at least 1 historical segment, most likely 2, and possibly up to all 4\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+  }\n+\n+  long getSumOfEventSequence(int numEvents)\n+  {\n+    return (numEvents * (1 + numEvents)) / 2;\n+  }\n+\n+  private void doMethodTeardown(GeneratedTestConfig generatedTestConfig, StreamEventWriter streamEventWriter)\n+  {\n+    try {\n+      streamEventWriter.flush();\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup as the writer may have already went Bye-Bye", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3Mjg2OA==", "bodyText": "BTW, I don't think ignoring exceptions is a good idea in any case. Probably it should log at least.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412672868", "createdAt": "2020-04-22T05:06:47Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void testIndexWithStreamReshardHelper(int newShardCount) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Reshard the supervisor by split from STREAM_SHARD_COUNT to newShardCount and waits until the resharding starts\n+      streamAdminClient.updateShardCount(generatedTestConfig.getStreamName(), newShardCount, true);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.isStreamActive(generatedTestConfig.getStreamName()),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.verfiyShardCountUpdated(generatedTestConfig.getStreamName(), STREAM_SHARD_COUNT, newShardCount),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void verifyIngestedData(GeneratedTestConfig generatedTestConfig) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for stream indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = generatedTestConfig.getStreamQueryPropsTransform().apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(generatedTestConfig.getSupervisorId());\n+    // wait for all indexing tasks to finish\n+    LOG.info(\"Waiting for all indexing tasks to finish\");\n+    ITRetryUtil.retryUntilTrue(\n+        () -> (indexer.getUncompletedTasksForDataSource(generatedTestConfig.getFullDatasourceName()).size() == 0),\n+        \"Waiting for Tasks Completion\"\n+    );\n+    // wait for segments to be handed off\n+    ITRetryUtil.retryUntil(\n+        () -> coordinator.areSegmentsLoaded(generatedTestConfig.getFullDatasourceName()),\n+        true,\n+        10000,\n+        30,\n+        \"Real-time generated segments loaded\"\n+    );\n+\n+    // this query will be answered by at least 1 historical segment, most likely 2, and possibly up to all 4\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+  }\n+\n+  long getSumOfEventSequence(int numEvents)\n+  {\n+    return (numEvents * (1 + numEvents)) / 2;\n+  }\n+\n+  private void doMethodTeardown(GeneratedTestConfig generatedTestConfig, StreamEventWriter streamEventWriter)\n+  {\n+    try {\n+      streamEventWriter.flush();\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup as the writer may have already went Bye-Bye", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3MjU1Mg=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NTQ5Mg==", "bodyText": "Probably better to call run() or generate() rather than start() since the generator is not something you can start/stop, but generates all events in start().", "url": "https://github.com/apache/druid/pull/9724#discussion_r412685492", "createdAt": "2020-04-22T05:42:41Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NjYxMw==", "bodyText": "remainding -> remaining?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412686613", "createdAt": "2020-04-22T05:45:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODQ0MQ==", "bodyText": "Can you add javadoc that explains what kind of tests should be done in parallel?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412688441", "createdAt": "2020-04-22T05:50:38Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/parallelized/ITKafkaIndexingServiceNonTransactionalParallelizedTest.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.parallelized;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.apache.druid.tests.indexer.AbstractKafkaIndexingServiceTest;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KAFKA_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKafkaIndexingServiceNonTransactionalParallelizedTest extends AbstractKafkaIndexingServiceTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODU2OQ==", "bodyText": "Similarly, can you add javadoc that explains what kind of tests should be done in sequential?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412688569", "createdAt": "2020-04-22T05:50:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceSerializedTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceSerializedTest extends AbstractKinesisIndexingServiceTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47d3f624cd0b171d343eb081df41db51a141250a", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/47d3f624cd0b171d343eb081df41db51a141250a", "committedDate": "2020-04-22T07:26:49Z", "message": "addressed comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4NDA1OTY4", "url": "https://github.com/apache/druid/pull/9724#pullrequestreview-398405968", "createdAt": "2020-04-22T17:16:19Z", "commit": {"oid": "47d3f624cd0b171d343eb081df41db51a141250a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2540, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}