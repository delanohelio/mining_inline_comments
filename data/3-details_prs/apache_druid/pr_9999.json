{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMwNjg3ODY2", "number": 9999, "title": "Optimize protobuf parsing for flatten data", "bodyText": "Fixes #9984 .\n\n\nDescription\nsee #9984\nIn order to improve the performance of protobuf parsing and solve the problem of JSONPathSpec mentioned in the end of #9984, we  apply different parsing methods for flatten data and nested data.\nSince Druid only allows defining ParseSpec of protobuf data with JSONParseSpec, the nested data refers to the data whose JSONPathSpec (defined in JSONParseSpec) is not null. The flatten data has null in JSONPathSpec, so its parsing process can be optimized by avoding transforming to JSON first. The modified class ProtobufInputRowParser has passed all the tests in ProtobufInputRowParserTest (If necessary, we can add a unit test for flatten data).\nBelow is the result of ProtobufParserBenchmark\n\nIt shows that parsing flatten data can be 4 times faster than parsing data using the optimized method.\n\n\n\nAdded test files\nWe added prototest.desc(which is a copy of prototest.desc in protobuf-extension test-classes)  and  ProtoFile(which is generated by a test in  ProtobuInputRowTest) under benchmarks/src/test/resources to simplify the process of test data preparation. The inputs for two benchmarks in ProtobufParserBenchmark are the same, but the results are a little different due to the request of JSONPathSpec.\n\n\n\n\n\nThis PR has:\n\n been self-reviewed.\n added unit tests or modified existing tests to cover new code paths.\n\n\n\nKey changed/added classes in this PR\n\nProtobufParserBenchmark\nProtobufInputRowParser", "createdAt": "2020-06-08T04:10:51Z", "url": "https://github.com/apache/druid/pull/9999", "merged": true, "mergeCommit": {"oid": "1596b3eacdb1ec127a9b1f4aa99ee8b8dd7a8b76"}, "closed": true, "closedAt": "2020-06-25T01:01:32Z", "author": {"login": "xhl0726"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcpHKlAAH2gAyNDMwNjg3ODY2OjM5NjgyNjExOWEzOWZlOGVmYTViY2ZjZWE4Mjc0ZDUwOTI1Nzk2YTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcuj9zhgFqTQzNzEwMjE5MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "396826119a39fe8efa5bcfcea8274d50925796a8", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/396826119a39fe8efa5bcfcea8274d50925796a8", "committedDate": "2020-06-08T02:37:52Z", "message": "optimize for protobuf parsing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cccd904827ef7e08f3d043b7544ff6ba4d779a12", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/cccd904827ef7e08f3d043b7544ff6ba4d779a12", "committedDate": "2020-06-08T08:26:53Z", "message": "fix import error and maven dependency"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/a7e5e23f39cf2d2306b7647bca28bac04423bb34", "committedDate": "2020-06-08T12:04:51Z", "message": "add unit test in protobufInputrowParserTest for flatten data"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMjE0MDk1", "url": "https://github.com/apache/druid/pull/9999#pullrequestreview-430214095", "createdAt": "2020-06-14T12:15:53Z", "commit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoxNTo1M1rOGjct0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoyNjo0N1rOGjcxLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyMzgyNg==", "bodyText": "I suggest refactoring these modifications to reduce code duplications.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n            \n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                    throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                } else {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    String json = JsonFormat.printer().print(message);\n          \n          \n            \n                    record = parser.parseToMap(json);\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException e) {\n          \n          \n            \n                    throw new ParseException(e, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                try {\n          \n          \n            \n                  DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                }\n          \n          \n            \n                catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                  throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                } else {\n          \n          \n            \n                  String json = JsonFormat.printer().print(message);\n          \n          \n            \n                  record = parser.parseToMap(json);\n          \n          \n            \n                }", "url": "https://github.com/apache/druid/pull/9999#discussion_r439823826", "createdAt": "2020-06-14T12:15:53Z", "author": {"login": "liran-funaro"}, "path": "extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java", "diffHunk": "@@ -100,16 +102,27 @@ void initDescriptor()\n       parser = parseSpec.makeParser();\n       initDescriptor();\n     }\n-    String json;\n-    try {\n-      DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n-      json = JsonFormat.printer().print(message);\n-    }\n-    catch (InvalidProtocolBufferException e) {\n-      throw new ParseException(e, \"Protobuf message could not be parsed\");\n+    Map<String, Object> record;\n+\n+    if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n+      }\n+      catch (InvalidProtocolBufferException ex) {\n+        throw new ParseException(ex, \"Protobuf message could not be parsed\");\n+      }\n+    } else {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        String json = JsonFormat.printer().print(message);\n+        record = parser.parseToMap(json);\n+      }\n+      catch (InvalidProtocolBufferException e) {\n+        throw new ParseException(e, \"Protobuf message could not be parsed\");\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDA2Mg==", "bodyText": "I don't think the benchmark class should have a main method. It seems useful for debugging, but I don't think it should exist in the master branch.", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824062", "createdAt": "2020-06-14T12:19:36Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeNestedData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = nestedParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+\n+  }\n+  private byte[] getProtoInputs(String fileName)\n+  {\n+    String filePath = this.getClass().getClassLoader().getResource(fileName).getPath();\n+    byte[] bytes = null;\n+    try {\n+      File file = new File(filePath);\n+      bytes = new byte[(int) file.length()];\n+      bytes = Files.toByteArray(file);\n+    }\n+    catch (FileNotFoundException e) {\n+      log.error(\"Cannot find the file: \" + filePath);\n+      e.printStackTrace();\n+    }\n+    catch (IOException e) {\n+      e.printStackTrace();\n+    }\n+    return bytes;\n+  }\n+\n+  public static void main(String[] args) throws RunnerException\n+  {\n+    Options opt = new OptionsBuilder()\n+        .include(ProtobufParserBenchmark.class.getSimpleName())\n+        .build();\n+    new Runner(opt).run();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ==", "bodyText": "I wonder what is the effect of ByteBuffer.wrap() on the measured performance.\nIs there a reason the wrapping is done inside the loop and not in the setup phase?", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824411", "createdAt": "2020-06-14T12:23:14Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDY4Ng==", "bodyText": "Is this logging necessary? It seems useful for debugging, but I don't think it should exist in the master branch.", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824686", "createdAt": "2020-06-14T12:26:47Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 84}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/95df02f8681cb66b3a0392d8dab64522e6aa4249", "committedDate": "2020-06-16T12:51:35Z", "message": "solve code duplication (remove the log and main())"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2NDQ2MTAy", "url": "https://github.com/apache/druid/pull/9999#pullrequestreview-436446102", "createdAt": "2020-06-24T08:46:12Z", "commit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo0NjoxMlrOGoIz7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo1MTowMFrOGoI_cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MDU4OA==", "bodyText": "nit: unnecessary println, though there are others in this test file prior to this change so no worries", "url": "https://github.com/apache/druid/pull/9999#discussion_r444740588", "createdAt": "2020-06-24T08:46:12Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception\n+  {\n+    //configure parser with desc file\n+    ProtobufInputRowParser parser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+\n+    //create binary of proto test event\n+    DateTime dateTime = new DateTime(2012, 7, 12, 9, 30, ISOChronology.getInstanceUTC());\n+    ProtoTestEventWrapper.ProtoTestEvent event = ProtoTestEventWrapper.ProtoTestEvent.newBuilder()\n+            .setDescription(\"description\")\n+            .setEventType(ProtoTestEventWrapper.ProtoTestEvent.EventCategory.CATEGORY_ONE)\n+            .setId(4711L)\n+            .setIsValid(true)\n+            .setSomeOtherId(4712)\n+            .setTimestamp(dateTime.toString())\n+            .setSomeFloatColumn(47.11F)\n+            .setSomeIntColumn(815)\n+            .setSomeLongColumn(816L)\n+            .build();\n+\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    event.writeTo(out);\n+\n+    InputRow row = parser.parseBatch(ByteBuffer.wrap(out.toByteArray())).get(0);\n+    System.out.println(row);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MjkwNQ==", "bodyText": "nit: should this be named flatParseSpec since it is for flat data and will not flatten the data since it has a null flattenSpec?", "url": "https://github.com/apache/druid/pull/9999#discussion_r444742905", "createdAt": "2020-06-24T08:49:59Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -76,6 +77,19 @@ public void setUp()\n         null\n     );\n \n+    flattenParseSpec = new JSONParseSpec(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzUzNw==", "bodyText": "nit: same suggestion about naming, should this be testParseFlatData since it isn't actively flattening nested data?", "url": "https://github.com/apache/druid/pull/9999#discussion_r444743537", "createdAt": "2020-06-24T08:51:00Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 42}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "178191f0a45d24256952aeaa461929413e234c91", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/178191f0a45d24256952aeaa461929413e234c91", "committedDate": "2020-06-24T10:21:39Z", "message": "rename 'flatten' to 'flat' to make it clearer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3MTAyMTkw", "url": "https://github.com/apache/druid/pull/9999#pullrequestreview-437102190", "createdAt": "2020-06-25T01:00:47Z", "commit": {"oid": "178191f0a45d24256952aeaa461929413e234c91"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2034, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}