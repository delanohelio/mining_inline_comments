{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMwNjg3ODY2", "number": 9999, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoxNTo1M1rOEFURUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo1MTowMFrOEIRuiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDEwNDUxOnYy", "diffSide": "RIGHT", "path": "extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoxNTo1M1rOGjct0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoxNTo1M1rOGjct0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyMzgyNg==", "bodyText": "I suggest refactoring these modifications to reduce code duplications.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n            \n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                    throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                } else {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    String json = JsonFormat.printer().print(message);\n          \n          \n            \n                    record = parser.parseToMap(json);\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException e) {\n          \n          \n            \n                    throw new ParseException(e, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                try {\n          \n          \n            \n                  DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                }\n          \n          \n            \n                catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                  throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                } else {\n          \n          \n            \n                  String json = JsonFormat.printer().print(message);\n          \n          \n            \n                  record = parser.parseToMap(json);\n          \n          \n            \n                }", "url": "https://github.com/apache/druid/pull/9999#discussion_r439823826", "createdAt": "2020-06-14T12:15:53Z", "author": {"login": "liran-funaro"}, "path": "extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java", "diffHunk": "@@ -100,16 +102,27 @@ void initDescriptor()\n       parser = parseSpec.makeParser();\n       initDescriptor();\n     }\n-    String json;\n-    try {\n-      DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n-      json = JsonFormat.printer().print(message);\n-    }\n-    catch (InvalidProtocolBufferException e) {\n-      throw new ParseException(e, \"Protobuf message could not be parsed\");\n+    Map<String, Object> record;\n+\n+    if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n+      }\n+      catch (InvalidProtocolBufferException ex) {\n+        throw new ParseException(ex, \"Protobuf message could not be parsed\");\n+      }\n+    } else {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        String json = JsonFormat.printer().print(message);\n+        record = parser.parseToMap(json);\n+      }\n+      catch (InvalidProtocolBufferException e) {\n+        throw new ParseException(e, \"Protobuf message could not be parsed\");\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDEwNjE3OnYy", "diffSide": "RIGHT", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoxOTozNlrOGjcuvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDoyNzo1NFrOGj0qiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDA2Mg==", "bodyText": "I don't think the benchmark class should have a main method. It seems useful for debugging, but I don't think it should exist in the master branch.", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824062", "createdAt": "2020-06-14T12:19:36Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeNestedData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = nestedParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+\n+  }\n+  private byte[] getProtoInputs(String fileName)\n+  {\n+    String filePath = this.getClass().getClassLoader().getResource(fileName).getPath();\n+    byte[] bytes = null;\n+    try {\n+      File file = new File(filePath);\n+      bytes = new byte[(int) file.length()];\n+      bytes = Files.toByteArray(file);\n+    }\n+    catch (FileNotFoundException e) {\n+      log.error(\"Cannot find the file: \" + filePath);\n+      e.printStackTrace();\n+    }\n+    catch (IOException e) {\n+      e.printStackTrace();\n+    }\n+    return bytes;\n+  }\n+\n+  public static void main(String[] args) throws RunnerException\n+  {\n+    Options opt = new OptionsBuilder()\n+        .include(ProtobufParserBenchmark.class.getSimpleName())\n+        .build();\n+    new Runner(opt).run();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIxNjIwMQ==", "bodyText": "Yes you're right~ It is just for debugging and should be removed.", "url": "https://github.com/apache/druid/pull/9999#discussion_r440216201", "createdAt": "2020-06-15T14:27:54Z", "author": {"login": "xhl0726"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeNestedData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = nestedParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+\n+  }\n+  private byte[] getProtoInputs(String fileName)\n+  {\n+    String filePath = this.getClass().getClassLoader().getResource(fileName).getPath();\n+    byte[] bytes = null;\n+    try {\n+      File file = new File(filePath);\n+      bytes = new byte[(int) file.length()];\n+      bytes = Files.toByteArray(file);\n+    }\n+    catch (FileNotFoundException e) {\n+      log.error(\"Cannot find the file: \" + filePath);\n+      e.printStackTrace();\n+    }\n+    catch (IOException e) {\n+      e.printStackTrace();\n+    }\n+    return bytes;\n+  }\n+\n+  public static void main(String[] args) throws RunnerException\n+  {\n+    Options opt = new OptionsBuilder()\n+        .include(ProtobufParserBenchmark.class.getSimpleName())\n+        .build();\n+    new Runner(opt).run();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDA2Mg=="}, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDEwODgwOnYy", "diffSide": "RIGHT", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoyMzoxNFrOGjcwGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwOTowMzozNVrOGoJcgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ==", "bodyText": "I wonder what is the effect of ByteBuffer.wrap() on the measured performance.\nIs there a reason the wrapping is done inside the loop and not in the setup phase?", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824411", "createdAt": "2020-06-14T12:23:14Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyMTA2MQ==", "bodyText": "Yeah your suggestion would make the loop clearer.  However, when I put the ByteBuffer.wrap() out of the loop, some parsing error occured. Since there is only one line in the input file ( I used the loop to monitor read many lines), the buffer would be empty after reading once. In another word, reading from a bytes.Buffer consumes the bytes that were read. This means if you try to read again, those will not be returned. When i = 0, it works. When i>0, it would report a parsing error due to the pos of the ByteBuffer allocated has been moved to the end. That's the reason why I put it in the loop. If you have any better solution to that, just tell me to make the loop easier to understand.", "url": "https://github.com/apache/druid/pull/9999#discussion_r440221061", "createdAt": "2020-06-15T14:34:36Z", "author": {"login": "xhl0726"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ=="}, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc1MDk3Ng==", "bodyText": "I see. It makes sense. It could be solved by buff.pos(0), but since ByteBuffer.wrap() only instantiate a single object with O(1) complexity, I don't think it would make much of a difference.", "url": "https://github.com/apache/druid/pull/9999#discussion_r444750976", "createdAt": "2020-06-24T09:03:35Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ=="}, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDExMDg3OnYy", "diffSide": "RIGHT", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxMjoyNjo0N1rOGjcxLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo0NzoxNFrOGj1h4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDY4Ng==", "bodyText": "Is this logging necessary? It seems useful for debugging, but I don't think it should exist in the master branch.", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824686", "createdAt": "2020-06-14T12:26:47Z", "author": {"login": "liran-funaro"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIzMDM2OA==", "bodyText": "Yes it can be removed. Thanks for pointing it out~", "url": "https://github.com/apache/druid/pull/9999#discussion_r440230368", "createdAt": "2020-06-15T14:47:14Z", "author": {"login": "xhl0726"}, "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDY4Ng=="}, "originalCommit": {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MTEyNjU0OnYy", "diffSide": "RIGHT", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo0NjoxMlrOGoIz7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo0NjoxMlrOGoIz7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MDU4OA==", "bodyText": "nit: unnecessary println, though there are others in this test file prior to this change so no worries", "url": "https://github.com/apache/druid/pull/9999#discussion_r444740588", "createdAt": "2020-06-24T08:46:12Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception\n+  {\n+    //configure parser with desc file\n+    ProtobufInputRowParser parser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+\n+    //create binary of proto test event\n+    DateTime dateTime = new DateTime(2012, 7, 12, 9, 30, ISOChronology.getInstanceUTC());\n+    ProtoTestEventWrapper.ProtoTestEvent event = ProtoTestEventWrapper.ProtoTestEvent.newBuilder()\n+            .setDescription(\"description\")\n+            .setEventType(ProtoTestEventWrapper.ProtoTestEvent.EventCategory.CATEGORY_ONE)\n+            .setId(4711L)\n+            .setIsValid(true)\n+            .setSomeOtherId(4712)\n+            .setTimestamp(dateTime.toString())\n+            .setSomeFloatColumn(47.11F)\n+            .setSomeIntColumn(815)\n+            .setSomeLongColumn(816L)\n+            .build();\n+\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    event.writeTo(out);\n+\n+    InputRow row = parser.parseBatch(ByteBuffer.wrap(out.toByteArray())).get(0);\n+    System.out.println(row);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MTE0MDk2OnYy", "diffSide": "RIGHT", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo0OTo1OVrOGoI8-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxMDoxNzowM1rOGoL-yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MjkwNQ==", "bodyText": "nit: should this be named flatParseSpec since it is for flat data and will not flatten the data since it has a null flattenSpec?", "url": "https://github.com/apache/druid/pull/9999#discussion_r444742905", "createdAt": "2020-06-24T08:49:59Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -76,6 +77,19 @@ public void setUp()\n         null\n     );\n \n+    flattenParseSpec = new JSONParseSpec(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc5MjUyMA==", "bodyText": "yeah you're right.", "url": "https://github.com/apache/druid/pull/9999#discussion_r444792520", "createdAt": "2020-06-24T10:17:03Z", "author": {"login": "xhl0726"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -76,6 +77,19 @@ public void setUp()\n         null\n     );\n \n+    flattenParseSpec = new JSONParseSpec(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MjkwNQ=="}, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MTE0NTA3OnYy", "diffSide": "RIGHT", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODo1MTowMFrOGoI_cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxMDoyNToxM1rOGoMP9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzUzNw==", "bodyText": "nit: same suggestion about naming, should this be testParseFlatData since it isn't actively flattening nested data?", "url": "https://github.com/apache/druid/pull/9999#discussion_r444743537", "createdAt": "2020-06-24T08:51:00Z", "author": {"login": "clintropolis"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc5NjkxNw==", "bodyText": "Your suggestion makes sense. I'll fix it soon.", "url": "https://github.com/apache/druid/pull/9999#discussion_r444796917", "createdAt": "2020-06-24T10:25:13Z", "author": {"login": "xhl0726"}, "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzUzNw=="}, "originalCommit": {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2304, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}