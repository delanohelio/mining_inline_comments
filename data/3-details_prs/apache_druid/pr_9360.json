{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MDg2MzM5", "number": 9360, "title": "Create splits of multiple files for parallel indexing", "bodyText": "Description\nFor now, the Parallel task creates a sub task per input file. This could be not very efficient when you have lots of small files because each task has an overhead for scheduling, JVM startup, etc.\nThis PR adds a new MaxSizeSplitHintSpec and allows the Parallel task to create splits of multiple files. If a split has only one files, that file could be larger than the configured maxSize. Otherwise, the total size of files in the same split cannot be larger than maxSize. This means, if you have a very large file, there will be only one task that processes the big file. This could be addressed in the future by creating multiple splits for the same file, each of which references to disjoint parts of the file.\nThis PR changes the default splitHintSpec from none to MaxSizeSplitHintSpec.\n\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-02-13T21:03:01Z", "url": "https://github.com/apache/druid/pull/9360", "merged": true, "mergeCommit": {"oid": "3bc7ae782c9e5989da88999fa91ddfb6fa559a31"}, "closed": true, "closedAt": "2020-02-25T01:34:40Z", "author": {"login": "jihoonson"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcEBUN5AH2gAyMzc1MDg2MzM5OjZjODM5YmI4ZGIzYWJlOTJjMmQzZTZiMWIwZjQ5M2Q4ODgwZWQyNjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcHn7MHAFqTM2MzgxODQwMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6c839bb8db3abe92c2d3e6b1b0f493d8880ed269", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6c839bb8db3abe92c2d3e6b1b0f493d8880ed269", "committedDate": "2020-02-13T20:53:46Z", "message": "Create splits of multiple files for parallel indexing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b78cf8b7fd49a8329499eb65f8fe314a536e81a", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/4b78cf8b7fd49a8329499eb65f8fe314a536e81a", "committedDate": "2020-02-13T22:33:36Z", "message": "fix wrong import and npe in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f812cd05f18b67339ce1745c51cb8adb8fdcdce", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6f812cd05f18b67339ce1745c51cb8adb8fdcdce", "committedDate": "2020-02-14T05:26:36Z", "message": "use the single file split in tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c00cc533905b33050fe66773783853a262d8a68b", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/c00cc533905b33050fe66773783853a262d8a68b", "committedDate": "2020-02-19T04:48:15Z", "message": "Merge branch 'master' of github.com:apache/druid into split-files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ae5271afc95e82a24a41c6b267feca018edd327", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/8ae5271afc95e82a24a41c6b267feca018edd327", "committedDate": "2020-02-19T05:24:11Z", "message": "rename"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/0210ba15127a05a68e51d2e2c87691f8c7627133", "committedDate": "2020-02-19T18:31:59Z", "message": "import order"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYxMzc5NjEy", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-361379612", "createdAt": "2020-02-19T19:36:51Z", "commit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTozNjo1MVrOFr05EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTo1NzoxNFrOFr1jWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ5OTY2NA==", "bodyText": "Extra \"and\"\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the whole indexing process. It splits the input data and and issues worker tasks\n          \n          \n            \n            the whole indexing process. It splits the input data and issues worker tasks", "url": "https://github.com/apache/druid/pull/9360#discussion_r381499664", "createdAt": "2020-02-19T19:36:51Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUwNjAwNg==", "bodyText": "It\u2019s a little unclear to me who's doing what in this. Is the following accurate/clearer?\n\u201cThe index_parallel task is a supervisor task that orchestrates the indexing process. The task splits input data for processing by Overlord worker tasks, which process the input splits assigned to them and create segments from the input. Once a worker task successfully processes all assigned input splits, it reports the generated segment list to the supervisor task.\u201d\nIf not, for a lighter edit, maybe just clarify that it's the worker tasks more specifically, rather than the overlord, that is processing input splits (if that's the case).", "url": "https://github.com/apache/druid/pull/9360#discussion_r381506006", "createdAt": "2020-02-19T19:48:43Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks\n+to the Overlord which actually process the assigned input split and create segments.\n+Once a worker task successfully processes all assigned input split, it reports the generated segment list to the supervisor task. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUwODE0MA==", "bodyText": "Light line edit..\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            until the number of retries reaches to the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalize the ingestion.\n          \n          \n            \n            until the number of retries reaches the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalizes ingestion.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381508140", "createdAt": "2020-02-19T19:52:39Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks\n+to the Overlord which actually process the assigned input split and create segments.\n+Once a worker task successfully processes all assigned input split, it reports the generated segment list to the supervisor task. \n The supervisor task periodically checks the status of worker tasks. If one of them fails, it retries the failed task\n-until the number of retries reaches to the configured limit. If all worker tasks succeed, then it publishes the reported segments at once and finalize the ingestion.\n+until the number of retries reaches to the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalize the ingestion.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUxMDQ4OA==", "bodyText": "Could this match the wording used below, so:\n\"....in a single task. (Files are never split across tasks.)", "url": "https://github.com/apache/druid/pull/9360#discussion_r381510488", "createdAt": "2020-02-19T19:57:14Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -226,7 +224,14 @@ The tuningConfig is optional and default parameters will be used if no tuningCon\n `SplitHintSpec` is used to give a hint when the supervisor task creates input splits.\n Note that each worker task processes a single input split. You can control the amount of data each worker task will read during the first phase.\n \n-Currently only one splitHintSpec, i.e., `segments`, is available.\n+#### `MaxSizeSplitHintSpec`\n+\n+`MaxSizeSplitHintSpec` is respected by all splittable input sources except for the HTTP input source.\n+\n+|property|description|default|required?|\n+|--------|-----------|-------|---------|\n+|type|This should always be `maxSize`.|none|yes|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (splitting a large file is not supported yet).|500MB|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYxNDk4NTUy", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-361498552", "createdAt": "2020-02-19T22:52:29Z", "commit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMjo1MjoyOVrOFr6pLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMjo1MjoyOVrOFr6pLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU5MzkwMA==", "bodyText": "If it isn't too much trouble, it seems like this would be better to just be a part of LocalInputSource to be more consistent with the cloud file input sources, rather than introducing a new type. Though if it is needlessly complicated then is probably fine as is.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381593900", "createdAt": "2020-02-19T22:52:29Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/SpecificFilesLocalInputSource.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input.impl;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SpecificFilesLocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "605ffb2ff1ef01fbfe43895f79e356233250c7c7", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/605ffb2ff1ef01fbfe43895f79e356233250c7c7", "committedDate": "2020-02-20T06:34:52Z", "message": "Remove specific local input source"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "383d2569332b4e369dfb1642aa1d2ef08ed54a8b", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/383d2569332b4e369dfb1642aa1d2ef08ed54a8b", "committedDate": "2020-02-20T06:46:33Z", "message": "Update docs/ingestion/native-batch.md\n\nCo-Authored-By: sthetland <steve.hetland@imply.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/76fb01c8c51efbd16562e79ff083ead309db0718", "committedDate": "2020-02-20T06:46:46Z", "message": "Update docs/ingestion/native-batch.md\n\nCo-Authored-By: sthetland <steve.hetland@imply.io>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYxNjg2NjE1", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-361686615", "createdAt": "2020-02-20T07:45:04Z", "commit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQwNzo0NTowNFrOFsI_sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQwNzo0NTowNFrOFsI_sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgyOTA0MQ==", "bodyText": "Is this better to accept both baseDir + filter and explicit files list, or should you specify one or the other exclusively?\nIf you think accepting both is better then this exception message should probably say 'At least one of ...' instead of 'Either one of'.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381829041", "createdAt": "2020-02-20T07:45:04Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -34,28 +39,46 @@\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files\n   )\n   {\n-    this.baseDir = Preconditions.checkNotNull(baseDir, \"baseDir\");\n-    this.filter = Preconditions.checkNotNull(filter, \"filter\");\n+    this.baseDir = baseDir;\n+    this.filter = baseDir != null ? Preconditions.checkNotNull(filter, \"filter\") : filter;\n+    this.files = files;\n+\n+    if (baseDir == null && CollectionUtils.isNullOrEmpty(files)) {\n+      throw new IAE(\"Either one of baseDir or files should be specified\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718"}, "originalPosition": 52}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8623f10291c4490f2c9cf2bba8020d4a1eb99cf8", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/8623f10291c4490f2c9cf2bba8020d4a1eb99cf8", "committedDate": "2020-02-20T20:03:27Z", "message": "doc and error msg"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyMjM1NDE3", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-362235417", "createdAt": "2020-02-20T21:09:02Z", "commit": {"oid": "8623f10291c4490f2c9cf2bba8020d4a1eb99cf8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bcbb345b0cc6fc65b205b8912c675c3467338f51", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/bcbb345b0cc6fc65b205b8912c675c3467338f51", "committedDate": "2020-02-21T23:09:20Z", "message": "Merge branch 'master' of github.com:apache/druid into split-files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/689d4671588b8e069cb17e99e1be195ccbb0ef11", "committedDate": "2020-02-22T00:31:19Z", "message": "fix build"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyOTg0ODIy", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-362984822", "createdAt": "2020-02-22T00:07:42Z", "commit": {"oid": "bcbb345b0cc6fc65b205b8912c675c3467338f51"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMDowNzo0MlrOFtH-ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMToyNTozMFrOFtIrUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2MDkyMw==", "bodyText": "Looks like the splitSize + size < maxSplitSize and current.isEmpty() block can be combined", "url": "https://github.com/apache/druid/pull/9360#discussion_r382860923", "createdAt": "2020-02-22T00:07:42Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcbb345b0cc6fc65b205b8912c675c3467338f51"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2OTk2OA==", "bodyText": "Can you add this new property to the LocalInputSource property docs?", "url": "https://github.com/apache/druid/pull/9360#discussion_r382869968", "createdAt": "2020-02-22T01:04:49Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDEyOA==", "bodyText": "nit: could call this sourceIterator", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870128", "createdAt": "2020-02-22T01:06:01Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDE1Nw==", "bodyText": "nit: could call this sourceCloseableIterator", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870157", "createdAt": "2020-02-22T01:06:17Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,\n       File temporaryDirectory\n   )\n   {\n-    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream.iterator()), temporaryDirectory);\n+    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream), temporaryDirectory);\n   }\n \n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      CloseableIterator<InputEntity> sourceIterator,\n+      CloseableIterator<? extends InputEntity> sourceIterator,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDY5NQ==", "bodyText": "Can add a @Nullable here", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870695", "createdAt": "2020-02-22T01:10:25Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MTcyMg==", "bodyText": "Should this propagate the exception instead? If we get an object with a byte size that can't be stored in a long, something seems very wrong", "url": "https://github.com/apache/druid/pull/9360#discussion_r382871722", "createdAt": "2020-02-22T01:18:57Z", "author": {"login": "jon-wei"}, "path": "extensions-core/google-extensions/src/main/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSource.java", "diffHunk": "@@ -59,23 +65,42 @@ public GoogleCloudStorageInputSource(\n   }\n \n   @Override\n-  protected GoogleCloudStorageEntity createEntity(InputSplit<CloudObjectLocation> split)\n+  protected InputEntity createEntity(CloudObjectLocation location)\n   {\n-    return new GoogleCloudStorageEntity(storage, split.get());\n+    return new GoogleCloudStorageEntity(storage, location);\n   }\n \n   @Override\n-  protected Stream<InputSplit<CloudObjectLocation>> getPrefixesSplitStream()\n+  protected Stream<InputSplit<List<CloudObjectLocation>>> getPrefixesSplitStream(@Nonnull SplitHintSpec splitHintSpec)\n   {\n-    return StreamSupport.stream(storageObjectIterable().spliterator(), false)\n-                        .map(this::byteSourceFromStorageObject)\n-                        .map(InputSplit::new);\n+    final Iterator<List<StorageObject>> splitIterator = splitHintSpec.split(\n+        storageObjectIterable().iterator(),\n+        storageObject -> {\n+          final BigInteger sizeInBigInteger = storageObject.getSize();\n+          long sizeInLong;\n+          if (sizeInBigInteger == null) {\n+            sizeInLong = Long.MAX_VALUE;\n+          } else {\n+            try {\n+              sizeInLong = sizeInBigInteger.longValueExact();\n+            }\n+            catch (ArithmeticException e) {\n+              sizeInLong = Long.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MjQwMA==", "bodyText": "Since it would get converted into a MaxSizeSplitHintSpec in createSplit, could this create a MaxSizeSplitHintSpec directly? (Does this also mean SegmentsSplitHintSpec is deprecated?)", "url": "https://github.com/apache/druid/pull/9360#discussion_r382872400", "createdAt": "2020-02-22T01:25:30Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -228,13 +232,15 @@ protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nu\n     // segmentIds is supposed to be specified by the supervisor task during the parallel indexing.\n     // If it's not null, segments are already split by the supervisor task and further split won't happen.\n     if (segmentIds == null) {\n-      return createSplits(\n-          coordinatorClient,\n-          retryPolicyFactory,\n-          dataSource,\n-          interval,\n-          splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec\n-      ).stream();\n+      return Streams.sequentialStreamFrom(\n+          createSplits(\n+              coordinatorClient,\n+              retryPolicyFactory,\n+              dataSource,\n+              interval,\n+              splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzMDU2MjU4", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-363056258", "createdAt": "2020-02-22T23:23:18Z", "commit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzoyMzoxOVrOFtNXpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzo0ODowNlrOFtNcUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTI4NQ==", "bodyText": "I think you should make spec classes be pure data objects (or beans). Adding methods like split to them makes them complicated and adds logic that makes it hard to version them in the future. We should think of data objects as literals, not as objects with business logic.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949285", "createdAt": "2020-02-22T23:23:19Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTgyMQ==", "bodyText": "I think you can simplify the logic of the next method below if you initialize peeking to inputIterator.next(), and only set peeking to null when inputIterator.hasNext() is false. In your next() below, you would just keeping shifting values from inputIterator into current after each iteration as long as there are more inputs.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949821", "createdAt": "2020-02-22T23:33:57Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTg0Mg==", "bodyText": "equals and hashCode need unit tests", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949842", "createdAt": "2020-02-22T23:34:34Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else {\n+            break;\n+          }\n+        }\n+        assert !current.isEmpty();\n+        return current;\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    MaxSizeSplitHintSpec that = (MaxSizeSplitHintSpec) o;\n+    return maxSplitSize == that.maxSplitSize;\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(maxSplitSize);\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTk4Mw==", "bodyText": "Seems like this method really doesn't belong here if not all subclasses or implementation need it? Or should this class be abstract instead?", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949983", "createdAt": "2020-02-22T23:37:42Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/SegmentsSplitHintSpec.java", "diffHunk": "@@ -56,6 +57,12 @@ public long getMaxInputSegmentBytesPerTask()\n     return maxInputSegmentBytesPerTask;\n   }\n \n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDI3MA==", "bodyText": "equals and hashCode need unit tests for maintainability.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382950270", "createdAt": "2020-02-22T23:43:37Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -131,14 +197,15 @@ public boolean equals(Object o)\n     if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-    LocalInputSource source = (LocalInputSource) o;\n-    return Objects.equals(baseDir, source.baseDir) &&\n-           Objects.equals(filter, source.filter);\n+    LocalInputSource that = (LocalInputSource) o;\n+    return Objects.equals(baseDir, that.baseDir) &&\n+           Objects.equals(filter, that.filter) &&\n+           Objects.equals(files, that.files);\n   }\n \n   @Override\n   public int hashCode()\n   {\n-    return Objects.hash(baseDir, filter);\n+    return Objects.hash(baseDir, filter, files);\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDQ4MA==", "bodyText": "tests for equals and hashcode please.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382950480", "createdAt": "2020-02-22T23:48:06Z", "author": {"login": "jnaous"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/firehose/WindowedSegmentId.java", "diffHunk": "@@ -56,6 +63,35 @@ public String getSegmentId()\n   @JsonProperty\n   public List<Interval> getIntervals()\n   {\n-    return intervals;\n+    return Collections.unmodifiableList(intervals);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    WindowedSegmentId segmentId1 = (WindowedSegmentId) o;\n+    return Objects.equals(segmentId, segmentId1.segmentId) &&\n+           Objects.equals(intervals, segmentId1.intervals);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(segmentId, intervals);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 48}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c10552a040a6e16ce725037bd1dc79f1ed374e9e", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/c10552a040a6e16ce725037bd1dc79f1ed374e9e", "committedDate": "2020-02-24T18:34:33Z", "message": "Merge branch 'master' of github.com:apache/druid into split-files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acaa8486ff8ca8476598ea7606b2c8466e456f1b", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/acaa8486ff8ca8476598ea7606b2c8466e456f1b", "committedDate": "2020-02-24T19:52:23Z", "message": "fix a test and address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzODE4NDAz", "url": "https://github.com/apache/druid/pull/9360#pullrequestreview-363818403", "createdAt": "2020-02-25T01:34:30Z", "commit": {"oid": "acaa8486ff8ca8476598ea7606b2c8466e456f1b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2907, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}