{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwODQ1NDE5", "number": 10419, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMjo1MzozMlrOEmSryw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMToyMzo1NlrOEmyafg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTg3NDY3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMjo1MzozMlrOHWPEeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDowOToxN1rOHWVoZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA3NzYyNg==", "bodyText": "Does it make sense to keep the @VisibleForTesting since this is now public (so that it can be used in PartialDimensionCardinalityTask)?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493077626", "createdAt": "2020-09-22T22:53:32Z", "author": {"login": "ccaominh"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions\n   }\n \n   @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n+  public static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTEyNQ==", "bodyText": "Removed the @VisibleForTesting", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185125", "createdAt": "2020-09-23T04:09:17Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions\n   }\n \n   @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n+  public static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA3NzYyNg=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTg5NTE4OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzowMjo1NFrOHWPQqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDowOToyOFrOHWVolQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc0Nw==", "bodyText": "Using \"single_dim\" instead of \"ranged\" in the error message maps better to the naming in the docs", "url": "https://github.com/apache/druid/pull/10419#discussion_r493080747", "createdAt": "2020-09-22T23:02:54Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTE3Mw==", "bodyText": "Changed the message to use \"single_dim\"", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185173", "createdAt": "2020-09-23T04:09:28Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc0Nw=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTkxMzU0OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzoxMTozMlrOHWPbbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDowOTozNlrOHWVotg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA==", "bodyText": "What do you think about changing the logging level to reduce the logging amount since this is printed for each row?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493083500", "createdAt": "2020-09-22T23:11:32Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExODExOQ==", "bodyText": "I'll remove this, it was a debugging-only message that I don't think should be retained", "url": "https://github.com/apache/druid/pull/10419#discussion_r493118119", "createdAt": "2020-09-23T01:13:52Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTIwNg==", "bodyText": "This has been removed", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185206", "createdAt": "2020-09-23T04:09:36Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTkyNDI2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzoxNTo1N1rOHWPhZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDowOTo1NlrOHWVpGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ==", "bodyText": "What do you think about using HllSketch instead of HyperLogLogCollector since HllSketch provides much more accurate estimates if the cardinality does not exceed the sketch's k value: http://datasketches.apache.org/docs/HLL/HllSketchVsDruidHyperLogLogCollector.html. Using HllSketch also will be more accurate and faster when the partial HLLs are merged.\nUsing HllSketch would mean that the implementation for parallel ingestion is different from the one for sequential ingestion though.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493085031", "createdAt": "2020-09-22T23:15:57Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());\n+\n+      HyperLogLogCollector hllCollector = intervalToCardinalities.computeIfAbsent(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExNzk2NQ==", "bodyText": "Using HllSketch would mean that the implementation for parallel ingestion is different from the one for sequential ingestion though.\n\nThat was my reasoning for using HyperLogLogCollector, but I think it makes sense to change these to use HllSketch. I'll update this one to use HllSketch and a follow-on could be to do the same for IndexTask.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493117965", "createdAt": "2020-09-23T01:13:17Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());\n+\n+      HyperLogLogCollector hllCollector = intervalToCardinalities.computeIfAbsent(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTMwNA==", "bodyText": "I've updated this to use HllSketch instead", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185304", "createdAt": "2020-09-23T04:09:56Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());\n+\n+      HyperLogLogCollector hllCollector = intervalToCardinalities.computeIfAbsent(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTk2NTYwOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzozNjoxM1rOHWP5gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMzoxNzoxNlrOHXEQTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ==", "bodyText": "I believe the logic in PartialHashSegmentGenerateTask.isReady() will need to be adjusted; otherwise if PartialDimensionCardinalityTask runs and grabs the locks here it'll get stuck. For example, PartialRangeSegmentGenerateTask.isReady() does not grab the locks since they're acquired earlier by PartialDimensionDistributionTask.\nI think a good place to add a test would be in HashPartitionMultiPhaseParallelIndexingTest. Currently, its test cases all specify a value of 2 for numShards, so we can add cases with null.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493091201", "createdAt": "2020-09-22T23:36:13Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExODI0Ng==", "bodyText": "Thanks, will look into this", "url": "https://github.com/apache/druid/pull/10419#discussion_r493118246", "createdAt": "2020-09-23T01:14:15Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTgwMg==", "bodyText": "I didn't see any failures when I was testing this on a cluster or in unit tests with the current locking (maybe related to the javadoc comment on isReady(): \"This method must be idempotent, as it may be run multiple times per task.\"?).\nI updated PartialHashSegmentGenerateTask.isReady() to skip the lock acquisition if the numShardsOverride is set (indicating that that cardinality phase ran).", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185802", "createdAt": "2020-09-23T04:12:01Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5OTg4Nw==", "bodyText": "I think acquiring locks here should be fine since it's idempotent. The supervisor task and all its subtasks share the same lock based on their groupId. I actually think it's better to call tryTimeChunkLock() in every subtask since it will make the task fail early when its lock is revoked. Otherwise, the task will fail when it publishes segments which happens at the last stage in batch ingestion.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493899887", "createdAt": "2020-09-23T21:15:18Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkyMDk4Mw==", "bodyText": "I seem to remember running into the self deadlock issue when implementing #8925, which is why the locks are only acquired in the first phase of the range partitioning subtasks. I don't remember if I discovered this via RangePartitionMultiPhaseParallelIndexingTest or ITPerfectRollupParallelIndexTest, but if the problem isn't showing up in this PR's tests, then that's good.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493920983", "createdAt": "2020-09-23T22:02:55Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0OTAwNQ==", "bodyText": "I wasn't able to reproduce the self deadlock with RangePartitionMultiPhaseParallelIndexingTest or ITPerfectRollupParallelIndexTest in master, so either I misremembered stuff or the issue has been fixed in the meantime.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493949005", "createdAt": "2020-09-23T23:17:16Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTk3NDQyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/indexer/partitions/HashedPartitionsSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo0MDoxMlrOHWP-sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDoxMjoyOFrOHWVreA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MjUyOQ==", "bodyText": "https://github.com/apache/druid/blob/master/docs/ingestion/native-batch.md#hash-based-partitioning needs to be updated to say that numShards is no longer required and also to mention the new partial dimension cardinality task.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493092529", "createdAt": "2020-09-22T23:40:12Z", "author": {"login": "ccaominh"}, "path": "core/src/main/java/org/apache/druid/indexer/partitions/HashedPartitionsSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public Integer getNumShards()\n   @Override\n   public String getForceGuaranteedRollupIncompatiblityReason()\n   {\n-    return getNumShards() == null ? NUM_SHARDS + \" must be specified\" : FORCE_GUARANTEED_ROLLUP_COMPATIBLE;\n+    return FORCE_GUARANTEED_ROLLUP_COMPATIBLE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTkxMg==", "bodyText": "Updated the docs to mark numShards as optional and added a description of the new cardinality scan phase.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185912", "createdAt": "2020-09-23T04:12:28Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/indexer/partitions/HashedPartitionsSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public Integer getNumShards()\n   @Override\n   public String getForceGuaranteedRollupIncompatiblityReason()\n   {\n-    return getNumShards() == null ? NUM_SHARDS + \" must be specified\" : FORCE_GUARANTEED_ROLLUP_COMPATIBLE;\n+    return FORCE_GUARANTEED_ROLLUP_COMPATIBLE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MjUyOQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTk4OTQyOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo0Nzo0MVrOHWQHdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwNDoxMjo0N1rOHWVr0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NDc3NA==", "bodyText": "Checking isForceGuaranteedRollup() is probably redundant here", "url": "https://github.com/apache/druid/pull/10419#discussion_r493094774", "createdAt": "2020-09-22T23:47:41Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",\n+          ingestionSchema.getTuningConfig().getPartitionsSpec()\n+      );\n+    }\n+\n+    final Integer numShardsOverride;\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n+    if (ingestionSchema.getTuningConfig().isForceGuaranteedRollup() && partitionsSpec.getNumShards() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NjAwMQ==", "bodyText": "Removed the isForceGuaranteedRollup check here", "url": "https://github.com/apache/druid/pull/10419#discussion_r493186001", "createdAt": "2020-09-23T04:12:47Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",\n+          ingestionSchema.getTuningConfig().getPartitionsSpec()\n+      );\n+    }\n+\n+    final Integer numShardsOverride;\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n+    if (ingestionSchema.getTuningConfig().isForceGuaranteedRollup() && partitionsSpec.getNumShards() == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NDc3NA=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTk5NzEyOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTaskTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo1MTozOVrOHWQMBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo1MTozOVrOHWQMBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NTk0MQ==", "bodyText": "I like the extensive set of tests!", "url": "https://github.com/apache/druid/pull/10419#discussion_r493095941", "createdAt": "2020-09-22T23:51:39Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTaskTest.java", "diffHunk": "@@ -0,0 +1,398 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import org.apache.druid.client.indexing.NoopIndexingServiceClient;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.data.input.impl.InlineInputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskState;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.DynamicPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.PartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskInfoProvider;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.stats.DropwizardRowIngestionMetersFactory;\n+import org.apache.druid.indexing.common.task.IndexTaskClientFactory;\n+import org.apache.druid.java.util.common.Intervals;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.UniformGranularitySpec;\n+import org.apache.druid.testing.junit.LoggerCaptureRule;\n+import org.apache.logging.log4j.core.LogEvent;\n+import org.easymock.Capture;\n+import org.easymock.EasyMock;\n+import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n+import org.joda.time.Interval;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+@RunWith(Enclosed.class)\n+public class PartialDimensionCardinalityTaskTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4Nzg5ODg0OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwOTo1NDoxOFrOHWiO5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MDo0MlrOHXF_3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzM5MTU4OA==", "bodyText": "how about using 0 instead here?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493391588", "createdAt": "2020-09-23T09:54:18Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzU2NA==", "bodyText": "Changed this to 0", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977564", "createdAt": "2020-09-24T00:50:42Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzM5MTU4OA=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4ODA1ODcxOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMDoyMjozNlrOHWj18g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MToxMFrOHXGAVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQxNzk3MA==", "bodyText": "out of curiosity, what is the effect if we pass jsonMapper.writeValueAsBytes(groupKey) directly to hllSketch? does it affect performance or accuracy in any way? hllSketch is already doing hashing on the byte array input.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493417970", "createdAt": "2020-09-23T10:22:36Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),\n+          inputRow\n+      );\n+\n+      try {\n+        hllSketch.update(\n+            IndexTask.HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(groupKey)).asBytes()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzY4Ng==", "bodyText": "Good point, I got rid of the first level of hashing there, it should be more accurate this way", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977686", "createdAt": "2020-09-24T00:51:10Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),\n+          inputRow\n+      );\n+\n+      try {\n+        hllSketch.update(\n+            IndexTask.HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(groupKey)).asBytes()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQxNzk3MA=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 226}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MDg5NjAzOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMDoyOTo1MlrOHW_ytQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MToyNFrOHXGAhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw==", "bodyText": "Please add the new parameter and its description.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493875893", "createdAt": "2020-09-23T20:29:52Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkzOTQyNg==", "bodyText": "I don't think I added any new parameters to HashedPartitionsSpec, were you referring to something else?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493939426", "createdAt": "2020-09-23T22:55:00Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0MDUxNw==", "bodyText": "Oh, I should have been more specific. I meant, the newly supported parameter, maxRowsPerSegment (or targetRowsPerSegment which I suggested below). Neither of them is not described here since we didn't support it before. Maybe https://github.com/apache/druid/blob/master/docs/ingestion/hadoop.md#hash-based-partitioning helps.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493940517", "createdAt": "2020-09-23T22:58:29Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzczMg==", "bodyText": "Added docs for targetRowsPerSegment", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977732", "createdAt": "2020-09-24T00:51:24Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MDk5NDI3OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMDo1ODo1OVrOHXAvig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MTo0MFrOHXGAwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MTQ2Ng==", "bodyText": "It seems like that it will guarantee that you will not have segments than the computed numShards, but it will not be guaranteed that number of rows per segment doesn't exceed maxRowsPerSegment since the partition dimensions can be skewed. Is this correct? Then, I suggest targetRowsPerSegment since it's not a hard limit.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493891466", "createdAt": "2020-09-23T20:58:59Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.\n+- The `partial_dimension_cardinality` phase is an optional phase that only runs if `numShards` is not specified.\n+The Parallel task splits the input data and assigns them to worker tasks based on the split hint spec.\n+Each worker task (type `partial_dimension_cardinality`) gathers estimates of partitioning dimensions cardinality for\n+each time chunk. The Parallel task will aggregate these estimates from the worker tasks and determine the highest\n+cardinality across all of the time chunks in the input data, dividing this cardinality by `maxRowsPerSegment` to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3Nzc5Mg==", "bodyText": "I went with targetRowsPerSegment", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977792", "createdAt": "2020-09-24T00:51:40Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.\n+- The `partial_dimension_cardinality` phase is an optional phase that only runs if `numShards` is not specified.\n+The Parallel task splits the input data and assigns them to worker tasks based on the split hint spec.\n+Each worker task (type `partial_dimension_cardinality`) gathers estimates of partitioning dimensions cardinality for\n+each time chunk. The Parallel task will aggregate these estimates from the worker tasks and determine the highest\n+cardinality across all of the time chunks in the input data, dividing this cardinality by `maxRowsPerSegment` to", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MTQ2Ng=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTAxNTkzOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTowNTozOVrOHXA8-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MjoxNVrOHXGBWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NDkwNQ==", "bodyText": "Hmm.. Should we fail instead? Since the timeline in the coordinator and the broker will explode if you have this many segments per interval.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493894905", "createdAt": "2020-09-23T21:05:39Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;\n+    for (Union union : finalCollectors.values()) {\n+      maxCardinality = Math.max(maxCardinality, (long) union.getEstimate());\n+    }\n+\n+    LOG.info(\"Estimated max cardinality: \" + maxCardinality);\n+\n+    // determine numShards based on maxRowsPerSegment and the highest per-interval cardinality\n+    long numShards = maxCardinality / maxRowsPerSegment;\n+    if (maxCardinality % maxRowsPerSegment != 0) {\n+      // if there's a remainder add 1 so we stay under maxRowsPerSegment\n+      numShards += 1;\n+    }\n+    try {\n+      return Math.toIntExact(numShards);\n+    }\n+    catch (ArithmeticException ae) {\n+      return Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3Nzk0Nw==", "bodyText": "I changed this to throw an exception now", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977947", "createdAt": "2020-09-24T00:52:15Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;\n+    for (Union union : finalCollectors.values()) {\n+      maxCardinality = Math.max(maxCardinality, (long) union.getEstimate());\n+    }\n+\n+    LOG.info(\"Estimated max cardinality: \" + maxCardinality);\n+\n+    // determine numShards based on maxRowsPerSegment and the highest per-interval cardinality\n+    long numShards = maxCardinality / maxRowsPerSegment;\n+    if (maxCardinality % maxRowsPerSegment != 0) {\n+      // if there's a remainder add 1 so we stay under maxRowsPerSegment\n+      numShards += 1;\n+    }\n+    try {\n+      return Math.toIntExact(numShards);\n+    }\n+    catch (ArithmeticException ae) {\n+      return Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NDkwNQ=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTAyNjc1OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTowOTowOFrOHXBDww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MzozMVrOHXGChA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NjY0Mw==", "bodyText": "When I wrote this, my intention was showing the name of phases (e.g., https://github.com/apache/druid/pull/10419/files#diff-05bbc55d565a3d9462353d9b4771cb09R34). This phase name is not shown anywhere currently, but will be available in the task live reports and metrics after #10352. How about removing underscores from the here too?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493896643", "createdAt": "2020-09-23T21:09:08Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODI0NA==", "bodyText": "Removed the underscores here", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978244", "createdAt": "2020-09-24T00:53:31Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NjY0Mw=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTA1OTQyOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMToxOToxN1rOHXBXUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1NDo1OFrOHXGD6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMTY0OA==", "bodyText": "This timestamp should be bucketed based on the query granularity. See https://github.com/apache/druid/blob/master/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java#L146.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493901648", "createdAt": "2020-09-23T21:19:17Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODYwMg==", "bodyText": "Ah, thanks, I fixed this and updated PartialDimensionCardinalityTaskTest.sendsCorrectReportWithMultipleIntervalsInData() to test that case", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978602", "createdAt": "2020-09-24T00:54:58Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMTY0OA=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTA2MzY2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMToyMDozN1rOHXBZxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1Mjo0MlrOHXGBvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMjI3OQ==", "bodyText": "nit: inputRow is safely non-null since FilteringCloseableInputRowIterator filters out all null rows. See AbstractBatchIndexTask.defaultRowFilter().", "url": "https://github.com/apache/druid/pull/10419#discussion_r493902279", "createdAt": "2020-09-23T21:20:37Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODA0NA==", "bodyText": "Removed the null check", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978044", "createdAt": "2020-09-24T00:52:42Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMjI3OQ=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTA3MzI2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMToyMzo1NlrOHXBfqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1MzoxNVrOHXGCTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMzc4NA==", "bodyText": "nit: DefaultIndexTaskInputRowIteratorBuilder effectively does nothing here since its core functionality has been moved to FilteringCloseableInputRowIterator in #10336. I haven't cleaned up this interface yet.  Now, it's only useful in range partitioning as some more useful inputRowHandlers are appended to the default builder.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493903784", "createdAt": "2020-09-23T21:23:56Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODE5MQ==", "bodyText": "I changed this to just use the iterator above this directly", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978191", "createdAt": "2020-09-24T00:53:15Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMzc4NA=="}, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 175}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2956, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}