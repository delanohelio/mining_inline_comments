{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY2NDcyNDQz", "number": 10267, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMToyOToyOVrOEXlb1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQyMzozOToyMlrOFqG_fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMTY2MDM4OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMToyOToyOVrOG_dHrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxMzo1MzowMFrOG_iVqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5MDU3Mw==", "bodyText": "will it make sense to move DimFilter outside the InputSource in the task json? It seems more natural to me to put the filters alongside transforms, dimensions, and metrics and leave only the data source properties inside the InputSource section. On the flip side, it could make the compatibility situation more complicated than it is.", "url": "https://github.com/apache/druid/pull/10267#discussion_r469190573", "createdAt": "2020-08-12T11:29:29Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -87,13 +91,21 @@\n   @Nullable\n   private final List<WindowedSegmentId> segmentIds;\n   private final DimFilter dimFilter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a4a97eb8b803545fcec896d592bfb21d537db86"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI3NjA3NQ==", "bodyText": "It's possible to specify a filter alongside transforms today! You can do it in two places:\n\nIn the transformSpec (this works with any input source / format, see https://druid.apache.org/docs/latest/ingestion/index.html#filter)\nIn the druid inputSource itself (of course, only works with this input source)\n\nIt's a little silly to have both, perhaps, but there's a practical reason: specifying a filter in the druid inputSource is faster, because it is applied while creating the cursor that reads the data, and therefore it can use indexes, etc. The filter in the transformSpec is applied after the cursor generates rows.\nBut I think in the future, it'd be better to support pushing down the transformSpec filter into the cursor, and then we could deprecate the filter parameter in the inputSource, because it wouldn't be useful anymore.\nFor now, I suggest we leave it as-is.", "url": "https://github.com/apache/druid/pull/10267#discussion_r469276075", "createdAt": "2020-08-12T13:53:00Z", "author": {"login": "gianm"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -87,13 +91,21 @@\n   @Nullable\n   private final List<WindowedSegmentId> segmentIds;\n   private final DimFilter dimFilter;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5MDU3Mw=="}, "originalCommit": {"oid": "6a4a97eb8b803545fcec896d592bfb21d537db86"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzOTI3NjM1OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/InputRowSchemas.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwMjoxMjo1NFrOHAlPug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo0NzoxMFrOHBELpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3MjI4Mg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10267#discussion_r470372282", "createdAt": "2020-08-14T02:12:54Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/InputRowSchemas.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.input;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.druid.data.input.ColumnsFilter;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.transform.Transform;\n+import org.apache.druid.segment.transform.TransformSpec;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utilities that are helpful when implementing {@link org.apache.druid.data.input.InputEntityReader}.\n+ */\n+public class InputRowSchemas\n+{\n+  private InputRowSchemas()\n+  {\n+    // No instantiation.\n+  }\n+\n+  /**\n+   * Creates an {@link InputRowSchema} from a given {@link DataSchema}.\n+   */\n+  public static InputRowSchema fromDataSchema(final DataSchema dataSchema)\n+  {\n+    return new InputRowSchema(\n+        dataSchema.getTimestampSpec(),\n+        dataSchema.getDimensionsSpec(),\n+        createColumnsFilter(\n+            dataSchema.getTimestampSpec(),\n+            dataSchema.getDimensionsSpec(),\n+            dataSchema.getTransformSpec(),\n+            dataSchema.getAggregators()\n+        )\n+    );\n+  }\n+\n+  /**\n+   * Build a {@link ColumnsFilter} that can filter down the list of columns that must be read after flattening.\n+   *\n+   * @see InputRowSchema#getColumnsFilter()\n+   */\n+  @VisibleForTesting\n+  static ColumnsFilter createColumnsFilter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a4a97eb8b803545fcec896d592bfb21d537db86"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY1MzMwMA==", "bodyText": "(What's the thumbs up for?)", "url": "https://github.com/apache/druid/pull/10267#discussion_r470653300", "createdAt": "2020-08-14T14:20:40Z", "author": {"login": "gianm"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/InputRowSchemas.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.input;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.druid.data.input.ColumnsFilter;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.transform.Transform;\n+import org.apache.druid.segment.transform.TransformSpec;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utilities that are helpful when implementing {@link org.apache.druid.data.input.InputEntityReader}.\n+ */\n+public class InputRowSchemas\n+{\n+  private InputRowSchemas()\n+  {\n+    // No instantiation.\n+  }\n+\n+  /**\n+   * Creates an {@link InputRowSchema} from a given {@link DataSchema}.\n+   */\n+  public static InputRowSchema fromDataSchema(final DataSchema dataSchema)\n+  {\n+    return new InputRowSchema(\n+        dataSchema.getTimestampSpec(),\n+        dataSchema.getDimensionsSpec(),\n+        createColumnsFilter(\n+            dataSchema.getTimestampSpec(),\n+            dataSchema.getDimensionsSpec(),\n+            dataSchema.getTransformSpec(),\n+            dataSchema.getAggregators()\n+        )\n+    );\n+  }\n+\n+  /**\n+   * Build a {@link ColumnsFilter} that can filter down the list of columns that must be read after flattening.\n+   *\n+   * @see InputRowSchema#getColumnsFilter()\n+   */\n+  @VisibleForTesting\n+  static ColumnsFilter createColumnsFilter(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3MjI4Mg=="}, "originalCommit": {"oid": "6a4a97eb8b803545fcec896d592bfb21d537db86"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3OTE0MQ==", "bodyText": "thought it was a good method \ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10267#discussion_r470879141", "createdAt": "2020-08-14T21:47:10Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/InputRowSchemas.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.input;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.druid.data.input.ColumnsFilter;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.transform.Transform;\n+import org.apache.druid.segment.transform.TransformSpec;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utilities that are helpful when implementing {@link org.apache.druid.data.input.InputEntityReader}.\n+ */\n+public class InputRowSchemas\n+{\n+  private InputRowSchemas()\n+  {\n+    // No instantiation.\n+  }\n+\n+  /**\n+   * Creates an {@link InputRowSchema} from a given {@link DataSchema}.\n+   */\n+  public static InputRowSchema fromDataSchema(final DataSchema dataSchema)\n+  {\n+    return new InputRowSchema(\n+        dataSchema.getTimestampSpec(),\n+        dataSchema.getDimensionsSpec(),\n+        createColumnsFilter(\n+            dataSchema.getTimestampSpec(),\n+            dataSchema.getDimensionsSpec(),\n+            dataSchema.getTransformSpec(),\n+            dataSchema.getAggregators()\n+        )\n+    );\n+  }\n+\n+  /**\n+   * Build a {@link ColumnsFilter} that can filter down the list of columns that must be read after flattening.\n+   *\n+   * @see InputRowSchema#getColumnsFilter()\n+   */\n+  @VisibleForTesting\n+  static ColumnsFilter createColumnsFilter(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3MjI4Mg=="}, "originalCommit": {"oid": "6a4a97eb8b803545fcec896d592bfb21d537db86"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NzI1MDY4OnYy", "diffSide": "RIGHT", "path": "docs/configuration/index.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTo1Mjo0OFrOIBDKhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMjoyODoxN1rOIBD_Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3MTMzNA==", "bodyText": "should be 0.20.1 at this point", "url": "https://github.com/apache/druid/pull/10267#discussion_r537971334", "createdAt": "2020-12-08T01:52:48Z", "author": {"login": "vogievetsky"}, "path": "docs/configuration/index.md", "diffHunk": "@@ -1249,6 +1249,7 @@ Additional peon configs include:\n |`druid.indexer.task.gracefulShutdownTimeout`|Wait this long on middleManager restart for restorable tasks to gracefully exit.|PT5M|\n |`druid.indexer.task.hadoopWorkingPath`|Temporary working directory for Hadoop tasks.|`/tmp/druid-indexing`|\n |`druid.indexer.task.restoreTasksOnRestart`|If true, MiddleManagers will attempt to stop tasks gracefully on shutdown and restore them on restart.|false|\n+|`druid.indexer.task.ignoreTimestampSpecForDruidInputSource`|If true, tasks using the [Druid input source](../ingestion/native-batch.md#druid-input-source) will ignore the provided timestampSpec, and will use the `__time` column of the input datasource. This option is provided for compatibility with ingestion specs written before Druid 0.20.0.|false|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7695d8a5e67ad7a8a8414805b7ff389ad93724cd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk4NDc3MA==", "bodyText": "I'll change it to 0.21.0, in the guess that this will be the next release.", "url": "https://github.com/apache/druid/pull/10267#discussion_r537984770", "createdAt": "2020-12-08T02:28:17Z", "author": {"login": "gianm"}, "path": "docs/configuration/index.md", "diffHunk": "@@ -1249,6 +1249,7 @@ Additional peon configs include:\n |`druid.indexer.task.gracefulShutdownTimeout`|Wait this long on middleManager restart for restorable tasks to gracefully exit.|PT5M|\n |`druid.indexer.task.hadoopWorkingPath`|Temporary working directory for Hadoop tasks.|`/tmp/druid-indexing`|\n |`druid.indexer.task.restoreTasksOnRestart`|If true, MiddleManagers will attempt to stop tasks gracefully on shutdown and restore them on restart.|false|\n+|`druid.indexer.task.ignoreTimestampSpecForDruidInputSource`|If true, tasks using the [Druid input source](../ingestion/native-batch.md#druid-input-source) will ignore the provided timestampSpec, and will use the `__time` column of the input datasource. This option is provided for compatibility with ingestion specs written before Druid 0.20.0.|false|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3MTMzNA=="}, "originalCommit": {"oid": "7695d8a5e67ad7a8a8414805b7ff389ad93724cd"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NzI1MTA0OnYy", "diffSide": "RIGHT", "path": "docs/configuration/index.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTo1Mjo1N1rOIBDKuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTo1Mjo1N1rOIBDKuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3MTM4NQ==", "bodyText": "ditto", "url": "https://github.com/apache/druid/pull/10267#discussion_r537971385", "createdAt": "2020-12-08T01:52:57Z", "author": {"login": "vogievetsky"}, "path": "docs/configuration/index.md", "diffHunk": "@@ -1313,6 +1314,7 @@ then the value from the configuration below is used:\n |`druid.indexer.task.gracefulShutdownTimeout`|Wait this long on Indexer restart for restorable tasks to gracefully exit.|PT5M|\n |`druid.indexer.task.hadoopWorkingPath`|Temporary working directory for Hadoop tasks.|`/tmp/druid-indexing`|\n |`druid.indexer.task.restoreTasksOnRestart`|If true, the Indexer will attempt to stop tasks gracefully on shutdown and restore them on restart.|false|\n+|`druid.indexer.task.ignoreTimestampSpecForDruidInputSource`|If true, tasks using the [Druid input source](../ingestion/native-batch.md#druid-input-source) will ignore the provided timestampSpec, and will use the `__time` column of the input datasource. This option is provided for compatibility with ingestion specs written before Druid 0.20.0.|false|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7695d8a5e67ad7a8a8414805b7ff389ad93724cd"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc5NTUzNzQ1OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQxNzo1MjoxOFrOI85UUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQwMTowNDo0NlrOI9IcLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNDU2Mw==", "bodyText": "Maybe good to suggest using auto compaction here instead of writing an ingestion spec.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600724563", "createdAt": "2021-03-24T17:52:18Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk3MjMzMg==", "bodyText": "Good idea. I added this.\nAlternatively, if your goals can be satisfied by [compaction](compaction.md),\nconsider that instead as a simpler approach.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600972332", "createdAt": "2021-03-25T01:04:46Z", "author": {"login": "gianm"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNDU2Mw=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc5NTU0Nzg4OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQxNzo1NDozMFrOI85a3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQwMTozMjowNFrOI9I-rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNjIzOQ==", "bodyText": "This part was not in the previous example. Was it intentional to use hashed partitionsSpec here? Seems unnecessary to me.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600726239", "createdAt": "2021-03-24T17:54:30Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.\n+\n+An example task spec is shown below. It reads from a hypothetical raw datasource `wikipedia_raw` and creates a new\n+rolled-up datasource `wikipedia_rollup` by grouping on hour, \"countryName\", and \"page\".\n \n ```json\n-...\n+{\n+  \"type\": \"index_parallel\",\n+  \"spec\": {\n+    \"dataSchema\": {\n+      \"dataSource\": \"wikipedia_rollup\",\n+      \"timestampSpec\": {\n+        \"column\": \"__time\",\n+        \"format\": \"millis\"\n+      },\n+      \"dimensionsSpec\": {\n+        \"dimensions\": [\n+          \"countryName\",\n+          \"page\"\n+        ]\n+      },\n+      \"metricsSpec\": [\n+        {\n+          \"type\": \"count\",\n+          \"name\": \"cnt\"\n+        }\n+      ],\n+      \"granularitySpec\": {\n+        \"type\": \"uniform\",\n+        \"queryGranularity\": \"HOUR\",\n+        \"segmentGranularity\": \"DAY\",\n+        \"intervals\": [\"2016-06-27/P1D\"],\n+        \"rollup\": true\n+      }\n+    },\n     \"ioConfig\": {\n       \"type\": \"index_parallel\",\n       \"inputSource\": {\n         \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\",\n-        \"dimensions\": [\n-          \"page\",\n-          \"user\"\n-        ],\n-        \"metrics\": [\n-          \"added\"\n-        ],\n-        \"filter\": {\n-          \"type\": \"selector\",\n-          \"dimension\": \"page\",\n-          \"value\": \"Druid\"\n-        }\n+        \"dataSource\": \"wikipedia_raw\",\n+        \"interval\": \"2016-06-27/P1D\"\n       }\n-      ...\n     },\n-...\n+    \"tuningConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"partitionsSpec\": {\n+        \"type\": \"hashed\",\n+        \"numShards\": 1\n+      },\n+      \"forceGuaranteedRollup\": true,\n+      \"maxNumConcurrentSubTasks\": 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk2Mjk2OA==", "bodyText": "What I was thinking was:\n\nI want to include a full ingest spec, not just the inputSource part, so people have a full example.\nThis spec uses rollup, so for a reindexing spec, it'd be good to use a partitionsSpec that guarantees rollup too.\n\nDo you have a better suggestion for what to put in the example?", "url": "https://github.com/apache/druid/pull/10267#discussion_r600962968", "createdAt": "2021-03-25T00:34:35Z", "author": {"login": "gianm"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.\n+\n+An example task spec is shown below. It reads from a hypothetical raw datasource `wikipedia_raw` and creates a new\n+rolled-up datasource `wikipedia_rollup` by grouping on hour, \"countryName\", and \"page\".\n \n ```json\n-...\n+{\n+  \"type\": \"index_parallel\",\n+  \"spec\": {\n+    \"dataSchema\": {\n+      \"dataSource\": \"wikipedia_rollup\",\n+      \"timestampSpec\": {\n+        \"column\": \"__time\",\n+        \"format\": \"millis\"\n+      },\n+      \"dimensionsSpec\": {\n+        \"dimensions\": [\n+          \"countryName\",\n+          \"page\"\n+        ]\n+      },\n+      \"metricsSpec\": [\n+        {\n+          \"type\": \"count\",\n+          \"name\": \"cnt\"\n+        }\n+      ],\n+      \"granularitySpec\": {\n+        \"type\": \"uniform\",\n+        \"queryGranularity\": \"HOUR\",\n+        \"segmentGranularity\": \"DAY\",\n+        \"intervals\": [\"2016-06-27/P1D\"],\n+        \"rollup\": true\n+      }\n+    },\n     \"ioConfig\": {\n       \"type\": \"index_parallel\",\n       \"inputSource\": {\n         \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\",\n-        \"dimensions\": [\n-          \"page\",\n-          \"user\"\n-        ],\n-        \"metrics\": [\n-          \"added\"\n-        ],\n-        \"filter\": {\n-          \"type\": \"selector\",\n-          \"dimension\": \"page\",\n-          \"value\": \"Druid\"\n-        }\n+        \"dataSource\": \"wikipedia_raw\",\n+        \"interval\": \"2016-06-27/P1D\"\n       }\n-      ...\n     },\n-...\n+    \"tuningConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"partitionsSpec\": {\n+        \"type\": \"hashed\",\n+        \"numShards\": 1\n+      },\n+      \"forceGuaranteedRollup\": true,\n+      \"maxNumConcurrentSubTasks\": 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNjIzOQ=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk2NDI5OQ==", "bodyText": "I see. It makes sense. If that's the case, I would suggest simply removing numShards from the spec. The parallel task will find the numShards automatically based on targetRowsPerSegment which is 5 million by default.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600964299", "createdAt": "2021-03-25T00:38:15Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.\n+\n+An example task spec is shown below. It reads from a hypothetical raw datasource `wikipedia_raw` and creates a new\n+rolled-up datasource `wikipedia_rollup` by grouping on hour, \"countryName\", and \"page\".\n \n ```json\n-...\n+{\n+  \"type\": \"index_parallel\",\n+  \"spec\": {\n+    \"dataSchema\": {\n+      \"dataSource\": \"wikipedia_rollup\",\n+      \"timestampSpec\": {\n+        \"column\": \"__time\",\n+        \"format\": \"millis\"\n+      },\n+      \"dimensionsSpec\": {\n+        \"dimensions\": [\n+          \"countryName\",\n+          \"page\"\n+        ]\n+      },\n+      \"metricsSpec\": [\n+        {\n+          \"type\": \"count\",\n+          \"name\": \"cnt\"\n+        }\n+      ],\n+      \"granularitySpec\": {\n+        \"type\": \"uniform\",\n+        \"queryGranularity\": \"HOUR\",\n+        \"segmentGranularity\": \"DAY\",\n+        \"intervals\": [\"2016-06-27/P1D\"],\n+        \"rollup\": true\n+      }\n+    },\n     \"ioConfig\": {\n       \"type\": \"index_parallel\",\n       \"inputSource\": {\n         \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\",\n-        \"dimensions\": [\n-          \"page\",\n-          \"user\"\n-        ],\n-        \"metrics\": [\n-          \"added\"\n-        ],\n-        \"filter\": {\n-          \"type\": \"selector\",\n-          \"dimension\": \"page\",\n-          \"value\": \"Druid\"\n-        }\n+        \"dataSource\": \"wikipedia_raw\",\n+        \"interval\": \"2016-06-27/P1D\"\n       }\n-      ...\n     },\n-...\n+    \"tuningConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"partitionsSpec\": {\n+        \"type\": \"hashed\",\n+        \"numShards\": 1\n+      },\n+      \"forceGuaranteedRollup\": true,\n+      \"maxNumConcurrentSubTasks\": 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNjIzOQ=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk4MTE2Ng==", "bodyText": "OK, I'll do that.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600981166", "createdAt": "2021-03-25T01:32:04Z", "author": {"login": "gianm"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1282,60 +1282,81 @@ no `inputFormat` field needs to be specified in the ingestion spec when using th\n |type|This should be \"druid\".|yes|\n |dataSource|A String defining the Druid datasource to fetch rows from|yes|\n |interval|A String representing an ISO-8601 interval, which defines the time range to fetch the data over.|yes|\n-|dimensions|A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. |no|\n-|metrics|The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned.|no|\n |filter| See [Filters](../querying/filters.md). Only rows that match the filter, if specified, will be returned.|no|\n \n-A minimal example DruidInputSource spec is shown below:\n+The Druid input source can be used for a variety of purposes, including:\n \n-```json\n-...\n-    \"ioConfig\": {\n-      \"type\": \"index_parallel\",\n-      \"inputSource\": {\n-        \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\"\n-      }\n-      ...\n-    },\n-...\n-```\n+- Creating new datasources that are rolled-up copies of existing datasources.\n+- Changing the [partitioning or sorting](index.md#partitioning) of a datasource to improve performance.\n+- Updating or removing rows using a [`transformSpec`](index.md#transformspec).\n \n-The spec above will read all existing dimension and metric columns from\n-the `wikipedia` datasource, including all rows with a timestamp (the `__time` column)\n-within the interval `2013-01-01/2013-01-02`.\n+When using the Druid input source, the timestamp column shows up as a numeric field named `__time` set to the number\n+of milliseconds since the epoch (January 1, 1970 00:00:00 UTC). It is common to use this in the timestampSpec, if you\n+want the output timestamp to be equivalent to the input timestamp. In this case, set the timestamp column to `__time`\n+and the format to `auto` or `millis`.\n \n-A spec that applies a filter and reads a subset of the original datasource's columns is shown below.\n+It is OK for the input and output datasources to be the same. In this case, newly generated data will overwrite the\n+previous data for the intervals specified in the `granularitySpec`. Generally, if you are going to do this, it is a\n+good idea to test out your reindexing by writing to a separate datasource before overwriting your main one.\n+\n+An example task spec is shown below. It reads from a hypothetical raw datasource `wikipedia_raw` and creates a new\n+rolled-up datasource `wikipedia_rollup` by grouping on hour, \"countryName\", and \"page\".\n \n ```json\n-...\n+{\n+  \"type\": \"index_parallel\",\n+  \"spec\": {\n+    \"dataSchema\": {\n+      \"dataSource\": \"wikipedia_rollup\",\n+      \"timestampSpec\": {\n+        \"column\": \"__time\",\n+        \"format\": \"millis\"\n+      },\n+      \"dimensionsSpec\": {\n+        \"dimensions\": [\n+          \"countryName\",\n+          \"page\"\n+        ]\n+      },\n+      \"metricsSpec\": [\n+        {\n+          \"type\": \"count\",\n+          \"name\": \"cnt\"\n+        }\n+      ],\n+      \"granularitySpec\": {\n+        \"type\": \"uniform\",\n+        \"queryGranularity\": \"HOUR\",\n+        \"segmentGranularity\": \"DAY\",\n+        \"intervals\": [\"2016-06-27/P1D\"],\n+        \"rollup\": true\n+      }\n+    },\n     \"ioConfig\": {\n       \"type\": \"index_parallel\",\n       \"inputSource\": {\n         \"type\": \"druid\",\n-        \"dataSource\": \"wikipedia\",\n-        \"interval\": \"2013-01-01/2013-01-02\",\n-        \"dimensions\": [\n-          \"page\",\n-          \"user\"\n-        ],\n-        \"metrics\": [\n-          \"added\"\n-        ],\n-        \"filter\": {\n-          \"type\": \"selector\",\n-          \"dimension\": \"page\",\n-          \"value\": \"Druid\"\n-        }\n+        \"dataSource\": \"wikipedia_raw\",\n+        \"interval\": \"2016-06-27/P1D\"\n       }\n-      ...\n     },\n-...\n+    \"tuningConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"partitionsSpec\": {\n+        \"type\": \"hashed\",\n+        \"numShards\": 1\n+      },\n+      \"forceGuaranteedRollup\": true,\n+      \"maxNumConcurrentSubTasks\": 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDcyNjIzOQ=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc5NTU4ODQwOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQxODowMjo1N1rOI850Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQwMToyODozMFrOI9I6Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDczMjY3OQ==", "bodyText": "Is it better to add this annotation at the class-level? Seems reasonable to not include any fields in JSON if they are null.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600732679", "createdAt": "2021-03-24T18:02:57Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -160,6 +184,7 @@ public String getDataSource()\n \n   @Nullable\n   @JsonProperty\n+  @JsonInclude(Include.NON_NULL)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk3OTk5OQ==", "bodyText": "Interesting question. To answer it, I had to add some tests to make sure it worked properly. The answer is yes, it does work. I'll make the change and keep the new tests (look for them in DruidInputSourceTest).", "url": "https://github.com/apache/druid/pull/10267#discussion_r600979999", "createdAt": "2021-03-25T01:28:30Z", "author": {"login": "gianm"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -160,6 +184,7 @@ public String getDataSource()\n \n   @Nullable\n   @JsonProperty\n+  @JsonInclude(Include.NON_NULL)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDczMjY3OQ=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc5NjE4MDA3OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidSegmentReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQxOTozODoxNVrOI8_Kyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQxOTozODoxNVrOI8_Kyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDgyMDQyNg==", "bodyText": "Would be better to cache inputRowSchema since this function is called per row.", "url": "https://github.com/apache/druid/pull/10267#discussion_r600820426", "createdAt": "2021-03-24T19:38:15Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidSegmentReader.java", "diffHunk": "@@ -122,8 +147,16 @@\n   @Override\n   protected List<InputRow> parseInputRows(Map<String, Object> intermediateRow) throws ParseException\n   {\n-    final DateTime timestamp = (DateTime) intermediateRow.get(ColumnHolder.TIME_COLUMN_NAME);\n-    return Collections.singletonList(new MapBasedInputRow(timestamp.getMillis(), dimensions, intermediateRow));\n+    return Collections.singletonList(\n+        MapInputRowParser.parse(\n+            new InputRowSchema(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc5Njk5MDcxOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNFQyMzozOToyMlrOI9Guig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQwMTo1NzoyMFrOI9JcXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk0NDI2Ng==", "bodyText": "Do you know why this changed?", "url": "https://github.com/apache/druid/pull/10267#discussion_r600944266", "createdAt": "2021-03-24T23:39:22Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -129,7 +129,7 @@ public void testAutoCompactionDutySubmitAndVerifyCompaction() throws Exception\n           fullDatasourceName,\n           AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n           0,\n-          22482,\n+          22481,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk2NzAwMg==", "bodyText": "I guessed it was because the metadata changed. By that, I mean the org.apache.druid.segment.Metadata object stored in the segment, which contains the TimestampSpec.\nIt adds up, I think, since -1 character is the difference between the old default timestamp + auto (13 chars) and the new default __time + millis (12 chars).", "url": "https://github.com/apache/druid/pull/10267#discussion_r600967002", "createdAt": "2021-03-25T00:46:55Z", "author": {"login": "gianm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -129,7 +129,7 @@ public void testAutoCompactionDutySubmitAndVerifyCompaction() throws Exception\n           fullDatasourceName,\n           AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n           0,\n-          22482,\n+          22481,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk0NDI2Ng=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk4ODc2Ng==", "bodyText": "Ah, that seems likely the reason. Thanks \ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10267#discussion_r600988766", "createdAt": "2021-03-25T01:57:20Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -129,7 +129,7 @@ public void testAutoCompactionDutySubmitAndVerifyCompaction() throws Exception\n           fullDatasourceName,\n           AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n           0,\n-          22482,\n+          22481,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk0NDI2Ng=="}, "originalCommit": {"oid": "8fb44d7cd1e7f7f19ada2357baf18ae773a2d454"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2268, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}