{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxOTI4MzYx", "number": 9171, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyMDoyOVrODYVKEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0MTowNVrODYrztw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODM5MDU2OnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyMDoyOVrOFeIUwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyMjowNVrOFeIXcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNzk4Ng==", "bodyText": "Do I need to add the s3 extension for this support or is it bundled with the hdfs extension somehow?", "url": "https://github.com/apache/druid/pull/9171#discussion_r367137986", "createdAt": "2020-01-15T22:20:29Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzODY3NQ==", "bodyText": "Sorry, please ignore I see that the hadoop-aws module needs to be added - mentioned below", "url": "https://github.com/apache/druid/pull/9171#discussion_r367138675", "createdAt": "2020-01-15T22:22:05Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNzk4Ng=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODQwMDQzOnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyNDoxNFrOFeIa4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMzoxOToxOFrOFeJkRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzOTU1NA==", "bodyText": "Is this still accurate? Have we done more recent tests?", "url": "https://github.com/apache/druid/pull/9171#discussion_r367139554", "createdAt": "2020-01-15T22:24:14Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE1ODM0Mw==", "bodyText": "It was tested before I started working on Druid and don't know what was the test coverage. There's no more recent tests that I'm aware of.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367158343", "createdAt": "2020-01-15T23:19:18Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzOTU1NA=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODQwNzM4OnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyNzoxMFrOFeIfOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNzoyM1rOFeoHog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MDY2Nw==", "bodyText": "What type of input source should I use instead of the hdfs input source? Why is this beneficial?", "url": "https://github.com/apache/druid/pull/9171#discussion_r367140667", "createdAt": "2020-01-15T22:27:10Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n+\n+## Reading data from HDFS or Cloud Storage\n+\n+### Native batch ingestion\n+\n+The [HDFS input source](../../ingestion/native-batch.md#hdfs-input-source) is supported by the [Parallel task](../../ingestion/native-batch.md#parallel-task)\n+to read files directly from the HDFS Storage. However, we highly recommend to use a proper", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE1ODM2Ng==", "bodyText": "It depends on the type of your cloud storage. The benefit of using it is simpler to use without the extra setup to read from the cloud storage using the hdfs library which is basically same with the steps described above. We currently support only s3 and google cloud storage input sources. So if you want to read from something else such as azure, you may want to use the hdfs input source. But I don't think we have tested this functionality very well and I also don't know how to set up properly for doing that.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367158366", "createdAt": "2020-01-15T23:19:22Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n+\n+## Reading data from HDFS or Cloud Storage\n+\n+### Native batch ingestion\n+\n+The [HDFS input source](../../ingestion/native-batch.md#hdfs-input-source) is supported by the [Parallel task](../../ingestion/native-batch.md#parallel-task)\n+to read files directly from the HDFS Storage. However, we highly recommend to use a proper", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MDY2Nw=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1ODkxNA==", "bodyText": "Added this to the doc.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367658914", "createdAt": "2020-01-16T21:27:23Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n+\n+## Reading data from HDFS or Cloud Storage\n+\n+### Native batch ingestion\n+\n+The [HDFS input source](../../ingestion/native-batch.md#hdfs-input-source) is supported by the [Parallel task](../../ingestion/native-batch.md#parallel-task)\n+to read files directly from the HDFS Storage. However, we highly recommend to use a proper", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MDY2Nw=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODQxMjY4OnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMjoyOToxN1rOFeIifw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQyMzoxOToyNFrOFeJkaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MTUwMw==", "bodyText": "Is one of these ingestion methods recommended over the other? How do I decide which one to use?", "url": "https://github.com/apache/druid/pull/9171#discussion_r367141503", "createdAt": "2020-01-15T22:29:17Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n+\n+## Reading data from HDFS or Cloud Storage\n+\n+### Native batch ingestion\n+\n+The [HDFS input source](../../ingestion/native-batch.md#hdfs-input-source) is supported by the [Parallel task](../../ingestion/native-batch.md#parallel-task)\n+to read files directly from the HDFS Storage. However, we highly recommend to use a proper\n+[Input Source](../../ingestion/native-batch.md#input-sources) instead to read objects from Cloud storage.\n+\n+### Hadoop-based ingestion\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE1ODM3OQ==", "bodyText": "You mean between the native batch ingestion and hadoop-based one? It's explained at https://github.com/apache/druid/pull/9171/files#diff-3ae520a063215c87a2a6c144eeb0bfc0R74-R80.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367158379", "createdAt": "2020-01-15T23:19:24Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,105 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n-```json\n-\"firehose\" : {\n-    \"type\" : \"hdfs\",\n-    \"paths\": \"/foo/bar,/foo/baz\"\n-}\n+|Property|Possible Values|Description|Default|\n+|--------|---------------|-----------|-------|\n+|`druid.storage.type`|hdfs||Must be set.|\n+|`druid.storage.storageDirectory`|gs://bucket/example/directory|Path to the deep storage|Must be set.|\n+\n+All services that need to access GCS need to have the [GCS connector jar](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md) in their class path.\n+One option is to place this jar in `${DRUID_HOME}/lib/` and `${DRUID_HOME}/extensions/druid-hdfs-storage/`.\n+\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [instructions to configure Hadoop](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md#configure-hadoop),\n+[GCS core default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml)\n+and [GCS core template](https://github.com/GoogleCloudPlatform/bdutil/blob/master/conf/hadoop2/gcs-core-template.xml).\n+\n+```xml\n+<property>\n+  <name>fs.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n+  <description>The FileSystem for gs: (GCS) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.gs.impl</name>\n+  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n+  <description>The AbstractFileSystem for gs: uris.</description>\n+</property>\n ```\n \n-This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if\n-`intervals` are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning\n-of files is slow.\n-\n-|Property|Description|Default|\n-|--------|-----------|-------|\n-|type|This should be `hdfs`.|none (required)|\n-|paths|HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like `*` are supported in these paths.|none (required)|\n-|maxCacheCapacityBytes|Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes.|1073741824|\n-|maxFetchCapacityBytes|Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read.|1073741824|\n-|prefetchTriggerBytes|Threshold to trigger prefetching files.|maxFetchCapacityBytes / 2|\n-|fetchTimeout|Timeout for fetching each file.|60000|\n-|maxFetchRetry|Maximum number of retries for fetching each file.|3|\n+Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n+\n+## Reading data from HDFS or Cloud Storage\n+\n+### Native batch ingestion\n+\n+The [HDFS input source](../../ingestion/native-batch.md#hdfs-input-source) is supported by the [Parallel task](../../ingestion/native-batch.md#parallel-task)\n+to read files directly from the HDFS Storage. However, we highly recommend to use a proper\n+[Input Source](../../ingestion/native-batch.md#input-sources) instead to read objects from Cloud storage.\n+\n+### Hadoop-based ingestion\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MTUwMw=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTEwNjc1OnYy", "diffSide": "LEFT", "path": "docs/development/extensions-core/kafka-ingestion.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxNzo0OToxNlrOFeiMdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNzozMlrOFeoH4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU2MTg0Nw==", "bodyText": "I didn't update the kafka tutorial to use this spec. I can follow up in a separate patch", "url": "https://github.com/apache/druid/pull/9171#discussion_r367561847", "createdAt": "2020-01-16T17:49:16Z", "author": {"login": "suneet-s"}, "path": "docs/development/extensions-core/kafka-ingestion.md", "diffHunk": "@@ -60,22 +60,16 @@ A sample supervisor spec is shown below:\n   \"type\": \"kafka\",\n   \"dataSchema\": {\n     \"dataSource\": \"metrics-kafka\",\n-    \"parser\": {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1ODk3Ng==", "bodyText": "Thanks.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367658976", "createdAt": "2020-01-16T21:27:32Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/kafka-ingestion.md", "diffHunk": "@@ -60,22 +60,16 @@ A sample supervisor spec is shown below:\n   \"type\": \"kafka\",\n   \"dataSchema\": {\n     \"dataSource\": \"metrics-kafka\",\n-    \"parser\": {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU2MTg0Nw=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTE0NDIyOnYy", "diffSide": "LEFT", "path": "website/package-lock.json", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxODowMTo1N1rOFeijbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNzozNVrOFeoIBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU2NzcyNQ==", "bodyText": "just curious why all of these were marked as optional before, but not needed any more", "url": "https://github.com/apache/druid/pull/9171#discussion_r367567725", "createdAt": "2020-01-16T18:01:57Z", "author": {"login": "suneet-s"}, "path": "website/package-lock.json", "diffHunk": "@@ -3913,8 +3913,7 @@\n             \"ansi-regex\": {\n               \"version\": \"2.1.1\",\n               \"bundled\": true,\n-              \"dev\": true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1OTAxMw==", "bodyText": "Oops, this is not supposed to be added. Reverted all changed in this file.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367659013", "createdAt": "2020-01-16T21:27:35Z", "author": {"login": "jihoonson"}, "path": "website/package-lock.json", "diffHunk": "@@ -3913,8 +3913,7 @@\n             \"ansi-regex\": {\n               \"version\": \"2.1.1\",\n               \"bundled\": true,\n-              \"dev\": true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU2NzcyNQ=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTE2Mzk3OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/index.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxODowOTozMFrOFeivxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNzozOFrOFeoIFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU3MDg4NQ==", "bodyText": "this should be index_parallel - same comment on line 299, 332, 349. I have a doc change coming up so I can fix in the next patch as well.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367570885", "createdAt": "2020-01-16T18:09:30Z", "author": {"login": "suneet-s"}, "path": "docs/ingestion/index.md", "diffHunk": "@@ -287,44 +289,31 @@ definition is an _ingestion spec_.\n \n Ingestion specs consists of three main components:\n \n-- [`dataSchema`](#dataschema), which configures the [datasource name](#datasource), [input row parser](#parser),\n-   [primary timestamp](#timestampspec), [flattening of nested data](#flattenspec) (if needed),\n-   [dimensions](#dimensionsspec), [metrics](#metricsspec), and [transforms and filters](#transformspec) (if needed).\n-- [`ioConfig`](#ioconfig), which tells Druid how to connect to the source system and . For more information, see the\n+- [`dataSchema`](#dataschema), which configures the [datasource name](#datasource),\n+   [primary timestamp](#timestampspec), [dimensions](#dimensionsspec), [metrics](#metricsspec), and [transforms and filters](#transformspec) (if needed).\n+- [`ioConfig`](#ioconfig), which tells Druid how to connect to the source system and how to parse data. For more information, see the\n    documentation for each [ingestion method](#ingestion-methods).\n - [`tuningConfig`](#tuningconfig), which controls various tuning parameters specific to each\n   [ingestion method](#ingestion-methods).\n \n-Example ingestion spec for task type \"index\" (native batch):\n+Example ingestion spec for task type `parallel_index` (native batch):\n \n ```\n {\n-  \"type\": \"index\",\n+  \"type\": \"parallel_index\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1OTAzMQ==", "bodyText": "Oops, thanks. Fixed.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367659031", "createdAt": "2020-01-16T21:27:38Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/index.md", "diffHunk": "@@ -287,44 +289,31 @@ definition is an _ingestion spec_.\n \n Ingestion specs consists of three main components:\n \n-- [`dataSchema`](#dataschema), which configures the [datasource name](#datasource), [input row parser](#parser),\n-   [primary timestamp](#timestampspec), [flattening of nested data](#flattenspec) (if needed),\n-   [dimensions](#dimensionsspec), [metrics](#metricsspec), and [transforms and filters](#transformspec) (if needed).\n-- [`ioConfig`](#ioconfig), which tells Druid how to connect to the source system and . For more information, see the\n+- [`dataSchema`](#dataschema), which configures the [datasource name](#datasource),\n+   [primary timestamp](#timestampspec), [dimensions](#dimensionsspec), [metrics](#metricsspec), and [transforms and filters](#transformspec) (if needed).\n+- [`ioConfig`](#ioconfig), which tells Druid how to connect to the source system and how to parse data. For more information, see the\n    documentation for each [ingestion method](#ingestion-methods).\n - [`tuningConfig`](#tuningconfig), which controls various tuning parameters specific to each\n   [ingestion method](#ingestion-methods).\n \n-Example ingestion spec for task type \"index\" (native batch):\n+Example ingestion spec for task type `parallel_index` (native batch):\n \n ```\n {\n-  \"type\": \"index\",\n+  \"type\": \"parallel_index\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU3MDg4NQ=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTE5MzQ1OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/data-formats.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxODoyMDoxNlrOFejClg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNzo0MFrOFeoIKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU3NTcwMg==", "bodyText": "Would it be more accurate to say the string parser is deprecated since we still need the parser for hadoop ingestion?", "url": "https://github.com/apache/druid/pull/9171#discussion_r367575702", "createdAt": "2020-01-16T18:20:16Z", "author": {"login": "suneet-s"}, "path": "docs/ingestion/data-formats.md", "diffHunk": "@@ -63,155 +65,968 @@ _TSV (Delimited)_\n \n Note that the CSV and TSV data do not contain column heads. This becomes important when you specify the data for ingesting.\n \n+Besides text formats, Druid also supports binary formats such as [Orc](#orc) and [Parquet](#parquet) formats.\n+\n ## Custom Formats\n \n Druid supports custom data formats and can use the `Regex` parser or the `JavaScript` parsers to parse these formats. Please note that using any of these parsers for\n parsing data will not be as efficient as writing a native Java parser or using an external stream processor. We welcome contributions of new Parsers.\n \n-## Configuration\n+## Input Format\n+\n+> The Input Format is a new way to specify the data format of your input data which was introduced in 0.17.0.\n+Unfortunately, the Input Format doesn't support all data formats or ingestion methods supported by Druid yet.\n+Especially if you want to use the Hadoop ingestion, you still need to use the [Parser](#parser-deprecated).\n+If your data is formatted in some format not listed in this section, please consider using the Parser instead.\n \n-All forms of Druid ingestion require some form of schema object. The format of the data to be ingested is specified using the`parseSpec` entry in your `dataSchema`.\n+All forms of Druid ingestion require some form of schema object. The format of the data to be ingested is specified using the `inputFormat` entry in your [`ioConfig`](index.md#ioconfig).\n \n ### JSON\n \n+The `inputFormat` to load data of JSON format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"json\"\n+  },\n+  ...\n+}\n+```\n+\n+The JSON `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `json`. | yes |\n+| flattenSpec | JSON Object | Specifies flattening configuration for nested JSON data. See [`flattenSpec`](#flattenspec) for more info. | no |\n+| featureSpec | JSON Object | [JSON parser features](https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features) supported by Jackson library. Those features will be applied when parsing the input JSON data. | no |\n+\n+### CSV\n+\n+The `inputFormat` to load data of the CSV format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"csv\",\n+    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"]\n+  },\n+  ...\n+}\n+```\n+\n+The CSV `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `csv`. | yes |\n+| listDelimiter | String | A custom delimiter for multi-value dimensions. | no (default == ctrl+A) |\n+| columns | JSON array | Specifies the columns of the data. The columns should be in the same order with the columns of your data. | yes if `findColumnsFromHeader` is false or missing |\n+| findColumnsFromHeader | Boolean | If this is set, the task will find the column names from the header row. Note that `skipHeaderRows` will be applied before finding column names from the header. For example, if you set `skipHeaderRows` to 2 and `findColumnsFromHeader` to true, the task will skip the first two lines and then extract column information from the third line. `columns` will be ignored if this is set to true. | no (default = false if `columns` is set; otherwise null) |\n+| skipHeaderRows | Integer | If this is set, the task will skip the first `skipHeaderRows` rows. | no (default = 0) |\n+\n+### TSV (Delimited)\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"tsv\",\n+    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n+    \"delimiter\":\"|\"\n+  },\n+  ...\n+}\n+```\n+\n+The `inputFormat` to load data of a delimited format. An example is:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `tsv`. | yes |\n+| delimiter | String | A custom delimiter for data values. | no (default == `\\t`) |\n+| listDelimiter | String | A custom delimiter for multi-value dimensions. | no (default == ctrl+A) |\n+| columns | JSON array | Specifies the columns of the data. The columns should be in the same order with the columns of your data. | yes if `findColumnsFromHeader` is false or missing |\n+| findColumnsFromHeader | Boolean | If this is set, the task will find the column names from the header row. Note that `skipHeaderRows` will be applied before finding column names from the header. For example, if you set `skipHeaderRows` to 2 and `findColumnsFromHeader` to true, the task will skip the first two lines and then extract column information from the third line. `columns` will be ignored if this is set to true. | no (default = false if `columns` is set; otherwise null) |\n+| skipHeaderRows | Integer | If this is set, the task will skip the first `skipHeaderRows` rows. | no (default = 0) |\n+\n+Be sure to change the `delimiter` to the appropriate delimiter for your data. Like CSV, you must specify the columns and which subset of the columns you want indexed.\n+\n+### ORC\n+\n+> You need to include the [`druid-orc-extensions`](../development/extensions-core/orc.md) as an extension to use the ORC input format.\n+\n+> If you are considering upgrading from earlier than 0.15.0 to 0.15.0 or a higher version,\n+> please read [Migration from 'contrib' extension](../development/extensions-core/orc.md#migration-from-contrib-extension) carefully.\n+\n+The `inputFormat` to load data of ORC format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"orc\",\n+    \"flattenSpec\": {\n+      \"useFieldDiscovery\": true,\n+      \"fields\": [\n+        {\n+          \"type\": \"path\",\n+          \"name\": \"nested\",\n+          \"expr\": \"$.path.to.nested\"\n+        }\n+      ]\n+    }\n+    \"binaryAsString\": false\n+  },\n+  ...\n+}\n+```\n+\n+The ORC `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `json`. | yes |\n+| flattenSpec | JSON Object | Specifies flattening configuration for nested JSON data. See [`flattenSpec`](#flattenspec) for more info. | no |\n+| binaryAsString | Boolean | Specifies if the binary orc column which is not logically marked as a string should be treated as a UTF-8 encoded string. | no (default == false) |\n+\n+### Parquet\n+\n+> You need to include the [`druid-parquet-extensions`](../development/extensions-core/parquet.md) as an extension to use the Parquet input format.\n+\n+The `inputFormat` to load data of Parquet format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"parquet\",\n+    \"flattenSpec\": {\n+      \"useFieldDiscovery\": true,\n+      \"fields\": [\n+        {\n+          \"type\": \"path\",\n+          \"name\": \"nested\",\n+          \"expr\": \"$.path.to.nested\"\n+        }\n+      ]\n+    }\n+    \"binaryAsString\": false\n+  },\n+  ...\n+}\n+```\n+\n+The Parquet `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+|type| String| This should be set to `parquet` to read Parquet file| yes |\n+|flattenSpec| JSON Object |Define a [`flattenSpec`](#flattenspec) to extract nested values from a Parquet file. Note that only 'path' expression are supported ('jq' is unavailable).| no (default will auto-discover 'root' level properties) |\n+| binaryAsString | Boolean | Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. | no (default == false) |\n+\n+### FlattenSpec\n+\n+The `flattenSpec` is located in `inputFormat` \u2192 `flattenSpec` and is responsible for\n+bridging the gap between potentially nested input data (such as JSON, Avro, etc) and Druid's flat data model.\n+An example `flattenSpec` is:\n+\n+```json\n+\"flattenSpec\": {\n+  \"useFieldDiscovery\": true,\n+  \"fields\": [\n+    { \"name\": \"baz\", \"type\": \"root\" },\n+    { \"name\": \"foo_bar\", \"type\": \"path\", \"expr\": \"$.foo.bar\" },\n+    { \"name\": \"first_food\", \"type\": \"jq\", \"expr\": \".thing.food[1]\" }\n+  ]\n+}\n+```\n+> Conceptually, after input data records are read, the `flattenSpec` is applied first before\n+> any other specs such as [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec),\n+> [`dimensionsSpec`](./index.md#dimensionsspec), or [`metricsSpec`](./index.md#metricsspec). Keep this in mind when writing\n+> your ingestion spec.\n+\n+Flattening is only supported for [data formats](data-formats.md) that support nesting, including `avro`, `json`, `orc`,\n+and `parquet`.\n+\n+A `flattenSpec` can have the following components:\n+\n+| Field | Description | Default |\n+|-------|-------------|---------|\n+| useFieldDiscovery | If true, interpret all root-level fields as available fields for usage by [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec), [`dimensionsSpec`](./index.md#dimensionsspec), and [`metricsSpec`](./index.md#metricsspec).<br><br>If false, only explicitly specified fields (see `fields`) will be available for use. | `true` |\n+| fields | Specifies the fields of interest and how they are accessed. [See below for details.](#field-flattening-specifications) | `[]` |\n+\n+#### Field flattening specifications\n+\n+Each entry in the `fields` list can have the following components:\n+\n+| Field | Description | Default |\n+|-------|-------------|---------|\n+| type | Options are as follows:<br><br><ul><li>`root`, referring to a field at the root level of the record. Only really useful if `useFieldDiscovery` is false.</li><li>`path`, referring to a field using [JsonPath](https://github.com/jayway/JsonPath) notation. Supported by most data formats that offer nesting, including `avro`, `json`, `orc`, and `parquet`.</li><li>`jq`, referring to a field using [jackson-jq](https://github.com/eiiches/jackson-jq) notation. Only supported for the `json` format.</li></ul> | none (required) |\n+| name | Name of the field after flattening. This name can be referred to by the [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec), [`dimensionsSpec`](./index.md#dimensionsspec), and [`metricsSpec`](./index.md#metricsspec).| none (required) |\n+| expr | Expression for accessing the field while flattening. For type `path`, this should be [JsonPath](https://github.com/jayway/JsonPath). For type `jq`, this should be [jackson-jq](https://github.com/eiiches/jackson-jq) notation. For other types, this parameter is ignored. | none (required for types `path` and `jq`) |\n+\n+#### Notes on flattening\n+\n+* For convenience, when defining a root-level field, it is possible to define only the field name, as a string, instead of a JSON object. For example, `{\"name\": \"baz\", \"type\": \"root\"}` is equivalent to `\"baz\"`.\n+* Enabling `useFieldDiscovery` will only automatically detect \"simple\" fields at the root level that correspond to data types that Druid supports. This includes strings, numbers, and lists of strings or numbers. Other types will not be automatically detected, and must be specified explicitly in the `fields` list.\n+* Duplicate field `name`s are not allowed. An exception will be thrown.\n+* If `useFieldDiscovery` is enabled, any discovered field with the same name as one already defined in the `fields` list will be skipped, rather than added twice.\n+* [http://jsonpath.herokuapp.com/](http://jsonpath.herokuapp.com/) is useful for testing `path`-type expressions.\n+* jackson-jq supports a subset of the full [jq](https://stedolan.github.io/jq/) syntax.  Please refer to the [jackson-jq documentation](https://github.com/eiiches/jackson-jq) for details.\n+\n+## Parser (Deprecated)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1OTA1MA==", "bodyText": "Good point. I changed as below:\n> The Parser is deprecated for [native batch tasks](./native-batch.md), [Kafka indexing service](../development/extensions-core/kafka-ingestion.md),\nand [Kinesis indexing service](../development/extensions-core/kinesis-ingestion.md).\nConsider using the [input format](#input-format) instead for these types of ingestion.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367659050", "createdAt": "2020-01-16T21:27:40Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/data-formats.md", "diffHunk": "@@ -63,155 +65,968 @@ _TSV (Delimited)_\n \n Note that the CSV and TSV data do not contain column heads. This becomes important when you specify the data for ingesting.\n \n+Besides text formats, Druid also supports binary formats such as [Orc](#orc) and [Parquet](#parquet) formats.\n+\n ## Custom Formats\n \n Druid supports custom data formats and can use the `Regex` parser or the `JavaScript` parsers to parse these formats. Please note that using any of these parsers for\n parsing data will not be as efficient as writing a native Java parser or using an external stream processor. We welcome contributions of new Parsers.\n \n-## Configuration\n+## Input Format\n+\n+> The Input Format is a new way to specify the data format of your input data which was introduced in 0.17.0.\n+Unfortunately, the Input Format doesn't support all data formats or ingestion methods supported by Druid yet.\n+Especially if you want to use the Hadoop ingestion, you still need to use the [Parser](#parser-deprecated).\n+If your data is formatted in some format not listed in this section, please consider using the Parser instead.\n \n-All forms of Druid ingestion require some form of schema object. The format of the data to be ingested is specified using the`parseSpec` entry in your `dataSchema`.\n+All forms of Druid ingestion require some form of schema object. The format of the data to be ingested is specified using the `inputFormat` entry in your [`ioConfig`](index.md#ioconfig).\n \n ### JSON\n \n+The `inputFormat` to load data of JSON format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"json\"\n+  },\n+  ...\n+}\n+```\n+\n+The JSON `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `json`. | yes |\n+| flattenSpec | JSON Object | Specifies flattening configuration for nested JSON data. See [`flattenSpec`](#flattenspec) for more info. | no |\n+| featureSpec | JSON Object | [JSON parser features](https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features) supported by Jackson library. Those features will be applied when parsing the input JSON data. | no |\n+\n+### CSV\n+\n+The `inputFormat` to load data of the CSV format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"csv\",\n+    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"]\n+  },\n+  ...\n+}\n+```\n+\n+The CSV `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `csv`. | yes |\n+| listDelimiter | String | A custom delimiter for multi-value dimensions. | no (default == ctrl+A) |\n+| columns | JSON array | Specifies the columns of the data. The columns should be in the same order with the columns of your data. | yes if `findColumnsFromHeader` is false or missing |\n+| findColumnsFromHeader | Boolean | If this is set, the task will find the column names from the header row. Note that `skipHeaderRows` will be applied before finding column names from the header. For example, if you set `skipHeaderRows` to 2 and `findColumnsFromHeader` to true, the task will skip the first two lines and then extract column information from the third line. `columns` will be ignored if this is set to true. | no (default = false if `columns` is set; otherwise null) |\n+| skipHeaderRows | Integer | If this is set, the task will skip the first `skipHeaderRows` rows. | no (default = 0) |\n+\n+### TSV (Delimited)\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"tsv\",\n+    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n+    \"delimiter\":\"|\"\n+  },\n+  ...\n+}\n+```\n+\n+The `inputFormat` to load data of a delimited format. An example is:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `tsv`. | yes |\n+| delimiter | String | A custom delimiter for data values. | no (default == `\\t`) |\n+| listDelimiter | String | A custom delimiter for multi-value dimensions. | no (default == ctrl+A) |\n+| columns | JSON array | Specifies the columns of the data. The columns should be in the same order with the columns of your data. | yes if `findColumnsFromHeader` is false or missing |\n+| findColumnsFromHeader | Boolean | If this is set, the task will find the column names from the header row. Note that `skipHeaderRows` will be applied before finding column names from the header. For example, if you set `skipHeaderRows` to 2 and `findColumnsFromHeader` to true, the task will skip the first two lines and then extract column information from the third line. `columns` will be ignored if this is set to true. | no (default = false if `columns` is set; otherwise null) |\n+| skipHeaderRows | Integer | If this is set, the task will skip the first `skipHeaderRows` rows. | no (default = 0) |\n+\n+Be sure to change the `delimiter` to the appropriate delimiter for your data. Like CSV, you must specify the columns and which subset of the columns you want indexed.\n+\n+### ORC\n+\n+> You need to include the [`druid-orc-extensions`](../development/extensions-core/orc.md) as an extension to use the ORC input format.\n+\n+> If you are considering upgrading from earlier than 0.15.0 to 0.15.0 or a higher version,\n+> please read [Migration from 'contrib' extension](../development/extensions-core/orc.md#migration-from-contrib-extension) carefully.\n+\n+The `inputFormat` to load data of ORC format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"orc\",\n+    \"flattenSpec\": {\n+      \"useFieldDiscovery\": true,\n+      \"fields\": [\n+        {\n+          \"type\": \"path\",\n+          \"name\": \"nested\",\n+          \"expr\": \"$.path.to.nested\"\n+        }\n+      ]\n+    }\n+    \"binaryAsString\": false\n+  },\n+  ...\n+}\n+```\n+\n+The ORC `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+| type | String | This should say `json`. | yes |\n+| flattenSpec | JSON Object | Specifies flattening configuration for nested JSON data. See [`flattenSpec`](#flattenspec) for more info. | no |\n+| binaryAsString | Boolean | Specifies if the binary orc column which is not logically marked as a string should be treated as a UTF-8 encoded string. | no (default == false) |\n+\n+### Parquet\n+\n+> You need to include the [`druid-parquet-extensions`](../development/extensions-core/parquet.md) as an extension to use the Parquet input format.\n+\n+The `inputFormat` to load data of Parquet format. An example is:\n+\n+```json\n+\"ioConfig\": {\n+  \"inputFormat\": {\n+    \"type\": \"parquet\",\n+    \"flattenSpec\": {\n+      \"useFieldDiscovery\": true,\n+      \"fields\": [\n+        {\n+          \"type\": \"path\",\n+          \"name\": \"nested\",\n+          \"expr\": \"$.path.to.nested\"\n+        }\n+      ]\n+    }\n+    \"binaryAsString\": false\n+  },\n+  ...\n+}\n+```\n+\n+The Parquet `inputFormat` has the following components:\n+\n+| Field | Type | Description | Required |\n+|-------|------|-------------|----------|\n+|type| String| This should be set to `parquet` to read Parquet file| yes |\n+|flattenSpec| JSON Object |Define a [`flattenSpec`](#flattenspec) to extract nested values from a Parquet file. Note that only 'path' expression are supported ('jq' is unavailable).| no (default will auto-discover 'root' level properties) |\n+| binaryAsString | Boolean | Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. | no (default == false) |\n+\n+### FlattenSpec\n+\n+The `flattenSpec` is located in `inputFormat` \u2192 `flattenSpec` and is responsible for\n+bridging the gap between potentially nested input data (such as JSON, Avro, etc) and Druid's flat data model.\n+An example `flattenSpec` is:\n+\n+```json\n+\"flattenSpec\": {\n+  \"useFieldDiscovery\": true,\n+  \"fields\": [\n+    { \"name\": \"baz\", \"type\": \"root\" },\n+    { \"name\": \"foo_bar\", \"type\": \"path\", \"expr\": \"$.foo.bar\" },\n+    { \"name\": \"first_food\", \"type\": \"jq\", \"expr\": \".thing.food[1]\" }\n+  ]\n+}\n+```\n+> Conceptually, after input data records are read, the `flattenSpec` is applied first before\n+> any other specs such as [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec),\n+> [`dimensionsSpec`](./index.md#dimensionsspec), or [`metricsSpec`](./index.md#metricsspec). Keep this in mind when writing\n+> your ingestion spec.\n+\n+Flattening is only supported for [data formats](data-formats.md) that support nesting, including `avro`, `json`, `orc`,\n+and `parquet`.\n+\n+A `flattenSpec` can have the following components:\n+\n+| Field | Description | Default |\n+|-------|-------------|---------|\n+| useFieldDiscovery | If true, interpret all root-level fields as available fields for usage by [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec), [`dimensionsSpec`](./index.md#dimensionsspec), and [`metricsSpec`](./index.md#metricsspec).<br><br>If false, only explicitly specified fields (see `fields`) will be available for use. | `true` |\n+| fields | Specifies the fields of interest and how they are accessed. [See below for details.](#field-flattening-specifications) | `[]` |\n+\n+#### Field flattening specifications\n+\n+Each entry in the `fields` list can have the following components:\n+\n+| Field | Description | Default |\n+|-------|-------------|---------|\n+| type | Options are as follows:<br><br><ul><li>`root`, referring to a field at the root level of the record. Only really useful if `useFieldDiscovery` is false.</li><li>`path`, referring to a field using [JsonPath](https://github.com/jayway/JsonPath) notation. Supported by most data formats that offer nesting, including `avro`, `json`, `orc`, and `parquet`.</li><li>`jq`, referring to a field using [jackson-jq](https://github.com/eiiches/jackson-jq) notation. Only supported for the `json` format.</li></ul> | none (required) |\n+| name | Name of the field after flattening. This name can be referred to by the [`timestampSpec`](./index.md#timestampspec), [`transformSpec`](./index.md#transformspec), [`dimensionsSpec`](./index.md#dimensionsspec), and [`metricsSpec`](./index.md#metricsspec).| none (required) |\n+| expr | Expression for accessing the field while flattening. For type `path`, this should be [JsonPath](https://github.com/jayway/JsonPath). For type `jq`, this should be [jackson-jq](https://github.com/eiiches/jackson-jq) notation. For other types, this parameter is ignored. | none (required for types `path` and `jq`) |\n+\n+#### Notes on flattening\n+\n+* For convenience, when defining a root-level field, it is possible to define only the field name, as a string, instead of a JSON object. For example, `{\"name\": \"baz\", \"type\": \"root\"}` is equivalent to `\"baz\"`.\n+* Enabling `useFieldDiscovery` will only automatically detect \"simple\" fields at the root level that correspond to data types that Druid supports. This includes strings, numbers, and lists of strings or numbers. Other types will not be automatically detected, and must be specified explicitly in the `fields` list.\n+* Duplicate field `name`s are not allowed. An exception will be thrown.\n+* If `useFieldDiscovery` is enabled, any discovered field with the same name as one already defined in the `fields` list will be skipped, rather than added twice.\n+* [http://jsonpath.herokuapp.com/](http://jsonpath.herokuapp.com/) is useful for testing `path`-type expressions.\n+* jackson-jq supports a subset of the full [jq](https://stedolan.github.io/jq/) syntax.  Please refer to the [jackson-jq documentation](https://github.com/eiiches/jackson-jq) for details.\n+\n+## Parser (Deprecated)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU3NTcwMg=="}, "originalCommit": {"oid": "2391262e708f38876efc65c4786b67d617506217"}, "originalPosition": 225}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTkyNjMwOnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMjo1Nzo0OVrOFeqMkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0ODoxMVrOFerFPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5Mjk0NA==", "bodyText": "Is there authentication configuration needed for accessing GCS? Could add that in a follow-on PR if so.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367692944", "createdAt": "2020-01-16T22:57:49Z", "author": {"login": "jon-wei"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,110 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNzQ1NQ==", "bodyText": "I added google.cloud.auth.service.account.enable property. Haven't checked how it works, but just copied from https://github.com/GoogleCloudDataproc/bigdata-interop/blob/master/gcs/INSTALL.md.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367707455", "createdAt": "2020-01-16T23:48:11Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,110 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5Mjk0NA=="}, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTk1ODQzOnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzoxNTowMVrOFeqgRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0ODowOVrOFerFNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5Nzk4OA==", "bodyText": "Google cloud Storage -> Google Cloud Storage", "url": "https://github.com/apache/druid/pull/9171#discussion_r367697988", "createdAt": "2020-01-16T23:15:01Z", "author": {"login": "jon-wei"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,110 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNzQ0Ng==", "bodyText": "Thanks, fixed.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367707446", "createdAt": "2020-01-16T23:48:09Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -36,49 +36,110 @@ To use this Apache Druid extension, make sure to [include](../../development/ext\n |`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n |`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n \n-If you are using the Hadoop indexer, set your output directory to be a location on Hadoop and it will work.\n+Besides the above settings, you also need to include all Hadoop configuration files (such as `core-site.xml`, `hdfs-site.xml`)\n+in the Druid classpath. One way to do this is copying all those files under `${DRUID_HOME}/conf/_common`.\n+\n+If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\n If you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set `druid.hadoop.security.kerberos.principal` and `druid.hadoop.security.kerberos.keytab`, this is an alternative to the cron job method that runs `kinit` command periodically.\n \n-### Configuration for Google Cloud Storage\n+### Configuration for Cloud Storage\n+\n+You can also use the AWS S3 or the Google Cloud Storage as the deep storage via HDFS.\n+\n+#### Configuration for AWS S3\n \n-The HDFS extension can also be used for GCS as deep storage.\n+To use the AWS S3 as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n \n |Property|Possible Values|Description|Default|\n |--------|---------------|-----------|-------|\n-|`druid.storage.type`|hdfs||Must be set.|\n-|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n+|`druid.storage.type`|hdfs| |Must be set.|\n+|`druid.storage.storageDirectory`|s3a://bucket/example/directory or s3n://bucket/example/directory|Path to the deep storage|Must be set.|\n \n-All services that need to access GCS need to have the [GCS connector jar](https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation) in their class path. One option is to place this jar in <druid>/lib/ and <druid>/extensions/druid-hdfs-storage/\n+You also need to include the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html), especially the `hadoop-aws.jar` in the Druid classpath.\n+Run the below command to install the `hadoop-aws.jar` file under `${DRUID_HOME}/extensions/druid-hdfs-storage` in all nodes.\n \n-Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\n-\n-<a name=\"firehose\"></a>\n+```bash\n+java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\";\n+cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n+```\n \n-## Native batch ingestion\n+Finally, you need to add the below properties in the `core-site.xml`.\n+For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+\n+```xml\n+<property>\n+  <name>fs.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n+  <description>The implementation class of the S3A Filesystem</description>\n+</property>\n+\n+<property>\n+  <name>fs.AbstractFileSystem.s3a.impl</name>\n+  <value>org.apache.hadoop.fs.s3a.S3A</value>\n+  <description>The implementation class of the S3A AbstractFileSystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.access.key</name>\n+  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your access key</value>\n+</property>\n+\n+<property>\n+  <name>fs.s3a.secret.key</name>\n+  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n+  <value>your secret key</value>\n+</property>\n+```\n \n-This firehose ingests events from a predefined list of files from a Hadoop filesystem.\n-This firehose is _splittable_ and can be used by [native parallel index tasks](../../ingestion/native-batch.md#parallel-task).\n-Since each split represents an HDFS file, each worker task of `index_parallel` will read an object.\n+#### Configuration for Google Cloud Storage\n \n-Sample spec:\n+To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5Nzk4OA=="}, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTk3MzU0OnYy", "diffSide": "RIGHT", "path": "docs/development/modules.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzoyMjowNFrOFeqpOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0Nzo1OFrOFerFDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMDI4Mg==", "bodyText": "suggest putting backticks around @JacksonInject", "url": "https://github.com/apache/druid/pull/9171#discussion_r367700282", "createdAt": "2020-01-16T23:22:04Z", "author": {"login": "jon-wei"}, "path": "docs/development/modules.md", "diffHunk": "@@ -148,29 +150,43 @@ To start a segment killing task, you need to access the old Coordinator console\n \n After the killing task ends, `index.zip` (`partitionNum_index.zip` for HDFS data storage) file should be deleted from the data storage.\n \n-### Adding a new Firehose\n+### Adding support for a new input source\n \n-There is an example of this in the `s3-extensions` module with the StaticS3FirehoseFactory.\n+Adding support for a new input source requires to implement three interfaces, i.e., `InputSource`, `InputEntity`, and `InputSourceReader`.\n+`InputSource` is to define where the input data is stored. `InputEntity` is to define how data can be read in parallel\n+in [native parallel indexing](../ingestion/native-batch.md).\n+`InputSourceReader` defines how to read your new input source and you can simply use the provided `InputEntityIteratingReader` in most cases.\n \n-Adding a Firehose is done almost entirely through the Jackson Modules instead of Guice.  Specifically, note the implementation\n+There is an example of this in the `druid-s3-extensions` module with the `S3InputSource` and `S3Entity`.\n+\n+Adding an InputSource is done almost entirely through the Jackson Modules instead of Guice. Specifically, note the implementation\n \n ``` java\n @Override\n public List<? extends Module> getJacksonModules()\n {\n   return ImmutableList.of(\n-          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n+          new SimpleModule().registerSubtypes(new NamedType(S3InputSource.class, \"s3\"))\n   );\n }\n ```\n \n-This is registering the FirehoseFactory with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"firehose\": { \"type\": \"static-s3\", ... }` in your realtime config, then the system will load this FirehoseFactory for your firehose.\n+This is registering the InputSource with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"inputSource\": { \"type\": \"s3\", ... }` in your IO config, then the system will load this InputSource for your `InputSource` implementation.\n+\n+Note that inside of Druid, we have made the @JacksonInject annotation for Jackson deserialized objects actually use the base Guice injector to resolve the object to be injected.  So, if your InputSource needs access to some object, you can add a @JacksonInject annotation on a setter and it will get set on instantiation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNzQwNA==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367707404", "createdAt": "2020-01-16T23:47:58Z", "author": {"login": "jihoonson"}, "path": "docs/development/modules.md", "diffHunk": "@@ -148,29 +150,43 @@ To start a segment killing task, you need to access the old Coordinator console\n \n After the killing task ends, `index.zip` (`partitionNum_index.zip` for HDFS data storage) file should be deleted from the data storage.\n \n-### Adding a new Firehose\n+### Adding support for a new input source\n \n-There is an example of this in the `s3-extensions` module with the StaticS3FirehoseFactory.\n+Adding support for a new input source requires to implement three interfaces, i.e., `InputSource`, `InputEntity`, and `InputSourceReader`.\n+`InputSource` is to define where the input data is stored. `InputEntity` is to define how data can be read in parallel\n+in [native parallel indexing](../ingestion/native-batch.md).\n+`InputSourceReader` defines how to read your new input source and you can simply use the provided `InputEntityIteratingReader` in most cases.\n \n-Adding a Firehose is done almost entirely through the Jackson Modules instead of Guice.  Specifically, note the implementation\n+There is an example of this in the `druid-s3-extensions` module with the `S3InputSource` and `S3Entity`.\n+\n+Adding an InputSource is done almost entirely through the Jackson Modules instead of Guice. Specifically, note the implementation\n \n ``` java\n @Override\n public List<? extends Module> getJacksonModules()\n {\n   return ImmutableList.of(\n-          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n+          new SimpleModule().registerSubtypes(new NamedType(S3InputSource.class, \"s3\"))\n   );\n }\n ```\n \n-This is registering the FirehoseFactory with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"firehose\": { \"type\": \"static-s3\", ... }` in your realtime config, then the system will load this FirehoseFactory for your firehose.\n+This is registering the InputSource with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"inputSource\": { \"type\": \"s3\", ... }` in your IO config, then the system will load this InputSource for your `InputSource` implementation.\n+\n+Note that inside of Druid, we have made the @JacksonInject annotation for Jackson deserialized objects actually use the base Guice injector to resolve the object to be injected.  So, if your InputSource needs access to some object, you can add a @JacksonInject annotation on a setter and it will get set on instantiation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMDI4Mg=="}, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTk3NDQ4OnYy", "diffSide": "RIGHT", "path": "docs/development/modules.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzoyMjozNlrOFeqp1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0Nzo1NlrOFerFBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMDQzOA==", "bodyText": "Suggest the following\n\"requires to implement two interfaces, i.e.,\" -> \"requires implementing two interfaces: \"", "url": "https://github.com/apache/druid/pull/9171#discussion_r367700438", "createdAt": "2020-01-16T23:22:36Z", "author": {"login": "jon-wei"}, "path": "docs/development/modules.md", "diffHunk": "@@ -148,29 +150,43 @@ To start a segment killing task, you need to access the old Coordinator console\n \n After the killing task ends, `index.zip` (`partitionNum_index.zip` for HDFS data storage) file should be deleted from the data storage.\n \n-### Adding a new Firehose\n+### Adding support for a new input source\n \n-There is an example of this in the `s3-extensions` module with the StaticS3FirehoseFactory.\n+Adding support for a new input source requires to implement three interfaces, i.e., `InputSource`, `InputEntity`, and `InputSourceReader`.\n+`InputSource` is to define where the input data is stored. `InputEntity` is to define how data can be read in parallel\n+in [native parallel indexing](../ingestion/native-batch.md).\n+`InputSourceReader` defines how to read your new input source and you can simply use the provided `InputEntityIteratingReader` in most cases.\n \n-Adding a Firehose is done almost entirely through the Jackson Modules instead of Guice.  Specifically, note the implementation\n+There is an example of this in the `druid-s3-extensions` module with the `S3InputSource` and `S3Entity`.\n+\n+Adding an InputSource is done almost entirely through the Jackson Modules instead of Guice. Specifically, note the implementation\n \n ``` java\n @Override\n public List<? extends Module> getJacksonModules()\n {\n   return ImmutableList.of(\n-          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n+          new SimpleModule().registerSubtypes(new NamedType(S3InputSource.class, \"s3\"))\n   );\n }\n ```\n \n-This is registering the FirehoseFactory with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"firehose\": { \"type\": \"static-s3\", ... }` in your realtime config, then the system will load this FirehoseFactory for your firehose.\n+This is registering the InputSource with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"inputSource\": { \"type\": \"s3\", ... }` in your IO config, then the system will load this InputSource for your `InputSource` implementation.\n+\n+Note that inside of Druid, we have made the @JacksonInject annotation for Jackson deserialized objects actually use the base Guice injector to resolve the object to be injected.  So, if your InputSource needs access to some object, you can add a @JacksonInject annotation on a setter and it will get set on instantiation.\n+\n+### Adding support for a new data format\n+\n+Adding support for a new data format requires to implement two interfaces, i.e., `InputFormat` and `InputEntityReader`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNzM5OA==", "bodyText": "Fixed, thanks.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367707398", "createdAt": "2020-01-16T23:47:56Z", "author": {"login": "jihoonson"}, "path": "docs/development/modules.md", "diffHunk": "@@ -148,29 +150,43 @@ To start a segment killing task, you need to access the old Coordinator console\n \n After the killing task ends, `index.zip` (`partitionNum_index.zip` for HDFS data storage) file should be deleted from the data storage.\n \n-### Adding a new Firehose\n+### Adding support for a new input source\n \n-There is an example of this in the `s3-extensions` module with the StaticS3FirehoseFactory.\n+Adding support for a new input source requires to implement three interfaces, i.e., `InputSource`, `InputEntity`, and `InputSourceReader`.\n+`InputSource` is to define where the input data is stored. `InputEntity` is to define how data can be read in parallel\n+in [native parallel indexing](../ingestion/native-batch.md).\n+`InputSourceReader` defines how to read your new input source and you can simply use the provided `InputEntityIteratingReader` in most cases.\n \n-Adding a Firehose is done almost entirely through the Jackson Modules instead of Guice.  Specifically, note the implementation\n+There is an example of this in the `druid-s3-extensions` module with the `S3InputSource` and `S3Entity`.\n+\n+Adding an InputSource is done almost entirely through the Jackson Modules instead of Guice. Specifically, note the implementation\n \n ``` java\n @Override\n public List<? extends Module> getJacksonModules()\n {\n   return ImmutableList.of(\n-          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n+          new SimpleModule().registerSubtypes(new NamedType(S3InputSource.class, \"s3\"))\n   );\n }\n ```\n \n-This is registering the FirehoseFactory with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"firehose\": { \"type\": \"static-s3\", ... }` in your realtime config, then the system will load this FirehoseFactory for your firehose.\n+This is registering the InputSource with Jackson's polymorphic serialization/deserialization layer.  More concretely, having this will mean that if you specify a `\"inputSource\": { \"type\": \"s3\", ... }` in your IO config, then the system will load this InputSource for your `InputSource` implementation.\n+\n+Note that inside of Druid, we have made the @JacksonInject annotation for Jackson deserialized objects actually use the base Guice injector to resolve the object to be injected.  So, if your InputSource needs access to some object, you can add a @JacksonInject annotation on a setter and it will get set on instantiation.\n+\n+### Adding support for a new data format\n+\n+Adding support for a new data format requires to implement two interfaces, i.e., `InputFormat` and `InputEntityReader`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMDQzOA=="}, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTk4NTc5OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/hadoop.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzoyODoyOFrOFeqwqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0Nzo1MVrOFerE4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMjE4Ng==", "bodyText": "Noting here that ${DRUID_HOME}/hadoop-dependencies doesn't work for this since the HDFS extension needs these libraries on the peon startup", "url": "https://github.com/apache/druid/pull/9171#discussion_r367702186", "createdAt": "2020-01-16T23:28:28Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/hadoop.md", "diffHunk": "@@ -145,7 +145,51 @@ A type of inputSpec where a static path to the data files is provided.\n For example, using the static input paths:\n \n ```\n-\"paths\" : \"s3n://billy-bucket/the/data/is/here/data.gz,s3n://billy-bucket/the/data/is/here/moredata.gz,s3n://billy-bucket/the/data/is/here/evenmoredata.gz\"\n+\"paths\" : \"hdfs://path/to/data/is/here/data.gz,hdfs://path/to/data/is/here/moredata.gz,hdfs://path/to/data/is/here/evenmoredata.gz\"\n+```\n+\n+You can also read from cloud storage such as AWS S3 or Google Cloud Storage.\n+To do so, you need to install the necessary library under `${DRUID_HOME}/hadoop-dependencies` in _all MiddleManager or Indexer processes_.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNzM2Mg==", "bodyText": "Hmm, yeah. Good point. Updated docs.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367707362", "createdAt": "2020-01-16T23:47:51Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/hadoop.md", "diffHunk": "@@ -145,7 +145,51 @@ A type of inputSpec where a static path to the data files is provided.\n For example, using the static input paths:\n \n ```\n-\"paths\" : \"s3n://billy-bucket/the/data/is/here/data.gz,s3n://billy-bucket/the/data/is/here/moredata.gz,s3n://billy-bucket/the/data/is/here/evenmoredata.gz\"\n+\"paths\" : \"hdfs://path/to/data/is/here/data.gz,hdfs://path/to/data/is/here/moredata.gz,hdfs://path/to/data/is/here/evenmoredata.gz\"\n+```\n+\n+You can also read from cloud storage such as AWS S3 or Google Cloud Storage.\n+To do so, you need to install the necessary library under `${DRUID_HOME}/hadoop-dependencies` in _all MiddleManager or Indexer processes_.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwMjE4Ng=="}, "originalCommit": {"oid": "5ee83d49f893c13d7fadb7baa6dca61906a4ac02"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MjEwMTY3OnYy", "diffSide": "RIGHT", "path": "docs/development/extensions-core/hdfs.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0MTowNVrOFer3dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNToyOTozN1rOFevNPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMDMwOQ==", "bodyText": "For the installation section below, I think we could point to https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md and say the following, and remove the parts where we duplicate their setup instructions:\n\nPlease follow the instructions at https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md for configuring your core-site.xml with the filesystem and authentication properties needed for GCS.\"\n\nWe can also add the following (it took me a while to find a download link for the connector):\n\nThe GCS connector library is available at https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters\n\nThe line below:\n\"Tested with Druid 0.9.0, Hadoop 2.7.2 and gcs-connector jar 1.4.4-hadoop2.\"\ncan be updated to\n\"Tested with Druid 0.17.0, Hadoop 2.8.5 and gcs-connector jar 2.0.0-hadoop2.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367720309", "createdAt": "2020-01-17T00:41:05Z", "author": {"login": "jon-wei"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -94,7 +94,7 @@ For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/d\n \n #### Configuration for Google Cloud Storage\n \n-To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n+To use the Google Cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59e2fef17e2e236834db402c3b7134298ad8d349"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzc3NTAzNw==", "bodyText": "Thanks, I made changed based on the suggestions. But I would still want to keep the example properties for GCS, since they are pretty mandatory. The similar pattern is applied to S3 configuration.", "url": "https://github.com/apache/druid/pull/9171#discussion_r367775037", "createdAt": "2020-01-17T05:29:37Z", "author": {"login": "jihoonson"}, "path": "docs/development/extensions-core/hdfs.md", "diffHunk": "@@ -94,7 +94,7 @@ For more configurations, see the [Hadoop AWS module](https://hadoop.apache.org/d\n \n #### Configuration for Google Cloud Storage\n \n-To use the Google cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.\n+To use the Google Cloud Storage as the deep storage, you need to configure `druid.storage.storageDirectory` properly.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMDMwOQ=="}, "originalCommit": {"oid": "59e2fef17e2e236834db402c3b7134298ad8d349"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2118, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}