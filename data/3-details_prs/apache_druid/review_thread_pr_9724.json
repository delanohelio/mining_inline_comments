{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NjAxNTE2", "number": 9724, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOToxNVrOD0CENA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo1MDo1OVrOD0kniw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODg2Mzg4OnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOToxNVrOGIxjkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNjozNTo0OFrOGI08IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MzcxMg==", "bodyText": "dependecy -> dependency", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853712", "createdAt": "2020-04-21T04:09:15Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkwOTE1Mg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r411909152", "createdAt": "2020-04-21T06:35:48Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MzcxMg=="}, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODg2NDE1OnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOToyNlrOGIxjuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNjozNjowN1rOGI088w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1Mzc1Mg==", "bodyText": "straving -> starving", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853752", "createdAt": "2020-04-21T04:09:26Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,\n+test does not use excessive memory straving other concurent task, test does not modify and/or use other task, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkwOTM2Mw==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r411909363", "createdAt": "2020-04-21T06:36:07Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" \n+test tag section in integration-tests/src/test/resources/testng.xml  \n+Please be mindful when adding tests to the \"AllParallelizedTests\" test tag that the tests can run in parallel with\n+other tests at the same time. i.e. test does not modify/restart/stop the druid cluster or other dependecy containers,\n+test does not use excessive memory straving other concurent task, test does not modify and/or use other task, ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1Mzc1Mg=="}, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODg2NDgwOnYy", "diffSide": "RIGHT", "path": "integration-tests/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOTo0MlrOGIxkBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDowOTo0MlrOGIxkBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MzgzMQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/9724#discussion_r411853831", "createdAt": "2020-04-21T04:09:42Z", "author": {"login": "jon-wei"}, "path": "integration-tests/pom.xml", "diffHunk": "@@ -385,6 +367,7 @@\n                                 <configuration>\n                                     <environmentVariables>\n                                     <DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>${start.hadoop.docker}</DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>\n+                                    <DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>${skip.start.docker}</DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODg3ODk0OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KinesisAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDoxNjoxNFrOGIxruA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNjozODo0MlrOGI1CIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NTgwMA==", "bodyText": "Suggest adding a brief comment here noting that Kinesis doesn't immediately drop the old shards after the resharding which is why the counts are summed", "url": "https://github.com/apache/druid/pull/9724#discussion_r411855800", "createdAt": "2020-04-21T04:16:14Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KinesisAdminClient.java", "diffHunk": "@@ -129,18 +132,28 @@ public void updateShardCount(String streamName, int newShardCount, boolean block\n     }\n   }\n \n+  @Override\n   public boolean isStreamActive(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return verifyStreamStatus(streamDescription, StreamStatus.ACTIVE);\n   }\n \n+  @Override\n   public int getStreamShardCount(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return getStreamShardCount(streamDescription);\n   }\n \n+  @Override\n+  public boolean verfiyShardCountUpdated(String streamName, int oldShardCount, int newShardCount)\n+  {\n+    int actualShardCount = getStreamShardCount(streamName);\n+    return actualShardCount == oldShardCount + newShardCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkxMDY5MQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r411910691", "createdAt": "2020-04-21T06:38:42Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KinesisAdminClient.java", "diffHunk": "@@ -129,18 +132,28 @@ public void updateShardCount(String streamName, int newShardCount, boolean block\n     }\n   }\n \n+  @Override\n   public boolean isStreamActive(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return verifyStreamStatus(streamDescription, StreamStatus.ACTIVE);\n   }\n \n+  @Override\n   public int getStreamShardCount(String streamName)\n   {\n     StreamDescription streamDescription = getStreamDescription(streamName);\n     return getStreamShardCount(streamDescription);\n   }\n \n+  @Override\n+  public boolean verfiyShardCountUpdated(String streamName, int oldShardCount, int newShardCount)\n+  {\n+    int actualShardCount = getStreamShardCount(streamName);\n+    return actualShardCount == oldShardCount + newShardCount;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NTgwMA=="}, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODg5MjQ1OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNDoyMjoyMlrOGIxyuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNjo0MTowMlrOGI1GuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NzU5NQ==", "bodyText": "Hmm, it's a bit strange that the stream indexing test extends something called \"batch index test\", maybe the base \"batch index\" class is generic enough that it should be renamed to something else, or some other refactoring", "url": "https://github.com/apache/druid/pull/9724#discussion_r411857595", "createdAt": "2020-04-21T04:22:22Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkxMTg2NQ==", "bodyText": "My bad. Can extend from AbstractIndexerTest instead.", "url": "https://github.com/apache/druid/pull/9724#discussion_r411911865", "createdAt": "2020-04-21T06:41:02Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1NzU5NQ=="}, "originalCommit": {"oid": "fcbfcc2403ab2c8bd90fbfef9a42bc5f4812e473"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDI3OTkzOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoxMDozOFrOGJidCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjoyNDo0N1rOGJlYpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NDg1Nw==", "bodyText": "Can you add javadoc for this class?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412654857", "createdAt": "2020-04-22T04:10:38Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();\n \n-  void flush();\n+  void flush() throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcwMjg4NA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412702884", "createdAt": "2020-04-22T06:24:47Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();\n \n-  void flush();\n+  void flush() throws Exception;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NDg1Nw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDI4NTc2OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoxMjo1MlrOGJigAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjoyNTo0M1rOGJlaIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NTYxNw==", "bodyText": "This method is not being called, but seems like it should be called in AbstractStreamIndexingTest.doMethodTeardown().", "url": "https://github.com/apache/druid/pull/9724#discussion_r412655617", "createdAt": "2020-04-22T04:12:52Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcwMzI2Ng==", "bodyText": "Yep. Done.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412703266", "createdAt": "2020-04-22T06:25:43Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamEventWriter.java", "diffHunk": "@@ -25,5 +25,11 @@\n \n   void shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NTYxNw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDI5MTU2OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoxNToyMFrOGJijAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjoyNjo0N1rOGJlcPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjM4NA==", "bodyText": "nit: probably better to name createStreamAdminClient() since it creates an instance rather than returning an existing one.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412656384", "createdAt": "2020-04-22T04:15:20Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcwMzgwNA==", "bodyText": "Done.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412703804", "createdAt": "2020-04-22T06:26:47Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjM4NA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDI5MzAwOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoxNTo1NVrOGJijwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjoyODoxM1rOGJlevg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjU3OA==", "bodyText": "nit: can be private.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412656578", "createdAt": "2020-04-22T04:15:55Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcwNDQ0Ng==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412704446", "createdAt": "2020-04-22T06:28:13Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1NjU3OA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDMwODI1OnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoyMjowOVrOGJir5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjozMzoxMlrOGJlnsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1ODY2MA==", "bodyText": "Does AllParallelizedTests parallelize tests in a single class? Or can it parallelize across classes? Probably worth mentioning here.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412658660", "createdAt": "2020-04-22T04:22:09Z", "author": {"login": "jihoonson"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcwNjczNw==", "bodyText": "Only \"methods inside the test cases are executed in parallel\". Updated.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412706737", "createdAt": "2020-04-22T06:33:12Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -303,3 +314,13 @@ This will tell the test framework that the test class needs to be constructed us\n 2) FromFileTestQueryHelper - reads queries with expected results from file and executes them and verifies the results using ResultVerifier\n \n Refer ITIndexerTest as an example on how to use dependency Injection\n+\n+### Running test methods in parallel\n+By default, test methods in a test class will be run in sequential order one at a time. Test methods for a given test \n+class can be set to run in parallel (multiple test methods of the given class running at the same time) by excluding\n+the given class/package from the \"AllSerializedTests\" test tag section and including it in the \"AllParallelizedTests\" ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY1ODY2MA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDMyMTc1OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDoyNzo0OVrOGJiy_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo0MTozNVrOGJl43A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MDQ3Ng==", "bodyText": "Can you add javadoc? It would be nice to explain where this class is used, how it is used, what is the contracts for implementations and so on.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412660476", "createdAt": "2020-04-22T04:27:49Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxMTEzMg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412711132", "createdAt": "2020-04-22T06:41:35Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MDQ3Ng=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDMyNjgyOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDozMDowOFrOGJi1wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo0NzoxNFrOGJmEcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTE4NA==", "bodyText": "SeekableStream series use the word \"partition\" instead of \"shard\". It would be nice to use a consistent name.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412661184", "createdAt": "2020-04-22T04:30:08Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient\n+{\n+  void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception;\n+\n+  void deleteStream(String streamName) throws Exception;\n+\n+  void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxMzAzNA==", "bodyText": "I am really sad Kinesis and Kafka don't just call everything the same. Which then extends to our Kafka and Kinesis supervisor spec like useEarliestSequenceNumber vs useEarliestOffset. I spent an hour setting the wrong one and wondering why it wasn't working.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412713034", "createdAt": "2020-04-22T06:45:20Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient\n+{\n+  void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception;\n+\n+  void deleteStream(String streamName) throws Exception;\n+\n+  void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTE4NA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxNDA5OQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412714099", "createdAt": "2020-04-22T06:47:14Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/StreamAdminClient.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import java.util.Map;\n+\n+public interface StreamAdminClient\n+{\n+  void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception;\n+\n+  void deleteStream(String streamName) throws Exception;\n+\n+  void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTE4NA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDMzMTA1OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDozMTo0M1rOGJi36g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo0OToxNVrOGJmIWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTczOA==", "bodyText": "nit: can be private final.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412661738", "createdAt": "2020-04-22T04:31:43Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxNTA5OA==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412715098", "createdAt": "2020-04-22T06:49:15Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MTczOA=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDMzNzQzOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDozNDo0MlrOGJi7dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo1MDoxNVrOGJmKSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MjY0Ng==", "bodyText": "nit: Intellij recommends using setProperty() instead of put(). I guess it's because the parameter type is more strict.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412662646", "createdAt": "2020-04-22T04:34:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxNTU5Mg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412715592", "createdAt": "2020-04-22T06:50:15Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2MjY0Ng=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM0OTEwOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDozOTo0MVrOGJjBuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo1MToyMVrOGJmM5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDI1MQ==", "bodyText": "What does \"active stream\" mean for Kafka?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412664251", "createdAt": "2020-04-22T04:39:41Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);\n+    adminClient = AdminClient.create(config);\n+  }\n+\n+  @Override\n+  public void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception\n+  {\n+    final short replicationFactor = 1;\n+    final NewTopic newTopic = new NewTopic(streamName, partitionCount, replicationFactor);\n+    final CreateTopicsResult createTopicsResult = adminClient.createTopics(Collections.singleton(newTopic));\n+    // Wait for create topic to compelte\n+    createTopicsResult.values().get(streamName).get();\n+  }\n+\n+  @Override\n+  public void deleteStream(String streamName) throws Exception\n+  {\n+    DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(ImmutableList.of(streamName));\n+    deleteTopicsResult.values().get(streamName).get();\n+  }\n+\n+  /**\n+   * This method can only increase the partition count of {@param streamName} to have a final partition\n+   * count of {@param newPartitionCount}\n+   * If {@param blocksUntilStarted} is set to true, then this method will blocks until the partitioning\n+   * started (but not nessesary finished), otherwise, the method will returns right after issue the reshard command\n+   */\n+  @Override\n+  public void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception\n+  {\n+    Map<String, NewPartitions> counts = new HashMap<>();\n+    counts.put(streamName, NewPartitions.increaseTo(newPartitionCount));\n+    CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(counts);\n+    if (blocksUntilStarted) {\n+      createPartitionsResult.values().get(streamName).get();\n+\n+    }\n+  }\n+\n+  @Override\n+  public boolean isStreamActive(String streamName)\n+  {\n+    return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxNjI2MQ==", "bodyText": "It doesn't means anything but since it implement the interface it needs the method. Added comment.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412716261", "createdAt": "2020-04-22T06:51:21Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaAdminClient.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import com.google.common.collect.ImmutableList;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.CreatePartitionsResult;\n+import org.apache.kafka.clients.admin.CreateTopicsResult;\n+import org.apache.kafka.clients.admin.DeleteTopicsResult;\n+import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.NewPartitions;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaAdminClient implements StreamAdminClient\n+{\n+  AdminClient adminClient;\n+\n+  public KafkaAdminClient(String kafkaInternalHost)\n+  {\n+    Properties config = new Properties();\n+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaInternalHost);\n+    adminClient = AdminClient.create(config);\n+  }\n+\n+  @Override\n+  public void createStream(String streamName, int partitionCount, Map<String, String> tags) throws Exception\n+  {\n+    final short replicationFactor = 1;\n+    final NewTopic newTopic = new NewTopic(streamName, partitionCount, replicationFactor);\n+    final CreateTopicsResult createTopicsResult = adminClient.createTopics(Collections.singleton(newTopic));\n+    // Wait for create topic to compelte\n+    createTopicsResult.values().get(streamName).get();\n+  }\n+\n+  @Override\n+  public void deleteStream(String streamName) throws Exception\n+  {\n+    DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(ImmutableList.of(streamName));\n+    deleteTopicsResult.values().get(streamName).get();\n+  }\n+\n+  /**\n+   * This method can only increase the partition count of {@param streamName} to have a final partition\n+   * count of {@param newPartitionCount}\n+   * If {@param blocksUntilStarted} is set to true, then this method will blocks until the partitioning\n+   * started (but not nessesary finished), otherwise, the method will returns right after issue the reshard command\n+   */\n+  @Override\n+  public void updateShardCount(String streamName, int newPartitionCount, boolean blocksUntilStarted) throws Exception\n+  {\n+    Map<String, NewPartitions> counts = new HashMap<>();\n+    counts.put(streamName, NewPartitions.increaseTo(newPartitionCount));\n+    CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(counts);\n+    if (blocksUntilStarted) {\n+      createPartitionsResult.values().get(streamName).get();\n+\n+    }\n+  }\n+\n+  @Override\n+  public boolean isStreamActive(String streamName)\n+  {\n+    return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDI1MQ=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM1MzAxOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaEventWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo0MToyMVrOGJjD0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNjo1NjoxNFrOGJmXjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDc4NQ==", "bodyText": "These 3 private variables can be final.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412664785", "createdAt": "2020-04-22T04:41:21Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaEventWriter.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import org.apache.druid.indexer.TaskIdUtils;\n+import org.apache.druid.testing.IntegrationTestingConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.Future;\n+\n+public class KafkaEventWriter implements StreamEventWriter\n+{\n+  private static final String TEST_PROPERTY_PREFIX = \"kafka.test.property.\";\n+  private KafkaProducer<String, String> producer;\n+  private boolean txnEnabled;\n+  private List<Future<RecordMetadata>> pendingWriteRecords = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcxODk4OQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412718989", "createdAt": "2020-04-22T06:56:14Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/KafkaEventWriter.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import org.apache.druid.indexer.TaskIdUtils;\n+import org.apache.druid.testing.IntegrationTestingConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.Future;\n+\n+public class KafkaEventWriter implements StreamEventWriter\n+{\n+  private static final String TEST_PROPERTY_PREFIX = \"kafka.test.property.\";\n+  private KafkaProducer<String, String> producer;\n+  private boolean txnEnabled;\n+  private List<Future<RecordMetadata>> pendingWriteRecords = new ArrayList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NDc4NQ=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM4NDQ0OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo1NToxMVrOGJjUrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxNzoxNTo1M1rOGKBtVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTEwMw==", "bodyText": "nit: it would be better to check the compaction config is set by calling the compaction config API.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669103", "createdAt": "2020-04-22T04:55:11Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -251,6 +251,9 @@ private void submitCompactionConfig(Integer maxRowsPerSegment, Period skipOffset\n                                                                                  null);\n     compactionResource.submitCompactionConfig(compactionConfig);\n \n+    // Wait for compaction config to persist\n+    Thread.sleep(2000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyMDk5NA==", "bodyText": "I actually do make calls to config/compaction and config/compaction/{datasource} to verify that the compaction is updated. But I did see that the old config was apply to the compaction when the test runs in the following sequence... post config 1 datasource A-> verify config 1 from the two get calls -> post config 2 datasource A-> verify config 2 from the two get calls -> force compaction datasource A\nWill look at this more closely in a separate PR. Lets keep this hear to reduce intermittent failure but I'll revisit later.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412720994", "createdAt": "2020-04-22T06:59:42Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -251,6 +251,9 @@ private void submitCompactionConfig(Integer maxRowsPerSegment, Period skipOffset\n                                                                                  null);\n     compactionResource.submitCompactionConfig(compactionConfig);\n \n+    // Wait for compaction config to persist\n+    Thread.sleep(2000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTEwMw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzE2NjkzNQ==", "bodyText": "Sounds good.", "url": "https://github.com/apache/druid/pull/9724#discussion_r413166935", "createdAt": "2020-04-22T17:15:53Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -251,6 +251,9 @@ private void submitCompactionConfig(Integer maxRowsPerSegment, Period skipOffset\n                                                                                  null);\n     compactionResource.submitCompactionConfig(compactionConfig);\n \n+    // Wait for compaction config to persist\n+    Thread.sleep(2000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTEwMw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM4NzMyOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractIndexerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo1NjoyMFrOGJjWKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzowOTozM1rOGJm1Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTQ4Mw==", "bodyText": "Why not injecting it in Kafka/Kinesis tests?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669483", "createdAt": "2020-04-22T04:56:20Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractIndexerTest.java", "diffHunk": "@@ -57,7 +57,7 @@\n   protected TestQueryHelper queryHelper;\n \n   @Inject\n-  private IntegrationTestingConfig config;\n+  public IntegrationTestingConfig config;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyMTM0OA==", "bodyText": "Got lazy. I can do that.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412721348", "createdAt": "2020-04-22T07:00:21Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractIndexerTest.java", "diffHunk": "@@ -57,7 +57,7 @@\n   protected TestQueryHelper queryHelper;\n \n   @Inject\n-  private IntegrationTestingConfig config;\n+  public IntegrationTestingConfig config;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTQ4Mw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyNjYwNg==", "bodyText": "done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412726606", "createdAt": "2020-04-22T07:09:33Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractIndexerTest.java", "diffHunk": "@@ -57,7 +57,7 @@\n   protected TestQueryHelper queryHelper;\n \n   @Inject\n-  private IntegrationTestingConfig config;\n+  public IntegrationTestingConfig config;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTQ4Mw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDM4ODczOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractKafkaIndexingServiceTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNDo1Njo1OVrOGJjW8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzowOToyNVrOGJm0_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTY4Mg==", "bodyText": "nit: can be protected.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412669682", "createdAt": "2020-04-22T04:56:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractKafkaIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.indexing.kafka.KafkaConsumerConfigs;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.testing.utils.KafkaAdminClient;\n+import org.apache.druid.testing.utils.KafkaEventWriter;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Function;\n+\n+public abstract class AbstractKafkaIndexingServiceTest extends AbstractStreamIndexingTest\n+{\n+  public abstract boolean isKafkaWriterTransactionalEnabled();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyNjUyNw==", "bodyText": "done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412726527", "createdAt": "2020-04-22T07:09:25Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractKafkaIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.indexing.kafka.KafkaConsumerConfigs;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.testing.utils.KafkaAdminClient;\n+import org.apache.druid.testing.utils.KafkaEventWriter;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Function;\n+\n+public abstract class AbstractKafkaIndexingServiceTest extends AbstractStreamIndexingTest\n+{\n+  public abstract boolean isKafkaWriterTransactionalEnabled();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2OTY4Mg=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDQwOTUxOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTowNTo1OVrOGJjiKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoxMDozNFrOGJm38w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3MjU1Mg==", "bodyText": "\"may have already gone\"?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412672552", "createdAt": "2020-04-22T05:05:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void testIndexWithStreamReshardHelper(int newShardCount) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Reshard the supervisor by split from STREAM_SHARD_COUNT to newShardCount and waits until the resharding starts\n+      streamAdminClient.updateShardCount(generatedTestConfig.getStreamName(), newShardCount, true);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.isStreamActive(generatedTestConfig.getStreamName()),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.verfiyShardCountUpdated(generatedTestConfig.getStreamName(), STREAM_SHARD_COUNT, newShardCount),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void verifyIngestedData(GeneratedTestConfig generatedTestConfig) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for stream indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = generatedTestConfig.getStreamQueryPropsTransform().apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(generatedTestConfig.getSupervisorId());\n+    // wait for all indexing tasks to finish\n+    LOG.info(\"Waiting for all indexing tasks to finish\");\n+    ITRetryUtil.retryUntilTrue(\n+        () -> (indexer.getUncompletedTasksForDataSource(generatedTestConfig.getFullDatasourceName()).size() == 0),\n+        \"Waiting for Tasks Completion\"\n+    );\n+    // wait for segments to be handed off\n+    ITRetryUtil.retryUntil(\n+        () -> coordinator.areSegmentsLoaded(generatedTestConfig.getFullDatasourceName()),\n+        true,\n+        10000,\n+        30,\n+        \"Real-time generated segments loaded\"\n+    );\n+\n+    // this query will be answered by at least 1 historical segment, most likely 2, and possibly up to all 4\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+  }\n+\n+  long getSumOfEventSequence(int numEvents)\n+  {\n+    return (numEvents * (1 + numEvents)) / 2;\n+  }\n+\n+  private void doMethodTeardown(GeneratedTestConfig generatedTestConfig, StreamEventWriter streamEventWriter)\n+  {\n+    try {\n+      streamEventWriter.flush();\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup as the writer may have already went Bye-Bye", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3Mjg2OA==", "bodyText": "BTW, I don't think ignoring exceptions is a good idea in any case. Probably it should log at least.", "url": "https://github.com/apache/druid/pull/9724#discussion_r412672868", "createdAt": "2020-04-22T05:06:47Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void testIndexWithStreamReshardHelper(int newShardCount) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Reshard the supervisor by split from STREAM_SHARD_COUNT to newShardCount and waits until the resharding starts\n+      streamAdminClient.updateShardCount(generatedTestConfig.getStreamName(), newShardCount, true);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.isStreamActive(generatedTestConfig.getStreamName()),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.verfiyShardCountUpdated(generatedTestConfig.getStreamName(), STREAM_SHARD_COUNT, newShardCount),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void verifyIngestedData(GeneratedTestConfig generatedTestConfig) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for stream indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = generatedTestConfig.getStreamQueryPropsTransform().apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(generatedTestConfig.getSupervisorId());\n+    // wait for all indexing tasks to finish\n+    LOG.info(\"Waiting for all indexing tasks to finish\");\n+    ITRetryUtil.retryUntilTrue(\n+        () -> (indexer.getUncompletedTasksForDataSource(generatedTestConfig.getFullDatasourceName()).size() == 0),\n+        \"Waiting for Tasks Completion\"\n+    );\n+    // wait for segments to be handed off\n+    ITRetryUtil.retryUntil(\n+        () -> coordinator.areSegmentsLoaded(generatedTestConfig.getFullDatasourceName()),\n+        true,\n+        10000,\n+        30,\n+        \"Real-time generated segments loaded\"\n+    );\n+\n+    // this query will be answered by at least 1 historical segment, most likely 2, and possibly up to all 4\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+  }\n+\n+  long getSumOfEventSequence(int numEvents)\n+  {\n+    return (numEvents * (1 + numEvents)) / 2;\n+  }\n+\n+  private void doMethodTeardown(GeneratedTestConfig generatedTestConfig, StreamEventWriter streamEventWriter)\n+  {\n+    try {\n+      streamEventWriter.flush();\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup as the writer may have already went Bye-Bye", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3MjU1Mg=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyNzI4Mw==", "bodyText": "hahaha\nDone", "url": "https://github.com/apache/druid/pull/9724#discussion_r412727283", "createdAt": "2020-04-22T07:10:34Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void testIndexWithStreamReshardHelper(int newShardCount) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Reshard the supervisor by split from STREAM_SHARD_COUNT to newShardCount and waits until the resharding starts\n+      streamAdminClient.updateShardCount(generatedTestConfig.getStreamName(), newShardCount, true);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.isStreamActive(generatedTestConfig.getStreamName()),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      ITRetryUtil.retryUntil(\n+          () -> streamAdminClient.verfiyShardCountUpdated(generatedTestConfig.getStreamName(), STREAM_SHARD_COUNT, newShardCount),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after resahrding\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  private void verifyIngestedData(GeneratedTestConfig generatedTestConfig) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for stream indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = generatedTestConfig.getStreamQueryPropsTransform().apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(generatedTestConfig.getSupervisorId());\n+    // wait for all indexing tasks to finish\n+    LOG.info(\"Waiting for all indexing tasks to finish\");\n+    ITRetryUtil.retryUntilTrue(\n+        () -> (indexer.getUncompletedTasksForDataSource(generatedTestConfig.getFullDatasourceName()).size() == 0),\n+        \"Waiting for Tasks Completion\"\n+    );\n+    // wait for segments to be handed off\n+    ITRetryUtil.retryUntil(\n+        () -> coordinator.areSegmentsLoaded(generatedTestConfig.getFullDatasourceName()),\n+        true,\n+        10000,\n+        30,\n+        \"Real-time generated segments loaded\"\n+    );\n+\n+    // this query will be answered by at least 1 historical segment, most likely 2, and possibly up to all 4\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+  }\n+\n+  long getSumOfEventSequence(int numEvents)\n+  {\n+    return (numEvents * (1 + numEvents)) / 2;\n+  }\n+\n+  private void doMethodTeardown(GeneratedTestConfig generatedTestConfig, StreamEventWriter streamEventWriter)\n+  {\n+    try {\n+      streamEventWriter.flush();\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup as the writer may have already went Bye-Bye", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY3MjU1Mg=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 361}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDUwMzAxOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo0Mjo0MVrOGJkUtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoxODo1NVrOGJnMmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NTQ5Mg==", "bodyText": "Probably better to call run() or generate() rather than start() since the generator is not something you can start/stop, but generates all events in start().", "url": "https://github.com/apache/druid/pull/9724#discussion_r412685492", "createdAt": "2020-04-22T05:42:41Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMjU2OQ==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412732569", "createdAt": "2020-04-22T07:18:55Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NTQ5Mg=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDUxMTM2OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo0NTo0MlrOGJkZFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoxOToyOFrOGJnN0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NjYxMw==", "bodyText": "remainding -> remaining?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412686613", "createdAt": "2020-04-22T05:45:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMjg4Mg==", "bodyText": "Done", "url": "https://github.com/apache/druid/pull/9724#discussion_r412732882", "createdAt": "2020-04-22T07:19:28Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractStreamIndexingTest.java", "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.StreamAdminClient;\n+import org.apache.druid.testing.utils.StreamEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+public abstract class AbstractStreamIndexingTest extends AbstractIndexerTest\n+{\n+  static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  // format for the querying interval\n+  static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  static final int EVENTS_PER_SECOND = 6;\n+  static final int TOTAL_NUMBER_OF_SECOND = 10;\n+  static final Logger LOG = new Logger(AbstractStreamIndexingTest.class);\n+  // Since this integration test can terminates or be killed un-expectedly, this tag is added to all streams created\n+  // to help make stream clean up easier. (Normally, streams should be cleanup automattically by the teardown method)\n+  // The value to this tag is a timestamp that can be used by a lambda function to remove unused stream.\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final int STREAM_SHARD_COUNT = 2;\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  private static final long CYCLE_PADDING_MS = 100;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private StreamAdminClient streamAdminClient;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+\n+  abstract StreamAdminClient getStreamAdminClient() throws Exception;\n+  abstract StreamEventWriter getStreamEventWriter() throws Exception;\n+  abstract Function<String, String> generateStreamIngestionPropsTransform(String streamName, String fullDatasourceName);\n+  abstract Function<String, String> generateStreamQueryPropsTransform(String streamName, String fullDatasourceName);\n+  public abstract String getTestNamePrefix();\n+\n+  protected void doBeforeClass() throws Exception\n+  {\n+    streamAdminClient = getStreamAdminClient();\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  protected void doClassTeardown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+  }\n+\n+  protected void doTestIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start data generator\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  void doTestIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  void doTestIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  void doTestIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  protected void doTestIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(generatedTestConfig.getSupervisorId());\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(generatedTestConfig.getSupervisorId());\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(generatedTestConfig);\n+    }\n+    finally {\n+      doMethodTeardown(generatedTestConfig, streamEventWriter);\n+    }\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardSplit() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT * 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT * 2);\n+  }\n+\n+  protected void doTestIndexDataWithStreamReshardMerge() throws Exception\n+  {\n+    // Reshard the stream from STREAM_SHARD_COUNT to STREAM_SHARD_COUNT / 2\n+    testIndexWithStreamReshardHelper(STREAM_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    StreamEventWriter streamEventWriter = getStreamEventWriter();\n+    final GeneratedTestConfig generatedTestConfig = new GeneratedTestConfig();\n+    try (\n+        final Closeable ignored1 = unloader(generatedTestConfig.getFullDatasourceName())\n+    ) {\n+      final String taskSpec = generatedTestConfig.getStreamIngestionPropsTransform().apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      generatedTestConfig.setSupervisorId(indexer.submitSupervisor(taskSpec));\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Verify supervisor is healthy before restart\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(generatedTestConfig.getSupervisorId())),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(generatedTestConfig.getStreamName(), streamEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4NjYxMw=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDUyNDExOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/parallelized/ITKafkaIndexingServiceNonTransactionalParallelizedTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo1MDozOFrOGJkgOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMDozN1rOGJnQuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODQ0MQ==", "bodyText": "Can you add javadoc that explains what kind of tests should be done in parallel?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412688441", "createdAt": "2020-04-22T05:50:38Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/parallelized/ITKafkaIndexingServiceNonTransactionalParallelizedTest.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.parallelized;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.apache.druid.tests.indexer.AbstractKafkaIndexingServiceTest;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KAFKA_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKafkaIndexingServiceNonTransactionalParallelizedTest extends AbstractKafkaIndexingServiceTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMzYyNA==", "bodyText": "The docs in integration-tests/README.md under \"Running test methods in parallel\" should cover that already. I will add explanation for this particular case why is it parallel vs sequential", "url": "https://github.com/apache/druid/pull/9724#discussion_r412733624", "createdAt": "2020-04-22T07:20:37Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/parallelized/ITKafkaIndexingServiceNonTransactionalParallelizedTest.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.parallelized;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.apache.druid.tests.indexer.AbstractKafkaIndexingServiceTest;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KAFKA_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKafkaIndexingServiceNonTransactionalParallelizedTest extends AbstractKafkaIndexingServiceTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODQ0MQ=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2NDUyNDkxOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceSerializedTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNTo1MDo1OVrOGJkguQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwNzoyMDo0MVrOGJnQ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODU2OQ==", "bodyText": "Similarly, can you add javadoc that explains what kind of tests should be done in sequential?", "url": "https://github.com/apache/druid/pull/9724#discussion_r412688569", "createdAt": "2020-04-22T05:50:59Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceSerializedTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceSerializedTest extends AbstractKinesisIndexingServiceTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMzY3Mg==", "bodyText": "The docs in integration-tests/README.md under \"Running test methods in parallel\" should cover that already. I will add explanation for this particular case why is it parallel vs sequential", "url": "https://github.com/apache/druid/pull/9724#discussion_r412733672", "createdAt": "2020-04-22T07:20:41Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceSerializedTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceSerializedTest extends AbstractKinesisIndexingServiceTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY4ODU2OQ=="}, "originalCommit": {"oid": "f31dc7ba95a1fc8a5d7df041c9a37368a4fb9b88"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2570, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}