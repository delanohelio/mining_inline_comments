{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgyNjE4NjE1", "number": 9449, "title": "Add Sql InputSource", "bodyText": "Add Sql InputSource support for ingesting events from RDBMS using parallel indexing.\n\n\n\n\n\n\n\n\n\n\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added unit tests or modified existing tests to cover new code paths.\n been tested in a test Druid cluster.", "createdAt": "2020-03-02T21:31:01Z", "url": "https://github.com/apache/druid/pull/9449", "merged": true, "mergeCommit": {"oid": "17cf8ea8f2cc6a4fe8e2c6e29e3b9f066a6ccc2b"}, "closed": true, "closedAt": "2020-06-09T19:55:21Z", "author": {"login": "a2l007"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcJ0kSbAH2gAyMzgyNjE4NjE1OjY0YjI0NzU4YmI5MWZmMjE3NGU4MGUyNGNkMDY4YjM5ODdjYzJjYTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcpqkT1gFqTQyNzQ5MTA3OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "64b24758bb91ff2174e80e24cd068b3987cc2ca3", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/64b24758bb91ff2174e80e24cd068b3987cc2ca3", "committedDate": "2020-03-02T21:26:06Z", "message": "Add Sql InputSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/61f62fe01a64534a485b0f8d9176ca6f18186b55", "committedDate": "2020-03-02T22:16:16Z", "message": "Add spelling"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxOTQ4NTMy", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-391948532", "createdAt": "2020-04-13T04:10:45Z", "commit": {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNzU3MjU0", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-401757254", "createdAt": "2020-04-28T11:50:33Z", "commit": {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMTo1MDozM1rOGNQMNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMTo1MDozM1rOGNQMNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA==", "bodyText": "can this be part of new module as InputSource seems to replacement for firehose related interfaces ?", "url": "https://github.com/apache/druid/pull/9449#discussion_r416549940", "createdAt": "2020-04-28T11:50:33Z", "author": {"login": "pjain1"}, "path": "server/src/main/java/org/apache/druid/guice/FirehoseModule.java", "diffHunk": "@@ -58,7 +59,8 @@ public void configure(Binder binder)\n                 new NamedType(CombiningFirehoseFactory.class, \"combining\"),\n                 new NamedType(FixedCountFirehoseFactory.class, \"fixedCount\"),\n                 new NamedType(SqlFirehoseFactory.class, \"sql\"),\n-                new NamedType(InlineFirehoseFactory.class, \"inline\")\n+                new NamedType(InlineFirehoseFactory.class, \"inline\"),\n+                new NamedType(SqlInputSource.class, \"sql\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55"}, "originalPosition": 14}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23a7c414b97cad0c871de331a8339c22333ace0c", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/23a7c414b97cad0c871de331a8339c22333ace0c", "committedDate": "2020-05-05T16:24:30Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d1185e49933fea9baf4e4b28c8d43417e7c543b0", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/d1185e49933fea9baf4e4b28c8d43417e7c543b0", "committedDate": "2020-05-05T19:18:46Z", "message": "Use separate DruidModule"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/7f9743b112d910d50e6348ddb098dd1532650027", "committedDate": "2020-05-05T20:05:20Z", "message": "Change module name"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2MjkzMzMz", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-406293333", "createdAt": "2020-05-06T04:11:53Z", "commit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA3ODY3MzEy", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-407867312", "createdAt": "2020-05-07T21:59:16Z", "commit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMTo1OToxN1rOGSR0PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMjoxODozMVrOGSSR3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTQ1Mg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/9449#discussion_r421819452", "createdAt": "2020-05-07T21:59:17Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/java/util/common/parsers/CloseableIterator.java", "diffHunk": "@@ -66,10 +66,10 @@ public void close() throws IOException\n \n     return new CloseableIterator<R>()\n     {\n-      CloseableIterator<R> iterator = findNextIeteratorIfNecessary();\n+      CloseableIterator<R> iterator = findNextIteratorIfNecessary();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMDU0Ng==", "bodyText": "Maybe worth mentioning one more time that these SQLs are executed in two sub tasks when you run a Parallel task.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421820546", "createdAt": "2020-05-07T22:01:53Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate sqls\n+within the interval `2013-01-01/2013-01-02`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTA5Nw==", "bodyText": "sqls should be properly capitalized as SQLs.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421821097", "createdAt": "2020-05-07T22:03:19Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate sqls", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNjQxOQ==", "bodyText": "Could you add a comment on why we fetch all result in local storage first? I remember this is to avoid holding database connections for too long time. It would help other developers.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421826419", "createdAt": "2020-05-07T22:16:47Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNzAzNw==", "bodyText": "nit: this doesn't have to be done in this PR, but how about making JsonIterator a CloseableIterator? It already implements Iterator and Closeable so it would be pretty simple.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421827037", "createdAt": "2020-05-07T22:18:31Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA3ODg3MDgw", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-407887080", "createdAt": "2020-05-07T22:44:20Z", "commit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMjo0NDoyMFrOGSS2Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMjo0OTozN1rOGSS9sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNjM1OA==", "bodyText": "Would you add more detailed docs for this parameter? It should probably mention that you have to load some extension to read from a particular type of database.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421836358", "createdAt": "2020-05-07T22:44:20Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODI1OA==", "bodyText": "One more thing, I remember that many people from our community have been asking about how to use SqlFirehose. What do you think about adding a section that explains how to use it in production environment? To be honest, it's not clear for me what are best practices to make a scalable and efficient pipeline using this input source. For example, how do you parallelize each ingestion task (which means, how do you split queries)? How do you handle data updates in database especially after ingestion job is done? How often do you run ingestion jobs? and so on.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421838258", "createdAt": "2020-05-07T22:49:37Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA3ODg1MzEx", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-407885311", "createdAt": "2020-05-07T22:39:53Z", "commit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMjozOTo1NFrOGSSwHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QyMzoyMzoyOVrOGSTozQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNDc4MA==", "bodyText": "Why did you choose this package for the sql ingestion classes?\nOther implementations of InputSource lives under druid-core not druid-server.\nAnd while we're on the subject of packages - have you thought about making this an extension?", "url": "https://github.com/apache/druid/pull/9449#discussion_r421834780", "createdAt": "2020-05-07T22:39:54Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNTIzMQ==", "bodyText": "javadocs please - same on all the new classes. Also this module name sounds very broad - have you thought about scoping this to a SqlInputModule ? Once we know the right package structure, I think the name of the module will make more sense.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421835231", "createdAt": "2020-05-07T22:41:12Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg==", "bodyText": "Can you write a ModuleTest that verifies all the injections are made correctly?", "url": "https://github.com/apache/druid/pull/9449#discussion_r421837636", "createdAt": "2020-05-07T22:47:51Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule\n+{\n+  @Override\n+  public List<? extends Module> getJacksonModules()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODY5Mg==", "bodyText": "Why can't open() do this for you? Callers of the interface won't know about the inner workings of each implementation, so if we have a remediation, we should do this automatically.\nInject a Supplier in to this class so you can get a temp dir, this will also make it easier to test.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421838692", "createdAt": "2020-05-07T22:50:55Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzOTY4Mg==", "bodyText": "Is this possible? The annotations indicate that this should always be NonNull\nIf this can be null, then I think it should be checked at the constructor - right now, there is the possiblilty that we create temp files that are never cleaned up if an exception is thrown on this line.", "url": "https://github.com/apache/druid/pull/9449#discussion_r421839682", "createdAt": "2020-05-07T22:53:56Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MDYxMQ==", "bodyText": "Can you add more comments in here for what this is trying to do", "url": "https://github.com/apache/druid/pull/9449#discussion_r421840611", "createdAt": "2020-05-07T22:56:46Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjEwNw==", "bodyText": "Are there any size restrictions here? What happens if I try to ingest a very large sql output? How big do the indexing machines need to be? How long can the db connection be kept open so that we can keep writing records to the temp file? Are there any logs that indicate what I need to do operationally to support this?", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842107", "createdAt": "2020-05-07T23:01:17Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0Mjg5Mw==", "bodyText": "Do we need null checks on all the json provided fields? Or is that handled by some annotation I'm not seeing?\nWhat is the default boolean value if foldCase is not specified?", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842893", "createdAt": "2020-05-07T23:03:52Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MzYxNw==", "bodyText": "EqualsVerifier test please for equals and hashcode", "url": "https://github.com/apache/druid/pull/9449#discussion_r421843617", "createdAt": "2020-05-07T23:06:05Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);\n+  }\n+\n+  @Override\n+  public int estimateNumSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.size();\n+  }\n+\n+  @Override\n+  public SplittableInputSource<String> withSplit(InputSplit<String> split)\n+  {\n+    return new SqlInputSource(\n+        Collections.singletonList(split.get()),\n+        foldCase,\n+        sqlFirehoseDatabaseConnector,\n+        objectMapper\n+    );\n+  }\n+\n+  @Override\n+  protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nullable File temporaryDirectory)\n+  {\n+    final SqlInputFormat inputFormat = new SqlInputFormat(objectMapper);\n+    return new InputEntityIteratingReader(\n+        inputRowSchema,\n+        inputFormat,\n+        createSplits(inputFormat, null)\n+            .map(split -> new SqlEntity(split.get(), sqlFirehoseDatabaseConnector, foldCase, objectMapper)).iterator(),\n+        temporaryDirectory\n+    );\n+  }\n+\n+  @Override\n+  public boolean needsFormat()\n+  {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    SqlInputSource that = (SqlInputSource) o;\n+    return foldCase == that.foldCase &&\n+           sqls.equals(that.sqls) &&\n+           sqlFirehoseDatabaseConnector.equals(that.sqlFirehoseDatabaseConnector) &&\n+           objectMapper.equals(that.objectMapper);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(sqls, sqlFirehoseDatabaseConnector, objectMapper, foldCase);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0OTI5Mw==", "bodyText": "Can you add unit tests for this class please - same with SqlInputFormat", "url": "https://github.com/apache/druid/pull/9449#discussion_r421849293", "createdAt": "2020-05-07T23:23:29Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()\n+    {\n+      @Override\n+      public File file()\n+      {\n+        return tempFile;\n+      }\n+\n+      @Override\n+      public void close()\n+      {\n+        if (!tempFile.delete()) {\n+          LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+        }\n+      }\n+    };\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 166}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "committedDate": "2020-05-25T12:52:52Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "633323490f39fc279a874ae72d549c6e34762211", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/633323490f39fc279a874ae72d549c6e34762211", "committedDate": "2020-05-28T22:18:49Z", "message": "Fix docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af4d5fab2495c0ee7c461d693053a2fa38c15f12", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/af4d5fab2495c0ee7c461d693053a2fa38c15f12", "committedDate": "2020-05-28T22:18:54Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/0328eaded9f4b85649316bfca72a79e5b7ed1db9", "committedDate": "2020-05-29T16:13:07Z", "message": "Use sqltestutils for tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxNDUzMzY5", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-421453369", "createdAt": "2020-05-31T00:55:18Z", "commit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMVQwMDo1NToxOVrOGc2FSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMVQwMTowNzoyN1rOGc2Hcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTQwMA==", "bodyText": "Where is the interval 2013-01-01/2013-01-02 from?", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899400", "createdAt": "2020-05-31T00:55:19Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs within the interval `2013-01-01/2013-01-02`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTY3NA==", "bodyText": "I'm not sure what it means by \"avoid unwanted data being retrieved and stored locally\". Does this mean the subtask can modify the sql to filter out data out of the interval in the granularitySpec? Would you point me out where it is implemented?", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899674", "createdAt": "2020-05-31T01:00:54Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs within the interval `2013-01-01/2013-01-02`.\n+Each of the SQL queries will be run in its own sub-task and thus for the above example, there would be two sub-tasks.\n+\n+Compared to the other native batch InputSources, SQL InputSource behaves differently in terms of reading the input data and so it would be helpful to consider the following points before using this InputSource in a production environment:\n+\n+* During indexing, each sub-task would execute one of the SQL queries and the results are stored locally on disk. The sub-tasks then proceed to read the data from these local input files and generate segments. Presently, there isn\u2019t any restriction on the size of the generated files and this would require the MiddleManagers or Indexers to have sufficient disk capacity based on the volume of data being indexed.\n+\n+* Filtering the SQL queries based on the intervals specified in the `granularitySpec` can avoid unwanted data being retrieved and stored locally by the indexing sub-tasks.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTgwOA==", "bodyText": "The tempFile will not be deleted if an exception is thrown in any lines above. We should catch all exceptions and delete the file properly.", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899808", "createdAt": "2020-05-31T01:04:07Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTg5NQ==", "bodyText": "Thanks for making the JsonIterator a CloseableIterator. Now you can return jsonIterator directly and remove this.", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899895", "createdAt": "2020-05-31T01:05:59Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Reader exclusively for {@link SqlEntity}\n+ */\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    //The results are fetched into local storage as this avoids having to keep a persistent database connection for a long time\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()\n+    {\n+    }, inputStream, closer, objectMapper);\n+    return new CloseableIterator<Map<String, Object>>()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTk1NA==", "bodyText": "I would definitely vote for having such a supervisor! That will be super useful.", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899954", "createdAt": "2020-05-31T01:07:27Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODI1OA=="}, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxNDUzOTI1", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-421453925", "createdAt": "2020-05-31T01:12:53Z", "commit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMVQwMToxMjo1M1rOGc2IfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMVQwMToxMjo1M1rOGc2IfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjkwMDIyMA==", "bodyText": "Please move jg to the above try-resource clause so that i can be closed safely.", "url": "https://github.com/apache/druid/pull/9449#discussion_r432900220", "createdAt": "2020-05-31T01:12:53Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03de8c0e6a40c61ed9e9399766bc633886d4682a", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/03de8c0e6a40c61ed9e9399766bc633886d4682a", "committedDate": "2020-06-01T13:41:38Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "committedDate": "2020-06-01T20:16:21Z", "message": "Add additional tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "committedDate": "2020-06-01T20:16:33Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/53cb148570ccb48903a3f6d587287bdc9220f0fc", "committedDate": "2020-06-01T21:14:36Z", "message": "Fix inspection"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzg4NjM2", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-423788636", "createdAt": "2020-06-03T17:44:32Z", "commit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzODA3MDc2", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-423807076", "createdAt": "2020-06-03T18:09:05Z", "commit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODowOTowNVrOGenm4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxOToxNzoyOVrOGep5Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg==", "bodyText": "Can we mark this as a big warning in the docs? I think a similar warning should be made on line 1316 indicating that this functionality is experimental and not yet recommended for production use.", "url": "https://github.com/apache/druid/pull/9449#discussion_r434759392", "createdAt": "2020-06-03T18:09:05Z", "author": {"login": "suneet-s"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\", \"SELECT * FROM table2 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs for the interval `2013-01-01/2013-01-02`.\n+Each of the SQL queries will be run in its own sub-task and thus for the above example, there would be two sub-tasks.\n+\n+Compared to the other native batch InputSources, SQL InputSource behaves differently in terms of reading the input data and so it would be helpful to consider the following points before using this InputSource in a production environment:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2Njc0NQ==", "bodyText": "I was thinking of a test where we verify that the \"sql\" named type is registered to the SqlInputSource class correctly. Here's an example of a module test I've added for a custom extension https://github.com/implydata/indexed-table-loader/blob/master/src/test/java/io/imply/druid/indextable/loader/IndexedTableLoaderDruidModuleTest.java", "url": "https://github.com/apache/druid/pull/9449#discussion_r434766745", "createdAt": "2020-06-03T18:22:15Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule\n+{\n+  @Override\n+  public List<? extends Module> getJacksonModules()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg=="}, "originalCommit": {"oid": "7f9743b112d910d50e6348ddb098dd1532650027"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODU4Nw==", "bodyText": "Sorry I missed this earlier: Why did we chose json serialization? What happens if the object returned from the result set can not be serialized by the json generator?", "url": "https://github.com/apache/druid/pull/9449#discussion_r434778587", "createdAt": "2020-06-03T18:43:20Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4MDIwMA==", "bodyText": "It looks like this logic is duplicated from SQLMetadataStorageActionHandler#isStatementException. Can you consolidate them please", "url": "https://github.com/apache/druid/pull/9449#discussion_r434780200", "createdAt": "2020-06-03T18:46:15Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NDQxMA==", "bodyText": "This breaks the contract of HashMap which allows the key to be null. StringUtils.toLowerCase(null) will throw an NPE", "url": "https://github.com/apache/druid/pull/9449#discussion_r434784410", "createdAt": "2020-06-03T18:54:06Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+      return new CleanableFile()\n+      {\n+        @Override\n+        public File file()\n+        {\n+          return tempFile;\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+          if (!tempFile.delete()) {\n+            LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+          }\n+        }\n+      };\n+    }\n+    catch (Exception e) {\n+      if (!tempFile.delete()) {\n+        LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private static class CaseFoldedMap extends HashMap<String, Object>\n+  {\n+    public static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public Object get(Object obj)\n+    {\n+      return super.get(StringUtils.toLowerCase((String) obj));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NzI4NA==", "bodyText": "Does this need to check either inputFormat or splitHintSpec to create the Stream of InputSplits? SImilar comment for function below. Not sure if I understand how these should be used.", "url": "https://github.com/apache/druid/pull/9449#discussion_r434787284", "createdAt": "2020-06-03T18:59:25Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ==", "bodyText": "Why do we need a smile object mapper instead of just the default object mapper", "url": "https://github.com/apache/druid/pull/9449#discussion_r434796895", "createdAt": "2020-06-03T19:17:29Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "committedDate": "2020-06-04T17:18:29Z", "message": "Add module test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd9d5bb96c010790725c081ae2d0efab15ae49b0", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/dd9d5bb96c010790725c081ae2d0efab15ae49b0", "committedDate": "2020-06-04T17:18:54Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "committedDate": "2020-06-04T18:09:23Z", "message": "Fix md in docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0OTM4OTQx", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-424938941", "createdAt": "2020-06-05T01:26:38Z", "commit": {"oid": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMToyNjozOFrOGfdc9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMToyNjozOFrOGfdc9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU5MA==", "bodyText": "nit: @VisibleForTesting is no longer true.", "url": "https://github.com/apache/druid/pull/9449#discussion_r435641590", "createdAt": "2020-06-05T01:26:38Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java", "diffHunk": "@@ -174,7 +174,7 @@ public void insert(\n   }\n \n   @VisibleForTesting", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c"}, "originalPosition": 3}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08b662647de8ad752194399561ff5102df492547", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/08b662647de8ad752194399561ff5102df492547", "committedDate": "2020-06-05T13:27:37Z", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9550f53ef3616ad7cb996a60782b462a706e3e07", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/9550f53ef3616ad7cb996a60782b462a706e3e07", "committedDate": "2020-06-05T14:07:46Z", "message": "Remove annotation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NDkxMDc4", "url": "https://github.com/apache/druid/pull/9449#pullrequestreview-427491078", "createdAt": "2020-06-09T19:52:40Z", "commit": {"oid": "9550f53ef3616ad7cb996a60782b462a706e3e07"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2989, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}