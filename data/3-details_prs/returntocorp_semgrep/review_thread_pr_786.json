{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMTY2MjM4", "number": 786, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0MToxMFrOD9226Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0Mzo0N1rOD927gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTg4NTIxOnYy", "diffSide": "RIGHT", "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0MToxMFrOGXmnhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0NzoyMFrOGXm47g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwMzE0MA==", "bodyText": "So, I'm kinda confused about the architecture here. From what I understand about this PR, we're performing these fixture operations when tests are collected, not when they're run. This gives us a performance hit because these fixtures take a while to run. Is that right?\nBased on above, I have a few questions:\n\nAre these fixtures being run because they don't have the benchmark argument/fixture? That's why adding it below causes the fixtures to be avoided unless we're actually benchmarking?\nWhy do we have two separate fixtures here that only appear to be used with each other? Could this be one fixture to simplify things?\nIt seems we could leverage existing pytest functionality rather than checking this information ourselves. Whether that's something like adding benchmark here to only run this fixture during benchmarking, or leveraging pytest.mark.parametrize with indirect=True to run the fixture during test-setup instead of test-collection. Am I missing something that makes that not possible?", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427403140", "createdAt": "2020-05-19T15:41:10Z", "author": {"login": "mschwager"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwNzU5OA==", "bodyText": "Basically I think all the questions are answered by: I assumed skipped tests' fixtures would not be executed. Sorry about that :(\nThis PR is the fastest way I found to fix the CI failures that are repo-wide, cause I'm supposed to dedicate all my time to semgrep-app now. The alternative I see is reverting the benchmarks commit.", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427407598", "createdAt": "2020-05-19T15:47:20Z", "author": {"login": "underyx"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwMzE0MA=="}, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTg5NjY1OnYy", "diffSide": "RIGHT", "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0Mzo0MlrOGXmuxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0Nzo1OVrOGXm6uA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwNDk5OQ==", "bodyText": "Does this avoid running the fixture unless we're benchmarking? The fact that this variable is unused makes me think there's a better way to solve this.", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427404999", "createdAt": "2020-05-19T15:43:42Z", "author": {"login": "mschwager"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):\n     repo_path = tmp_path_factory.mktemp(\"repo\")\n+\n+    getoption = request.config.getoption\n+    is_disabled = getoption(\"benchmark_skip\") or getoption(\"benchmark_disable\")\n+    is_force_enabled = getoption(\"benchmark_enable\") or getoption(\"benchmark_only\")\n+    if is_disabled and not is_force_enabled:\n+        return repo_path  # not cloning since we're not gonna run benchmarks\n+\n     subprocess.check_output(\n         [\"git\", \"clone\", \"https://github.com/returntocorp/semgrep-rules\", repo_path]\n     )\n     subprocess.check_output(\n-        [\"git\", \"--git-dir\", repo_path / \".git\", \"checkout\", \"c3196b4\"]  # May 16, 2020\n+        [\"git\", \"checkout\", \"c3196b4\"], cwd=repo_path  # May 16, 2020\n     )\n \n-    yield repo_path\n+    return repo_path\n \n \n @pytest.fixture(params=RULE_PATHS.strip().splitlines())\n-def semgrep_rules_rule(semgrep_rules_repo, request, tmp_path):\n+def semgrep_rules_rule(semgrep_rules_repo, request, tmp_path, benchmark):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwODA1Ng==", "bodyText": "Yep, this makes pytest-benchmark decide if it should run. I'd need to research alternative ways to accomplish the same.", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427408056", "createdAt": "2020-05-19T15:47:59Z", "author": {"login": "underyx"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):\n     repo_path = tmp_path_factory.mktemp(\"repo\")\n+\n+    getoption = request.config.getoption\n+    is_disabled = getoption(\"benchmark_skip\") or getoption(\"benchmark_disable\")\n+    is_force_enabled = getoption(\"benchmark_enable\") or getoption(\"benchmark_only\")\n+    if is_disabled and not is_force_enabled:\n+        return repo_path  # not cloning since we're not gonna run benchmarks\n+\n     subprocess.check_output(\n         [\"git\", \"clone\", \"https://github.com/returntocorp/semgrep-rules\", repo_path]\n     )\n     subprocess.check_output(\n-        [\"git\", \"--git-dir\", repo_path / \".git\", \"checkout\", \"c3196b4\"]  # May 16, 2020\n+        [\"git\", \"checkout\", \"c3196b4\"], cwd=repo_path  # May 16, 2020\n     )\n \n-    yield repo_path\n+    return repo_path\n \n \n @pytest.fixture(params=RULE_PATHS.strip().splitlines())\n-def semgrep_rules_rule(semgrep_rules_repo, request, tmp_path):\n+def semgrep_rules_rule(semgrep_rules_repo, request, tmp_path, benchmark):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwNDk5OQ=="}, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTg5Njk5OnYy", "diffSide": "RIGHT", "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0Mzo0N1rOGXmu_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo0OTowM1rOGXm9hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwNTA1Mg==", "bodyText": "Doesn't pytest have some existing functionality to do this for us? My concern is that we're duplicating internal pytest logic here, and that we could be missing an edge case they cover or that things could change out from underneath us and our logic no longer matches theirs. Thoughts?", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427405052", "createdAt": "2020-05-19T15:43:47Z", "author": {"login": "mschwager"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):\n     repo_path = tmp_path_factory.mktemp(\"repo\")\n+\n+    getoption = request.config.getoption\n+    is_disabled = getoption(\"benchmark_skip\") or getoption(\"benchmark_disable\")\n+    is_force_enabled = getoption(\"benchmark_enable\") or getoption(\"benchmark_only\")\n+    if is_disabled and not is_force_enabled:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwODc3NA==", "bodyText": "Yep, it is slightly scary. It's actually pytest-benchmark logic we're duplicating for the record.", "url": "https://github.com/returntocorp/semgrep/pull/786#discussion_r427408774", "createdAt": "2020-05-19T15:49:03Z", "author": {"login": "underyx"}, "path": "semgrep/tests/performance/test_semgrep_rules_repo_perf.py", "diffHunk": "@@ -221,20 +221,27 @@\n \n \n @pytest.fixture(scope=\"session\")\n-def semgrep_rules_repo(tmp_path_factory):\n+def semgrep_rules_repo(request, tmp_path_factory):\n     repo_path = tmp_path_factory.mktemp(\"repo\")\n+\n+    getoption = request.config.getoption\n+    is_disabled = getoption(\"benchmark_skip\") or getoption(\"benchmark_disable\")\n+    is_force_enabled = getoption(\"benchmark_enable\") or getoption(\"benchmark_only\")\n+    if is_disabled and not is_force_enabled:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQwNTA1Mg=="}, "originalCommit": {"oid": "66cd46ef2a431282f394d9ad2d304b53f2a552a0"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4718, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}