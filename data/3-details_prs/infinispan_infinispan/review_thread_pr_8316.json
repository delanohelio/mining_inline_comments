{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3NDE3MDU2", "number": 8316, "reviewThreads": {"totalCount": 66, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNDowOVrOD8lOiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzowNToyOFrOD-T7-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODUxMDgxOnYy", "diffSide": "RIGHT", "path": "commons-test/src/main/java/org/infinispan/commons/test/TestNGTestListener.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNDowOVrOGVnFXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNDowOVrOGVnFXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMzYyOA==", "bodyText": "remove?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425313628", "createdAt": "2020-05-14T17:34:09Z", "author": {"login": "pruivo"}, "path": "commons-test/src/main/java/org/infinispan/commons/test/TestNGTestListener.java", "diffHunk": "@@ -89,6 +89,7 @@ public void onStart(ISuite suite) {\n       try {\n          Class.forName(\"reactor.blockhound.BlockHound\");\n          log.info(\"BlockHound on classpath, installing non blocking checks!\");\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODUxOTU5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/util/logging/Log.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNjo0MlrOGVnLNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNjo0MlrOGVnLNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNTEyNA==", "bodyText": "use the xml name?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               @Message(value = \"Store cannot be configured with purgeOnStartup or shared if it is read only!\", id = 590)\n          \n          \n            \n               @Message(value = \"Store cannot be configured with purge or shared if it is read only!\", id = 590)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425315124", "createdAt": "2020-05-14T17:36:42Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/util/logging/Log.java", "diffHunk": "@@ -1999,4 +2000,20 @@ TimeoutException coordinatorTimeoutWaitingForView(int expectedViewId, int curren\n \n    @Message(value = \"Cannot change max-count since max-size is already defined\", id = 588)\n    CacheException cannotIncreaseMaxCount();\n+\n+   @Message(value = \"Store cannot be configured with both read and write only!\", id = 589)\n+   CacheConfigurationException storeBothReadAndWriteOnly();\n+\n+   @Message(value = \"Store cannot be configured with purgeOnStartup or shared if it is read only!\", id = 590)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODUyMTg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/util/logging/Log.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNzoyN1rOGVnM0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzozNzoyN1rOGVnM0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNTUzOA==", "bodyText": "typo: fetchPersistenceStage => fetchPersistenceState\nor use the xml name:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               @Message(value = \"Store cannot be configured with fetchPersistenceStage or preload if it is write only!\", id = 591)\n          \n          \n            \n               @Message(value = \"Store cannot be configured with fetch-state or preload if it is write only!\", id = 591)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425315538", "createdAt": "2020-05-14T17:37:27Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/util/logging/Log.java", "diffHunk": "@@ -1999,4 +2000,20 @@ TimeoutException coordinatorTimeoutWaitingForView(int expectedViewId, int curren\n \n    @Message(value = \"Cannot change max-count since max-size is already defined\", id = 588)\n    CacheException cannotIncreaseMaxCount();\n+\n+   @Message(value = \"Store cannot be configured with both read and write only!\", id = 589)\n+   CacheConfigurationException storeBothReadAndWriteOnly();\n+\n+   @Message(value = \"Store cannot be configured with purgeOnStartup or shared if it is read only!\", id = 590)\n+   CacheConfigurationException storeReadOnlyExceptions();\n+\n+   @Message(value = \"Store cannot be configured with fetchPersistenceStage or preload if it is write only!\", id = 591)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODU0NDgzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/configuration/cache/StoreConfigurationChildBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo0NDoxMlrOGVncHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTozMjo1MVrOGXE0Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxOTQ1NA==", "bodyText": "is this still valid? only replication and invalidation?\nbased on org.infinispan.configuration.cache.AbstractStoreConfigurationBuilder#validateStoreAttributes, invalidation isn't supported.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425319454", "createdAt": "2020-05-14T17:44:12Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/configuration/cache/StoreConfigurationChildBuilder.java", "diffHunk": "@@ -35,8 +24,25 @@\n     */\n    S purgeOnStartup(boolean b);\n \n+   /**\n+    * If true, this cache store will be only used to write entries.\n+    */\n+   S writeOnly(boolean b);\n+\n+   /**\n+    * If true, fetch persistent state when joining a cluster. If multiple cache stores are chained,\n+    * only one of them can have this property enabled. Persistent state transfer with a shared cache\n+    * store does not make sense, as the same persistent store that provides the data will just end\n+    * up receiving it. Therefore, if a shared cache store is used, the cache will not allow a\n+    * persistent state transfer even if a cache store has this property set to true. Finally,\n+    * setting it to true only makes sense if in a clustered environment, and only 'replication' and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0OTI5NQ==", "bodyText": "I have removed that part from the old description.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r426849295", "createdAt": "2020-05-18T19:32:51Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/configuration/cache/StoreConfigurationChildBuilder.java", "diffHunk": "@@ -35,8 +24,25 @@\n     */\n    S purgeOnStartup(boolean b);\n \n+   /**\n+    * If true, this cache store will be only used to write entries.\n+    */\n+   S writeOnly(boolean b);\n+\n+   /**\n+    * If true, fetch persistent state when joining a cluster. If multiple cache stores are chained,\n+    * only one of them can have this property enabled. Persistent state transfer with a shared cache\n+    * store does not make sense, as the same persistent store that provides the data will just end\n+    * up receiving it. Therefore, if a shared cache store is used, the cache will not allow a\n+    * persistent state transfer even if a cache store has this property set to true. Finally,\n+    * setting it to true only makes sense if in a clustered environment, and only 'replication' and", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxOTQ1NA=="}, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODU4MTg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/factories/EmptyConstructorNamedCacheFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo1Mjo0N1rOGVn1JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo1Mjo0N1rOGVn1JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyNTg2MQ==", "bodyText": "unnecessary change?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425325861", "createdAt": "2020-05-14T17:52:47Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/factories/EmptyConstructorNamedCacheFactory.java", "diffHunk": "@@ -116,7 +116,7 @@ public Object construct(String componentName) {\n          return new CommandsFactoryImpl();\n       } else if (componentName.equals(PersistenceManager.class.getName())) {\n          if (configuration.persistence().usingStores()) {\n-            PersistenceManagerImpl realPersistenceManager = new PersistenceManagerImpl();\n+            PersistenceManager realPersistenceManager = new PersistenceManagerImpl();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODYwNDk5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/interceptors/impl/CacheWriterInterceptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo1ODo1NlrOGVoEIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo1ODo1NlrOGVoEIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyOTY5Ng==", "bodyText": "IMO, return (int) Math.min(size, Integer.MAX_VALUE); looks better (as you did above)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425329696", "createdAt": "2020-05-14T17:58:56Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/interceptors/impl/CacheWriterInterceptor.java", "diffHunk": "@@ -477,7 +477,11 @@ public long getWritesToTheStores() {\n          displayName = \"Number of persisted entries\"\n    )\n    public int getNumberOfPersistedEntries() {\n-      return CompletionStages.join(persistenceManager.size());\n+      long size = CompletionStages.join(persistenceManager.size());\n+      if (size > Integer.MAX_VALUE) {\n+         return Integer.MAX_VALUE;\n+      }\n+      return (int) size;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODYyNTAyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODowNDo0MFrOGVoRVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODowNDo0MFrOGVoRVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMzMzA3OQ==", "bodyText": "typo\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                * @return a stage that when complete signals taht this store has been successfully started\n          \n          \n            \n                * @return a stage that when complete signals that this store has been successfully started", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425333079", "createdAt": "2020-05-14T18:04:40Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,389 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals taht this store has been successfully started", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODY2MzE5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxNTozOFrOGVop0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoyODo1OVrOGVpGyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMzOTM0NA==", "bodyText": "stop() invokes start()?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425339344", "createdAt": "2020-05-14T18:15:38Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.infinispan.persistence.support;\n+\n+import java.util.Objects;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.function.Predicate;\n+\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.persistence.internal.PersistenceUtil;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.ExternalStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+public class SegmentedAdvancedLoadWriteStoreAdapter<K, V> implements SegmentedAdvancedLoadWriteStore<K, V> {\n+   private final CacheLoader<K, V> cacheLoader;\n+   private final CacheWriter<K, V> cacheWriter;\n+\n+   private KeyPartitioner keyPartitioner;\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheLoader<K, V> cacheLoader) {\n+      this.cacheLoader = Objects.requireNonNull(cacheLoader);\n+      this.cacheWriter = cacheLoader instanceof CacheWriter ? (CacheWriter) cacheLoader : null;\n+   }\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheWriter<K, V> cacheWriter) {\n+      this.cacheWriter = Objects.requireNonNull(cacheWriter);\n+      this.cacheLoader = cacheWriter instanceof CacheLoader ? (CacheLoader) cacheWriter : null;\n+   }\n+\n+   @Override\n+   public void init(InitializationContext ctx) {\n+      keyPartitioner = ctx.getKeyPartitioner();\n+      if (cacheLoader != null) {\n+         cacheLoader.init(ctx);\n+      } else {\n+         cacheWriter.init(ctx);\n+      }\n+   }\n+\n+   @Override\n+   public void start() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void stop() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0NjMzMg==", "bodyText": "I didn't clean this part of the PR up yet. This class is actually to be deleted :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425346332", "createdAt": "2020-05-14T18:28:10Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.infinispan.persistence.support;\n+\n+import java.util.Objects;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.function.Predicate;\n+\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.persistence.internal.PersistenceUtil;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.ExternalStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+public class SegmentedAdvancedLoadWriteStoreAdapter<K, V> implements SegmentedAdvancedLoadWriteStore<K, V> {\n+   private final CacheLoader<K, V> cacheLoader;\n+   private final CacheWriter<K, V> cacheWriter;\n+\n+   private KeyPartitioner keyPartitioner;\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheLoader<K, V> cacheLoader) {\n+      this.cacheLoader = Objects.requireNonNull(cacheLoader);\n+      this.cacheWriter = cacheLoader instanceof CacheWriter ? (CacheWriter) cacheLoader : null;\n+   }\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheWriter<K, V> cacheWriter) {\n+      this.cacheWriter = Objects.requireNonNull(cacheWriter);\n+      this.cacheLoader = cacheWriter instanceof CacheLoader ? (CacheLoader) cacheWriter : null;\n+   }\n+\n+   @Override\n+   public void init(InitializationContext ctx) {\n+      keyPartitioner = ctx.getKeyPartitioner();\n+      if (cacheLoader != null) {\n+         cacheLoader.init(ctx);\n+      } else {\n+         cacheWriter.init(ctx);\n+      }\n+   }\n+\n+   @Override\n+   public void start() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void stop() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMzOTM0NA=="}, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0Njc2MQ==", "bodyText": "It was an initial attempt which has been superseded by NonBlockingStoreAdapter", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425346761", "createdAt": "2020-05-14T18:28:59Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.infinispan.persistence.support;\n+\n+import java.util.Objects;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.function.Predicate;\n+\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.persistence.internal.PersistenceUtil;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.ExternalStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+public class SegmentedAdvancedLoadWriteStoreAdapter<K, V> implements SegmentedAdvancedLoadWriteStore<K, V> {\n+   private final CacheLoader<K, V> cacheLoader;\n+   private final CacheWriter<K, V> cacheWriter;\n+\n+   private KeyPartitioner keyPartitioner;\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheLoader<K, V> cacheLoader) {\n+      this.cacheLoader = Objects.requireNonNull(cacheLoader);\n+      this.cacheWriter = cacheLoader instanceof CacheWriter ? (CacheWriter) cacheLoader : null;\n+   }\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheWriter<K, V> cacheWriter) {\n+      this.cacheWriter = Objects.requireNonNull(cacheWriter);\n+      this.cacheLoader = cacheWriter instanceof CacheLoader ? (CacheLoader) cacheWriter : null;\n+   }\n+\n+   @Override\n+   public void init(InitializationContext ctx) {\n+      keyPartitioner = ctx.getKeyPartitioner();\n+      if (cacheLoader != null) {\n+         cacheLoader.init(ctx);\n+      } else {\n+         cacheWriter.init(ctx);\n+      }\n+   }\n+\n+   @Override\n+   public void start() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void stop() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMzOTM0NA=="}, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODY2NTA0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxNjoxNVrOGVorHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxNjoxNVrOGVorHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMzOTY3OQ==", "bodyText": "but cacheLoader can be null", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425339679", "createdAt": "2020-05-14T18:16:15Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.infinispan.persistence.support;\n+\n+import java.util.Objects;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.function.Predicate;\n+\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.persistence.internal.PersistenceUtil;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.ExternalStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+public class SegmentedAdvancedLoadWriteStoreAdapter<K, V> implements SegmentedAdvancedLoadWriteStore<K, V> {\n+   private final CacheLoader<K, V> cacheLoader;\n+   private final CacheWriter<K, V> cacheWriter;\n+\n+   private KeyPartitioner keyPartitioner;\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheLoader<K, V> cacheLoader) {\n+      this.cacheLoader = Objects.requireNonNull(cacheLoader);\n+      this.cacheWriter = cacheLoader instanceof CacheWriter ? (CacheWriter) cacheLoader : null;\n+   }\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheWriter<K, V> cacheWriter) {\n+      this.cacheWriter = Objects.requireNonNull(cacheWriter);\n+      this.cacheLoader = cacheWriter instanceof CacheLoader ? (CacheLoader) cacheWriter : null;\n+   }\n+\n+   @Override\n+   public void init(InitializationContext ctx) {\n+      keyPartitioner = ctx.getKeyPartitioner();\n+      if (cacheLoader != null) {\n+         cacheLoader.init(ctx);\n+      } else {\n+         cacheWriter.init(ctx);\n+      }\n+   }\n+\n+   @Override\n+   public void start() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void stop() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void destroy() {\n+      // ExternalStore is both loader and writer so can just check one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODY3MTYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxODowMlrOGVovDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxODowMlrOGVovDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0MDY4Nw==", "bodyText": "why return false?\nshouldn't the \"write\" methods check if cacheWriter is null? same for \"read\" methods.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425340687", "createdAt": "2020-05-14T18:18:02Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/support/SegmentedAdvancedLoadWriteStoreAdapter.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.infinispan.persistence.support;\n+\n+import java.util.Objects;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.function.Predicate;\n+\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.persistence.internal.PersistenceUtil;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.ExternalStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+public class SegmentedAdvancedLoadWriteStoreAdapter<K, V> implements SegmentedAdvancedLoadWriteStore<K, V> {\n+   private final CacheLoader<K, V> cacheLoader;\n+   private final CacheWriter<K, V> cacheWriter;\n+\n+   private KeyPartitioner keyPartitioner;\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheLoader<K, V> cacheLoader) {\n+      this.cacheLoader = Objects.requireNonNull(cacheLoader);\n+      this.cacheWriter = cacheLoader instanceof CacheWriter ? (CacheWriter) cacheLoader : null;\n+   }\n+\n+   public SegmentedAdvancedLoadWriteStoreAdapter(CacheWriter<K, V> cacheWriter) {\n+      this.cacheWriter = Objects.requireNonNull(cacheWriter);\n+      this.cacheLoader = cacheWriter instanceof CacheLoader ? (CacheLoader) cacheWriter : null;\n+   }\n+\n+   @Override\n+   public void init(InitializationContext ctx) {\n+      keyPartitioner = ctx.getKeyPartitioner();\n+      if (cacheLoader != null) {\n+         cacheLoader.init(ctx);\n+      } else {\n+         cacheWriter.init(ctx);\n+      }\n+   }\n+\n+   @Override\n+   public void start() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void stop() {\n+      if (cacheLoader != null) {\n+         cacheLoader.start();\n+      } else {\n+         cacheWriter.start();\n+      }\n+   }\n+\n+   @Override\n+   public void destroy() {\n+      // ExternalStore is both loader and writer so can just check one\n+      if (cacheLoader instanceof ExternalStore) {\n+         ((ExternalStore<K, V>) cacheLoader).destroy();\n+      } else {\n+         stop();\n+      }\n+   }\n+\n+   @Override\n+   public boolean isAvailable() {\n+      if (cacheLoader != null) {\n+         return cacheLoader.isAvailable();\n+      } else {\n+         return cacheWriter.isAvailable();\n+      }\n+   }\n+\n+   @Override\n+   public MarshallableEntry<K, V> loadEntry(Object key) {\n+      return cacheLoader.loadEntry(key);\n+   }\n+\n+   @Override\n+   public MarshallableEntry<K, V> get(int segment, Object key) {\n+      return loadEntry(key);\n+   }\n+\n+   @Override\n+   public boolean contains(Object key) {\n+      return cacheLoader.contains(key);\n+   }\n+\n+   @Override\n+   public boolean contains(int segment, Object key) {\n+      return contains(key);\n+   }\n+\n+   @Override\n+   public void write(MarshallableEntry<? extends K, ? extends V> entry) {\n+      cacheWriter.write(entry);\n+   }\n+\n+   @Override\n+   public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      write(entry);\n+   }\n+\n+   @Override\n+   public boolean delete(Object key) {\n+      return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODY3NzYwOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/distribution/rehash/RehashWithSharedStoreTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoxOTo1NFrOGVoy0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTozNzowNFrOGXE74g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0MTY1MA==", "bodyText": "assertEquals()", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425341650", "createdAt": "2020-05-14T18:19:54Z", "author": {"login": "pruivo"}, "path": "core/src/test/java/org/infinispan/distribution/rehash/RehashWithSharedStoreTest.java", "diffHunk": "@@ -55,7 +56,8 @@ public void testRehashes() throws PersistenceException {\n \n       // Ensure the loader is shared!\n       for (Cache<Object, String> c: Arrays.asList(c1, c2, c3)) {\n-         assert TestingUtil.getFirstLoader(c).contains(k) : format(\"CacheStore on %s should contain key %s\", c, k);\n+         DummyInMemoryStore dims = TestingUtil.getFirstStore(c);\n+         assert dims.contains(k) : format(\"CacheStore on %s should contain key %s\", c, k);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MTI5OA==", "bodyText": "Changed all the asserts.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r426851298", "createdAt": "2020-05-18T19:37:04Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/distribution/rehash/RehashWithSharedStoreTest.java", "diffHunk": "@@ -55,7 +56,8 @@ public void testRehashes() throws PersistenceException {\n \n       // Ensure the loader is shared!\n       for (Cache<Object, String> c: Arrays.asList(c1, c2, c3)) {\n-         assert TestingUtil.getFirstLoader(c).contains(k) : format(\"CacheStore on %s should contain key %s\", c, k);\n+         DummyInMemoryStore dims = TestingUtil.getFirstStore(c);\n+         assert dims.contains(k) : format(\"CacheStore on %s should contain key %s\", c, k);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0MTY1MA=="}, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODY4MDc2OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/expiration/impl/ExpirationStoreListenerFunctionalTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoyMDo0NVrOGVo0tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTozODowMlrOGXE94A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0MjEzMg==", "bodyText": "isn't this always true?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r425342132", "createdAt": "2020-05-14T18:20:45Z", "author": {"login": "pruivo"}, "path": "core/src/test/java/org/infinispan/expiration/impl/ExpirationStoreListenerFunctionalTest.java", "diffHunk": "@@ -84,7 +84,7 @@ public void testExpirationOfStoreWhenDataNotInMemory() throws Exception {\n       assertFalse(event.isPre());\n       assertNotNull(event.getKey());\n       // The dummy store produces value and metadata so lets make sure\n-      if (TestingUtil.getCacheLoader(cache) instanceof DummyInMemoryStore) {\n+      if (TestingUtil.getFirstStore(cache) instanceof DummyInMemoryStore) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MTgwOA==", "bodyText": "No, this is extended and sometimes it is SingleFileStore.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r426851808", "createdAt": "2020-05-18T19:38:02Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/expiration/impl/ExpirationStoreListenerFunctionalTest.java", "diffHunk": "@@ -84,7 +84,7 @@ public void testExpirationOfStoreWhenDataNotInMemory() throws Exception {\n       assertFalse(event.isPre());\n       assertNotNull(event.getKey());\n       // The dummy store produces value and metadata so lets make sure\n-      if (TestingUtil.getCacheLoader(cache) instanceof DummyInMemoryStore) {\n+      if (TestingUtil.getFirstStore(cache) instanceof DummyInMemoryStore) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0MjEzMg=="}, "originalCommit": {"oid": "c0f20cf06079da7852a8c0d9219cccac821ce404"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDQ1ODIwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/configuration/cache/AbstractStoreConfigurationBuilder.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOTo0NDo1MVrOGXYd_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNToxNjoyNFrOGXlfvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3MTMyNg==", "bodyText": "I'm not sure that readOnly and shared is an issue, as you could have a read-only jdbc store that is shared amongst the nodes. Currently shared being configured only impacts writes, however I don't think logically the two are incompatible.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427171326", "createdAt": "2020-05-19T09:44:51Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/configuration/cache/AbstractStoreConfigurationBuilder.java", "diffHunk": "@@ -173,12 +181,26 @@ private void validateStoreAttributes() {\n       boolean fetchPersistentState = attributes.attribute(FETCH_PERSISTENT_STATE).get();\n       boolean purgeOnStartup = attributes.attribute(PURGE_ON_STARTUP).get();\n       boolean transactional = attributes.attribute(TRANSACTIONAL).get();\n+      boolean readOnly = attributes.attribute(IGNORE_MODIFICATIONS).get();\n+      boolean writeOnly = attributes.attribute(WRITE_ONLY).get();\n       ConfigurationBuilder builder = getBuilder();\n \n       if (purgeOnStartup && preload) {\n          throw CONFIG.preloadAndPurgeOnStartupConflict();\n       }\n \n+      if (readOnly && writeOnly) {\n+         throw CONFIG.storeBothReadAndWriteOnly();\n+      }\n+\n+      if (readOnly && (purgeOnStartup || shared)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwNzkyNw==", "bodyText": "I had tossed back and forth with this. The config value shared only applies to a store that you write with, it has nothing to do with reading. So I figured that if a user had both they most likely misconfigured something. If you want I can revert though, as I mentioned I was trying to clamp down on configurations that may not make sense.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427307927", "createdAt": "2020-05-19T13:37:11Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/configuration/cache/AbstractStoreConfigurationBuilder.java", "diffHunk": "@@ -173,12 +181,26 @@ private void validateStoreAttributes() {\n       boolean fetchPersistentState = attributes.attribute(FETCH_PERSISTENT_STATE).get();\n       boolean purgeOnStartup = attributes.attribute(PURGE_ON_STARTUP).get();\n       boolean transactional = attributes.attribute(TRANSACTIONAL).get();\n+      boolean readOnly = attributes.attribute(IGNORE_MODIFICATIONS).get();\n+      boolean writeOnly = attributes.attribute(WRITE_ONLY).get();\n       ConfigurationBuilder builder = getBuilder();\n \n       if (purgeOnStartup && preload) {\n          throw CONFIG.preloadAndPurgeOnStartupConflict();\n       }\n \n+      if (readOnly && writeOnly) {\n+         throw CONFIG.storeBothReadAndWriteOnly();\n+      }\n+\n+      if (readOnly && (purgeOnStartup || shared)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3MTMyNg=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM4NDc2NA==", "bodyText": "Keep the check in place, probably best to be strict on second thought. I suppose we can always remove this check if we want shared to be applicable in read-only stores.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427384764", "createdAt": "2020-05-19T15:16:24Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/configuration/cache/AbstractStoreConfigurationBuilder.java", "diffHunk": "@@ -173,12 +181,26 @@ private void validateStoreAttributes() {\n       boolean fetchPersistentState = attributes.attribute(FETCH_PERSISTENT_STATE).get();\n       boolean purgeOnStartup = attributes.attribute(PURGE_ON_STARTUP).get();\n       boolean transactional = attributes.attribute(TRANSACTIONAL).get();\n+      boolean readOnly = attributes.attribute(IGNORE_MODIFICATIONS).get();\n+      boolean writeOnly = attributes.attribute(WRITE_ONLY).get();\n       ConfigurationBuilder builder = getBuilder();\n \n       if (purgeOnStartup && preload) {\n          throw CONFIG.preloadAndPurgeOnStartupConflict();\n       }\n \n+      if (readOnly && writeOnly) {\n+         throw CONFIG.storeBothReadAndWriteOnly();\n+      }\n+\n+      if (readOnly && (purgeOnStartup || shared)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3MTMyNg=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDQ5MDY4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOTo1MzozNlrOGXYyfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOTo1MzozNlrOGXYyfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3NjU3NA==", "bodyText": "/s/method/interface", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427176574", "createdAt": "2020-05-19T09:53:36Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDUwNTIyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOTo1NzozNlrOGXY7zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNToxNzowMFrOGXlhhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3ODk1Ng==", "bodyText": "Ideally this should occur in the validate method of a store builder, but I guess for a lot of user implementations they probably won't bother creating a configuration builder.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427178956", "createdAt": "2020-05-19T09:57:36Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NTEyNA==", "bodyText": "Also, shouldn't start(InitializationContext) return a CompletionStage that has been completed exceptionally instead of thowing an exception?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427195124", "createdAt": "2020-05-19T10:25:21Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3ODk1Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMDUwOQ==", "bodyText": "I am trying to ease this for users. While I agree that \"we\" should be returning a CompletionStage with the exception in it, I don't expect a user to be as vigilant as we are. So I think supporting either should be okay.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427310509", "createdAt": "2020-05-19T13:40:41Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3ODk1Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM4NTIyMA==", "bodyText": "Supporting both sounds good, but we should push them towards the correct way in the docs.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427385220", "createdAt": "2020-05-19T15:17:00Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE3ODk1Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDYxMDg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyNjoyNFrOGXZ9FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo0MToxM1rOGXg_WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NTY2OQ==", "bodyText": "Missing docs.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427195669", "createdAt": "2020-05-19T10:26:24Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMDkzNg==", "bodyText": "I will try to add if I can - I am trying to prioritize getting this in first though so we can find issues in CR1 as needed.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427310936", "createdAt": "2020-05-19T13:41:13Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NTY2OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDYxMjM1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyNjo1NVrOGXZ-Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo0MToyNlrOGXg_5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NTkzMQ==", "bodyText": "Unnecessary <p>?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427195931", "createdAt": "2020-05-19T10:26:55Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMTA3OA==", "bodyText": "I will have more docs above it :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427311078", "createdAt": "2020-05-19T13:41:26Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NTkzMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDYxMzk4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyNzozMFrOGXZ_RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo0MTo1MVrOGXhBOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NjIyOQ==", "bodyText": "Is start also guaranteed to no be invoked concurrently?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427196229", "createdAt": "2020-05-19T10:27:30Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMTQxNw==", "bodyText": "Yes. I just didn't add it yet :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427311417", "createdAt": "2020-05-19T13:41:51Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5NjIyOQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDYxODM0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyODo0M1rOGXaCEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo0MjoxOFrOGXhCfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5Njk0Nw==", "bodyText": "Is this still required?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427196947", "createdAt": "2020-05-19T10:28:43Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMTc0MA==", "bodyText": "I will just remove this method and keep the documentation I have in JIRA for now.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427311740", "createdAt": "2020-05-19T13:42:18Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5Njk0Nw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDYzOTU2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDozNDo1MlrOGXaPKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDozNDo1MlrOGXaPKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwMDI5OA==", "bodyText": "I think the wording here is confusing. How about:\n\"the media type desired for key parameters passed to this store and the media type of returned key values.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427200298", "createdAt": "2020-05-19T10:34:52Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit\n+    * Note that if the returned MediaType is binary, that the provided and returned values will/must be raw unwrapped\n+    * byte[] instances. Due to this the users should not rely upon the equality of such instances as each instance\n+    * will possibly be different objects.\n+    * <p>\n+    * The {@link MediaType#MATCH_ALL} is a special media type that if returned will signal that the store should\n+    * receive all values\n+    * @implSpec\n+    * The default implementation just returns the storageMediaType\n+    * @param storageMediaType how the key is stored in memory\n+    * @param supportedMediaTypes what media types Infinispan can convert to automatically on behalf of the store\n+    * @return the media type the store desires for keys to be in when invoked and will return in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDY0ODA3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDozNzozM1rOGXaUlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDozNzozM1rOGXaUlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwMTY4Nw==", "bodyText": "Wrong method name in the error message.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427201687", "createdAt": "2020-05-19T10:37:33Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit\n+    * Note that if the returned MediaType is binary, that the provided and returned values will/must be raw unwrapped\n+    * byte[] instances. Due to this the users should not rely upon the equality of such instances as each instance\n+    * will possibly be different objects.\n+    * <p>\n+    * The {@link MediaType#MATCH_ALL} is a special media type that if returned will signal that the store should\n+    * receive all values\n+    * @implSpec\n+    * The default implementation just returns the storageMediaType\n+    * @param storageMediaType how the key is stored in memory\n+    * @param supportedMediaTypes what media types Infinispan can convert to automatically on behalf of the store\n+    * @return the media type the store desires for keys to be in when invoked and will return in\n+    */\n+   default MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @param storageMediaType\n+    * @param supportedMediaTypes\n+    * @return\n+    */\n+   default MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default CompletionStage<Boolean> isAvailable() {\n+      return CompletableFutures.completedTrue();\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   default CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param entry\n+    * @return\n+    */\n+   CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<Boolean> delete(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> addSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement addSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> removeSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement removeSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   CompletionStage<Void> clear();\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkWrite(int publisherCount, Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(me -> Completable.fromCompletionStage(write(sp.getSegment(), me)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkDelete(int publisherCount, Publisher<SegmentedPublisher<Object>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(obj -> Completable.fromCompletionStage(delete(sp.getSegment(), obj)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> size(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement size\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> approximateSize(IntSet segments) {\n+      return size(segments);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @param filter\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement entryPublisher\");\n+   }\n+\n+   /**\n+    * Publishes keys from this store that are in one of the provided segments and also pass the provided filter.\n+    * The returned publisher must support being subscribed to any number of times. That is subsequent invocations of\n+    * {@link Publisher#subscribe(Subscriber)} should provide independent views of the underlying keys to the Subscribers.\n+    * Keys should not retrieved until a given Subscriber requests them via the\n+    * {@link org.reactivestreams.Subscription#request(long)} method.\n+    * <p>\n+    * Subscribing to the returned {@link Publisher} should not block the invoking thread. It is up to the store\n+    * implementation to ensure this occurs. If the underlying store implementation has non blocking support the\n+    * recommended approach is to return a Publisher that observes its values on the provided\n+    * {@link InitializationContext#getNonBlockingExecutor()}. If however the store must block to perform an operation it\n+    * is recommended to wrap your Publisher before returning with the\n+    * {@link org.infinispan.util.concurrent.BlockingManager#blockingPublisher(Publisher)} method and it will handle\n+    * subscription and observation on the blocking and non blocking executors respectively.\n+    * <p>\n+    * <h4>Summary of Characteristics Effects</h4>\n+    * <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" summary=\"Summary of Characteristics Effects\">\n+    *    <tr>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Characteristic</th>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Effect</th>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#BULK_READ}</td>\n+    *       <td valign=\"top\">When set this store must implement this method</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#EXPIRATION}</td>\n+    *       <td valign=\"top\">When set this store must not return expired keys</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#SEGMENTABLE}</td>\n+    *       <td valign=\"top\">When this is not set the provided {@code segments} parameter should be ignored</td>\n+    *    </tr>\n+    * </table>\n+    * <p>\n+    * @implSpec\n+    * A default implementation is provided that does the following:\n+    * <pre>{@code\n+    * return Flowable.fromPublisher(publishEntries(segments, filter))\n+    *     .map(MarshallableEntry::getKey);}\n+    * </pre>\n+    * @param segments A set of segments to filter keys by. This will always be non null.\n+    * @param filter A filter to filter they keys by. If this is null then no additional filtering should be done after segments.\n+    * @return a publisher that will provide the keys from the store\n+    */\n+   default Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.EXPIRATION + \", but it does not implement purgeExpired\");\n+   }\n+\n+   /**\n+    * Write modifications to the store in the prepare phase, as this is the only way we know the FINAL values of the entries.\n+    * This is required to handle scenarios where an objects value is changed after the put command has been executed, but\n+    * before the commit is called on the Tx.\n+    *\n+    * @param transaction the current transactional context.\n+    * @param batchModification an object containing the write/remove operations required for this transaction.\n+    */\n+   default CompletionStage<Void> prepareWithModifications(Transaction transaction, BatchModification batchModification) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.TRANSACTIONAL + \", but it does not implement rollback\");\n+   }\n+\n+   /**\n+    * Commit the provided transaction's changes to the underlying store.\n+    *\n+    * @param transaction the current transactional context.\n+    */\n+   default CompletionStage<Void> commit(Transaction transaction) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.TRANSACTIONAL + \", but it does not implement rollback\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 356}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDY0ODI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDozNzozOFrOGXaUtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo0Mjo1MFrOGXhELw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwMTcxOQ==", "bodyText": "Wrong method name in the error message.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427201719", "createdAt": "2020-05-19T10:37:38Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit\n+    * Note that if the returned MediaType is binary, that the provided and returned values will/must be raw unwrapped\n+    * byte[] instances. Due to this the users should not rely upon the equality of such instances as each instance\n+    * will possibly be different objects.\n+    * <p>\n+    * The {@link MediaType#MATCH_ALL} is a special media type that if returned will signal that the store should\n+    * receive all values\n+    * @implSpec\n+    * The default implementation just returns the storageMediaType\n+    * @param storageMediaType how the key is stored in memory\n+    * @param supportedMediaTypes what media types Infinispan can convert to automatically on behalf of the store\n+    * @return the media type the store desires for keys to be in when invoked and will return in\n+    */\n+   default MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @param storageMediaType\n+    * @param supportedMediaTypes\n+    * @return\n+    */\n+   default MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default CompletionStage<Boolean> isAvailable() {\n+      return CompletableFutures.completedTrue();\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   default CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param entry\n+    * @return\n+    */\n+   CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<Boolean> delete(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> addSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement addSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> removeSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement removeSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   CompletionStage<Void> clear();\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkWrite(int publisherCount, Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(me -> Completable.fromCompletionStage(write(sp.getSegment(), me)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkDelete(int publisherCount, Publisher<SegmentedPublisher<Object>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(obj -> Completable.fromCompletionStage(delete(sp.getSegment(), obj)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> size(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement size\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> approximateSize(IntSet segments) {\n+      return size(segments);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @param filter\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement entryPublisher\");\n+   }\n+\n+   /**\n+    * Publishes keys from this store that are in one of the provided segments and also pass the provided filter.\n+    * The returned publisher must support being subscribed to any number of times. That is subsequent invocations of\n+    * {@link Publisher#subscribe(Subscriber)} should provide independent views of the underlying keys to the Subscribers.\n+    * Keys should not retrieved until a given Subscriber requests them via the\n+    * {@link org.reactivestreams.Subscription#request(long)} method.\n+    * <p>\n+    * Subscribing to the returned {@link Publisher} should not block the invoking thread. It is up to the store\n+    * implementation to ensure this occurs. If the underlying store implementation has non blocking support the\n+    * recommended approach is to return a Publisher that observes its values on the provided\n+    * {@link InitializationContext#getNonBlockingExecutor()}. If however the store must block to perform an operation it\n+    * is recommended to wrap your Publisher before returning with the\n+    * {@link org.infinispan.util.concurrent.BlockingManager#blockingPublisher(Publisher)} method and it will handle\n+    * subscription and observation on the blocking and non blocking executors respectively.\n+    * <p>\n+    * <h4>Summary of Characteristics Effects</h4>\n+    * <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" summary=\"Summary of Characteristics Effects\">\n+    *    <tr>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Characteristic</th>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Effect</th>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#BULK_READ}</td>\n+    *       <td valign=\"top\">When set this store must implement this method</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#EXPIRATION}</td>\n+    *       <td valign=\"top\">When set this store must not return expired keys</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#SEGMENTABLE}</td>\n+    *       <td valign=\"top\">When this is not set the provided {@code segments} parameter should be ignored</td>\n+    *    </tr>\n+    * </table>\n+    * <p>\n+    * @implSpec\n+    * A default implementation is provided that does the following:\n+    * <pre>{@code\n+    * return Flowable.fromPublisher(publishEntries(segments, filter))\n+    *     .map(MarshallableEntry::getKey);}\n+    * </pre>\n+    * @param segments A set of segments to filter keys by. This will always be non null.\n+    * @param filter A filter to filter they keys by. If this is null then no additional filtering should be done after segments.\n+    * @return a publisher that will provide the keys from the store\n+    */\n+   default Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.EXPIRATION + \", but it does not implement purgeExpired\");\n+   }\n+\n+   /**\n+    * Write modifications to the store in the prepare phase, as this is the only way we know the FINAL values of the entries.\n+    * This is required to handle scenarios where an objects value is changed after the put command has been executed, but\n+    * before the commit is called on the Tx.\n+    *\n+    * @param transaction the current transactional context.\n+    * @param batchModification an object containing the write/remove operations required for this transaction.\n+    */\n+   default CompletionStage<Void> prepareWithModifications(Transaction transaction, BatchModification batchModification) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.TRANSACTIONAL + \", but it does not implement rollback\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 347}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMjE3NQ==", "bodyText": "Copy paste storm.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427312175", "createdAt": "2020-05-19T13:42:50Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/spi/NonBlockingStore.java", "diffHunk": "@@ -0,0 +1,390 @@\n+package org.infinispan.persistence.spi;\n+\n+import java.util.EnumSet;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.util.Experimental;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.support.BatchModification;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+import io.reactivex.rxjava3.core.Completable;\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * <p>\n+ * Implementations of this store must be thread safe if concurrent operations are performed on it. This should include\n+ * possibly invoking start or stop multiple times\n+ * <p>\n+ * Note that this method is Experimental and its methods may change slightly over time until it has matured.\n+ * @author William Burns\n+ * @since 11.0\n+ * @param <K> key value type\n+ * @param <V> value value type\n+ */\n+@Experimental\n+public interface NonBlockingStore<K, V> {\n+\n+   enum Characteristic {\n+      /**\n+       * Whether this cache can be shared between multiple nodes. An example would be an external system, such as\n+       * a database.\n+       */\n+      SHAREABLE,\n+      /**\n+       * If this store only supports being read from.  Any write based operations will never be invoked on this store.\n+       */\n+      READ_ONLY,\n+      /**\n+       * If this store only supports being written to. Any read based operations will never be invoked on this store.\n+       */\n+      WRITE_ONLY,\n+      /**\n+       * If this store supports bulk read operations.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #publishKeys(IntSet, Predicate)},\n+       * {@link #publishEntries(IntSet, Predicate, boolean)}  and {@link #size(IntSet)} methods.\n+       */\n+      BULK_READ,\n+      /**\n+       * If this store supports being invoked in a transactional context with a prepare and commit or rollback phases.\n+       * <p>\n+       * Stores that have this characteristic must override the\n+       * {@link #prepareWithModifications(Transaction, BatchModification)}, {@link #commit(Transaction)} and\n+       * {@link #rollback(Transaction)} methods.\n+       */\n+      TRANSACTIONAL,\n+      /**\n+       * Whether this store supports being segmented. All methods in this SPI take as an argument a way to map a given\n+       * entry to a segment. A segment in Infinispan is an int that acts as a bucket for many keys. Many store\n+       * implementations may be able to store and load entries in a more performant way if they segment their data\n+       * accordingly.\n+       * <p>\n+       * If this store is not segmentable then invokers of this SPI are not required to calculate these segments before\n+       * invoking these methods and thus these methods may be invoked with any int value, null or equivalent. Please\n+       * see each method to see how they may be affected when this store is not segmentable.\n+       * <p>\n+       * Note that a store may also be configured at runtime to be segmented or not. If this store is configured to not\n+       * be segmented this store will be treated as if it does not have the SEGMENTABLE characteristic (causing possible\n+       * parameters to be null or invalid segment numbers). A store implementation may want to block this configuration\n+       * by throwing an exception in the {@link #start(InitializationContext)} method if it does not want to support this.\n+       * <p>\n+       * While it is possible that a SEGMENTABLE store can be configured as not segmented, a store that is not\n+       * SEGMENTABLE will never be allowed to be configured as segmented.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #addSegments(IntSet)} and\n+       * {@link #removeSegments(IntSet)} methods.\n+       */\n+      SEGMENTABLE,\n+      /**\n+       * If this store supports storing expiration metadata. Certain methods may or may not include expired entries.\n+       * <p>\n+       * Stores that have this characteristic must override the {@link #purgeExpired()} method.\n+       */\n+      EXPIRATION\n+   }\n+\n+   /**\n+    *\n+    * @param ctx initialization context used to initialize this store\n+    * @return a stage that when complete signals that this store has been successfully started\n+    */\n+   CompletionStage<Void> start(InitializationContext ctx);\n+\n+   /**\n+    * <p>\n+    * This method is guaranteed to not be invoked concurrently with other operations.\n+    * @return a stage that when complete signals that this store has been stopped\n+    */\n+   CompletionStage<Void> stop();\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Set<Characteristic> characteristics() {\n+      return EnumSet.noneOf(Characteristic.class);\n+   }\n+\n+   /**\n+    * Provides a way for the store to communicate which media type it requires for its key arguments. Infinispan\n+    * will invoke this method immediately after the store is started providing the media type that in memory objects are\n+    * stored via {@code storageMediaType} as well as all the media types this running instance can support converting\n+    * to. The store then must choose from any of the provided media types. Any methods that are invoked after that\n+    * accept a key argument will be guaranteed to have this key be in the desired media type. Also any return values\n+    * from this store should be in this media type\n+    * <p>\n+    * Note that choosing a media type other than the storage media type will incur the cost of converting the key\n+    * to and from the storage media type for input and output parameters.\n+    * <p>\n+    * TODO: update once this detection is more explicit\n+    * Note that if the returned MediaType is binary, that the provided and returned values will/must be raw unwrapped\n+    * byte[] instances. Due to this the users should not rely upon the equality of such instances as each instance\n+    * will possibly be different objects.\n+    * <p>\n+    * The {@link MediaType#MATCH_ALL} is a special media type that if returned will signal that the store should\n+    * receive all values\n+    * @implSpec\n+    * The default implementation just returns the storageMediaType\n+    * @param storageMediaType how the key is stored in memory\n+    * @param supportedMediaTypes what media types Infinispan can convert to automatically on behalf of the store\n+    * @return the media type the store desires for keys to be in when invoked and will return in\n+    */\n+   default MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @param storageMediaType\n+    * @param supportedMediaTypes\n+    * @return\n+    */\n+   default MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return storageMediaType;\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default CompletionStage<Boolean> isAvailable() {\n+      return CompletableFutures.completedTrue();\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   default CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param entry\n+    * @return\n+    */\n+   CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry);\n+\n+   /**\n+    *\n+    * @param segment\n+    * @param key\n+    * @return\n+    */\n+   CompletionStage<Boolean> delete(int segment, Object key);\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> addSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement addSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Void> removeSegments(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.SEGMENTABLE + \", but it does not implement removeSegments\");\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   CompletionStage<Void> clear();\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkWrite(int publisherCount, Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(me -> Completable.fromCompletionStage(write(sp.getSegment(), me)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param publisherCount\n+    * @param publisher\n+    * @return\n+    */\n+   default CompletionStage<Void> bulkDelete(int publisherCount, Publisher<SegmentedPublisher<Object>> publisher) {\n+      return Flowable.fromPublisher(publisher)\n+            .concatMapCompletable(sp ->\n+                        Flowable.fromPublisher(sp)\n+                              .concatMapCompletable(obj -> Completable.fromCompletionStage(delete(sp.getSegment(), obj)))\n+                  , publisherCount)\n+            .toCompletionStage(null);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> size(IntSet segments) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement size\");\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @return\n+    */\n+   default CompletionStage<Long> approximateSize(IntSet segments) {\n+      return size(segments);\n+   }\n+\n+   /**\n+    *\n+    * @param segments\n+    * @param filter\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.BULK_READ + \", but it does not implement entryPublisher\");\n+   }\n+\n+   /**\n+    * Publishes keys from this store that are in one of the provided segments and also pass the provided filter.\n+    * The returned publisher must support being subscribed to any number of times. That is subsequent invocations of\n+    * {@link Publisher#subscribe(Subscriber)} should provide independent views of the underlying keys to the Subscribers.\n+    * Keys should not retrieved until a given Subscriber requests them via the\n+    * {@link org.reactivestreams.Subscription#request(long)} method.\n+    * <p>\n+    * Subscribing to the returned {@link Publisher} should not block the invoking thread. It is up to the store\n+    * implementation to ensure this occurs. If the underlying store implementation has non blocking support the\n+    * recommended approach is to return a Publisher that observes its values on the provided\n+    * {@link InitializationContext#getNonBlockingExecutor()}. If however the store must block to perform an operation it\n+    * is recommended to wrap your Publisher before returning with the\n+    * {@link org.infinispan.util.concurrent.BlockingManager#blockingPublisher(Publisher)} method and it will handle\n+    * subscription and observation on the blocking and non blocking executors respectively.\n+    * <p>\n+    * <h4>Summary of Characteristics Effects</h4>\n+    * <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" summary=\"Summary of Characteristics Effects\">\n+    *    <tr>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Characteristic</th>\n+    *       <th bgcolor=\"#CCCCFF\" align=\"left\">Effect</th>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#BULK_READ}</td>\n+    *       <td valign=\"top\">When set this store must implement this method</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#EXPIRATION}</td>\n+    *       <td valign=\"top\">When set this store must not return expired keys</td>\n+    *    </tr>\n+    *    <tr>\n+    *       <td valign=\"top\">{@link Characteristic#SEGMENTABLE}</td>\n+    *       <td valign=\"top\">When this is not set the provided {@code segments} parameter should be ignored</td>\n+    *    </tr>\n+    * </table>\n+    * <p>\n+    * @implSpec\n+    * A default implementation is provided that does the following:\n+    * <pre>{@code\n+    * return Flowable.fromPublisher(publishEntries(segments, filter))\n+    *     .map(MarshallableEntry::getKey);}\n+    * </pre>\n+    * @param segments A set of segments to filter keys by. This will always be non null.\n+    * @param filter A filter to filter they keys by. If this is null then no additional filtering should be done after segments.\n+    * @return a publisher that will provide the keys from the store\n+    */\n+   default Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   /**\n+    *\n+    * @return\n+    */\n+   default Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.EXPIRATION + \", but it does not implement purgeExpired\");\n+   }\n+\n+   /**\n+    * Write modifications to the store in the prepare phase, as this is the only way we know the FINAL values of the entries.\n+    * This is required to handle scenarios where an objects value is changed after the put command has been executed, but\n+    * before the commit is called on the Tx.\n+    *\n+    * @param transaction the current transactional context.\n+    * @param batchModification an object containing the write/remove operations required for this transaction.\n+    */\n+   default CompletionStage<Void> prepareWithModifications(Transaction transaction, BatchModification batchModification) {\n+      throw new UnsupportedOperationException(\"Store characteristic included \" + Characteristic.TRANSACTIONAL + \", but it does not implement rollback\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwMTcxOQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 347}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDY3MjA5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDo0NTowNFrOGXajxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNToyOTo0N1rOGXmIIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwNTU3NA==", "bodyText": "It doesn't have to be addressed in this PR, but is CacheStoreFactoryRegistry required anymore? AFAIK this was only ever needed for the WFLY subsystem integration with deployable stores.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427205574", "createdAt": "2020-05-19T10:45:04Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxMjQ3MA==", "bodyText": "I would really love to remove it as well. I don't know tbh, I asked Paul about it and he didn't know about it.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427312470", "createdAt": "2020-05-19T13:43:13Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwNTU3NA=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM4NzMzMg==", "bodyText": "AFAIK Wildfly doesn't allow custom store implementations to be deployed to the server, so I don't think they will ever need it. @pferraro Please speak up if you envisage needing this behaviour.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427387332", "createdAt": "2020-05-19T15:19:45Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwNTU3NA=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM5NTEwNQ==", "bodyText": "https://issues.redhat.com/browse/ISPN-11862", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427395105", "createdAt": "2020-05-19T15:29:47Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIwNTU3NA=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDcxNTc3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDo1Nzo1MVrOGXa-5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo1MTowMVrOGXhaxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxMjUxNw==", "bodyText": "I assume we're performing these lookups everytime for isReadOnly and isWriteOnly to take into account stores being disabled at runtime?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427212517", "createdAt": "2020-05-19T10:57:51Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxNzk1OA==", "bodyText": "Yeah, sadly. I have debated about different ways to do disabling of stores, but that is too much for this PR. If we find this is problematic for performance we can revisit later.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427317958", "createdAt": "2020-05-19T13:51:01Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxMjUxNw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 193}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDcyMjY5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDo1OTo1N1rOGXbDVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODoyNjozMVrOGYArRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxMzY1Mw==", "bodyText": "Not strictly related to the non-blocking aspects, but should we add a PersistenceMarshallerFactory so that we only create and register the component if configuration.persistence().usingStores()?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427213653", "createdAt": "2020-05-19T10:59:57Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxOTEwNw==", "bodyText": "We handle that already in https://github.com/infinispan/infinispan/blob/master/core/src/main/java/org/infinispan/factories/EmptyConstructorNamedCacheFactory.java#L121\nBut it should have probably been a separate factory. Something for later me thinks.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427319107", "createdAt": "2020-05-19T13:52:48Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxMzY1Mw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgzMDA4Nw==", "bodyText": "With the current factory we utilise the PersistenceManagerStub if no stores are configured, so I'm not sure if the following code is required:\n      if (!enabled)\n         return;\n\nHappy to address in another PR as I'm not sure why we even create a stub if no stores are configured.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427830087", "createdAt": "2020-05-20T08:26:31Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxMzY1Mw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 225}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDc0NjQ5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTowNjo1NVrOGXbSJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTowNjo1NVrOGXbSJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIxNzQ0Nw==", "bodyText": "Typo: guarantees", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427217447", "createdAt": "2020-05-19T11:06:55Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDc2NzYwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMToxMzoxMlrOGXbfWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo1NTowNFrOGXhlPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyMDgyNQ==", "bodyText": "Should we add a trace log of the exception before propogating?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427220825", "createdAt": "2020-05-19T11:13:12Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyMDYzNw==", "bodyText": "Sure, I don't see why not :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427320637", "createdAt": "2020-05-19T13:55:04Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyMDgyNQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 308}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDgxMDcxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMToyNjo0MlrOGXb6bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMToyNjo0MlrOGXb6bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyNzc1Nw==", "bodyText": "This is so much simpler than before \ud83d\ude42", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427227757", "createdAt": "2020-05-19T11:26:42Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);\n+         throw t;\n+      }\n    }\n \n-   protected void pollStoreAvailability() {\n-      acquireReadLock();\n-      try {\n-         boolean availabilityChanged = false;\n-         boolean failureDetected = false;\n-         for (StoreStatus status : storeStatuses.values()) {\n-            if (status.availabilityChanged())\n-               availabilityChanged = true;\n-            if (availabilityChanged && !status.availability && !failureDetected) {\n-               failureDetected = true;\n-               unavailableException = new StoreUnavailableException(String.format(\"Store %s is unavailable\", status.store));\n-               CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n-            }\n+   private Set<Characteristic> updateCharacteristics(NonBlockingStore store, Set<Characteristic> characteristics,\n+         StoreConfiguration storeConfiguration) {\n+      if (storeConfiguration.ignoreModifications()) {\n+         if (characteristics.contains(Characteristic.WRITE_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.WRITE_ONLY,\n+                  Characteristic.READ_ONLY);\n          }\n-         if (!failureDetected && availabilityChanged) {\n-            unavailableException = null;\n-            CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(true));\n+         characteristics.add(Characteristic.READ_ONLY);\n+      }\n+      if (storeConfiguration.writeOnly()) {\n+         if (characteristics.contains(Characteristic.READ_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.READ_ONLY,\n+                  Characteristic.WRITE_ONLY);\n          }\n-      } finally {\n-         releaseReadLock();\n+         characteristics.add(Characteristic.WRITE_ONLY);\n       }\n+      return characteristics;\n    }\n \n-   /**\n-    * Returns the next trace number identifier, always 0 or higher\n-    */\n-   private static int getNextTraceNumber() {\n-      return asyncExecutionId.getAndUpdate(prev -> Math.max(prev + 1, 0));\n+   protected CompletionStage<Void> pollStoreAvailability() {\n+      if (trace) {\n+         log.trace(\"Polling Store availability\");\n+      }\n+      // This maybe will always be empty - used when all stores are available\n+      Maybe<NonBlockingStore<Object, Object>> allAvailableMaybe = Maybe.defer(() -> {\n+         if (unavailableExceptionMessage != null) {\n+            unavailableExceptionMessage = null;\n+            return Maybe.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(true)\n+               .thenApply(CompletableFutures.toNullFunction()));\n+         }\n+         return Maybe.empty();\n+      });\n+      return Completable.using(this::acquireReadLock,\n+            ignore -> Flowable.fromIterable(stores.values())\n+                  .flatMapMaybe(storeStatus -> {\n+                     CompletionStage<Boolean> availableStage = storeStatus.store.isAvailable();\n+                     return Maybe.fromCompletionStage(availableStage.thenApply(isAvailable -> {\n+                        synchronized (storeStatus) {\n+                           storeStatus.availability = isAvailable;\n+                        }\n+                        if (!isAvailable) {\n+                           return storeStatus.store();\n+                        }\n+                        return null;\n+                     }));\n+                  }).firstElement()\n+                  // If it is empty that means all stores were available\n+                  .switchIfEmpty(allAvailableMaybe)\n+                  .concatMapCompletable(unavailableStore -> {\n+                     if (unavailableExceptionMessage == null) {\n+                        log.debugf(\"Store %s is unavailable!\", unavailableStore);\n+                        unavailableExceptionMessage = \"Store \" + unavailableStore + \" is unavailable\";\n+                        return Completable.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n+                     }\n+                     return Completable.complete();\n+                  }),\n+            this::releaseReadLock)\n+            .toCompletionStage(null);\n+   }\n+\n+   private NonBlockingStore<?, ?> storeFromConfiguration(StoreConfiguration cfg) {\n+      final Object bareInstance;\n+      if (cfg.segmented() && cfg instanceof AbstractSegmentedStoreConfiguration) {\n+         bareInstance = new ComposedSegmentedLoadWriteStore<>((AbstractSegmentedStoreConfiguration) cfg);\n+      } else {\n+         bareInstance = cacheStoreFactoryRegistry.createInstance(cfg);\n+      }\n+      if (!(bareInstance instanceof NonBlockingStore)) {\n+         // All prior stores implemented at least Lifecycle\n+         return new NonBlockingStoreAdapter<>((Lifecycle) bareInstance);\n+      }\n+      return (NonBlockingStore<?, ?>) bareInstance;\n    }\n \n    @Override\n    @Stop\n    public void stop() {\n-      storesMutex.writeLock().lock();\n-      publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+      long stamp = lock.writeLock();\n       try {\n+         stopAvailabilityTask();\n+         Flowable<NonBlockingStore<?, ?>> flowable = Flowable.fromIterable(stores.keySet());\n          // If needed, clear the persistent store before stopping\n          if (clearOnStop) {\n-            clearAllStoresSync(AccessMode.BOTH, getNextTraceNumber());\n+            flowable = flowable\n+                  .delay(store -> Completable.fromCompletionStage(store.clear()).toFlowable());\n          }\n+         flowable = flowable.delay(store -> Completable.fromCompletionStage(store.stop()).toFlowable());\n \n-         Set<Lifecycle> undelegated = new HashSet<>();\n-         Consumer<CacheWriter> stopWriters = writer -> {\n-            writer.stop();\n-            if (writer instanceof DelegatingCacheWriter) {\n-               CacheWriter actual = undelegate(writer);\n-               actual.stop();\n-               undelegated.add(actual);\n-            } else {\n-               undelegated.add(writer);\n-            }\n-         };\n-         if (availabilityFuture != null)\n-            availabilityFuture.cancel(true);\n-         nonTxWriters.forEach(stopWriters);\n-         nonTxWriters.clear();\n-         txWriters.forEach(stopWriters);\n-         txWriters.clear();\n-\n-         for (CacheLoader l : loaders) {\n-            if (!undelegated.contains(l)) {\n-               l.stop();\n-            }\n-            if (l instanceof DelegatingCacheLoader) {\n-               CacheLoader actual = undelegate(l);\n-               if (!undelegated.contains(actual)) {\n-                  actual.stop();\n-               }\n-            }\n-         }\n-         loaders.clear();\n+         // Wait until it completes\n+         blockingSubscribe(flowable);\n+         stores.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 459}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDgxMzI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMToyNzoyMlrOGXb72Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTozMjoyOVrOGXmPtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyODEyMQ==", "bodyText": "Do we have a Jira for this? \\cc @danberindei", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427228121", "createdAt": "2020-05-19T11:27:22Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);\n+         throw t;\n+      }\n    }\n \n-   protected void pollStoreAvailability() {\n-      acquireReadLock();\n-      try {\n-         boolean availabilityChanged = false;\n-         boolean failureDetected = false;\n-         for (StoreStatus status : storeStatuses.values()) {\n-            if (status.availabilityChanged())\n-               availabilityChanged = true;\n-            if (availabilityChanged && !status.availability && !failureDetected) {\n-               failureDetected = true;\n-               unavailableException = new StoreUnavailableException(String.format(\"Store %s is unavailable\", status.store));\n-               CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n-            }\n+   private Set<Characteristic> updateCharacteristics(NonBlockingStore store, Set<Characteristic> characteristics,\n+         StoreConfiguration storeConfiguration) {\n+      if (storeConfiguration.ignoreModifications()) {\n+         if (characteristics.contains(Characteristic.WRITE_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.WRITE_ONLY,\n+                  Characteristic.READ_ONLY);\n          }\n-         if (!failureDetected && availabilityChanged) {\n-            unavailableException = null;\n-            CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(true));\n+         characteristics.add(Characteristic.READ_ONLY);\n+      }\n+      if (storeConfiguration.writeOnly()) {\n+         if (characteristics.contains(Characteristic.READ_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.READ_ONLY,\n+                  Characteristic.WRITE_ONLY);\n          }\n-      } finally {\n-         releaseReadLock();\n+         characteristics.add(Characteristic.WRITE_ONLY);\n       }\n+      return characteristics;\n    }\n \n-   /**\n-    * Returns the next trace number identifier, always 0 or higher\n-    */\n-   private static int getNextTraceNumber() {\n-      return asyncExecutionId.getAndUpdate(prev -> Math.max(prev + 1, 0));\n+   protected CompletionStage<Void> pollStoreAvailability() {\n+      if (trace) {\n+         log.trace(\"Polling Store availability\");\n+      }\n+      // This maybe will always be empty - used when all stores are available\n+      Maybe<NonBlockingStore<Object, Object>> allAvailableMaybe = Maybe.defer(() -> {\n+         if (unavailableExceptionMessage != null) {\n+            unavailableExceptionMessage = null;\n+            return Maybe.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(true)\n+               .thenApply(CompletableFutures.toNullFunction()));\n+         }\n+         return Maybe.empty();\n+      });\n+      return Completable.using(this::acquireReadLock,\n+            ignore -> Flowable.fromIterable(stores.values())\n+                  .flatMapMaybe(storeStatus -> {\n+                     CompletionStage<Boolean> availableStage = storeStatus.store.isAvailable();\n+                     return Maybe.fromCompletionStage(availableStage.thenApply(isAvailable -> {\n+                        synchronized (storeStatus) {\n+                           storeStatus.availability = isAvailable;\n+                        }\n+                        if (!isAvailable) {\n+                           return storeStatus.store();\n+                        }\n+                        return null;\n+                     }));\n+                  }).firstElement()\n+                  // If it is empty that means all stores were available\n+                  .switchIfEmpty(allAvailableMaybe)\n+                  .concatMapCompletable(unavailableStore -> {\n+                     if (unavailableExceptionMessage == null) {\n+                        log.debugf(\"Store %s is unavailable!\", unavailableStore);\n+                        unavailableExceptionMessage = \"Store \" + unavailableStore + \" is unavailable\";\n+                        return Completable.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n+                     }\n+                     return Completable.complete();\n+                  }),\n+            this::releaseReadLock)\n+            .toCompletionStage(null);\n+   }\n+\n+   private NonBlockingStore<?, ?> storeFromConfiguration(StoreConfiguration cfg) {\n+      final Object bareInstance;\n+      if (cfg.segmented() && cfg instanceof AbstractSegmentedStoreConfiguration) {\n+         bareInstance = new ComposedSegmentedLoadWriteStore<>((AbstractSegmentedStoreConfiguration) cfg);\n+      } else {\n+         bareInstance = cacheStoreFactoryRegistry.createInstance(cfg);\n+      }\n+      if (!(bareInstance instanceof NonBlockingStore)) {\n+         // All prior stores implemented at least Lifecycle\n+         return new NonBlockingStoreAdapter<>((Lifecycle) bareInstance);\n+      }\n+      return (NonBlockingStore<?, ?>) bareInstance;\n    }\n \n    @Override\n    @Stop\n    public void stop() {\n-      storesMutex.writeLock().lock();\n-      publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+      long stamp = lock.writeLock();\n       try {\n+         stopAvailabilityTask();\n+         Flowable<NonBlockingStore<?, ?>> flowable = Flowable.fromIterable(stores.keySet());\n          // If needed, clear the persistent store before stopping\n          if (clearOnStop) {\n-            clearAllStoresSync(AccessMode.BOTH, getNextTraceNumber());\n+            flowable = flowable\n+                  .delay(store -> Completable.fromCompletionStage(store.clear()).toFlowable());\n          }\n+         flowable = flowable.delay(store -> Completable.fromCompletionStage(store.stop()).toFlowable());\n \n-         Set<Lifecycle> undelegated = new HashSet<>();\n-         Consumer<CacheWriter> stopWriters = writer -> {\n-            writer.stop();\n-            if (writer instanceof DelegatingCacheWriter) {\n-               CacheWriter actual = undelegate(writer);\n-               actual.stop();\n-               undelegated.add(actual);\n-            } else {\n-               undelegated.add(writer);\n-            }\n-         };\n-         if (availabilityFuture != null)\n-            availabilityFuture.cancel(true);\n-         nonTxWriters.forEach(stopWriters);\n-         nonTxWriters.clear();\n-         txWriters.forEach(stopWriters);\n-         txWriters.clear();\n-\n-         for (CacheLoader l : loaders) {\n-            if (!undelegated.contains(l)) {\n-               l.stop();\n-            }\n-            if (l instanceof DelegatingCacheLoader) {\n-               CacheLoader actual = undelegate(l);\n-               if (!undelegated.contains(actual)) {\n-                  actual.stop();\n-               }\n-            }\n-         }\n-         loaders.clear();\n+         // Wait until it completes\n+         blockingSubscribe(flowable);\n+         stores.clear();\n          preloaded = false;\n       } finally {\n-         publisherSemaphore.release(Integer.MAX_VALUE);\n-         storesMutex.writeLock().unlock();\n+         lock.unlockWrite(stamp);\n       }\n    }\n \n-   @Override\n-   public boolean hasWriter() {\n-      if (!enabled) {\n-         return false;\n-      }\n-      acquireReadLock();\n-      try {\n-         return !nonTxWriters.isEmpty() || !txWriters.isEmpty();\n-      } finally {\n-         releaseReadLock();\n+   private void stopAvailabilityTask() {\n+      AutoCloseable taskToClose = availabilityTask;\n+      if (taskToClose != null) {\n+         try {\n+            taskToClose.close();\n+         } catch (Exception e) {\n+            log.warn(\"There was a problem stopping availability task\", e);\n+         }\n       }\n    }\n \n+   // This here solely to document that we are using a blocking method. This is because the start/stop lifecycle\n+   // methods themselves are blocking but our API is not. This can be removed if lifecycle ever allows for non\n+   // blocking, but don't hold your breath for it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 491}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyMjcxOQ==", "bodyText": "Not that I am aware of. I am guessing we may have this in Infinispan 14 \ud83e\udd23", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427322719", "createdAt": "2020-05-19T13:58:08Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);\n+         throw t;\n+      }\n    }\n \n-   protected void pollStoreAvailability() {\n-      acquireReadLock();\n-      try {\n-         boolean availabilityChanged = false;\n-         boolean failureDetected = false;\n-         for (StoreStatus status : storeStatuses.values()) {\n-            if (status.availabilityChanged())\n-               availabilityChanged = true;\n-            if (availabilityChanged && !status.availability && !failureDetected) {\n-               failureDetected = true;\n-               unavailableException = new StoreUnavailableException(String.format(\"Store %s is unavailable\", status.store));\n-               CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n-            }\n+   private Set<Characteristic> updateCharacteristics(NonBlockingStore store, Set<Characteristic> characteristics,\n+         StoreConfiguration storeConfiguration) {\n+      if (storeConfiguration.ignoreModifications()) {\n+         if (characteristics.contains(Characteristic.WRITE_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.WRITE_ONLY,\n+                  Characteristic.READ_ONLY);\n          }\n-         if (!failureDetected && availabilityChanged) {\n-            unavailableException = null;\n-            CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(true));\n+         characteristics.add(Characteristic.READ_ONLY);\n+      }\n+      if (storeConfiguration.writeOnly()) {\n+         if (characteristics.contains(Characteristic.READ_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.READ_ONLY,\n+                  Characteristic.WRITE_ONLY);\n          }\n-      } finally {\n-         releaseReadLock();\n+         characteristics.add(Characteristic.WRITE_ONLY);\n       }\n+      return characteristics;\n    }\n \n-   /**\n-    * Returns the next trace number identifier, always 0 or higher\n-    */\n-   private static int getNextTraceNumber() {\n-      return asyncExecutionId.getAndUpdate(prev -> Math.max(prev + 1, 0));\n+   protected CompletionStage<Void> pollStoreAvailability() {\n+      if (trace) {\n+         log.trace(\"Polling Store availability\");\n+      }\n+      // This maybe will always be empty - used when all stores are available\n+      Maybe<NonBlockingStore<Object, Object>> allAvailableMaybe = Maybe.defer(() -> {\n+         if (unavailableExceptionMessage != null) {\n+            unavailableExceptionMessage = null;\n+            return Maybe.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(true)\n+               .thenApply(CompletableFutures.toNullFunction()));\n+         }\n+         return Maybe.empty();\n+      });\n+      return Completable.using(this::acquireReadLock,\n+            ignore -> Flowable.fromIterable(stores.values())\n+                  .flatMapMaybe(storeStatus -> {\n+                     CompletionStage<Boolean> availableStage = storeStatus.store.isAvailable();\n+                     return Maybe.fromCompletionStage(availableStage.thenApply(isAvailable -> {\n+                        synchronized (storeStatus) {\n+                           storeStatus.availability = isAvailable;\n+                        }\n+                        if (!isAvailable) {\n+                           return storeStatus.store();\n+                        }\n+                        return null;\n+                     }));\n+                  }).firstElement()\n+                  // If it is empty that means all stores were available\n+                  .switchIfEmpty(allAvailableMaybe)\n+                  .concatMapCompletable(unavailableStore -> {\n+                     if (unavailableExceptionMessage == null) {\n+                        log.debugf(\"Store %s is unavailable!\", unavailableStore);\n+                        unavailableExceptionMessage = \"Store \" + unavailableStore + \" is unavailable\";\n+                        return Completable.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n+                     }\n+                     return Completable.complete();\n+                  }),\n+            this::releaseReadLock)\n+            .toCompletionStage(null);\n+   }\n+\n+   private NonBlockingStore<?, ?> storeFromConfiguration(StoreConfiguration cfg) {\n+      final Object bareInstance;\n+      if (cfg.segmented() && cfg instanceof AbstractSegmentedStoreConfiguration) {\n+         bareInstance = new ComposedSegmentedLoadWriteStore<>((AbstractSegmentedStoreConfiguration) cfg);\n+      } else {\n+         bareInstance = cacheStoreFactoryRegistry.createInstance(cfg);\n+      }\n+      if (!(bareInstance instanceof NonBlockingStore)) {\n+         // All prior stores implemented at least Lifecycle\n+         return new NonBlockingStoreAdapter<>((Lifecycle) bareInstance);\n+      }\n+      return (NonBlockingStore<?, ?>) bareInstance;\n    }\n \n    @Override\n    @Stop\n    public void stop() {\n-      storesMutex.writeLock().lock();\n-      publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+      long stamp = lock.writeLock();\n       try {\n+         stopAvailabilityTask();\n+         Flowable<NonBlockingStore<?, ?>> flowable = Flowable.fromIterable(stores.keySet());\n          // If needed, clear the persistent store before stopping\n          if (clearOnStop) {\n-            clearAllStoresSync(AccessMode.BOTH, getNextTraceNumber());\n+            flowable = flowable\n+                  .delay(store -> Completable.fromCompletionStage(store.clear()).toFlowable());\n          }\n+         flowable = flowable.delay(store -> Completable.fromCompletionStage(store.stop()).toFlowable());\n \n-         Set<Lifecycle> undelegated = new HashSet<>();\n-         Consumer<CacheWriter> stopWriters = writer -> {\n-            writer.stop();\n-            if (writer instanceof DelegatingCacheWriter) {\n-               CacheWriter actual = undelegate(writer);\n-               actual.stop();\n-               undelegated.add(actual);\n-            } else {\n-               undelegated.add(writer);\n-            }\n-         };\n-         if (availabilityFuture != null)\n-            availabilityFuture.cancel(true);\n-         nonTxWriters.forEach(stopWriters);\n-         nonTxWriters.clear();\n-         txWriters.forEach(stopWriters);\n-         txWriters.clear();\n-\n-         for (CacheLoader l : loaders) {\n-            if (!undelegated.contains(l)) {\n-               l.stop();\n-            }\n-            if (l instanceof DelegatingCacheLoader) {\n-               CacheLoader actual = undelegate(l);\n-               if (!undelegated.contains(actual)) {\n-                  actual.stop();\n-               }\n-            }\n-         }\n-         loaders.clear();\n+         // Wait until it completes\n+         blockingSubscribe(flowable);\n+         stores.clear();\n          preloaded = false;\n       } finally {\n-         publisherSemaphore.release(Integer.MAX_VALUE);\n-         storesMutex.writeLock().unlock();\n+         lock.unlockWrite(stamp);\n       }\n    }\n \n-   @Override\n-   public boolean hasWriter() {\n-      if (!enabled) {\n-         return false;\n-      }\n-      acquireReadLock();\n-      try {\n-         return !nonTxWriters.isEmpty() || !txWriters.isEmpty();\n-      } finally {\n-         releaseReadLock();\n+   private void stopAvailabilityTask() {\n+      AutoCloseable taskToClose = availabilityTask;\n+      if (taskToClose != null) {\n+         try {\n+            taskToClose.close();\n+         } catch (Exception e) {\n+            log.warn(\"There was a problem stopping availability task\", e);\n+         }\n       }\n    }\n \n+   // This here solely to document that we are using a blocking method. This is because the start/stop lifecycle\n+   // methods themselves are blocking but our API is not. This can be removed if lifecycle ever allows for non\n+   // blocking, but don't hold your breath for it.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyODEyMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 491}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM5NzA0NQ==", "bodyText": "That sounds overly optimistic \ud83d\ude03", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427397045", "createdAt": "2020-05-19T15:32:29Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -74,255 +61,286 @@\n import org.infinispan.metadata.impl.InternalMetadataImpl;\n import org.infinispan.notifications.cachelistener.CacheNotifier;\n import org.infinispan.persistence.InitializationContextImpl;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheLoader;\n-import org.infinispan.persistence.async.AdvancedAsyncCacheWriter;\n-import org.infinispan.persistence.async.AsyncCacheLoader;\n-import org.infinispan.persistence.async.AsyncCacheWriter;\n-import org.infinispan.persistence.async.State;\n+import org.infinispan.persistence.async.AsyncNonBlockingStore;\n import org.infinispan.persistence.factory.CacheStoreFactoryRegistry;\n-import org.infinispan.persistence.internal.PersistenceUtil;\n-import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n-import org.infinispan.persistence.spi.AdvancedCacheLoader;\n-import org.infinispan.persistence.spi.AdvancedCacheWriter;\n-import org.infinispan.persistence.spi.CacheLoader;\n-import org.infinispan.persistence.spi.CacheWriter;\n-import org.infinispan.persistence.spi.FlagAffectedStore;\n import org.infinispan.persistence.spi.LocalOnlyCacheLoader;\n import org.infinispan.persistence.spi.MarshallableEntry;\n import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.NonBlockingStore.Characteristic;\n import org.infinispan.persistence.spi.PersistenceException;\n-import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n import org.infinispan.persistence.spi.StoreUnavailableException;\n-import org.infinispan.persistence.spi.TransactionalCacheWriter;\n import org.infinispan.persistence.support.BatchModification;\n import org.infinispan.persistence.support.ComposedSegmentedLoadWriteStore;\n-import org.infinispan.persistence.support.DelegatingCacheLoader;\n-import org.infinispan.persistence.support.DelegatingCacheWriter;\n-import org.infinispan.remoting.transport.Transport;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.util.concurrent.AggregateCompletionStage;\n import org.infinispan.util.concurrent.BlockingManager;\n import org.infinispan.util.concurrent.CompletableFutures;\n import org.infinispan.util.concurrent.CompletionStages;\n-import org.infinispan.util.concurrent.WithinThreadExecutor;\n+import org.infinispan.util.concurrent.NonBlockingManager;\n import org.infinispan.util.logging.Log;\n import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n \n import io.reactivex.rxjava3.core.Completable;\n import io.reactivex.rxjava3.core.Flowable;\n import io.reactivex.rxjava3.core.Maybe;\n import io.reactivex.rxjava3.core.Single;\n-import io.reactivex.rxjava3.functions.Supplier;\n import net.jcip.annotations.GuardedBy;\n \n @Scope(Scopes.NAMED_CACHE)\n public class PersistenceManagerImpl implements PersistenceManager {\n \n-   private static final Log log = LogFactory.getLog(PersistenceManagerImpl.class);\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n    private static final boolean trace = log.isTraceEnabled();\n-   private static final AtomicInteger asyncExecutionId = new AtomicInteger();\n \n    @Inject Configuration configuration;\n    @Inject GlobalConfiguration globalConfiguration;\n    @Inject ComponentRef<AdvancedCache<Object, Object>> cache;\n-   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n-   PersistenceMarshaller m;\n-   @Inject TransactionManager transactionManager;\n+   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n+   @Inject KeyPartitioner keyPartitioner;\n    @Inject TimeService timeService;\n-   @Inject @ComponentName(EXPIRATION_SCHEDULED_EXECUTOR)\n-   ScheduledExecutorService scheduledExecutor;\n+   @Inject TransactionManager transactionManager;\n+   @Inject @ComponentName(KnownComponentNames.PERSISTENCE_MARSHALLER)\n+   PersistenceMarshaller persistenceMarshaller;\n    @Inject ByteBufferFactory byteBufferFactory;\n+   @Inject CacheNotifier<Object, Object> cacheNotifier;\n    @Inject MarshallableEntryFactory marshallableEntryFactory;\n-   @Inject CacheStoreFactoryRegistry cacheStoreFactoryRegistry;\n-   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n-   @Inject CacheNotifier cacheNotifier;\n-   @Inject KeyPartitioner keyPartitioner;\n-   @Inject Transport transport;\n-   @Inject BlockingManager handler;\n-   @Inject ComponentRef<InvocationHelper> invocationHelper;\n    @Inject ComponentRef<CommandsFactory> commandsFactory;\n+   @ComponentName(KnownComponentNames.NON_BLOCKING_EXECUTOR)\n+   @Inject Executor nonBlockingExecutor;\n+   @Inject BlockingManager blockingManager;\n+   @Inject NonBlockingManager nonBlockingManager;\n+   @Inject ComponentRef<InvocationHelper> invocationHelper;\n+   @Inject ComponentRef<InternalExpirationManager<Object, Object>> expirationManager;\n \n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheLoader> loaders = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<CacheWriter> nonTxWriters = new ArrayList<>();\n-   @GuardedBy(\"storesMutex\")\n-   private final List<TransactionalCacheWriter> txWriters = new ArrayList<>();\n-   private final Semaphore publisherSemaphore = new Semaphore(Integer.MAX_VALUE);\n-   private final ReadWriteLock storesMutex = new ReentrantReadWriteLock();\n-   @GuardedBy(\"storesMutex\")\n-   private final Map<Object, StoreStatus> storeStatuses = new HashMap<>();\n-   private AdvancedPurgeListener<Object, Object> advancedListener;\n-   private final Supplier<Semaphore> publisherSemaphoreCallable = () -> {\n-      publisherSemaphore.acquire();\n-      return publisherSemaphore;\n-   };\n-\n-   /**\n-    * making it volatile as it might change after @Start, so it needs the visibility.\n-    */\n+   // We use stamped lock since we require releasing locks in threads that may be the same that acquired it\n+   private final StampedLock lock = new StampedLock();\n+   // making it volatile as it might change after @Start, so it needs the visibility.\n    private volatile boolean enabled;\n+   private volatile boolean preloaded;\n    private volatile boolean clearOnStop;\n-   private volatile boolean readOnly;\n-   private boolean preloaded;\n-   private Future availabilityFuture;\n-   private volatile StoreUnavailableException unavailableException;\n+   private volatile AutoCloseable availabilityTask;\n+   private volatile String unavailableExceptionMessage;\n+\n+   private int segmentCount;\n+\n+   @GuardedBy(\"lock\")\n+   private final Map<NonBlockingStore<?, ?>, StoreStatus> stores = new LinkedHashMap<>();\n+\n+   private <K, V> NonBlockingStore<K, V> getStore(Predicate<StoreStatus> predicate) {\n+      // We almost always will be doing reads, so optimistic should be faster\n+      // Writes are only done during startup, shutdown and if removing a store\n+      long stamp = lock.tryOptimisticRead();\n+      NonBlockingStore<K, V> store = getStoreLocked(predicate);\n+      if (!lock.validate(stamp)) {\n+         stamp = acquireReadLock();\n+         try {\n+            store = getStoreLocked(predicate);\n+         } finally {\n+            releaseReadLock(stamp);\n+         }\n+      }\n+      return store;\n+   }\n+\n+   @GuardedBy(\"lock#readLock\")\n+   private <K, V> NonBlockingStore<K, V> getStoreLocked(Predicate<StoreStatus> predicate) {\n+      for (Map.Entry<NonBlockingStore<?, ?>, StoreStatus> entry : stores.entrySet()) {\n+         if (predicate.test(entry.getValue())) {\n+            return (NonBlockingStore<K, V>) entry.getKey();\n+         }\n+      }\n+      return null;\n+   }\n \n    @Override\n-   @Start()\n+   @Start\n    public void start() {\n-      advancedListener = new AdvancedPurgeListener<>(expirationManager.wired());\n-      preloaded = false;\n       enabled = configuration.persistence().usingStores();\n       if (!enabled)\n          return;\n+\n+      preloaded = false;\n+      segmentCount = configuration.clustering().hash().numSegments();\n+\n+      long stamp = lock.writeLock();\n       try {\n-         createLoadersAndWriters();\n-         Transaction xaTx = null;\n-         if (transactionManager != null) {\n-            xaTx = transactionManager.suspend();\n-         }\n-         storesMutex.writeLock().lock();\n-         try {\n-            Set<Lifecycle> undelegated = new HashSet<>();\n-            nonTxWriters.forEach(w -> startWriter(w, undelegated));\n-            txWriters.forEach(w -> startWriter(w, undelegated));\n-            loaders.forEach(l -> startLoader(l, undelegated));\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            // Ensure that after writers and loaders have started, they are classified as available by their isAvailable impl\n-            pollStoreAvailability();\n-\n-            // Now schedule the availability check\n-            long interval = configuration.persistence().availabilityInterval();\n-            if (interval > 0)\n-               availabilityFuture = scheduledExecutor.scheduleAtFixedRate(this::pollStoreAvailability, interval, interval, TimeUnit.MILLISECONDS);\n-         } finally {\n-            if (xaTx != null) {\n-               transactionManager.resume(xaTx);\n-            }\n-            storesMutex.writeLock().unlock();\n+         Completable storeStartup = Flowable.fromIterable(configuration.persistence().stores())\n+               // We have to ensure stores are started in configured order to ensure the stores map retains that order\n+               .concatMapSingle(storeConfiguration -> {\n+                  NonBlockingStore<?, ?> actualStore = storeFromConfiguration(storeConfiguration);\n+                  NonBlockingStore<?, ?> nonBlockingStore;\n+                  if (storeConfiguration.async().enabled()) {\n+                     nonBlockingStore = new AsyncNonBlockingStore<>(actualStore);\n+                  } else {\n+                     nonBlockingStore = actualStore;\n+                  }\n+                  StoreConfiguration processedConfiguration = cacheStoreFactoryRegistry.processStoreConfiguration(storeConfiguration);\n+                  InitializationContextImpl ctx =\n+                        new InitializationContextImpl(processedConfiguration, cache.wired(), keyPartitioner, persistenceMarshaller,\n+                              timeService, byteBufferFactory, marshallableEntryFactory, nonBlockingExecutor, globalConfiguration, blockingManager);\n+                  CompletionStage<Void> stage = nonBlockingStore.start(ctx);\n+                  return Completable.fromCompletionStage(stage)\n+                        .toSingle(() -> new StoreStatus(nonBlockingStore, processedConfiguration,\n+                              updateCharacteristics(nonBlockingStore, nonBlockingStore.characteristics(), storeConfiguration)));\n+               })\n+               // This relies upon visibility guarnatees of reactive streams for publishing map values\n+               .doOnNext(status -> stores.put(status.store, status))\n+               .delay(status -> {\n+                  if (status.config.purgeOnStartup()) {\n+                     return Flowable.fromCompletable(Completable.fromCompletionStage(status.store.clear()));\n+                  }\n+                  return Flowable.empty();\n+               })\n+               .ignoreElements();\n+\n+         long interval = configuration.persistence().availabilityInterval();\n+         if (interval > 0) {\n+            storeStartup = storeStartup.doOnComplete(() ->\n+               availabilityTask = nonBlockingManager.scheduleWithFixedDelay(this::pollStoreAvailability, interval, interval, MILLISECONDS));\n          }\n-      } catch (Exception e) {\n-         throw new CacheException(\"Unable to start cache loaders\", e);\n-      }\n-   }\n \n-   /**\n-    * Returns how many publisher invocations are currently active.\n-    * @return count of active publisher instances\n-    */\n-   public int activePublisherInvocations() {\n-      return Integer.MAX_VALUE - publisherSemaphore.availablePermits();\n+         storeStartup.doOnComplete(() -> lock.unlockWrite(stamp))\n+               // Blocks here waiting for stores and availability task to start if needed\n+               .blockingAwait();\n+      } catch (Throwable t) {\n+         lock.unlockWrite(stamp);\n+         throw t;\n+      }\n    }\n \n-   protected void pollStoreAvailability() {\n-      acquireReadLock();\n-      try {\n-         boolean availabilityChanged = false;\n-         boolean failureDetected = false;\n-         for (StoreStatus status : storeStatuses.values()) {\n-            if (status.availabilityChanged())\n-               availabilityChanged = true;\n-            if (availabilityChanged && !status.availability && !failureDetected) {\n-               failureDetected = true;\n-               unavailableException = new StoreUnavailableException(String.format(\"Store %s is unavailable\", status.store));\n-               CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n-            }\n+   private Set<Characteristic> updateCharacteristics(NonBlockingStore store, Set<Characteristic> characteristics,\n+         StoreConfiguration storeConfiguration) {\n+      if (storeConfiguration.ignoreModifications()) {\n+         if (characteristics.contains(Characteristic.WRITE_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.WRITE_ONLY,\n+                  Characteristic.READ_ONLY);\n          }\n-         if (!failureDetected && availabilityChanged) {\n-            unavailableException = null;\n-            CompletionStages.join(cacheNotifier.notifyPersistenceAvailabilityChanged(true));\n+         characteristics.add(Characteristic.READ_ONLY);\n+      }\n+      if (storeConfiguration.writeOnly()) {\n+         if (characteristics.contains(Characteristic.READ_ONLY)) {\n+            throw log.storeConfiguredHasBothReadAndWriteOnly(store.getClass().getName(), Characteristic.READ_ONLY,\n+                  Characteristic.WRITE_ONLY);\n          }\n-      } finally {\n-         releaseReadLock();\n+         characteristics.add(Characteristic.WRITE_ONLY);\n       }\n+      return characteristics;\n    }\n \n-   /**\n-    * Returns the next trace number identifier, always 0 or higher\n-    */\n-   private static int getNextTraceNumber() {\n-      return asyncExecutionId.getAndUpdate(prev -> Math.max(prev + 1, 0));\n+   protected CompletionStage<Void> pollStoreAvailability() {\n+      if (trace) {\n+         log.trace(\"Polling Store availability\");\n+      }\n+      // This maybe will always be empty - used when all stores are available\n+      Maybe<NonBlockingStore<Object, Object>> allAvailableMaybe = Maybe.defer(() -> {\n+         if (unavailableExceptionMessage != null) {\n+            unavailableExceptionMessage = null;\n+            return Maybe.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(true)\n+               .thenApply(CompletableFutures.toNullFunction()));\n+         }\n+         return Maybe.empty();\n+      });\n+      return Completable.using(this::acquireReadLock,\n+            ignore -> Flowable.fromIterable(stores.values())\n+                  .flatMapMaybe(storeStatus -> {\n+                     CompletionStage<Boolean> availableStage = storeStatus.store.isAvailable();\n+                     return Maybe.fromCompletionStage(availableStage.thenApply(isAvailable -> {\n+                        synchronized (storeStatus) {\n+                           storeStatus.availability = isAvailable;\n+                        }\n+                        if (!isAvailable) {\n+                           return storeStatus.store();\n+                        }\n+                        return null;\n+                     }));\n+                  }).firstElement()\n+                  // If it is empty that means all stores were available\n+                  .switchIfEmpty(allAvailableMaybe)\n+                  .concatMapCompletable(unavailableStore -> {\n+                     if (unavailableExceptionMessage == null) {\n+                        log.debugf(\"Store %s is unavailable!\", unavailableStore);\n+                        unavailableExceptionMessage = \"Store \" + unavailableStore + \" is unavailable\";\n+                        return Completable.fromCompletionStage(cacheNotifier.notifyPersistenceAvailabilityChanged(false));\n+                     }\n+                     return Completable.complete();\n+                  }),\n+            this::releaseReadLock)\n+            .toCompletionStage(null);\n+   }\n+\n+   private NonBlockingStore<?, ?> storeFromConfiguration(StoreConfiguration cfg) {\n+      final Object bareInstance;\n+      if (cfg.segmented() && cfg instanceof AbstractSegmentedStoreConfiguration) {\n+         bareInstance = new ComposedSegmentedLoadWriteStore<>((AbstractSegmentedStoreConfiguration) cfg);\n+      } else {\n+         bareInstance = cacheStoreFactoryRegistry.createInstance(cfg);\n+      }\n+      if (!(bareInstance instanceof NonBlockingStore)) {\n+         // All prior stores implemented at least Lifecycle\n+         return new NonBlockingStoreAdapter<>((Lifecycle) bareInstance);\n+      }\n+      return (NonBlockingStore<?, ?>) bareInstance;\n    }\n \n    @Override\n    @Stop\n    public void stop() {\n-      storesMutex.writeLock().lock();\n-      publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+      long stamp = lock.writeLock();\n       try {\n+         stopAvailabilityTask();\n+         Flowable<NonBlockingStore<?, ?>> flowable = Flowable.fromIterable(stores.keySet());\n          // If needed, clear the persistent store before stopping\n          if (clearOnStop) {\n-            clearAllStoresSync(AccessMode.BOTH, getNextTraceNumber());\n+            flowable = flowable\n+                  .delay(store -> Completable.fromCompletionStage(store.clear()).toFlowable());\n          }\n+         flowable = flowable.delay(store -> Completable.fromCompletionStage(store.stop()).toFlowable());\n \n-         Set<Lifecycle> undelegated = new HashSet<>();\n-         Consumer<CacheWriter> stopWriters = writer -> {\n-            writer.stop();\n-            if (writer instanceof DelegatingCacheWriter) {\n-               CacheWriter actual = undelegate(writer);\n-               actual.stop();\n-               undelegated.add(actual);\n-            } else {\n-               undelegated.add(writer);\n-            }\n-         };\n-         if (availabilityFuture != null)\n-            availabilityFuture.cancel(true);\n-         nonTxWriters.forEach(stopWriters);\n-         nonTxWriters.clear();\n-         txWriters.forEach(stopWriters);\n-         txWriters.clear();\n-\n-         for (CacheLoader l : loaders) {\n-            if (!undelegated.contains(l)) {\n-               l.stop();\n-            }\n-            if (l instanceof DelegatingCacheLoader) {\n-               CacheLoader actual = undelegate(l);\n-               if (!undelegated.contains(actual)) {\n-                  actual.stop();\n-               }\n-            }\n-         }\n-         loaders.clear();\n+         // Wait until it completes\n+         blockingSubscribe(flowable);\n+         stores.clear();\n          preloaded = false;\n       } finally {\n-         publisherSemaphore.release(Integer.MAX_VALUE);\n-         storesMutex.writeLock().unlock();\n+         lock.unlockWrite(stamp);\n       }\n    }\n \n-   @Override\n-   public boolean hasWriter() {\n-      if (!enabled) {\n-         return false;\n-      }\n-      acquireReadLock();\n-      try {\n-         return !nonTxWriters.isEmpty() || !txWriters.isEmpty();\n-      } finally {\n-         releaseReadLock();\n+   private void stopAvailabilityTask() {\n+      AutoCloseable taskToClose = availabilityTask;\n+      if (taskToClose != null) {\n+         try {\n+            taskToClose.close();\n+         } catch (Exception e) {\n+            log.warn(\"There was a problem stopping availability task\", e);\n+         }\n       }\n    }\n \n+   // This here solely to document that we are using a blocking method. This is because the start/stop lifecycle\n+   // methods themselves are blocking but our API is not. This can be removed if lifecycle ever allows for non\n+   // blocking, but don't hold your breath for it.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyODEyMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 491}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDgyMDg0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMToyOTo0OFrOGXcAvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo1ODozN1rOGXhuxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyOTM3Mg==", "bodyText": "Nitpick: s/storeStatus/store and it will easily fit on one line.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427229372", "createdAt": "2020-05-19T11:29:48Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -332,39 +350,26 @@ public boolean isPreloaded() {\n \n    @Override\n    public CompletionStage<Void> preload() {\n-      if (!enabled)\n-         return CompletableFutures.completedNull();\n-\n-      AdvancedCacheLoader<Object, Object> preloadCl = null;\n-\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            if (getStoreConfig(l).preload()) {\n-               if (!(l instanceof AdvancedCacheLoader)) {\n-                  throw new PersistenceException(\"Cannot preload from cache loader '\" + l.getClass().getName()\n-                        + \"' as it doesn't implement '\" + AdvancedCacheLoader.class.getName() + \"'\");\n-               }\n-               preloadCl = (AdvancedCacheLoader) l;\n-               if (preloadCl instanceof AdvancedAsyncCacheLoader)\n-                  preloadCl = (AdvancedCacheLoader) ((AdvancedAsyncCacheLoader) preloadCl).undelegate();\n-               break;\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      if (preloadCl == null) {\n+      long stamp = acquireReadLock();\n+      NonBlockingStore<Object, Object> nonBlockingStore = getStoreLocked(storeStatus ->\n+            storeStatus.config.preload());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 552}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyMzA3OA==", "bodyText": "Sure, I will just change to status.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427323078", "createdAt": "2020-05-19T13:58:37Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -332,39 +350,26 @@ public boolean isPreloaded() {\n \n    @Override\n    public CompletionStage<Void> preload() {\n-      if (!enabled)\n-         return CompletableFutures.completedNull();\n-\n-      AdvancedCacheLoader<Object, Object> preloadCl = null;\n-\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            if (getStoreConfig(l).preload()) {\n-               if (!(l instanceof AdvancedCacheLoader)) {\n-                  throw new PersistenceException(\"Cannot preload from cache loader '\" + l.getClass().getName()\n-                        + \"' as it doesn't implement '\" + AdvancedCacheLoader.class.getName() + \"'\");\n-               }\n-               preloadCl = (AdvancedCacheLoader) l;\n-               if (preloadCl instanceof AdvancedAsyncCacheLoader)\n-                  preloadCl = (AdvancedCacheLoader) ((AdvancedAsyncCacheLoader) preloadCl).undelegate();\n-               break;\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      if (preloadCl == null) {\n+      long stamp = acquireReadLock();\n+      NonBlockingStore<Object, Object> nonBlockingStore = getStoreLocked(storeStatus ->\n+            storeStatus.config.preload());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyOTM3Mg=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 552}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDgzNTcxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTozMzo1N1rOGXcJvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTozMzo1N1rOGXcJvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzMTY3OQ==", "bodyText": "Additional whitespace", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427231679", "createdAt": "2020-05-19T11:33:57Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 761}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDg0Nzg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTozNzoxMFrOGXcQ6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTozNzoxMFrOGXcQ6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzMzUxMw==", "bodyText": "MarshallableEntry<Object, Object>?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427233513", "createdAt": "2020-05-19T11:37:10Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 584}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDg1NDc2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTozOToxMVrOGXcVSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowMToyOVrOGXh3Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNDYzMg==", "bodyText": "Is preloadEntry more accurate as we're executing putKeyValueCommand?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427234632", "createdAt": "2020-05-19T11:39:11Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 584}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyNTI0Mg==", "bodyText": "Sure. This was just the prior name.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427325242", "createdAt": "2020-05-19T14:01:29Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNDYzMg=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 584}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDg3NTA1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo0NToyM1rOGXciDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowNjowM1rOGXiEqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNzkwMQ==", "bodyText": "I don't understand why the <suspend|begin|resume|commit>IfNeeded methods are required?\nconfiguration.transaction().transactionMode().isTransactional() && transactionManager != null is always  true when these methods are called. Can't we just execute the transactionManager. methods inline?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427237901", "createdAt": "2020-05-19T11:45:23Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 610}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyNzI1OQ==", "bodyText": "Sadly this is how it was before. TBH I don't know and I didn't want to mess with this code :D\nI can make a JIRA to reevaluate this though.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427327259", "createdAt": "2020-05-19T14:04:06Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNzkwMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 610}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyODE2NA==", "bodyText": "It might be that it needed the blocking executor when transactions were blocking. We might be able to remove it now that I made that \"non blocking\" for the caller when in a tx.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427328164", "createdAt": "2020-05-19T14:05:18Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNzkwMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 610}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyODY4MA==", "bodyText": "I created https://issues.redhat.com/browse/ISPN-11858", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427328680", "createdAt": "2020-05-19T14:06:03Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzNzkwMQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 610}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDg4MzQzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo0Nzo1M1rOGXcnEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo0Nzo1M1rOGXcnEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIzOTE4NA==", "bodyText": "It would be could to suppress the \"unchecked\" warning here", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427239184", "createdAt": "2020-05-19T11:47:53Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 644}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDkwMjk4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo1MzoxNVrOGXczJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDoxMjozNFrOGXiXMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0MjI3Ng==", "bodyText": "Can we ever reach this state? or persistenceWithoutCacheWriteInterceptor?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427242276", "createdAt": "2020-05-19T11:53:15Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 850}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzMzExMg==", "bodyText": "I have no clue tbh, this is just what we had before https://github.com/infinispan/infinispan/blob/master/core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java#L406", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427333112", "createdAt": "2020-05-19T14:12:09Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0MjI3Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 850}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzMzQyNA==", "bodyText": "I would be very surprised that you could.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427333424", "createdAt": "2020-05-19T14:12:34Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0MjI3Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 850}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDkxMDg1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo1NToyOFrOGXc34Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMTo1NToyOFrOGXc34Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0MzQ4OQ==", "bodyText": "storeClass::isInstance", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427243489", "createdAt": "2020-05-19T11:55:28Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 912}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDkzNjY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjowMzoyOVrOGXdIpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDoxMzo1N1rOGXibOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0Nzc4MQ==", "bodyText": "Shouldn't the WRITE_ONLY check be part of the allowLoad method?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427247781", "createdAt": "2020-05-19T12:03:29Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzNDQ1OQ==", "bodyText": "Sounds good to me.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427334459", "createdAt": "2020-05-19T14:13:57Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0Nzc4MQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDk0NDEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjowNToyOVrOGXdNXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo1NDowMFrOGXnLXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0ODk4OQ==", "bodyText": "The only implementation of LocalOnlyCacheLoader is ClusterLoader ... when is that ever used?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427248989", "createdAt": "2020-05-19T12:05:29Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzNjc2OA==", "bodyText": "I forget the use case of ClusterLoader, I have a feeling it was something with invalidation caches. @danberindei might remember. I would like to remove this if possible, but I don't think we can yet. I think another JIRA or discussion would be required first.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427336768", "createdAt": "2020-05-19T14:16:50Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0ODk4OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQxMjMxOQ==", "bodyText": "The decision is to deprecate and then remove in 14.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427412319", "createdAt": "2020-05-19T15:54:00Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI0ODk4OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDk2NzgxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoxMjoyNVrOGXdcjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTozNDowMVrOGYRtCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1Mjg3Nw==", "bodyText": "Should we be testing against shouldWrite here? Or should ignoreCommandWithFlags be ignored for transactions?\nIf so, commitAllTxStores and rollbackAllTxStores also needs updating.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427252877", "createdAt": "2020-05-19T12:12:25Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1460}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwNTYwNw==", "bodyText": "I had debated about transactions. This whole thing still rubs me the wrong way. I really don't like it.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427305607", "createdAt": "2020-05-19T13:33:52Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1Mjg3Nw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1460}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzODk4Nw==", "bodyText": "It being the way store writes are skipped with rolling upgrade and remote store.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427338987", "createdAt": "2020-05-19T14:19:43Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1Mjg3Nw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1460}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEwOTA2NA==", "bodyText": "This will be further handled in https://issues.redhat.com/browse/ISPN-11859", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428109064", "createdAt": "2020-05-20T15:34:01Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1Mjg3Nw=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1460}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDk3Nzg0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoxNToxMVrOGXdixQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDoyNTowOFrOGXjA8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1NDQ2OQ==", "bodyText": "shouldWrite?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427254469", "createdAt": "2020-05-19T12:15:11Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the prepare work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().prepareWithModifications(transaction, batchModification)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> commitAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.commit(transaction), traceId),\n-            \"Committing tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Committing transaction %s to stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the commit work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> rollbackAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.rollback(transaction), traceId),\n-            \"Rolling back tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Rolling back transaction %s for stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration = getStoreConfig(l);\n-            if (predicate.test(storeConfiguration) && l instanceof AdvancedCacheLoader) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((AdvancedCacheLoader) l).size();\n-               }, \"Retrieving size with predicate for id %d\");\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return CompletableFuture.completedFuture(-1);\n+   public <K, V> CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry<K, V>> entries, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<MarshallableEntry<K, V>>> flowable = Flowable.fromIterable(entries)\n+            .groupBy(me -> keyPartitioner.getSegment(me.getKey()))\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Writing batch to stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).bulkWrite(segmentCount, flowable)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(IntSet segments) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  ((storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented())) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((SegmentedAdvancedLoadWriteStore) l).size(segments);\n-               }, \"Retrieving size with segments for id %d\");\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Calculating size of store via publisher for segments %s\", segments);\n-         }\n-\n-         return Flowable.fromPublisher(publishKeys(segments, null, AccessMode.BOTH))\n-               .count()\n-               .map(count -> {\n-                  long longValue = count;\n-                  if (longValue > Integer.MAX_VALUE) {\n-                     return Integer.MAX_VALUE;\n-                  }\n-                  return (int) longValue;\n-               })\n-               .toCompletionStage();\n-\n-      } finally {\n-         releaseReadLock();\n-      }\n+   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<Object>> flowable = Flowable.fromIterable(keys)\n+            .groupBy(keyPartitioner::getSegment)\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Deleting batch of entries from stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1602}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM0MTcyNQ==", "bodyText": "Technically we are doing the same thing as the old PersistenceManagerImpl as the flags are only looked at for the single value write. I think I will leave it this way and create a JIRA. Because I would really like to figure out some other way to handle this rolling upgrade case, but I don't know how yet.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427341725", "createdAt": "2020-05-19T14:22:55Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the prepare work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().prepareWithModifications(transaction, batchModification)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> commitAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.commit(transaction), traceId),\n-            \"Committing tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Committing transaction %s to stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the commit work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> rollbackAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.rollback(transaction), traceId),\n-            \"Rolling back tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Rolling back transaction %s for stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration = getStoreConfig(l);\n-            if (predicate.test(storeConfiguration) && l instanceof AdvancedCacheLoader) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((AdvancedCacheLoader) l).size();\n-               }, \"Retrieving size with predicate for id %d\");\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return CompletableFuture.completedFuture(-1);\n+   public <K, V> CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry<K, V>> entries, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<MarshallableEntry<K, V>>> flowable = Flowable.fromIterable(entries)\n+            .groupBy(me -> keyPartitioner.getSegment(me.getKey()))\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Writing batch to stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).bulkWrite(segmentCount, flowable)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(IntSet segments) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  ((storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented())) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((SegmentedAdvancedLoadWriteStore) l).size(segments);\n-               }, \"Retrieving size with segments for id %d\");\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Calculating size of store via publisher for segments %s\", segments);\n-         }\n-\n-         return Flowable.fromPublisher(publishKeys(segments, null, AccessMode.BOTH))\n-               .count()\n-               .map(count -> {\n-                  long longValue = count;\n-                  if (longValue > Integer.MAX_VALUE) {\n-                     return Integer.MAX_VALUE;\n-                  }\n-                  return (int) longValue;\n-               })\n-               .toCompletionStage();\n-\n-      } finally {\n-         releaseReadLock();\n-      }\n+   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<Object>> flowable = Flowable.fromIterable(keys)\n+            .groupBy(keyPartitioner::getSegment)\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Deleting batch of entries from stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1NDQ2OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1602}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM0NDExNA==", "bodyText": "I created https://issues.redhat.com/browse/ISPN-11859", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427344114", "createdAt": "2020-05-19T14:25:08Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<?> preloadKey(long flags, MarshallableEntry me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore  = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(store -> storeClass.isInstance(store))\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.WRITE_ONLY)\n+                                 && allowLoad(entry.getValue(), localInvocation, includeStores))\n+                     // Only do 1 request at a time\n+                     .concatMapMaybe(entry -> Maybe.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).load(segment, key)), 1)\n+                     .firstElement();\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n+   private boolean allowLoad(StoreStatus storeStatus, boolean localInvocation, boolean includeStores) {\n+      return (localInvocation || !isLocalOnlyLoader(storeStatus.store)) &&\n+            (includeStores || storeStatus.characteristics.contains(Characteristic.READ_ONLY) || storeStatus.config.ignoreModifications());\n+   }\n+\n+   private boolean isLocalOnlyLoader(NonBlockingStore store) {\n+      if (store instanceof LocalOnlyCacheLoader) return true;\n+      NonBlockingStore unwrappedStore;\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         unwrappedStore = ((DelegatingNonBlockingStore) store).delegate();\n+      } else {\n+         unwrappedStore = store;\n       }\n+      if (unwrappedStore instanceof LocalOnlyCacheLoader) {\n+         return true;\n+      }\n+      if (unwrappedStore instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter) unwrappedStore).getActualStore() instanceof LocalOnlyCacheLoader;\n+      }\n+      return false;\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, int segment, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(Predicate<? super StoreConfiguration> predicate) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with segment %d includeStores %s for id: %d\",\n-                  key, segment, includeStores, traceId);\n+            log.tracef(\"Obtaining size from stores\");\n          }\n-         MarshallableEntry load = null;\n-         Set<CacheLoader> attemptedLoaders = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores) && l instanceof SegmentedAdvancedLoadWriteStore) {\n-               load = ((SegmentedAdvancedLoadWriteStore) l).get(segment, key);\n-               if (load != null)\n-                  break;\n-               if (attemptedLoaders == null) {\n-                  attemptedLoaders = new HashSet<>(loaders.size());\n-               }\n-               attemptedLoaders.add(l);\n-            }\n-         }\n-         if (load == null) {\n-            for (CacheLoader l : loaders) {\n-               if (allowLoad(l, localInvocation, includeStores) && (attemptedLoaders == null || !attemptedLoaders.contains(l))) {\n-                  load = l.loadEntry(key);\n-                  if (load != null)\n-                     break;\n-               }\n-            }\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+         return nonBlockingStore.size(IntSets.immutableRangeSet(segmentCount))\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment, boolean localInvocation, boolean includeStores) {\n-      return supplyOnPersistenceExAndContinue(traceId -> loadFromAllStoresSync(key, segment, localInvocation, includeStores, traceId),\n-            \"Loading from first store for id %d\");\n-   }\n-\n-   private boolean allowLoad(CacheLoader loader, boolean localInvocation, boolean includeStores) {\n-      return (localInvocation || !isLocalOnlyLoader(loader)) && (includeStores || !(loader instanceof CacheWriter));\n-   }\n-\n-   private boolean isLocalOnlyLoader(CacheLoader loader) {\n-      if (loader instanceof LocalOnlyCacheLoader) return true;\n-      if (loader instanceof DelegatingCacheLoader) {\n-         CacheLoader unwrappedLoader = ((DelegatingCacheLoader) loader).undelegate();\n-         return unwrappedLoader instanceof LocalOnlyCacheLoader;\n-      }\n-      return false;\n-   }\n-\n-   private void writeToAllNonTxStoresSync(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags, int traceId) {\n-      acquireReadLock();\n+   public CompletionStage<Long> size(IntSet segments) {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Writing entry %s for id: %d\", marshalledEntry, traceId);\n+            log.tracef(\"Obtaining size from stores for segments %s\", segments);\n          }\n-         //noinspection unchecked\n-         nonTxWriters.stream()\n-               .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-               .filter(writer -> predicate.test(getStoreConfig(writer)))\n-               .forEach(writer -> {\n-                  if (writer instanceof SegmentedAdvancedLoadWriteStore) {\n-                     ((SegmentedAdvancedLoadWriteStore) writer).write(segment, marshalledEntry);\n-                  } else {\n-                     writer.write(marshalledEntry);\n-                  }\n-               });\n-      } finally {\n-         releaseReadLock();\n+         NonBlockingStore<?, ?> nonBlockingStore = getStoreLocked(storeStatus -> storeStatus.characteristics.contains(\n+               Characteristic.BULK_READ));\n+         if (nonBlockingStore == null) {\n+            releaseReadLock(stamp);\n+            return CompletableFuture.completedFuture(-1L);\n+         }\n+         return nonBlockingStore.size(segments)\n+               .whenComplete((ignore, ignoreT) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n    @Override\n-   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      return runOnPersistenceExAndContinue(traceId -> writeToAllNonTxStoresSync(marshalledEntry, segment, predicate, flags, traceId),\n-            \"Writing to all stores for id %d\");\n+   public void setClearOnStop(boolean clearOnStop) {\n+      this.clearOnStop = clearOnStop;\n    }\n \n    @Override\n-   public CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry> entries,\n+   public CompletionStage<Void> writeToAllNonTxStores(MarshallableEntry marshalledEntry, int segment,\n          Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!entries.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      int id = getNextTraceNumber(\"Submitting persistence async operation of id %d to write a batch\");\n-\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         return Completable.using(publisherSemaphoreCallable,\n-               semaphore -> Flowable.fromIterable(nonTxWriters)\n-                     .filter(writer -> !(writer instanceof FlagAffectedStore) || FlagAffectedStore.class.cast(writer).shouldWrite(flags))\n-                     .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                     .map(writer -> {\n-                        Flowable<MarshallableEntry> flowable = Flowable.fromIterable(entries);\n-                        if (trace) {\n-                           // Note this trace message will be on the persistence thread as it is subscribed below\n-                           flowable = flowable.doOnSubscribe(s -> log.tracef(\"Continuing write batch for id %d\", id));\n-                        }\n-                        // Invoking blockingPublisher here makes this async\n-                        return writer.bulkUpdate(handler.blockingPublisher(flowable));\n-                        // Only prefetch one to limit async subscriptions to a single one\n-                     }).concatMapCompletable(Completable::fromCompletionStage, 1),\n-               Semaphore::release)\n-               .toCompletionStage(null);\n-      } finally {\n-         releaseReadLock();\n-      }\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Writing entry %s for with segment: %d\", marshalledEntry, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the write work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().write(segment, marshalledEntry)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n-   @Override\n-   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys,\n-         Predicate<? super StoreConfiguration> predicate, long flags) {\n-      if (!keys.iterator().hasNext())\n-         return CompletableFutures.completedNull();\n-\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing delete batch for id %d\", traceId);\n-            }\n-            nonTxWriters.stream()\n-                  .filter(writer -> predicate.test(getStoreConfig(writer)))\n-                  .forEach(writer -> writer.deleteBatch(keys));\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Submitting persistence async operation of id %d to write a batch\");\n+   private boolean shouldWrite(StoreStatus storeStatus, Predicate<? super StoreConfiguration> userPredicate, long flags) {\n+      return !storeStatus.characteristics.contains(Characteristic.READ_ONLY)\n+            && userPredicate.test(storeStatus.config)\n+            && !storeStatus.store.ignoreCommandWithFlags(flags);\n    }\n \n    @Override\n    public CompletionStage<Void> prepareAllTxStores(Transaction transaction, BatchModification batchModification,\n          Predicate<? super StoreConfiguration> predicate) throws PersistenceException {\n-      return runOnPersistenceExAndContinue(traceId -> {\n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            if (trace) {\n-               log.tracef(\"Continuing prepare batch for id %d\", traceId);\n-            }\n-            for (CacheWriter writer : txWriters) {\n-               if (predicate.test(getStoreConfig(writer)) || configuration.clustering().cacheMode().equals(CacheMode.LOCAL)) {\n-                  TransactionalCacheWriter txWriter = (TransactionalCacheWriter) undelegate(writer);\n-                  txWriter.prepareWithModifications(transaction, batchModification);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Preparing batch for store: %s on transaction %s\", batchModification, transaction);\n                }\n-            }\n-         } finally {\n-            releaseReadLock();\n-         }\n-      }, \"Preparing all tx stores for id %d\");\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the prepare work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().prepareWithModifications(transaction, batchModification)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> commitAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.commit(transaction), traceId),\n-            \"Committing tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Committing transaction %s to stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the commit work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n    public CompletionStage<Void> rollbackAllTxStores(Transaction transaction, Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> performOnAllTxStores(predicate, writer -> writer.rollback(transaction), traceId),\n-            \"Rolling back tx for all stores for id %d\");\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Rolling back transaction %s for stores\", transaction);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(entry.getKey().commit(transaction)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration = getStoreConfig(l);\n-            if (predicate.test(storeConfiguration) && l instanceof AdvancedCacheLoader) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((AdvancedCacheLoader) l).size();\n-               }, \"Retrieving size with predicate for id %d\");\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return CompletableFuture.completedFuture(-1);\n+   public <K, V> CompletionStage<Void> writeBatchToAllNonTxStores(Iterable<MarshallableEntry<K, V>> entries, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<MarshallableEntry<K, V>>> flowable = Flowable.fromIterable(entries)\n+            .groupBy(me -> keyPartitioner.getSegment(me.getKey()))\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Writing batch to stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry -> shouldWrite(entry.getValue(), predicate, flags))\n+                     // Let the rollback work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           PersistenceManagerImpl.<K, V>storeForEntry(entry).bulkWrite(segmentCount, flowable)));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n    }\n \n    @Override\n-   public CompletionStage<Integer> size(IntSet segments) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  ((storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented())) {\n-               return supplyOnPersistenceExAndContinue(traceId -> {\n-                  if (trace) {\n-                     log.tracef(\"Continuing size operation for id %d\", traceId);\n-                  }\n-                  return ((SegmentedAdvancedLoadWriteStore) l).size(segments);\n-               }, \"Retrieving size with segments for id %d\");\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Calculating size of store via publisher for segments %s\", segments);\n-         }\n-\n-         return Flowable.fromPublisher(publishKeys(segments, null, AccessMode.BOTH))\n-               .count()\n-               .map(count -> {\n-                  long longValue = count;\n-                  if (longValue > Integer.MAX_VALUE) {\n-                     return Integer.MAX_VALUE;\n-                  }\n-                  return (int) longValue;\n-               })\n-               .toCompletionStage();\n-\n-      } finally {\n-         releaseReadLock();\n-      }\n+   public CompletionStage<Void> deleteBatchFromAllNonTxStores(Iterable<Object> keys, Predicate<? super StoreConfiguration> predicate, long flags) {\n+      Flowable<NonBlockingStore.SegmentedPublisher<Object>> flowable = Flowable.fromIterable(keys)\n+            .groupBy(keyPartitioner::getSegment)\n+            .map(SegmentPublisherWrapper::new);\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.trace(\"Deleting batch of entries from stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1NDQ2OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 1602}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MDk4OTk4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoxODo0MFrOGXdqiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzozMTo0M1rOGXgkwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1NjQ1Ng==", "bodyText": "\ud83e\udd2e  ... shows how crazy it was before!", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427256456", "createdAt": "2020-05-19T12:18:40Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwNDEyOQ==", "bodyText": "Yeah, blargh.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427304129", "createdAt": "2020-05-19T13:31:43Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1NjQ1Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTAwMDI0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyMToxOVrOGXdw3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTo1ODozNlrOGXnYNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1ODA3Ng==", "bodyText": "Now that we have Characteristics, I don't think there is any need for org.infinispan.commons.persistence.Store annotation. We should deprecate this so that we can remove it in Infinispan 14.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427258076", "createdAt": "2020-05-19T12:21:19Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwNDAyMg==", "bodyText": "\ud83d\udc4d to deprecate. I thought about but just forgot to do so.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427304022", "createdAt": "2020-05-19T13:31:34Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1ODA3Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQxNTYwNQ==", "bodyText": "https://issues.redhat.com/browse/ISPN-11865\nI'll create a PR", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427415605", "createdAt": "2020-05-19T15:58:36Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1ODA3Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTAwMTY2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyMTo0M1rOGXdxyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyMTo0M1rOGXdxyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1ODMxMw==", "bodyText": "Redundant whitespace.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427258313", "createdAt": "2020-05-19T12:21:43Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);\n+      if (storeAnnotation != null && storeAnnotation.shared()) {\n+         characteristics.add(Characteristic.SHAREABLE);\n+      }\n+\n+      // Transactional is a special interface that could be true on a segment or not segmented store both\n+      if (storeImpl instanceof TransactionalCacheWriter) {\n+         characteristics.add(Characteristic.TRANSACTIONAL);\n+      }\n+      return characteristics;\n+   }\n+\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTAxMTQxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyNDoxOVrOGXd3xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTozMjo1N1rOGYRp9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1OTg0Ng==", "bodyText": "I would say we probably should err on the side of caution and assume that the creation blocks.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427259846", "createdAt": "2020-05-19T12:24:19Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);\n+      if (storeAnnotation != null && storeAnnotation.shared()) {\n+         characteristics.add(Characteristic.SHAREABLE);\n+      }\n+\n+      // Transactional is a special interface that could be true on a segment or not segmented store both\n+      if (storeImpl instanceof TransactionalCacheWriter) {\n+         characteristics.add(Characteristic.TRANSACTIONAL);\n+      }\n+      return characteristics;\n+   }\n+\n+\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      blockingManager = ctx.getBlockingManager();\n+      marshallableEntryFactory = ctx.getMarshallableEntryFactory();\n+      return blockingManager.runBlocking(() -> {\n+         if (isReadOnly()) {\n+            loader().init(ctx);\n+         } else {\n+            writer().init(ctx);\n+         }\n+         oldStoreImpl.start();\n+      }, nextTraceId());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(oldStoreImpl::stop, nextTraceId());\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return characteristics;\n+   }\n+\n+   @Override\n+   public MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return blockingManager.supplyBlocking(() ->\n+            isSegmented() ? segmentedStore().size(segments) : advancedLoader().size(), nextTraceId())\n+            .thenApply(Integer::longValue);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      // Old SPI didn't support approximations\n+      return size(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      Publisher<MarshallableEntry<K, V>> publisher;\n+      if (isSegmented()) {\n+         publisher = segmentedStore().entryPublisher(segments, filter, includeValues, true);\n+      } else {\n+         publisher = advancedLoader().entryPublisher(filter, includeValues, true);\n+      }\n+      // Despite this being a publisher, we assume the subscription is blocking as the SPI never enforced this\n+      // We do however assume the creation of the Publisher is not blocking... maybe we should?\n+      return blockingManager.blockingPublisher(publisher);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM0OTYwMg==", "bodyText": "Yeah I was having a hard time with this one, because it is a lot of context switching to create it on the blocking thread and then subscribe again on the blocking thread. All of our impls do not block for the creation, just the subscription.\nBut I guess a user may not know how to implement Publisher correctly. For example they may acquire the DB connection in this method instead of on subscription.\nBut then again I don't think a user will be implementing the old SPI moving forward, hopefully. I plan on adding a bunch of examples of how to implement the SPI given specific use cases for Final.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427349602", "createdAt": "2020-05-19T14:31:55Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);\n+      if (storeAnnotation != null && storeAnnotation.shared()) {\n+         characteristics.add(Characteristic.SHAREABLE);\n+      }\n+\n+      // Transactional is a special interface that could be true on a segment or not segmented store both\n+      if (storeImpl instanceof TransactionalCacheWriter) {\n+         characteristics.add(Characteristic.TRANSACTIONAL);\n+      }\n+      return characteristics;\n+   }\n+\n+\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      blockingManager = ctx.getBlockingManager();\n+      marshallableEntryFactory = ctx.getMarshallableEntryFactory();\n+      return blockingManager.runBlocking(() -> {\n+         if (isReadOnly()) {\n+            loader().init(ctx);\n+         } else {\n+            writer().init(ctx);\n+         }\n+         oldStoreImpl.start();\n+      }, nextTraceId());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(oldStoreImpl::stop, nextTraceId());\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return characteristics;\n+   }\n+\n+   @Override\n+   public MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return blockingManager.supplyBlocking(() ->\n+            isSegmented() ? segmentedStore().size(segments) : advancedLoader().size(), nextTraceId())\n+            .thenApply(Integer::longValue);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      // Old SPI didn't support approximations\n+      return size(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      Publisher<MarshallableEntry<K, V>> publisher;\n+      if (isSegmented()) {\n+         publisher = segmentedStore().entryPublisher(segments, filter, includeValues, true);\n+      } else {\n+         publisher = advancedLoader().entryPublisher(filter, includeValues, true);\n+      }\n+      // Despite this being a publisher, we assume the subscription is blocking as the SPI never enforced this\n+      // We do however assume the creation of the Publisher is not blocking... maybe we should?\n+      return blockingManager.blockingPublisher(publisher);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1OTg0Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEwODI3Ng==", "bodyText": "I have not changed this btw. I think it is safe enough to assume it isn't blocking for now. And as mentioned this is only for the old SPI.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428108276", "createdAt": "2020-05-20T15:32:57Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);\n+      if (storeAnnotation != null && storeAnnotation.shared()) {\n+         characteristics.add(Characteristic.SHAREABLE);\n+      }\n+\n+      // Transactional is a special interface that could be true on a segment or not segmented store both\n+      if (storeImpl instanceof TransactionalCacheWriter) {\n+         characteristics.add(Characteristic.TRANSACTIONAL);\n+      }\n+      return characteristics;\n+   }\n+\n+\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      blockingManager = ctx.getBlockingManager();\n+      marshallableEntryFactory = ctx.getMarshallableEntryFactory();\n+      return blockingManager.runBlocking(() -> {\n+         if (isReadOnly()) {\n+            loader().init(ctx);\n+         } else {\n+            writer().init(ctx);\n+         }\n+         oldStoreImpl.start();\n+      }, nextTraceId());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(oldStoreImpl::stop, nextTraceId());\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return characteristics;\n+   }\n+\n+   @Override\n+   public MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return blockingManager.supplyBlocking(() ->\n+            isSegmented() ? segmentedStore().size(segments) : advancedLoader().size(), nextTraceId())\n+            .thenApply(Integer::longValue);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      // Old SPI didn't support approximations\n+      return size(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      Publisher<MarshallableEntry<K, V>> publisher;\n+      if (isSegmented()) {\n+         publisher = segmentedStore().entryPublisher(segments, filter, includeValues, true);\n+      } else {\n+         publisher = advancedLoader().entryPublisher(filter, includeValues, true);\n+      }\n+      // Despite this being a publisher, we assume the subscription is blocking as the SPI never enforced this\n+      // We do however assume the creation of the Publisher is not blocking... maybe we should?\n+      return blockingManager.blockingPublisher(publisher);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI1OTg0Ng=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTAxMzI0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyNDo0OVrOGXd48A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjoyNDo0OVrOGXd48A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI2MDE0NA==", "bodyText": "The same as publishEntries", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427260144", "createdAt": "2020-05-19T12:24:49Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.dataconversion.MediaType;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {\n+      return id != null ? \"StoreAdapter-\" + id.getAndIncrement() : null;\n+   }\n+\n+   static private Set<Characteristic> determineCharacteristics(Object storeImpl) {\n+      EnumSet<Characteristic> characteristics;\n+      if (storeImpl instanceof SegmentedAdvancedLoadWriteStore) {\n+          characteristics = EnumSet.of(Characteristic.SEGMENTABLE, Characteristic.EXPIRATION,\n+               Characteristic.BULK_READ);\n+      } else {\n+         characteristics = EnumSet.noneOf(Characteristic.class);\n+         if (storeImpl instanceof AdvancedCacheLoader) {\n+            characteristics.add(Characteristic.BULK_READ);\n+         } else if (!(storeImpl instanceof CacheLoader)) {\n+            characteristics.add(Characteristic.WRITE_ONLY);\n+         }\n+\n+         if (storeImpl instanceof AdvancedCacheWriter) {\n+            characteristics.add(Characteristic.EXPIRATION);\n+         } else if (!(storeImpl instanceof CacheWriter)) {\n+            characteristics.add(Characteristic.READ_ONLY);\n+         }\n+      }\n+\n+      Store storeAnnotation = storeImpl.getClass().getAnnotation(Store.class);\n+      if (storeAnnotation != null && storeAnnotation.shared()) {\n+         characteristics.add(Characteristic.SHAREABLE);\n+      }\n+\n+      // Transactional is a special interface that could be true on a segment or not segmented store both\n+      if (storeImpl instanceof TransactionalCacheWriter) {\n+         characteristics.add(Characteristic.TRANSACTIONAL);\n+      }\n+      return characteristics;\n+   }\n+\n+\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      blockingManager = ctx.getBlockingManager();\n+      marshallableEntryFactory = ctx.getMarshallableEntryFactory();\n+      return blockingManager.runBlocking(() -> {\n+         if (isReadOnly()) {\n+            loader().init(ctx);\n+         } else {\n+            writer().init(ctx);\n+         }\n+         oldStoreImpl.start();\n+      }, nextTraceId());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(oldStoreImpl::stop, nextTraceId());\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return characteristics;\n+   }\n+\n+   @Override\n+   public MediaType getKeyMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public MediaType getValueMediaType(MediaType storageMediaType, Set<MediaType> supportedMediaTypes) {\n+      return MediaType.MATCH_ALL;\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return blockingManager.supplyBlocking(() ->\n+            isSegmented() ? segmentedStore().size(segments) : advancedLoader().size(), nextTraceId())\n+            .thenApply(Integer::longValue);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      // Old SPI didn't support approximations\n+      return size(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      Publisher<MarshallableEntry<K, V>> publisher;\n+      if (isSegmented()) {\n+         publisher = segmentedStore().entryPublisher(segments, filter, includeValues, true);\n+      } else {\n+         publisher = advancedLoader().entryPublisher(filter, includeValues, true);\n+      }\n+      // Despite this being a publisher, we assume the subscription is blocking as the SPI never enforced this\n+      // We do however assume the creation of the Publisher is not blocking... maybe we should?\n+      return blockingManager.blockingPublisher(publisher);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      Publisher<K> publisher;\n+      if (isSegmented()) {\n+         publisher = segmentedStore().publishKeys(segments, filter);\n+      } else {\n+         publisher = advancedLoader().publishKeys(filter);\n+      }\n+      // Despite this being a publisher, we assume the subscription is blocking as the SPI never enforced this\n+      // We do however assume the creation of the Publisher is not blocking... maybe we should?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTA1MzA1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjozNTozMFrOGXeSDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzozMToxN1rOGXgjkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI2NjU3NA==", "bodyText": "Are the JavaDocs coming as part of this PR or the follow up for the SPI docs?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427266574", "createdAt": "2020-05-19T12:35:30Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManager.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+/**\n+ * TODO:\n+ */\n+public interface NonBlockingManager {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMzgyNA==", "bodyText": "I think at this point I would say they might come in the next one. They will contain a lot of stuff to write and would rather get the SPI being tested out in CR1.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427303824", "createdAt": "2020-05-19T13:31:17Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManager.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+/**\n+ * TODO:\n+ */\n+public interface NonBlockingManager {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI2NjU3NA=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTA5MTY4OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo0NToyM1rOGXeqPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDozMjozOFrOGXjYpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Mjc2NQ==", "bodyText": "Can we add this comment as part of the javadoc.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427272765", "createdAt": "2020-05-19T12:45:23Z", "author": {"login": "ryanemerson"}, "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "diffHunk": "@@ -0,0 +1,660 @@\n+package org.infinispan.persistence;\n+\n+import static org.infinispan.test.TestingUtil.allEntries;\n+import static org.testng.AssertJUnit.assertEquals;\n+import static org.testng.AssertJUnit.assertFalse;\n+import static org.testng.AssertJUnit.assertNotNull;\n+import static org.testng.AssertJUnit.assertNull;\n+import static org.testng.AssertJUnit.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import org.infinispan.commons.marshall.Marshaller;\n+import org.infinispan.commons.marshall.ProtoStreamMarshaller;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.marshall.WrappedBytes;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.commons.util.IntSets;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.ConfigurationBuilder;\n+import org.infinispan.container.entries.InternalCacheEntry;\n+import org.infinispan.container.entries.InternalCacheValue;\n+import org.infinispan.container.impl.InternalEntryFactory;\n+import org.infinispan.container.impl.InternalEntryFactoryImpl;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.marshall.TestObjectStreamMarshaller;\n+import org.infinispan.marshall.persistence.PersistenceMarshaller;\n+import org.infinispan.marshall.persistence.impl.MarshalledEntryUtil;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.PersistenceException;\n+import org.infinispan.persistence.support.EnsureNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.WaitNonBlockingStore;\n+import org.infinispan.protostream.ProtobufUtil;\n+import org.infinispan.protostream.SerializationContext;\n+import org.infinispan.protostream.SerializationContextInitializer;\n+import org.infinispan.test.AbstractInfinispanTest;\n+import org.infinispan.test.TestDataSCI;\n+import org.infinispan.test.TestingUtil;\n+import org.infinispan.test.data.Key;\n+import org.infinispan.test.data.Person;\n+import org.infinispan.test.fwk.NonBlockingTest;\n+import org.infinispan.test.fwk.TestCacheManagerFactory;\n+import org.infinispan.test.fwk.TestInternalCacheEntryFactory;\n+import org.infinispan.util.ControlledTimeService;\n+import org.infinispan.util.PersistenceMockUtil;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * This is a base class containing various unit tests for each and every different CacheStore implementations. If you\n+ * need to add Cache/CacheManager tests that need to be run for each cache store/loader implementation, then use\n+ * BaseStoreFunctionalTest.\n+ */\n+// this needs to be here for the test to run in an IDE", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMzI3Mw==", "bodyText": "Sorry this was just duplicated from the BaseStoreTest. Tbh I will just remove the comment :D", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427303273", "createdAt": "2020-05-19T13:30:28Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "diffHunk": "@@ -0,0 +1,660 @@\n+package org.infinispan.persistence;\n+\n+import static org.infinispan.test.TestingUtil.allEntries;\n+import static org.testng.AssertJUnit.assertEquals;\n+import static org.testng.AssertJUnit.assertFalse;\n+import static org.testng.AssertJUnit.assertNotNull;\n+import static org.testng.AssertJUnit.assertNull;\n+import static org.testng.AssertJUnit.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import org.infinispan.commons.marshall.Marshaller;\n+import org.infinispan.commons.marshall.ProtoStreamMarshaller;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.marshall.WrappedBytes;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.commons.util.IntSets;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.ConfigurationBuilder;\n+import org.infinispan.container.entries.InternalCacheEntry;\n+import org.infinispan.container.entries.InternalCacheValue;\n+import org.infinispan.container.impl.InternalEntryFactory;\n+import org.infinispan.container.impl.InternalEntryFactoryImpl;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.marshall.TestObjectStreamMarshaller;\n+import org.infinispan.marshall.persistence.PersistenceMarshaller;\n+import org.infinispan.marshall.persistence.impl.MarshalledEntryUtil;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.PersistenceException;\n+import org.infinispan.persistence.support.EnsureNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.WaitNonBlockingStore;\n+import org.infinispan.protostream.ProtobufUtil;\n+import org.infinispan.protostream.SerializationContext;\n+import org.infinispan.protostream.SerializationContextInitializer;\n+import org.infinispan.test.AbstractInfinispanTest;\n+import org.infinispan.test.TestDataSCI;\n+import org.infinispan.test.TestingUtil;\n+import org.infinispan.test.data.Key;\n+import org.infinispan.test.data.Person;\n+import org.infinispan.test.fwk.NonBlockingTest;\n+import org.infinispan.test.fwk.TestCacheManagerFactory;\n+import org.infinispan.test.fwk.TestInternalCacheEntryFactory;\n+import org.infinispan.util.ControlledTimeService;\n+import org.infinispan.util.PersistenceMockUtil;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * This is a base class containing various unit tests for each and every different CacheStore implementations. If you\n+ * need to add Cache/CacheManager tests that need to be run for each cache store/loader implementation, then use\n+ * BaseStoreFunctionalTest.\n+ */\n+// this needs to be here for the test to run in an IDE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Mjc2NQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM1MDE4MQ==", "bodyText": "I have just removed this for now, this was a copy over from the previous SPI BaseStoreTest.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427350181", "createdAt": "2020-05-19T14:32:38Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "diffHunk": "@@ -0,0 +1,660 @@\n+package org.infinispan.persistence;\n+\n+import static org.infinispan.test.TestingUtil.allEntries;\n+import static org.testng.AssertJUnit.assertEquals;\n+import static org.testng.AssertJUnit.assertFalse;\n+import static org.testng.AssertJUnit.assertNotNull;\n+import static org.testng.AssertJUnit.assertNull;\n+import static org.testng.AssertJUnit.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import org.infinispan.commons.marshall.Marshaller;\n+import org.infinispan.commons.marshall.ProtoStreamMarshaller;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.marshall.WrappedBytes;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.commons.util.IntSets;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.ConfigurationBuilder;\n+import org.infinispan.container.entries.InternalCacheEntry;\n+import org.infinispan.container.entries.InternalCacheValue;\n+import org.infinispan.container.impl.InternalEntryFactory;\n+import org.infinispan.container.impl.InternalEntryFactoryImpl;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.marshall.TestObjectStreamMarshaller;\n+import org.infinispan.marshall.persistence.PersistenceMarshaller;\n+import org.infinispan.marshall.persistence.impl.MarshalledEntryUtil;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.PersistenceException;\n+import org.infinispan.persistence.support.EnsureNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.WaitNonBlockingStore;\n+import org.infinispan.protostream.ProtobufUtil;\n+import org.infinispan.protostream.SerializationContext;\n+import org.infinispan.protostream.SerializationContextInitializer;\n+import org.infinispan.test.AbstractInfinispanTest;\n+import org.infinispan.test.TestDataSCI;\n+import org.infinispan.test.TestingUtil;\n+import org.infinispan.test.data.Key;\n+import org.infinispan.test.data.Person;\n+import org.infinispan.test.fwk.NonBlockingTest;\n+import org.infinispan.test.fwk.TestCacheManagerFactory;\n+import org.infinispan.test.fwk.TestInternalCacheEntryFactory;\n+import org.infinispan.util.ControlledTimeService;\n+import org.infinispan.util.PersistenceMockUtil;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * This is a base class containing various unit tests for each and every different CacheStore implementations. If you\n+ * need to add Cache/CacheManager tests that need to be run for each cache store/loader implementation, then use\n+ * BaseStoreFunctionalTest.\n+ */\n+// this needs to be here for the test to run in an IDE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Mjc2NQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTA5NTAxOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo0NjoyM1rOGXeshw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDozNToxOFrOGXjhKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3MzM1MQ==", "bodyText": "Neither asNonBlockingStore method are ever called.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427273351", "createdAt": "2020-05-19T12:46:23Z", "author": {"login": "ryanemerson"}, "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "diffHunk": "@@ -0,0 +1,660 @@\n+package org.infinispan.persistence;\n+\n+import static org.infinispan.test.TestingUtil.allEntries;\n+import static org.testng.AssertJUnit.assertEquals;\n+import static org.testng.AssertJUnit.assertFalse;\n+import static org.testng.AssertJUnit.assertNotNull;\n+import static org.testng.AssertJUnit.assertNull;\n+import static org.testng.AssertJUnit.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import org.infinispan.commons.marshall.Marshaller;\n+import org.infinispan.commons.marshall.ProtoStreamMarshaller;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.marshall.WrappedBytes;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.commons.util.IntSets;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.ConfigurationBuilder;\n+import org.infinispan.container.entries.InternalCacheEntry;\n+import org.infinispan.container.entries.InternalCacheValue;\n+import org.infinispan.container.impl.InternalEntryFactory;\n+import org.infinispan.container.impl.InternalEntryFactoryImpl;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.marshall.TestObjectStreamMarshaller;\n+import org.infinispan.marshall.persistence.PersistenceMarshaller;\n+import org.infinispan.marshall.persistence.impl.MarshalledEntryUtil;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.PersistenceException;\n+import org.infinispan.persistence.support.EnsureNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.WaitNonBlockingStore;\n+import org.infinispan.protostream.ProtobufUtil;\n+import org.infinispan.protostream.SerializationContext;\n+import org.infinispan.protostream.SerializationContextInitializer;\n+import org.infinispan.test.AbstractInfinispanTest;\n+import org.infinispan.test.TestDataSCI;\n+import org.infinispan.test.TestingUtil;\n+import org.infinispan.test.data.Key;\n+import org.infinispan.test.data.Person;\n+import org.infinispan.test.fwk.NonBlockingTest;\n+import org.infinispan.test.fwk.TestCacheManagerFactory;\n+import org.infinispan.test.fwk.TestInternalCacheEntryFactory;\n+import org.infinispan.util.ControlledTimeService;\n+import org.infinispan.util.PersistenceMockUtil;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * This is a base class containing various unit tests for each and every different CacheStore implementations. If you\n+ * need to add Cache/CacheManager tests that need to be run for each cache store/loader implementation, then use\n+ * BaseStoreFunctionalTest.\n+ */\n+// this needs to be here for the test to run in an IDE\n+@Test(groups = \"unit\", testName = \"persistence.BaseNonBlockingStoreTest\")\n+@NonBlockingTest\n+public abstract class BaseNonBlockingStoreTest extends AbstractInfinispanTest {\n+\n+   protected static final int WRITE_DELETE_BATCH_MIN_ENTRIES = 80;\n+   protected static final int WRITE_DELETE_BATCH_MAX_ENTRIES = 120;\n+   protected TestObjectStreamMarshaller marshaller;\n+\n+   protected WaitNonBlockingStore<Object, Object> store;\n+   protected ControlledTimeService timeService;\n+   private InternalEntryFactory factory;\n+   private final IntSet segments = IntSets.immutableSet(0);\n+\n+   protected static <K, V> NonBlockingStore<K, V> asNonBlockingStore(CacheLoader<K, V> loader) {\n+      return new NonBlockingStoreAdapter<>(loader);\n+   }\n+\n+   protected static <K, V> NonBlockingStore<K, V> asNonBlockingStore(CacheWriter<K, V> writer) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM1MjM2MA==", "bodyText": "This is for future. My thought was to convert the previous BaseStoreTest to extend this one and just use these methods to convert. I will try to squeeze that in here possibl. This way we don't have 2 base test classes doing essentially the same thing.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427352360", "createdAt": "2020-05-19T14:35:18Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/persistence/BaseNonBlockingStoreTest.java", "diffHunk": "@@ -0,0 +1,660 @@\n+package org.infinispan.persistence;\n+\n+import static org.infinispan.test.TestingUtil.allEntries;\n+import static org.testng.AssertJUnit.assertEquals;\n+import static org.testng.AssertJUnit.assertFalse;\n+import static org.testng.AssertJUnit.assertNotNull;\n+import static org.testng.AssertJUnit.assertNull;\n+import static org.testng.AssertJUnit.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import org.infinispan.commons.marshall.Marshaller;\n+import org.infinispan.commons.marshall.ProtoStreamMarshaller;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.marshall.WrappedBytes;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.commons.util.IntSets;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.ConfigurationBuilder;\n+import org.infinispan.container.entries.InternalCacheEntry;\n+import org.infinispan.container.entries.InternalCacheValue;\n+import org.infinispan.container.impl.InternalEntryFactory;\n+import org.infinispan.container.impl.InternalEntryFactoryImpl;\n+import org.infinispan.distribution.ch.KeyPartitioner;\n+import org.infinispan.marshall.TestObjectStreamMarshaller;\n+import org.infinispan.marshall.persistence.PersistenceMarshaller;\n+import org.infinispan.marshall.persistence.impl.MarshalledEntryUtil;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.PersistenceException;\n+import org.infinispan.persistence.support.EnsureNonBlockingStore;\n+import org.infinispan.persistence.support.NonBlockingStoreAdapter;\n+import org.infinispan.persistence.support.WaitNonBlockingStore;\n+import org.infinispan.protostream.ProtobufUtil;\n+import org.infinispan.protostream.SerializationContext;\n+import org.infinispan.protostream.SerializationContextInitializer;\n+import org.infinispan.test.AbstractInfinispanTest;\n+import org.infinispan.test.TestDataSCI;\n+import org.infinispan.test.TestingUtil;\n+import org.infinispan.test.data.Key;\n+import org.infinispan.test.data.Person;\n+import org.infinispan.test.fwk.NonBlockingTest;\n+import org.infinispan.test.fwk.TestCacheManagerFactory;\n+import org.infinispan.test.fwk.TestInternalCacheEntryFactory;\n+import org.infinispan.util.ControlledTimeService;\n+import org.infinispan.util.PersistenceMockUtil;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+\n+/**\n+ * This is a base class containing various unit tests for each and every different CacheStore implementations. If you\n+ * need to add Cache/CacheManager tests that need to be run for each cache store/loader implementation, then use\n+ * BaseStoreFunctionalTest.\n+ */\n+// this needs to be here for the test to run in an IDE\n+@Test(groups = \"unit\", testName = \"persistence.BaseNonBlockingStoreTest\")\n+@NonBlockingTest\n+public abstract class BaseNonBlockingStoreTest extends AbstractInfinispanTest {\n+\n+   protected static final int WRITE_DELETE_BATCH_MIN_ENTRIES = 80;\n+   protected static final int WRITE_DELETE_BATCH_MAX_ENTRIES = 120;\n+   protected TestObjectStreamMarshaller marshaller;\n+\n+   protected WaitNonBlockingStore<Object, Object> store;\n+   protected ControlledTimeService timeService;\n+   private InternalEntryFactory factory;\n+   private final IntSet segments = IntSets.immutableSet(0);\n+\n+   protected static <K, V> NonBlockingStore<K, V> asNonBlockingStore(CacheLoader<K, V> loader) {\n+      return new NonBlockingStoreAdapter<>(loader);\n+   }\n+\n+   protected static <K, V> NonBlockingStore<K, V> asNonBlockingStore(CacheWriter<K, V> writer) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3MzM1MQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTExMDUyOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/persistence/CacheLoaderFunctionalTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMjo1MDoyMVrOGXe2ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDozNToyNFrOGXjhcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3NTg5OQ==", "bodyText": "This needs uncommenting.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427275899", "createdAt": "2020-05-19T12:50:21Z", "author": {"login": "ryanemerson"}, "path": "core/src/test/java/org/infinispan/persistence/CacheLoaderFunctionalTest.java", "diffHunk": "@@ -95,7 +92,7 @@ protected String parameters() {\n    @Factory\n    public Object[] factory() {\n       return new Object[]{\n-            new CacheLoaderFunctionalTest().segmented(true),\n+//            new CacheLoaderFunctionalTest().segmented(true),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM1MjQzMg==", "bodyText": "Oops, yeah.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427352432", "createdAt": "2020-05-19T14:35:24Z", "author": {"login": "wburns"}, "path": "core/src/test/java/org/infinispan/persistence/CacheLoaderFunctionalTest.java", "diffHunk": "@@ -95,7 +92,7 @@ protected String parameters() {\n    @Factory\n    public Object[] factory() {\n       return new Object[]{\n-            new CacheLoaderFunctionalTest().segmented(true),\n+//            new CacheLoaderFunctionalTest().segmented(true),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3NTg5OQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTM3MzUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo1MTo1OVrOGXhdAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNTozMjo0NVrOGXmQaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxODUzMA==", "bodyText": "?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427318530", "createdAt": "2020-05-19T13:51:59Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,695 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Allows multiple subscribers but only a single value - perfect for reuse of signalling messages\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // We need to create one per start/stop cycle\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   private Disposable startSub;\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending replication being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   @GuardedBy(\"this + Flowable\")\n+   private Map<Object, Modification> modificationMap = new HashMap<>();\n+   @GuardedBy(\"this\")\n+   private boolean hasClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> modificationsToReplicate;\n+   @GuardedBy(\"this\")\n+   private boolean clearToReplicate;\n+\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      // TODO: worry if segmented", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM5NzIyNQ==", "bodyText": "I think originally I had some concerns with segmentation. But now that I have implemented it everything should be essentially a pass through for segmentation. I will just remove this.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427397225", "createdAt": "2020-05-19T15:32:45Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,695 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Allows multiple subscribers but only a single value - perfect for reuse of signalling messages\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // We need to create one per start/stop cycle\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   private Disposable startSub;\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending replication being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   @GuardedBy(\"this + Flowable\")\n+   private Map<Object, Modification> modificationMap = new HashMap<>();\n+   @GuardedBy(\"this\")\n+   private boolean hasClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> modificationsToReplicate;\n+   @GuardedBy(\"this\")\n+   private boolean clearToReplicate;\n+\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      // TODO: worry if segmented", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMxODUzMA=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTQwMjE5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzo1ODo1NlrOGXhvkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNToxMTowMlrOGXlPyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyMzI4MQ==", "bodyText": "variable is never used.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427323281", "createdAt": "2020-05-19T13:58:56Z", "author": {"login": "ryanemerson"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,695 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Allows multiple subscribers but only a single value - perfect for reuse of signalling messages\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // We need to create one per start/stop cycle\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   private Disposable startSub;\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending replication being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   @GuardedBy(\"this + Flowable\")\n+   private Map<Object, Modification> modificationMap = new HashMap<>();\n+   @GuardedBy(\"this\")\n+   private boolean hasClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> modificationsToReplicate;\n+   @GuardedBy(\"this\")\n+   private boolean clearToReplicate;\n+\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      // TODO: worry if segmented\n+      segmentCount = ctx.getCache().getCacheConfiguration().clustering().hash().numSegments();\n+      persistenceConfiguration = ctx.getCache().getCacheConfiguration().persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      asyncConfiguration = ctx.getConfiguration().async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         startSub.dispose();\n+         startSub = null;\n+         asyncStage = waitUntilStop();\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   private CompletionStage<Void> waitUntilStop() {\n+      CompletionStage<Void> stage;\n+      boolean pendingChanges;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM4MDY4MQ==", "bodyText": "Yeah, sorry forgot to remove. I had that temporarily when testing stop ensuring all entries were replicated.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r427380681", "createdAt": "2020-05-19T15:11:02Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,695 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Allows multiple subscribers but only a single value - perfect for reuse of signalling messages\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // We need to create one per start/stop cycle\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   private Disposable startSub;\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending replication being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   @GuardedBy(\"this + Flowable\")\n+   private Map<Object, Modification> modificationMap = new HashMap<>();\n+   @GuardedBy(\"this\")\n+   private boolean hasClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> modificationsToReplicate;\n+   @GuardedBy(\"this\")\n+   private boolean clearToReplicate;\n+\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      // TODO: worry if segmented\n+      segmentCount = ctx.getCache().getCacheConfiguration().clustering().hash().numSegments();\n+      persistenceConfiguration = ctx.getCache().getCacheConfiguration().persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      asyncConfiguration = ctx.getConfiguration().async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         startSub.dispose();\n+         startSub = null;\n+         asyncStage = waitUntilStop();\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   private CompletionStage<Void> waitUntilStop() {\n+      CompletionStage<Void> stage;\n+      boolean pendingChanges;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyMzI4MQ=="}, "originalCommit": {"oid": "5f1ae60093a60253b530d3b9b6e98c495e548906"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTc4Nzk2OnYy", "diffSide": "RIGHT", "path": "commons-test/src/main/java/org/infinispan/commons/test/BlockHoundHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzo1NDowN1rOGYM5NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzo1NjoyOVrOGYNBEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzMDI2MQ==", "bodyText": "it looks like reactor/BlockHound#121 is already fixed. you can remove this line \ud83e\udd23", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428030261", "createdAt": "2020-05-20T13:54:07Z", "author": {"login": "pruivo"}, "path": "commons-test/src/main/java/org/infinispan/commons/test/BlockHoundHelper.java", "diffHunk": "@@ -17,6 +17,7 @@ static void installBlockHound() {\n       // This is a terrible hack but gets around the issue that blockhound doesn't allow the registering thread\n       // to be a dynamic blocking thread - in which case our checks in\n       // AbstractInfinispanTest#currentThreadRequiresNonBlocking will never be evaluated\n+      // To be removed when https://github.com/reactor/BlockHound/issues/121 is fixed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93e74ef08630adcd03cbb9204353db067d2e8879"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzMjI3Mg==", "bodyText": "Sure :D", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428032272", "createdAt": "2020-05-20T13:56:29Z", "author": {"login": "wburns"}, "path": "commons-test/src/main/java/org/infinispan/commons/test/BlockHoundHelper.java", "diffHunk": "@@ -17,6 +17,7 @@ static void installBlockHound() {\n       // This is a terrible hack but gets around the issue that blockhound doesn't allow the registering thread\n       // to be a dynamic blocking thread - in which case our checks in\n       // AbstractInfinispanTest#currentThreadRequiresNonBlocking will never be evaluated\n+      // To be removed when https://github.com/reactor/BlockHound/issues/121 is fixed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzMDI2MQ=="}, "originalCommit": {"oid": "93e74ef08630adcd03cbb9204353db067d2e8879"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTk0OTIxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/configuration/cache/AsyncStoreConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDoyNDo1MlrOGYOcAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDoyODo1M1rOGYOoJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NTU1Mg==", "bodyText": "org.infinispan.configuration.serializing.AbstractStoreSerializer still serializes the thread pool size.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428055552", "createdAt": "2020-05-20T14:24:52Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/configuration/cache/AsyncStoreConfiguration.java", "diffHunk": "@@ -68,6 +68,7 @@ public int modificationQueueSize() {\n    /**\n     * Size of the thread pool whose threads are responsible for applying the modifications.\n     */\n+   @Deprecated\n    public int threadPoolSize() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0aed247aa1188c4d2b63acb45d68a2a1559e45"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1ODY2Mw==", "bodyText": "Updated to remove, now have to wait for CI to confirm it didn't break anything :(", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428058663", "createdAt": "2020-05-20T14:28:53Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/configuration/cache/AsyncStoreConfiguration.java", "diffHunk": "@@ -68,6 +68,7 @@ public int modificationQueueSize() {\n    /**\n     * Size of the thread pool whose threads are responsible for applying the modifications.\n     */\n+   @Deprecated\n    public int threadPoolSize() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NTU1Mg=="}, "originalCommit": {"oid": "de0aed247aa1188c4d2b63acb45d68a2a1559e45"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjAyNTA3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0MTowNVrOGYPM1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTowOToyNlrOGYQlFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2ODA1NQ==", "bodyText": "nitpick: can be static\ndo you think it would be useful to have the \"operation name\"? like StoreAdapter-<operation>-<id>", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428068055", "createdAt": "2020-05-20T14:41:05Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,327 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5MDY0NA==", "bodyText": "I debated about that. I can add it in quickly enough.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428090644", "createdAt": "2020-05-20T15:09:26Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/support/NonBlockingStoreAdapter.java", "diffHunk": "@@ -0,0 +1,327 @@\n+package org.infinispan.persistence.support;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+\n+import javax.transaction.Transaction;\n+\n+import org.infinispan.commons.api.Lifecycle;\n+import org.infinispan.commons.persistence.Store;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.persistence.spi.AdvancedCacheExpirationWriter;\n+import org.infinispan.persistence.spi.AdvancedCacheLoader;\n+import org.infinispan.persistence.spi.AdvancedCacheWriter;\n+import org.infinispan.persistence.spi.CacheLoader;\n+import org.infinispan.persistence.spi.CacheWriter;\n+import org.infinispan.persistence.spi.FlagAffectedStore;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.MarshallableEntryFactory;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.spi.SegmentedAdvancedLoadWriteStore;\n+import org.infinispan.persistence.spi.TransactionalCacheWriter;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.BlockingManager;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class NonBlockingStoreAdapter<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private static final AtomicInteger id = trace ? new AtomicInteger() : null;\n+\n+   private final Lifecycle oldStoreImpl;\n+   private final Set<Characteristic> characteristics;\n+\n+   private BlockingManager blockingManager;\n+   private MarshallableEntryFactory<K, V> marshallableEntryFactory;\n+\n+   public NonBlockingStoreAdapter(Lifecycle oldStoreImpl) {\n+      this.oldStoreImpl = oldStoreImpl;\n+      this.characteristics = determineCharacteristics(oldStoreImpl);\n+   }\n+\n+   public Lifecycle getActualStore() {\n+      return oldStoreImpl;\n+   }\n+\n+   private String nextTraceId() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2ODA1NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjAzNzYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0MzozN1rOGYPU1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTowODo0OVrOGYQjUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MDEwMA==", "bodyText": "nitpick: log and trace can be final.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428070100", "createdAt": "2020-05-20T14:43:37Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.factories.annotations.ComponentName;\n+import org.infinispan.factories.annotations.Inject;\n+import org.infinispan.factories.scopes.Scope;\n+import org.infinispan.factories.scopes.Scopes;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+\n+import net.jcip.annotations.GuardedBy;\n+\n+@Scope(Scopes.GLOBAL)\n+public class NonBlockingManagerImpl implements NonBlockingManager {\n+   private static Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5MDE5NA==", "bodyText": "Oops they were supposed to be. :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428090194", "createdAt": "2020-05-20T15:08:49Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.factories.annotations.ComponentName;\n+import org.infinispan.factories.annotations.Inject;\n+import org.infinispan.factories.scopes.Scope;\n+import org.infinispan.factories.scopes.Scopes;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+\n+import net.jcip.annotations.GuardedBy;\n+\n+@Scope(Scopes.GLOBAL)\n+public class NonBlockingManagerImpl implements NonBlockingManager {\n+   private static Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MDEwMA=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjA0NDI2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0NTowNFrOGYPZMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTowODozM1rOGYQijA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTIxNg==", "bodyText": "synchronize?\nif initialDelay is zero, the scheduler may run the \"task\" before the futureis set.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428071216", "createdAt": "2020-05-20T14:45:04Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.factories.annotations.ComponentName;\n+import org.infinispan.factories.annotations.Inject;\n+import org.infinispan.factories.scopes.Scope;\n+import org.infinispan.factories.scopes.Scopes;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+\n+import net.jcip.annotations.GuardedBy;\n+\n+@Scope(Scopes.GLOBAL)\n+public class NonBlockingManagerImpl implements NonBlockingManager {\n+   private static Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static boolean trace = log.isTraceEnabled();\n+\n+   @ComponentName(KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR)\n+   @Inject ScheduledExecutorService scheduler;\n+\n+   @Override\n+   public AutoCloseable scheduleWithFixedDelay(Supplier<CompletionStage<?>> supplier, long initialDelay, long delay, TimeUnit unit) {\n+      ReschedulingTask task = new ReschedulingTask(supplier, delay, unit);\n+      task.future = scheduler.schedule(task, initialDelay, unit);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4OTk5Ng==", "bodyText": "Agreed, we should plug that up.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428089996", "createdAt": "2020-05-20T15:08:33Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/util/concurrent/NonBlockingManagerImpl.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.infinispan.util.concurrent;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.factories.annotations.ComponentName;\n+import org.infinispan.factories.annotations.Inject;\n+import org.infinispan.factories.scopes.Scope;\n+import org.infinispan.factories.scopes.Scopes;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+\n+import net.jcip.annotations.GuardedBy;\n+\n+@Scope(Scopes.GLOBAL)\n+public class NonBlockingManagerImpl implements NonBlockingManager {\n+   private static Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static boolean trace = log.isTraceEnabled();\n+\n+   @ComponentName(KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR)\n+   @Inject ScheduledExecutorService scheduler;\n+\n+   @Override\n+   public AutoCloseable scheduleWithFixedDelay(Supplier<CompletionStage<?>> supplier, long initialDelay, long delay, TimeUnit unit) {\n+      ReschedulingTask task = new ReschedulingTask(supplier, delay, unit);\n+      task.future = scheduler.schedule(task, initialDelay, unit);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTIxNg=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjA1Mjc4OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/infinispan/api/flags/FlagsEnabledTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0Njo0OFrOGYPelg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0Njo0OFrOGYPelg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MjU5OA==", "bodyText": "nitpick:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   assertEquals(expected1, (int) store1.stats().get(\"load\"));\n          \n          \n            \n                  assertEquals(expected1, (int) store1.stats().get(\"load\"));", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428072598", "createdAt": "2020-05-20T14:46:48Z", "author": {"login": "pruivo"}, "path": "core/src/test/java/org/infinispan/api/flags/FlagsEnabledTest.java", "diffHunk": "@@ -198,8 +198,8 @@ public CustomDelegateCache(AdvancedCache<K, V> cache) {\n    private void assertLoadsAndReset(Cache<?, ?> cache1, int expected1, Cache<?, ?> cache2, int expected2) {\n       DummyInMemoryStore store1 = getCacheStore(cache1);\n       DummyInMemoryStore store2 = getCacheStore(cache2);\n-      assertEquals(expected1, (int) store1.stats().get(\"loadEntry\"));\n-      assertEquals(expected2, (int) store2.stats().get(\"loadEntry\"));\n+       assertEquals(expected1, (int) store1.stats().get(\"load\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjE3MjYzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxMjoxOVrOGYQtOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxMjoxOVrOGYQtOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5MjcyOA==", "bodyText": "typo\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               // This map contains all the modifcations currently being replicated to the delegating store\n          \n          \n            \n               // This map contains all the modifications currently being replicated to the delegating store", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428092728", "createdAt": "2020-05-20T15:12:19Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifcations currently being replicated to the delegating store", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjE3Mzk2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxMjozNVrOGYQuBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxMjozNVrOGYQuBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5MjkzNA==", "bodyText": "typo\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               private Map<Object, Modification> replicatingModications;\n          \n          \n            \n               private Map<Object, Modification> replicatingModifications;", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428092934", "createdAt": "2020-05-20T15:12:35Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifcations currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModications;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjE4MDcwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxNDowNlrOGYQyfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxNDowNlrOGYQyfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5NDA3Ng==", "bodyText": "typos? also, does it make sense to change  is being ran => is running?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               // True if their is an oustanding clear that is being ran on the delegating store\n          \n          \n            \n               // True if there is an outstanding clear that is being ran on the delegating store", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428094076", "createdAt": "2020-05-20T15:14:06Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifcations currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModications;\n+   // True if their is an oustanding clear that is being ran on the delegating store", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjIwMzc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToxOToxNVrOGYRByw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjoyMDo1N1rOGYTttw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ==", "bodyText": "Intellij is reporting a lot of access outside a synchronized block for most of the fields!", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428097995", "createdAt": "2020-05-20T15:19:15Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEwMTM1Mg==", "bodyText": "I just double checked and this field is fine. Maybe the Modification#apply is tricking it?", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428101352", "createdAt": "2020-05-20T15:23:57Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEwNjMxNw==", "bodyText": "I did find a couple in the next two, should be fixed now.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428106317", "createdAt": "2020-05-20T15:30:14Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODExNDk4MA==", "bodyText": "yep, Modification#apply is still complaining.\nyou can add a method in AsyncNonBlockingStore with the apply() logic (the logic is similar between put/remove)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428114980", "createdAt": "2020-05-20T15:41:53Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzMzkyOA==", "bodyText": "I am not a fan of changing the code to a do a switch inside another method, where using polymorphism is quite cleaner. I just wish there was a @GuardedBy value where you can specify an argument.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428133928", "createdAt": "2020-05-20T16:08:49Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzNDc1Mw==", "bodyText": "oh sorry I misunderstood what you said. Let me do that.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428134753", "createdAt": "2020-05-20T16:10:05Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE0MjAwNw==", "bodyText": "Updated, should be satisfied now :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428142007", "createdAt": "2020-05-20T16:20:57Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5Nzk5NQ=="}, "originalCommit": {"oid": "ffc19aa40ed10472611372221637e2a0f5629c39"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjIxMjQ3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToyMToxOVrOGYRHbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNToyNToyMVrOGYRTMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5OTQzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  return batchFuture.thenCompose(ignore -> awaitQuiescence());\n          \n          \n            \n                  return stage.thenCompose(ignore -> awaitQuiescence());", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428099439", "createdAt": "2020-05-20T15:21:19Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifcations currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModications;\n+   // True if their is an oustanding clear that is being ran on the delegating store\n+   @GuardedBy(\"this\")\n+   private boolean isReplicatingClear;\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      Configuration cacheConfiguration = ctx.getCache().getCacheConfiguration();\n+      persistenceConfiguration = cacheConfiguration.persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      StoreConfiguration storeConfiguration = ctx.getConfiguration();\n+      segmentCount = storeConfiguration.segmented() ? cacheConfiguration.clustering().hash().numSegments() : 1;\n+      asyncConfiguration = storeConfiguration.async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         asyncStage = awaitQuiescence().whenComplete((ignore, t) -> {\n+            // We can only dispose of the subscription after we are sure we are totally stopped\n+            if (startSub != null) {\n+               startSub.dispose();\n+               startSub = null;\n+            }\n+         });\n+\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   /**\n+    * Returns a stage that when complete, this store has submitted and completed all pending modifications\n+    */\n+   private CompletionStage<Void> awaitQuiescence() {\n+      CompletionStage<Void> stage;\n+      synchronized (this) {\n+         stage = batchFuture;\n+      }\n+      if (stage == null) {\n+         return CompletableFutures.completedNull();\n+      }\n+      if (trace) {\n+         log.tracef(\"Must wait until prior batch completes for %s\", actual);\n+      }\n+      return batchFuture.thenCompose(ignore -> awaitQuiescence());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99654b6edde63893bcf692f540e9bcaa2d7e44a6"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEwMjQ0OA==", "bodyText": "Good catch.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428102448", "createdAt": "2020-05-20T15:25:21Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,755 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifcations currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModications;\n+   // True if their is an oustanding clear that is being ran on the delegating store\n+   @GuardedBy(\"this\")\n+   private boolean isReplicatingClear;\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      Configuration cacheConfiguration = ctx.getCache().getCacheConfiguration();\n+      persistenceConfiguration = cacheConfiguration.persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      StoreConfiguration storeConfiguration = ctx.getConfiguration();\n+      segmentCount = storeConfiguration.segmented() ? cacheConfiguration.clustering().hash().numSegments() : 1;\n+      asyncConfiguration = storeConfiguration.async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         asyncStage = awaitQuiescence().whenComplete((ignore, t) -> {\n+            // We can only dispose of the subscription after we are sure we are totally stopped\n+            if (startSub != null) {\n+               startSub.dispose();\n+               startSub = null;\n+            }\n+         });\n+\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   /**\n+    * Returns a stage that when complete, this store has submitted and completed all pending modifications\n+    */\n+   private CompletionStage<Void> awaitQuiescence() {\n+      CompletionStage<Void> stage;\n+      synchronized (this) {\n+         stage = batchFuture;\n+      }\n+      if (stage == null) {\n+         return CompletableFutures.completedNull();\n+      }\n+      if (trace) {\n+         log.tracef(\"Must wait until prior batch completes for %s\", actual);\n+      }\n+      return batchFuture.thenCompose(ignore -> awaitQuiescence());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5OTQzOQ=="}, "originalCommit": {"oid": "99654b6edde63893bcf692f540e9bcaa2d7e44a6"}, "originalPosition": 181}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjMyNzI4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo0NjoyMlrOGYSQsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo1MToxNFrOGYSeCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODExODE5Mg==", "bodyText": "I'm having some trouble following this method logic... but I was expecting to check RemoveModification to prevents those keys to be published.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428118192", "createdAt": "2020-05-20T15:46:22Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,758 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifications currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModifications;\n+   // True if there is an outstanding clear that is being ran on the delegating store\n+   @GuardedBy(\"this\")\n+   private boolean isReplicatingClear;\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      Configuration cacheConfiguration = ctx.getCache().getCacheConfiguration();\n+      persistenceConfiguration = cacheConfiguration.persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      StoreConfiguration storeConfiguration = ctx.getConfiguration();\n+      segmentCount = storeConfiguration.segmented() ? cacheConfiguration.clustering().hash().numSegments() : 1;\n+      asyncConfiguration = storeConfiguration.async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         asyncStage = awaitQuiescence().whenComplete((ignore, t) -> {\n+            // We can only dispose of the subscription after we are sure we are totally stopped\n+            if (startSub != null) {\n+               startSub.dispose();\n+               startSub = null;\n+            }\n+         });\n+\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   /**\n+    * Returns a stage that when complete, this store has submitted and completed all pending modifications\n+    */\n+   private CompletionStage<Void> awaitQuiescence() {\n+      CompletionStage<Void> stage;\n+      synchronized (this) {\n+         stage = batchFuture;\n+      }\n+      if (stage == null) {\n+         return CompletableFutures.completedNull();\n+      }\n+      if (trace) {\n+         log.tracef(\"Must wait until prior batch completes for %s\", actual);\n+      }\n+      return stage.thenCompose(ignore -> awaitQuiescence());\n+   }\n+\n+   /**\n+    * This method is invoked every time a new batch of entries is generated. When the Flowable is completed, any\n+    * enqueued values should be replicated to the underlying store.\n+    * @param modificationFlowable the next stream of values to enqueue and eventually send\n+    */\n+   @Override\n+   public void accept(Flowable<Modification> modificationFlowable) {\n+      modificationFlowable.subscribe(modification -> {\n+         synchronized (this) {\n+            modification.apply(this);\n+         }\n+      }, RxJavaInterop.emptyConsumer(), () -> {\n+         Map<Object, Modification> newMap = new HashMap<>();\n+         if (trace) {\n+            log.tracef(\"Starting new batch with id %s\", System.identityHashCode(newMap));\n+         }\n+         boolean ourClearToReplicate;\n+         Map<Object, Modification> ourModificationsToReplicate;\n+         synchronized (this) {\n+            assert replicatingModifications == null || replicatingModifications.isEmpty();\n+            replicatingModifications = pendingModifications;\n+            ourModificationsToReplicate = pendingModifications;\n+            pendingModifications = newMap;\n+            isReplicatingClear = hasPendingClear;\n+            ourClearToReplicate = hasPendingClear;\n+            hasPendingClear = false;\n+         }\n+\n+\n+         CompletionStage<Void> asyncBatchStage;\n+         if (ourClearToReplicate) {\n+            if (trace) {\n+               log.tracef(\"Sending clear to underlying store for id %s\", System.identityHashCode(ourModificationsToReplicate));\n+            }\n+            asyncBatchStage = retry(actual::clear, persistenceConfiguration.connectionAttempts()).whenComplete((ignore, t) -> {\n+               synchronized (this) {\n+                  isReplicatingClear = false;\n+               }\n+            });\n+         } else {\n+            asyncBatchStage = CompletableFutures.completedNull();\n+         }\n+\n+         if (!ourModificationsToReplicate.isEmpty()) {\n+            asyncBatchStage = asyncBatchStage.thenCompose(ignore -> {\n+               if (trace) {\n+                  log.tracef(\"Sending batch write/remove operations %s to underlying store with id %s\", ourModificationsToReplicate.values(),\n+                        System.identityHashCode(ourModificationsToReplicate));\n+               }\n+               return retry(() -> replicateModifications(ourModificationsToReplicate), persistenceConfiguration.connectionAttempts()).whenComplete((ignore2, t) -> {\n+                  synchronized (this) {\n+                     replicatingModifications = null;\n+                  }\n+               });\n+            });\n+         }\n+\n+         asyncBatchStage.whenComplete((ignore, t) -> {\n+            if (trace) {\n+               log.tracef(\"Async operations completed for id %s\", System.identityHashCode(ourModificationsToReplicate));\n+            }\n+            boolean submitNewBatch;\n+            CompletableFuture<Void> future;\n+            synchronized (this) {\n+               submitNewBatch = !pendingModifications.isEmpty() || hasPendingClear;\n+               future = batchFuture;\n+               batchFuture = submitNewBatch ? new CompletableFuture<>() : null;\n+            }\n+            if (t != null) {\n+               future.completeExceptionally(t);\n+            } else {\n+               future.complete(null);\n+            }\n+            if (submitNewBatch) {\n+               if (trace) {\n+                  log.trace(\"Submitting new batch after completion of prior\");\n+               }\n+               requestFlowable.onNext(requestFlowable);\n+            }\n+         });\n+      });\n+   }\n+\n+   /**\n+    * Attempts to run the given supplier, checking the stage if it contains an error. It will rerun the Supplier\n+    * until a supplied stage doesn't contain an exception or it has encountered retries amount of exceptions. In the\n+    * latter case it will complete the returned stage with the last throwable encountered.\n+    * <p>\n+    * The supplier is only invoked on the delegating store if it is actually available and will wait for it to\n+    * become so if necessary.\n+    * @param operationSupplier supplies the stage to test if a throwable was encountered\n+    * @param retries how many attempts to make before giving up and propagating the exception\n+    * @return a stage that is completed when the underlying supplied stage completed normally or has encountered a\n+    * throwable retries times\n+    */\n+   private CompletionStage<Void> retry(Supplier<CompletionStage<Void>> operationSupplier, int retries) {\n+      return CompletionStages.handleAndCompose(getAvailabilityDelayStage().thenCompose(ignore -> operationSupplier.get()), (ignore, throwable) -> {\n+         if (throwable != null) {\n+            if (retries > 0) {\n+               int waitTime = persistenceConfiguration.availabilityInterval();\n+               log.debugf(throwable,\"Failed to process async operation - retrying with delay of %d ms\", waitTime);\n+               if (waitTime > 0) {\n+                  RunnableCompletionStage rcs = new RunnableCompletionStage(() -> retry(operationSupplier,retries - 1));\n+                  scheduler.schedule(rcs, waitTime, TimeUnit.MILLISECONDS);\n+                  return rcs;\n+               }\n+               return retry(operationSupplier,retries - 1);\n+            } else {\n+               log.debug(\"Failed to process async operation - no more retries\", throwable);\n+               return CompletableFutures.completedExceptionFuture(throwable);\n+            }\n+         }\n+         return CompletableFutures.completedNull();\n+      });\n+   }\n+\n+   private static class RunnableCompletionStage extends CompletableFuture<Void> implements Runnable {\n+      private final Supplier<CompletionStage<Void>> supplier;\n+\n+      private RunnableCompletionStage(Supplier<CompletionStage<Void>> supplier) {\n+         this.supplier = supplier;\n+      }\n+\n+      @Override\n+      public void run() {\n+         supplier.get().whenComplete((ignore, throwable) -> {\n+            if (throwable != null) {\n+               completeExceptionally(throwable);\n+            } else {\n+               complete(null);\n+            }\n+         });\n+      }\n+   }\n+\n+   private CompletionStage<Void> replicateModifications(Map<Object, Modification> modifications) {\n+      // Use a connected flowable, so we don't have to iterate over the modifications twice\n+      ConnectableFlowable<Modification> connectableModifications = Flowable.fromIterable(modifications.values())\n+            .publish();\n+\n+      // The below two methods may lazily subscribe to the Flowable, thus we must auto connect after both are\n+      // subscribed to (e.g. NonBlockingStoreAdapter subscribes on a blocking thread)\n+      Flowable<Modification> modificationFlowable = connectableModifications.autoConnect(2);\n+\n+      CompletionStage<Void> putStage = actual.bulkWrite(segmentCount, modificationFlowable.ofType(PutModification.class)\n+            .groupBy(Modification::getSegment, PutModification::<K, V>getEntry)\n+            .map(SegmentPublisherWrapper::new));\n+      CompletionStage<Void> removeStage = actual.bulkDelete(segmentCount, modificationFlowable.ofType(RemoveModification.class)\n+            .groupBy(Modification::getSegment, RemoveModification::getKey)\n+            .map(SegmentPublisherWrapper::new));\n+\n+      return CompletionStages.allOf(removeStage, putStage);\n+   }\n+\n+   private CompletionStage<Void> getAvailabilityDelayStage() {\n+      if (asyncConfiguration.failSilently()) {\n+         return CompletableFutures.completedNull();\n+      }\n+      CompletableFuture<Void> availabilityFuture;\n+      synchronized (this) {\n+         availabilityFuture = delegateAvailableFuture;\n+      }\n+      return availabilityFuture == null ? CompletableFutures.completedNull() : availabilityFuture;\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return Flowable.defer(() -> {\n+         assertNotStopped();\n+         if (trace) {\n+            log.tracef(\"Publisher subscribed to retrieve entries for segments %s\", segments);\n+         }\n+         return abstractPublish(segments, filter, PutModification::getEntry, MarshallableEntry::getKey,\n+               (innerSegments, predicate) -> actual.publishEntries(innerSegments, predicate, includeValues));\n+      });\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.defer(() -> {\n+         assertNotStopped();\n+         if (trace) {\n+            log.tracef(\"Publisher subscribed to retrieve keys for segments %s\", segments);\n+         }\n+         return abstractPublish(segments, filter, putModification -> putModification.<K, Object>getEntry().getKey(),\n+               RxJavaInterop.identityFunction(), actual::publishKeys);\n+      });\n+   }\n+\n+   private <E> Publisher<E> abstractPublish(IntSet segments, Predicate<? super K> filter, Function<PutModification, E> putFunction,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84854db8a00dc1b4ec21b1393cc645a6994187ae"}, "originalPosition": 373}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEyMTYwOA==", "bodyText": "It should be. The entryModifications map will contain the RemoveModification, which isn't published in the modPublisher but is filtered via combinedPredicate", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428121608", "createdAt": "2020-05-20T15:51:14Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/async/AsyncNonBlockingStore.java", "diffHunk": "@@ -0,0 +1,758 @@\n+package org.infinispan.persistence.async;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.AbstractMap;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+\n+import org.infinispan.commons.CacheException;\n+import org.infinispan.commons.IllegalLifecycleStateException;\n+import org.infinispan.commons.marshall.WrappedByteArray;\n+import org.infinispan.commons.util.IntSet;\n+import org.infinispan.configuration.cache.AsyncStoreConfiguration;\n+import org.infinispan.configuration.cache.Configuration;\n+import org.infinispan.configuration.cache.PersistenceConfiguration;\n+import org.infinispan.configuration.cache.StoreConfiguration;\n+import org.infinispan.factories.KnownComponentNames;\n+import org.infinispan.persistence.spi.InitializationContext;\n+import org.infinispan.persistence.spi.MarshallableEntry;\n+import org.infinispan.persistence.spi.NonBlockingStore;\n+import org.infinispan.persistence.support.DelegatingNonBlockingStore;\n+import org.infinispan.persistence.support.SegmentPublisherWrapper;\n+import org.infinispan.reactive.RxJavaInterop;\n+import org.infinispan.util.concurrent.CompletableFutures;\n+import org.infinispan.util.concurrent.CompletionStages;\n+import org.infinispan.util.logging.Log;\n+import org.infinispan.util.logging.LogFactory;\n+import org.reactivestreams.Publisher;\n+\n+import io.reactivex.rxjava3.core.Flowable;\n+import io.reactivex.rxjava3.disposables.Disposable;\n+import io.reactivex.rxjava3.flowables.ConnectableFlowable;\n+import io.reactivex.rxjava3.functions.Consumer;\n+import io.reactivex.rxjava3.functions.Function;\n+import io.reactivex.rxjava3.processors.FlowableProcessor;\n+import io.reactivex.rxjava3.processors.MulticastProcessor;\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+import net.jcip.annotations.GuardedBy;\n+\n+/**\n+ * A delegating NonBlockingStore implementation that batches write operations and runs the resulting batches on the\n+ * delegate store in a non overlapping manner. That is that only a single batch will be running at a time.\n+ * <p>\n+ * Whenever a write operation is performed it will also attempt to start a batch write immediately to the delegate store.\n+ * Any concurrent writes during this time may be included in the batch. Any additional writes will be enqueued until\n+ * the batch completes in which case it will automatically submit the pending batch, if there is one.  Write operations\n+ * to the same key in the same batch will be coalesced with only the last write being written to the underlying store.\n+ * If the number of enqueued pending write operations becomes equal or larger than the modification queue, then any\n+ * subsequent write will be added to the queue, but the returned Stage will not complete until the current batch completes\n+ * in an attempt to provide some backpressure to slow writes.\n+ * <p>\n+ * Read operations may be resolved by this store immediately if the given key is still being updated in the\n+ * delegate store or if it is enqueued for the next batch. If the key is in neither it will query the underlying store\n+ * to acquire it.\n+ * @author wburns\n+ * @since 11.0\n+ * @param <K> key type for the store\n+ * @param <V> value type for the store\n+ */\n+public class AsyncNonBlockingStore<K, V> extends DelegatingNonBlockingStore<K, V> implements Consumer<Flowable<AsyncNonBlockingStore.Modification>> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass());\n+   private static final boolean trace = log.isTraceEnabled();\n+   private final NonBlockingStore<K, V> actual;\n+\n+   // Any non-null value can be passed to `onNext` when a new batch should be submitted - A value should only be\n+   // submitted to this if the `batchFuture` was null and the caller was able to assign it to a new value\n+   private final MulticastProcessor<Object> requestFlowable;\n+   // Submit new Modifications to this on every write\n+   private volatile FlowableProcessor<Modification> submissionFlowable;\n+   // Closing this will stop the subsmissionFlowable subscription\n+   private volatile Disposable startSub;\n+\n+   private Executor nonBlockingExecutor;\n+   private int segmentCount;\n+   private int modificationQueueSize;\n+   private PersistenceConfiguration persistenceConfiguration;\n+   private AsyncStoreConfiguration asyncConfiguration;\n+\n+   // \"Non blocking\" scheduler used for the purpose of delaying retry batch operations on failures\n+   private ScheduledExecutorService scheduler;\n+\n+   // This variable will be non null if there is a pending batch being sent to the underlying store\n+   // If a request causes the modification queue to overflow it will receive a stage back that is only complete\n+   // when this future is completed (aka. previous replication has completed)\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> batchFuture;\n+\n+   // This variable will be non null if the underlying store has been found to be not available\n+   // Note that the async store will still be available as long as the queue size (ie. modificationMap.size) is not\n+   // greater than the configured modificationQueueSize\n+   @GuardedBy(\"this\")\n+   private CompletableFuture<Void> delegateAvailableFuture;\n+\n+   // Any pending modifications will be enqueued in this map\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> pendingModifications = new HashMap<>();\n+   // If there is a pending clear this will be true\n+   @GuardedBy(\"this\")\n+   private boolean hasPendingClear;\n+   // The next two variables are held temporarily until a replication of the values is complete. We need to retain\n+   // these values until we are sure the entries are actually in the store - note these variables are only written to\n+   // via reference (thus the map is safe to read outside of this lock, but the reference must be read in synchronized)\n+   // This map contains all the modifications currently being replicated to the delegating store\n+   @GuardedBy(\"this\")\n+   private Map<Object, Modification> replicatingModifications;\n+   // True if there is an outstanding clear that is being ran on the delegating store\n+   @GuardedBy(\"this\")\n+   private boolean isReplicatingClear;\n+\n+   public AsyncNonBlockingStore(NonBlockingStore<K, V> actual) {\n+      this.actual = actual;\n+      this.requestFlowable = MulticastProcessor.create(1);\n+      requestFlowable.start();\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      Configuration cacheConfiguration = ctx.getCache().getCacheConfiguration();\n+      persistenceConfiguration = cacheConfiguration.persistence();\n+      scheduler = ctx.getCache().getCacheManager().getGlobalComponentRegistry().getComponent(\n+            ScheduledExecutorService.class, KnownComponentNames.TIMEOUT_SCHEDULE_EXECUTOR);\n+      StoreConfiguration storeConfiguration = ctx.getConfiguration();\n+      segmentCount = storeConfiguration.segmented() ? cacheConfiguration.clustering().hash().numSegments() : 1;\n+      asyncConfiguration = storeConfiguration.async();\n+      modificationQueueSize = asyncConfiguration.modificationQueueSize();\n+      // It is possible for multiple threads to write to this processor at the same time\n+      submissionFlowable = UnicastProcessor.<Modification>create(1).toSerialized();\n+      nonBlockingExecutor = ctx.getNonBlockingExecutor();\n+      startSub = submissionFlowable.window(requestFlowable).subscribe(this);\n+      return actual.start(ctx);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      CompletionStage<Void> asyncStage;\n+      if (submissionFlowable != null) {\n+         if (trace) {\n+            log.tracef(\"Stopping async store containing store %s\", actual);\n+         }\n+         submissionFlowable = null;\n+         asyncStage = awaitQuiescence().whenComplete((ignore, t) -> {\n+            // We can only dispose of the subscription after we are sure we are totally stopped\n+            if (startSub != null) {\n+               startSub.dispose();\n+               startSub = null;\n+            }\n+         });\n+\n+      } else {\n+         asyncStage = CompletableFutures.completedNull();\n+      }\n+      return asyncStage.thenCompose(ignore -> {\n+         if (trace) {\n+            log.tracef(\"Stopping store %s from async store\", actual);\n+         }\n+         return actual.stop();\n+      });\n+   }\n+\n+   /**\n+    * Returns a stage that when complete, this store has submitted and completed all pending modifications\n+    */\n+   private CompletionStage<Void> awaitQuiescence() {\n+      CompletionStage<Void> stage;\n+      synchronized (this) {\n+         stage = batchFuture;\n+      }\n+      if (stage == null) {\n+         return CompletableFutures.completedNull();\n+      }\n+      if (trace) {\n+         log.tracef(\"Must wait until prior batch completes for %s\", actual);\n+      }\n+      return stage.thenCompose(ignore -> awaitQuiescence());\n+   }\n+\n+   /**\n+    * This method is invoked every time a new batch of entries is generated. When the Flowable is completed, any\n+    * enqueued values should be replicated to the underlying store.\n+    * @param modificationFlowable the next stream of values to enqueue and eventually send\n+    */\n+   @Override\n+   public void accept(Flowable<Modification> modificationFlowable) {\n+      modificationFlowable.subscribe(modification -> {\n+         synchronized (this) {\n+            modification.apply(this);\n+         }\n+      }, RxJavaInterop.emptyConsumer(), () -> {\n+         Map<Object, Modification> newMap = new HashMap<>();\n+         if (trace) {\n+            log.tracef(\"Starting new batch with id %s\", System.identityHashCode(newMap));\n+         }\n+         boolean ourClearToReplicate;\n+         Map<Object, Modification> ourModificationsToReplicate;\n+         synchronized (this) {\n+            assert replicatingModifications == null || replicatingModifications.isEmpty();\n+            replicatingModifications = pendingModifications;\n+            ourModificationsToReplicate = pendingModifications;\n+            pendingModifications = newMap;\n+            isReplicatingClear = hasPendingClear;\n+            ourClearToReplicate = hasPendingClear;\n+            hasPendingClear = false;\n+         }\n+\n+\n+         CompletionStage<Void> asyncBatchStage;\n+         if (ourClearToReplicate) {\n+            if (trace) {\n+               log.tracef(\"Sending clear to underlying store for id %s\", System.identityHashCode(ourModificationsToReplicate));\n+            }\n+            asyncBatchStage = retry(actual::clear, persistenceConfiguration.connectionAttempts()).whenComplete((ignore, t) -> {\n+               synchronized (this) {\n+                  isReplicatingClear = false;\n+               }\n+            });\n+         } else {\n+            asyncBatchStage = CompletableFutures.completedNull();\n+         }\n+\n+         if (!ourModificationsToReplicate.isEmpty()) {\n+            asyncBatchStage = asyncBatchStage.thenCompose(ignore -> {\n+               if (trace) {\n+                  log.tracef(\"Sending batch write/remove operations %s to underlying store with id %s\", ourModificationsToReplicate.values(),\n+                        System.identityHashCode(ourModificationsToReplicate));\n+               }\n+               return retry(() -> replicateModifications(ourModificationsToReplicate), persistenceConfiguration.connectionAttempts()).whenComplete((ignore2, t) -> {\n+                  synchronized (this) {\n+                     replicatingModifications = null;\n+                  }\n+               });\n+            });\n+         }\n+\n+         asyncBatchStage.whenComplete((ignore, t) -> {\n+            if (trace) {\n+               log.tracef(\"Async operations completed for id %s\", System.identityHashCode(ourModificationsToReplicate));\n+            }\n+            boolean submitNewBatch;\n+            CompletableFuture<Void> future;\n+            synchronized (this) {\n+               submitNewBatch = !pendingModifications.isEmpty() || hasPendingClear;\n+               future = batchFuture;\n+               batchFuture = submitNewBatch ? new CompletableFuture<>() : null;\n+            }\n+            if (t != null) {\n+               future.completeExceptionally(t);\n+            } else {\n+               future.complete(null);\n+            }\n+            if (submitNewBatch) {\n+               if (trace) {\n+                  log.trace(\"Submitting new batch after completion of prior\");\n+               }\n+               requestFlowable.onNext(requestFlowable);\n+            }\n+         });\n+      });\n+   }\n+\n+   /**\n+    * Attempts to run the given supplier, checking the stage if it contains an error. It will rerun the Supplier\n+    * until a supplied stage doesn't contain an exception or it has encountered retries amount of exceptions. In the\n+    * latter case it will complete the returned stage with the last throwable encountered.\n+    * <p>\n+    * The supplier is only invoked on the delegating store if it is actually available and will wait for it to\n+    * become so if necessary.\n+    * @param operationSupplier supplies the stage to test if a throwable was encountered\n+    * @param retries how many attempts to make before giving up and propagating the exception\n+    * @return a stage that is completed when the underlying supplied stage completed normally or has encountered a\n+    * throwable retries times\n+    */\n+   private CompletionStage<Void> retry(Supplier<CompletionStage<Void>> operationSupplier, int retries) {\n+      return CompletionStages.handleAndCompose(getAvailabilityDelayStage().thenCompose(ignore -> operationSupplier.get()), (ignore, throwable) -> {\n+         if (throwable != null) {\n+            if (retries > 0) {\n+               int waitTime = persistenceConfiguration.availabilityInterval();\n+               log.debugf(throwable,\"Failed to process async operation - retrying with delay of %d ms\", waitTime);\n+               if (waitTime > 0) {\n+                  RunnableCompletionStage rcs = new RunnableCompletionStage(() -> retry(operationSupplier,retries - 1));\n+                  scheduler.schedule(rcs, waitTime, TimeUnit.MILLISECONDS);\n+                  return rcs;\n+               }\n+               return retry(operationSupplier,retries - 1);\n+            } else {\n+               log.debug(\"Failed to process async operation - no more retries\", throwable);\n+               return CompletableFutures.completedExceptionFuture(throwable);\n+            }\n+         }\n+         return CompletableFutures.completedNull();\n+      });\n+   }\n+\n+   private static class RunnableCompletionStage extends CompletableFuture<Void> implements Runnable {\n+      private final Supplier<CompletionStage<Void>> supplier;\n+\n+      private RunnableCompletionStage(Supplier<CompletionStage<Void>> supplier) {\n+         this.supplier = supplier;\n+      }\n+\n+      @Override\n+      public void run() {\n+         supplier.get().whenComplete((ignore, throwable) -> {\n+            if (throwable != null) {\n+               completeExceptionally(throwable);\n+            } else {\n+               complete(null);\n+            }\n+         });\n+      }\n+   }\n+\n+   private CompletionStage<Void> replicateModifications(Map<Object, Modification> modifications) {\n+      // Use a connected flowable, so we don't have to iterate over the modifications twice\n+      ConnectableFlowable<Modification> connectableModifications = Flowable.fromIterable(modifications.values())\n+            .publish();\n+\n+      // The below two methods may lazily subscribe to the Flowable, thus we must auto connect after both are\n+      // subscribed to (e.g. NonBlockingStoreAdapter subscribes on a blocking thread)\n+      Flowable<Modification> modificationFlowable = connectableModifications.autoConnect(2);\n+\n+      CompletionStage<Void> putStage = actual.bulkWrite(segmentCount, modificationFlowable.ofType(PutModification.class)\n+            .groupBy(Modification::getSegment, PutModification::<K, V>getEntry)\n+            .map(SegmentPublisherWrapper::new));\n+      CompletionStage<Void> removeStage = actual.bulkDelete(segmentCount, modificationFlowable.ofType(RemoveModification.class)\n+            .groupBy(Modification::getSegment, RemoveModification::getKey)\n+            .map(SegmentPublisherWrapper::new));\n+\n+      return CompletionStages.allOf(removeStage, putStage);\n+   }\n+\n+   private CompletionStage<Void> getAvailabilityDelayStage() {\n+      if (asyncConfiguration.failSilently()) {\n+         return CompletableFutures.completedNull();\n+      }\n+      CompletableFuture<Void> availabilityFuture;\n+      synchronized (this) {\n+         availabilityFuture = delegateAvailableFuture;\n+      }\n+      return availabilityFuture == null ? CompletableFutures.completedNull() : availabilityFuture;\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return Flowable.defer(() -> {\n+         assertNotStopped();\n+         if (trace) {\n+            log.tracef(\"Publisher subscribed to retrieve entries for segments %s\", segments);\n+         }\n+         return abstractPublish(segments, filter, PutModification::getEntry, MarshallableEntry::getKey,\n+               (innerSegments, predicate) -> actual.publishEntries(innerSegments, predicate, includeValues));\n+      });\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.defer(() -> {\n+         assertNotStopped();\n+         if (trace) {\n+            log.tracef(\"Publisher subscribed to retrieve keys for segments %s\", segments);\n+         }\n+         return abstractPublish(segments, filter, putModification -> putModification.<K, Object>getEntry().getKey(),\n+               RxJavaInterop.identityFunction(), actual::publishKeys);\n+      });\n+   }\n+\n+   private <E> Publisher<E> abstractPublish(IntSet segments, Predicate<? super K> filter, Function<PutModification, E> putFunction,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODExODE5Mg=="}, "originalCommit": {"oid": "84854db8a00dc1b4ec21b1393cc645a6994187ae"}, "originalPosition": 373}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjY0Mzg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzowMzo1MVrOGYVeQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzoyMjoyMlrOGYWItQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3MDgxOA==", "bodyText": "StoreStatus contains a reference to the store. you could use stores.values().\nAnd, couple line below, just do value.store().load(segment, key)\nAlso, I'm wondering the stores map can be converted to a list or array.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428170818", "createdAt": "2020-05-20T17:03:51Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<Object> preloadEntry(long flags, MarshallableEntry<Object, Object> me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         //noinspection unchecked\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   @GuardedBy(\"lock#readLock\")\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(storeClass::isInstance)\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "feea39f84c113b38c4f4db09940dc5e048a128c0"}, "originalPosition": 1204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3ODAxNw==", "bodyText": "I had debated about changing it to a List at one point, but didn't mess with the idea too much. I can try to bake something up real quick. But tbh I would rather this not prevent integration, unless there is something else more important.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428178017", "createdAt": "2020-05-20T17:16:10Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<Object> preloadEntry(long flags, MarshallableEntry<Object, Object> me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         //noinspection unchecked\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   @GuardedBy(\"lock#readLock\")\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(storeClass::isInstance)\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3MDgxOA=="}, "originalCommit": {"oid": "feea39f84c113b38c4f4db09940dc5e048a128c0"}, "originalPosition": 1204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE4MDA1Mg==", "bodyText": "I'm ok to be merged as is @wburns  :) I've finished my quick look around \ud83d\udc4d", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428180052", "createdAt": "2020-05-20T17:19:37Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<Object> preloadEntry(long flags, MarshallableEntry<Object, Object> me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         //noinspection unchecked\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   @GuardedBy(\"lock#readLock\")\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(storeClass::isInstance)\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3MDgxOA=="}, "originalCommit": {"oid": "feea39f84c113b38c4f4db09940dc5e048a128c0"}, "originalPosition": 1204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE4MTY4NQ==", "bodyText": "Thanks a ton, I am trying to finish up the conversion to ArrayList real quick if that is it.", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428181685", "createdAt": "2020-05-20T17:22:22Z", "author": {"login": "wburns"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<Object> preloadEntry(long flags, MarshallableEntry<Object, Object> me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         //noinspection unchecked\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   @GuardedBy(\"lock#readLock\")\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(storeClass::isInstance)\n+               .map(store -> (T) store)\n+               .collect(Collectors.toCollection(HashSet::new));\n+      } finally {\n+         releaseReadLock(stamp);\n+      }\n    }\n \n-   private void clearAllStoresSync(Predicate<? super StoreConfiguration> predicate, int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public Collection<String> getStoresAsString() {\n+      long stamp = acquireReadLock();\n       try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Clearing persistence stores for id: %d\", traceId);\n-         }\n-         // Apply to txWriters as well as clear does not happen in a Tx context\n-         Consumer<CacheWriter> clearWriter = writer -> {\n-            if (writer instanceof AdvancedCacheWriter) {\n-               if (predicate.test(getStoreConfig(writer))) {\n-                  ((AdvancedCacheWriter) writer).clear();\n-               }\n-            }\n-         };\n-         nonTxWriters.forEach(clearWriter);\n-         txWriters.forEach(clearWriter);\n+         return stores.keySet().stream()\n+               .map(store -> store.getClass().getName())\n+               .collect(Collectors.toCollection(ArrayList::new));\n       } finally {\n-         releaseReadLock();\n+         releaseReadLock(stamp);\n       }\n    }\n \n-   private boolean deleteFromAllStoresSync(Object key, int segment, Predicate<? super StoreConfiguration> predicate,\n-         int traceId) {\n-      acquireReadLock();\n+   @Override\n+   public CompletionStage<Void> purgeExpired() {\n+      long stamp = acquireReadLock();\n       try {\n          checkStoreAvailability();\n          if (trace) {\n-            log.tracef(\"Deleting entry for key %s from stores for id: %d\", key, traceId);\n+            log.tracef(\"Purging entries from stores\");\n          }\n-         boolean removed = false;\n-         for (CacheWriter w : nonTxWriters) {\n-            if (predicate.test(getStoreConfig(w))) {\n-               if (w instanceof SegmentedAdvancedLoadWriteStore) {\n-                  removed |= ((SegmentedAdvancedLoadWriteStore) w).delete(segment, key);\n-               } else {\n-                  removed |= w.delete(key);\n-               }\n+         AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+         for (StoreStatus storeStatus : stores.values()) {\n+            if (storeStatus.characteristics.contains(Characteristic.EXPIRATION)) {\n+               Flowable<MarshallableEntry<Object, Object>> flowable = Flowable.fromPublisher(storeStatus.store().purgeExpired());\n+               Completable completable = flowable.concatMapCompletable(me -> Completable.fromCompletionStage(\n+                        expirationManager.running().handleInStoreExpirationInternal(me)));\n+               aggregateCompletionStage.dependsOn(completable.toCompletionStage(null));\n             }\n          }\n-         if (trace) {\n-            log.tracef(\"Entry was removed: %s for key %s from stores for id: %d\", removed, key, traceId);\n-         }\n-         return removed;\n-      } finally {\n-         releaseReadLock();\n+         return aggregateCompletionStage.freeze()\n+               .whenComplete((v, t) -> releaseReadLock(stamp));\n+      } catch (Throwable t) {\n+         releaseReadLock(stamp);\n+         throw t;\n       }\n    }\n \n+   @Override\n+   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n+      return Completable.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Clearing all stores\");\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the clear work in parallel across the stores\n+                     .flatMapCompletable(entry -> Completable.fromCompletionStage(\n+                           entry.getKey().clear()));\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage(null);\n+   }\n+\n    @Override\n    public CompletionStage<Boolean> deleteFromAllStores(Object key, int segment, Predicate<? super StoreConfiguration> predicate) {\n-      Objects.requireNonNull(key);\n-      return supplyOnPersistenceExAndContinue(traceId -> deleteFromAllStoresSync(key, segment, predicate, traceId),\n-            \"Deleting from all stores for id %d\");\n+      return Single.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Deleting entry for key %s from stores\", key);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())\n+                     .filter(entry ->\n+                           !entry.getValue().characteristics.contains(Characteristic.READ_ONLY)\n+                                 && predicate.test(entry.getValue().config))\n+                     // Let the delete work in parallel across the stores\n+                     .flatMapSingle(entry -> Single.fromCompletionStage(\n+                           entry.getKey().delete(segment, key)))\n+                     // Can't use any, as we have to reduce to ensure that all stores are updated\n+                     .reduce(Boolean.FALSE, (removed1, removed2) -> removed1 || removed2);\n+            },\n+            this::releaseReadLock\n+      ).toCompletionStage();\n    }\n \n-   private <K, V> AdvancedCacheLoader<K, V> getFirstAdvancedCacheLoader(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader loader : loaders) {\n-            if (predicate.test(getStoreConfig(loader)) && loader instanceof AdvancedCacheLoader) {\n-               return ((AdvancedCacheLoader<K, V>) loader);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(boolean fetchValue, boolean fetchMetadata) {\n+      return publishEntries(k -> true, fetchValue, fetchMetadata, k -> true);\n    }\n \n-   <K, V> SegmentedAdvancedLoadWriteStore<K, V> getFirstSegmentedStore(Predicate<? super StoreConfiguration> predicate) {\n-      acquireReadLock();\n-      try {\n-         for (CacheLoader l : loaders) {\n-            StoreConfiguration storeConfiguration;\n-            if (l instanceof SegmentedAdvancedLoadWriteStore &&\n-                  (storeConfiguration = getStoreConfig(l)) != null && storeConfiguration.segmented() &&\n-                  predicate.test(storeConfiguration)) {\n-               return ((SegmentedAdvancedLoadWriteStore<K, V>) l);\n-            }\n-         }\n-      } finally {\n-         releaseReadLock();\n-      }\n-      return null;\n+   @Override\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n+         boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return publishEntries(IntSets.immutableRangeSet(segmentCount), filter, fetchValue, fetchMetadata, predicate);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(Predicate<? super K> filter, boolean fetchValue,\n-                                                                   boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, V> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.entryPublisher(filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K, V> Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing entries for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, V> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishEntries(segments, filter, fetchValue);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K, V> Flowable<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata, Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, V> segmentedStore = getFirstSegmentedStore(predicate);\n-      if (segmentedStore != null) {\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.entryPublisher(segments, filter, fetchValue, fetchMetadata)),\n-                     Semaphore::release);\n-      }\n-      return publishEntries(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), fetchValue, fetchMetadata, predicate);\n+   public <K> Publisher<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return publishKeys(IntSets.immutableRangeSet(segmentCount), filter, predicate);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n-      AdvancedCacheLoader<K, ?> advancedCacheLoader = getFirstAdvancedCacheLoader(predicate);\n-\n-      if (advancedCacheLoader != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(advancedCacheLoader.publishKeys(filter)),\n-                     Semaphore::release);\n-      }\n-      return Flowable.empty();\n+   public <K> Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter, Predicate<? super StoreConfiguration> predicate) {\n+      return Flowable.using(this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Publishing keys for segments %s\", segments);\n+               }\n+               NonBlockingStore<K, ?> nonBlockingStore = getStoreLocked(storeStatus ->\n+                     storeStatus.characteristics.contains(Characteristic.BULK_READ) && predicate.test(storeStatus.config));\n+               return nonBlockingStore == null ? Flowable.empty() : nonBlockingStore.publishKeys(segments, filter);\n+            },\n+            this::releaseReadLock);\n    }\n \n    @Override\n-   public <K> Flowable<K> publishKeys(IntSet segments, Predicate<? super K> filter,\n-         Predicate<? super StoreConfiguration> predicate) {\n-      SegmentedAdvancedLoadWriteStore<K, ?> segmentedStore = getFirstSegmentedStore(predicate);\n-\n-      if (segmentedStore != null) {\n-         // We have to acquire the read lock on the stores mutex to be sure that no concurrent stop or store removal\n-         // is done while processing data\n-         return Flowable\n-               .using(publisherSemaphoreCallable,\n-                     semaphore -> handler.blockingPublisher(segmentedStore.publishKeys(segments, filter)),\n-                     Semaphore::release);\n-      }\n-\n-      return publishKeys(PersistenceUtil.combinePredicate(segments, keyPartitioner, filter), predicate);\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, boolean localInvocation,\n+         boolean includeStores) {\n+      return loadFromAllStores(key, keyPartitioner.getSegment(key), localInvocation, includeStores);\n    }\n \n-   private <K, V> MarshallableEntry<K, V> loadFromAllStoresSync(Object key, boolean localInvocation, boolean includeStores, int traceId) {\n-      acquireReadLock();\n-      try {\n-         checkStoreAvailability();\n-         if (trace) {\n-            log.tracef(\"Loading entry for key %s from stores with includeStores %s for id: %d\",\n-                  key, includeStores, traceId);\n-         }\n-         MarshallableEntry load = null;\n-         for (CacheLoader l : loaders) {\n-            if (allowLoad(l, localInvocation, includeStores)) {\n-               load = l.loadEntry(key);\n-               if (load != null)\n-                  break;\n-            }\n-         }\n-         if (trace) {\n-            log.tracef(\"Entry was loaded: %s for key %s from stores for id: %d\", load, key, traceId);\n-         }\n-         return load;\n-      } finally {\n-         releaseReadLock();\n+   @Override\n+   public <K, V> CompletionStage<MarshallableEntry<K, V>> loadFromAllStores(Object key, int segment,\n+         boolean localInvocation, boolean includeStores) {\n+      return Maybe.using(\n+            this::acquireReadLock,\n+            ignore -> {\n+               checkStoreAvailability();\n+               if (trace) {\n+                  log.tracef(\"Loading entry for key %s with segment %d\", key, segment);\n+               }\n+               return Flowable.fromIterable(stores.entrySet())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3MDgxOA=="}, "originalCommit": {"oid": "feea39f84c113b38c4f4db09940dc5e048a128c0"}, "originalPosition": 1204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjY0OTU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzowNToyOFrOGYVh8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzowNToyOFrOGYVh8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3MTc2Mw==", "bodyText": ".map(storeClass::cast) would avoid the warning :)", "url": "https://github.com/infinispan/infinispan/pull/8316#discussion_r428171763", "createdAt": "2020-05-20T17:05:28Z", "author": {"login": "pruivo"}, "path": "core/src/main/java/org/infinispan/persistence/manager/PersistenceManagerImpl.java", "diffHunk": "@@ -375,1083 +380,674 @@ public boolean isPreloaded() {\n             });\n    }\n \n-   @Override\n-   public void disableStore(String storeType) {\n-      if (enabled) {\n-         boolean noMoreStores;\n-         storesMutex.writeLock().lock();\n-         publisherSemaphore.acquireUninterruptibly(Integer.MAX_VALUE);\n+   private Single<Object> preloadEntry(long flags, MarshallableEntry<Object, Object> me, DataConversion keyDataConversion, DataConversion valueDataConversion) {\n+      // CallInterceptor will preserve the timestamps if the metadata is an InternalMetadataImpl instance\n+      InternalMetadataImpl metadata = new InternalMetadataImpl(me.getMetadata(), me.created(), me.lastUsed());\n+      Object key = keyDataConversion.toStorage(me.getKey());\n+      Object value = valueDataConversion.toStorage(me.getValue());\n+      PutKeyValueCommand cmd = commandsFactory.wired().buildPutKeyValueCommand(key, value, keyPartitioner.getSegment(key), metadata, flags);\n+      cmd.setInternalMetadata(me.getInternalMetadata());\n+\n+      CompletionStage<Object> stage;\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         final Transaction transaction = suspendIfNeeded();\n+         CompletionStage<Transaction> putStage;\n          try {\n-            removeCacheLoader(storeType, loaders);\n-            removeCacheWriter(storeType, nonTxWriters);\n-            removeCacheWriter(storeType, txWriters);\n-            noMoreStores = loaders.isEmpty() && nonTxWriters.isEmpty() && txWriters.isEmpty();\n-            readOnly = nonTxWriters.isEmpty() && txWriters.isEmpty();\n-\n-            if (!noMoreStores) {\n-               // Immediately poll store availability as the disabled store may have been the cause of the unavailability\n-               pollStoreAvailability();\n-            }\n-         } finally {\n-            publisherSemaphore.release(Integer.MAX_VALUE);\n-            storesMutex.writeLock().unlock();\n+            beginIfNeeded();\n+            putStage = invocationHelper.wired().invokeAsync(cmd, 1)\n+                  .thenApply(ignore -> {\n+                     try {\n+                        return transactionManager.suspend();\n+                     } catch (SystemException e) {\n+                        throw new PersistenceException(\"Unable to preload!\", e);\n+                     }\n+                  });\n+         } catch (Exception e) {\n+            throw new PersistenceException(\"Unable to preload!\", e);\n          }\n-\n-         if (noMoreStores) {\n-            if (availabilityFuture != null)\n-               availabilityFuture.cancel(true);\n-\n-            AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n-            AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n-            if (loaderInterceptor == null) {\n-               PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n-            } else {\n-               chain.removeInterceptor(loaderInterceptor.getClass());\n-            }\n-            AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n-            if (writerInterceptor == null) {\n-               writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n-               if (writerInterceptor == null) {\n-                  PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n-               } else {\n-                  chain.removeInterceptor(writerInterceptor.getClass());\n-               }\n-            } else {\n-               chain.removeInterceptor(writerInterceptor.getClass());\n+         //noinspection unchecked\n+         stage = (CompletionStage) blockingManager.whenCompleteBlocking(putStage, (pendingTransaction, t) -> {\n+            try {\n+               transactionManager.resume(pendingTransaction);\n+               commitIfNeeded(t == null);\n+            } catch (InvalidTransactionException | SystemException e) {\n+               throw new PersistenceException(\"Unable to preload!\", e);\n+            } finally {\n+               resumeIfNeeded(transaction);\n             }\n-            enabled = false;\n+         }, me.getKey());\n+      } else {\n+         stage = invocationHelper.wired().invokeAsync(cmd, 1);\n+      }\n+      return Maybe.fromCompletionStage(stage)\n+            .defaultIfEmpty(me);\n+   }\n+\n+   private void resumeIfNeeded(Transaction transaction) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null &&\n+            transaction != null) {\n+         try {\n+            transactionManager.resume(transaction);\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n       }\n    }\n \n-   @Override\n-   public <T> Set<T> getStores(Class<T> storeClass) {\n-      acquireReadLock();\n-      try {\n-         Set<T> result = new HashSet<>();\n-         for (CacheLoader l : loaders) {\n-            CacheLoader real = undelegate(l);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n+   private Transaction suspendIfNeeded() {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            return transactionManager.suspend();\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n          }\n+      }\n+      return null;\n+   }\n \n-         Consumer<CacheWriter> getWriters = writer -> {\n-            CacheWriter real = undelegate(writer);\n-            if (storeClass.isInstance(real)) {\n-               result.add(storeClass.cast(real));\n-            }\n-         };\n-         nonTxWriters.forEach(getWriters);\n-         txWriters.forEach(getWriters);\n+   private void beginIfNeeded() throws SystemException, NotSupportedException {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         transactionManager.begin();\n+      }\n+   }\n \n-         return result;\n-      } finally {\n-         releaseReadLock();\n+   private void commitIfNeeded(boolean success) {\n+      if (configuration.transaction().transactionMode().isTransactional() && transactionManager != null) {\n+         try {\n+            if (success) {\n+               transactionManager.commit();\n+            } else {\n+               transactionManager.rollback();\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n       }\n    }\n \n-   @Override\n-   public Collection<String> getStoresAsString() {\n-      acquireReadLock();\n-      try {\n-         Set<String> loaderTypes = new HashSet<>(loaders.size());\n-         for (CacheLoader loader : loaders)\n-            loaderTypes.add(undelegate(loader).getClass().getName());\n-         for (CacheWriter writer : nonTxWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         for (CacheWriter writer : txWriters)\n-            loaderTypes.add(undelegate(writer).getClass().getName());\n-         return loaderTypes;\n-      } finally {\n-         releaseReadLock();\n+   private long getMaxEntries() {\n+      long maxCount;\n+      if (configuration.memory().isEvictionEnabled() && (maxCount = configuration.memory().maxCount()) > 0) {\n+         return maxCount;\n       }\n+      return Long.MAX_VALUE;\n    }\n \n-   private static class AdvancedPurgeListener<K, V> implements AdvancedCacheExpirationWriter.ExpirationPurgeListener<K, V> {\n-      private final InternalExpirationManager<K, V> expirationManager;\n+   @GuardedBy(\"lock#readLock\")\n+   private long getFlagsForStateInsertion() {\n+      long flags = FlagBitSets.CACHE_MODE_LOCAL |\n+            FlagBitSets.SKIP_OWNERSHIP_CHECK |\n+            FlagBitSets.IGNORE_RETURN_VALUES |\n+            FlagBitSets.SKIP_CACHE_STORE |\n+            FlagBitSets.SKIP_LOCKING |\n+            FlagBitSets.SKIP_XSITE_BACKUP |\n+            FlagBitSets.IRAC_STATE;\n \n-      private AdvancedPurgeListener(InternalExpirationManager<K, V> expirationManager) {\n-         this.expirationManager = expirationManager;\n-      }\n+      boolean hasSharedStore = getStoreLocked(storeStatus -> storeStatus.config.shared()) != null;\n \n-      @Override\n-      public void marshalledEntryPurged(MarshallableEntry<K, V> entry) {\n-         expirationManager.handleInStoreExpiration(entry);\n+      if (!hasSharedStore  || !configuration.indexing().isVolatile()) {\n+         flags = EnumUtil.mergeBitSets(flags, FlagBitSets.SKIP_INDEXING);\n       }\n \n-      @Override\n-      public void entryPurged(K key) {\n-         expirationManager.handleInStoreExpiration(key);\n-      }\n+      return flags;\n    }\n \n    @Override\n-   public void purgeExpired() {\n-      if (!enabled)\n-         return;\n-      long start = -1;\n+   public CompletionStage<Void> disableStore(String storeType) {\n+      if (!enabled) {\n+         return CompletableFutures.completedNull();\n+      }\n+      boolean stillHasAStore = false;\n+      AggregateCompletionStage<Void> aggregateCompletionStage = CompletionStages.aggregateCompletionStage();\n+      long stamp = lock.writeLock();\n       try {\n-         if (trace) {\n-            log.trace(\"Purging cache store of expired entries\");\n-            start = timeService.time();\n+         boolean allAvailable = true;\n+         Iterator<StoreStatus> statusIterator = stores.values().iterator();\n+         while (statusIterator.hasNext()) {\n+            StoreStatus status = statusIterator.next();\n+            NonBlockingStore<?, ?> nonBlockingStore = unwrapStore(status.store());\n+            if (nonBlockingStore.getClass().getName().equals(storeType) || containedInAdapter(nonBlockingStore, storeType)) {\n+               statusIterator.remove();\n+               aggregateCompletionStage.dependsOn(nonBlockingStore.stop()\n+                     .whenComplete((v, t) -> {\n+                        if (t != null) {\n+                           log.warn(\"There was an error stopping the store\", t);\n+                        }\n+                     }));\n+            } else {\n+               stillHasAStore = true;\n+               allAvailable = allAvailable && status.availability;\n+            }\n          }\n \n-         acquireReadLock();\n-         try {\n-            checkStoreAvailability();\n-            Consumer<CacheWriter> purgeWriter = writer -> {\n-               // ISPN-6711 Shared stores should only be purged by the coordinator\n-               if (globalConfiguration.isClustered() && getStoreConfig(writer).shared() && !transport.isCoordinator())\n-                  return;\n-\n-               if (writer instanceof AdvancedCacheExpirationWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheExpirationWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               } else if (writer instanceof AdvancedCacheWriter) {\n-                  //noinspection unchecked\n-                  ((AdvancedCacheWriter)writer).purge(new WithinThreadExecutor(), advancedListener);\n-               }\n-            };\n-            nonTxWriters.forEach(purgeWriter);\n-            txWriters.forEach(purgeWriter);\n-         } finally {\n-            releaseReadLock();\n+         if (!stillHasAStore) {\n+            unavailableExceptionMessage = null;\n+            enabled = false;\n+            stopAvailabilityTask();\n+         } else if (allAvailable) {\n+            unavailableExceptionMessage = null;\n          }\n+      } finally {\n+         lock.unlockWrite(stamp);\n+      }\n \n-         if (trace) {\n-            log.tracef(\"Purging cache store completed in %s\",\n-                  Util.prettyPrintTime(timeService.timeDuration(start, TimeUnit.MILLISECONDS)));\n+      if (!stillHasAStore) {\n+         AsyncInterceptorChain chain = cache.wired().getAsyncInterceptorChain();\n+         AsyncInterceptor loaderInterceptor = chain.findInterceptorExtending(CacheLoaderInterceptor.class);\n+         if (loaderInterceptor == null) {\n+            PERSISTENCE.persistenceWithoutCacheLoaderInterceptor();\n+         } else {\n+            chain.removeInterceptor(loaderInterceptor.getClass());\n+         }\n+         AsyncInterceptor writerInterceptor = chain.findInterceptorExtending(CacheWriterInterceptor.class);\n+         if (writerInterceptor == null) {\n+            writerInterceptor = chain.findInterceptorWithClass(TransactionalStoreInterceptor.class);\n+            if (writerInterceptor == null) {\n+               PERSISTENCE.persistenceWithoutCacheWriteInterceptor();\n+            } else {\n+               chain.removeInterceptor(writerInterceptor.getClass());\n+            }\n+         } else {\n+            chain.removeInterceptor(writerInterceptor.getClass());\n          }\n-      } catch (Exception e) {\n-         PERSISTENCE.exceptionPurgingDataContainer(e);\n       }\n+      return aggregateCompletionStage.freeze();\n    }\n \n-   private <V> CompletionStage<V> supplyOnPersistenceExAndContinue(IntFunction<V> function, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.supplyBlocking(() -> function.apply(traceId), traceId);\n+   private <K, V> NonBlockingStore<K, V> unwrapStore(NonBlockingStore<K, V> store) {\n+      if (store instanceof DelegatingNonBlockingStore) {\n+         return ((DelegatingNonBlockingStore<K, V>) store).delegate();\n+      }\n+      return store;\n    }\n \n-   private CompletionStage<Void> runOnPersistenceExAndContinue(IntConsumer consumer, String traceMessage) {\n-      int traceId = getNextTraceNumber(traceMessage);\n-      return handler.runBlocking(() -> consumer.accept(traceId), traceId);\n+   private Object unwrapOldSPI(NonBlockingStore<?, ?> store) {\n+      if (store instanceof NonBlockingStoreAdapter) {\n+         return ((NonBlockingStoreAdapter<?, ?>) store).getActualStore();\n+      }\n+      return store;\n    }\n \n-   private static int getNextTraceNumber(String message) {\n-      if (trace) {\n-         int traceId = getNextTraceNumber();\n-         log.tracef(message, traceId);\n-         return traceId;\n-      }\n-      return -1;\n+   private boolean containedInAdapter(NonBlockingStore nonBlockingStore, String adaptedClassName) {\n+      return nonBlockingStore instanceof NonBlockingStoreAdapter &&\n+            ((NonBlockingStoreAdapter<?, ?>) nonBlockingStore).getActualStore().getClass().getName().equals(adaptedClassName);\n    }\n \n    @Override\n-   public CompletionStage<Void> clearAllStores(Predicate<? super StoreConfiguration> predicate) {\n-      return runOnPersistenceExAndContinue(traceId -> clearAllStoresSync(predicate, traceId), \"Clearing all stores for id %d\");\n+   public <T> Set<T> getStores(Class<T> storeClass) {\n+      long stamp = acquireReadLock();\n+      try {\n+         return stores.keySet().stream()\n+               .map(this::unwrapStore)\n+               .map(this::unwrapOldSPI)\n+               .filter(storeClass::isInstance)\n+               .map(store -> (T) store)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "feea39f84c113b38c4f4db09940dc5e048a128c0"}, "originalPosition": 918}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4208, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}