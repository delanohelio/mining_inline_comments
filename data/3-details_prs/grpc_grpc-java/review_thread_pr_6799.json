{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMTk4NTYz", "number": 6799, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NDoyMFrODrbTjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMjo0NDowM1rODwl0fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODYyNzMzOnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NDoyMFrOF7xKpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxNjozNVrOF-qOkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTg0Nw==", "bodyText": "Returns previous value if exists?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398215847", "createdAt": "2020-03-25T22:44:20Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0Nzg5MQ==", "bodyText": "yes, added to the doc.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401247891", "createdAt": "2020-03-31T22:16:35Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTg0Nw=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODc0ODE3OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMzozNjoyM1rOF7yTMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjowMzozNlrOF-p5fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIzNDQxOQ==", "bodyText": "@code or no tag instead of @link because it is itself.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398234419", "createdAt": "2020-03-25T23:36:23Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0MjQ5Mg==", "bodyText": "done.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401242492", "createdAt": "2020-03-31T22:03:36Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIzNDQxOQ=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTAyODU4OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwMjowODozM1rOF704YQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxNjo0NlrOF-qO5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3NjcwNQ==", "bodyText": "Returns the current value?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398276705", "createdAt": "2020-03-26T02:08:33Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);\n+\n+  /**\n+   * Returns cached value for given key if exists, otherwise {@code null}. This operation doesn't\n+   * return already expired cache entry.\n+   */\n+  @Nullable\n+  @CheckReturnValue\n+  V read(K key);\n+\n+  /**\n+   * Invalidates an entry for given key if exists. This operation will trigger {@link\n+   * EvictionListener} with {@link EvictionType#EXPLICIT}.\n+   */\n+  @Nullable\n+  V invalidate(K key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0Nzk3Mw==", "bodyText": "added to the doc.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401247973", "createdAt": "2020-03-31T22:16:46Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);\n+\n+  /**\n+   * Returns cached value for given key if exists, otherwise {@code null}. This operation doesn't\n+   * return already expired cache entry.\n+   */\n+  @Nullable\n+  @CheckReturnValue\n+  V read(K key);\n+\n+  /**\n+   * Invalidates an entry for given key if exists. This operation will trigger {@link\n+   * EvictionListener} with {@link EvictionType#EXPLICIT}.\n+   */\n+  @Nullable\n+  V invalidate(K key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3NjcwNQ=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTA0NTQ4OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwMjoxNzo0M1rOF71CHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxOTozNFrOF-qTDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3OTE5Ng==", "bodyText": "Noop if the key or value does not exist?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398279196", "createdAt": "2020-03-26T02:17:43Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);\n+\n+  /**\n+   * Returns cached value for given key if exists, otherwise {@code null}. This operation doesn't\n+   * return already expired cache entry.\n+   */\n+  @Nullable\n+  @CheckReturnValue\n+  V read(K key);\n+\n+  /**\n+   * Invalidates an entry for given key if exists. This operation will trigger {@link\n+   * EvictionListener} with {@link EvictionType#EXPLICIT}.\n+   */\n+  @Nullable\n+  V invalidate(K key);\n+\n+  /**\n+   * Invalidates cache entries for given keys. This operation will trigger {@link EvictionListener}\n+   * with {@link EvictionType#EXPLICIT}.\n+   */\n+  void invalidateAll(Iterable<K> keys);\n+\n+  /** Returns {@code true} if given key is cached. */\n+  @CheckReturnValue\n+  boolean hasCacheEntry(K key);\n+\n+  /**\n+   * Returns the estimated number of entry of the cache. Note that the size can be larger than its\n+   * true size, because there might be already expired cache.\n+   */\n+  @CheckReturnValue\n+  int estimatedSize();\n+\n+  /** Closes underlying resources. */\n+  void close();\n+\n+  /** A Listener notifies cache eviction events. */\n+  interface EvictionListener<K, V> {\n+\n+    /**\n+     * Notifies the listener when any cache entry is evicted. Implementation can assume that this\n+     * method is called serially. Implementation should be non blocking, for long running task\n+     * consider offloading the task to {@link java.util.concurrent.Executor}.\n+     */\n+    void onEviction(K key, V value, EvictionType cause);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0OTAzOQ==", "bodyText": "this method is called not for calling. when this listener is called, all values should be present.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401249039", "createdAt": "2020-03-31T22:19:34Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LruCache.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+\n+/** An LruCache is a cache with least recently used eviction. */\n+interface LruCache<K, V> {\n+\n+  /**\n+   * Populates a cache entry. If the cache entry for given key already exists, the value will be\n+   * replaced to the new value.\n+   */\n+  @Nullable\n+  V cache(K key, V value);\n+\n+  /**\n+   * Returns cached value for given key if exists, otherwise {@code null}. This operation doesn't\n+   * return already expired cache entry.\n+   */\n+  @Nullable\n+  @CheckReturnValue\n+  V read(K key);\n+\n+  /**\n+   * Invalidates an entry for given key if exists. This operation will trigger {@link\n+   * EvictionListener} with {@link EvictionType#EXPLICIT}.\n+   */\n+  @Nullable\n+  V invalidate(K key);\n+\n+  /**\n+   * Invalidates cache entries for given keys. This operation will trigger {@link EvictionListener}\n+   * with {@link EvictionType#EXPLICIT}.\n+   */\n+  void invalidateAll(Iterable<K> keys);\n+\n+  /** Returns {@code true} if given key is cached. */\n+  @CheckReturnValue\n+  boolean hasCacheEntry(K key);\n+\n+  /**\n+   * Returns the estimated number of entry of the cache. Note that the size can be larger than its\n+   * true size, because there might be already expired cache.\n+   */\n+  @CheckReturnValue\n+  int estimatedSize();\n+\n+  /** Closes underlying resources. */\n+  void close();\n+\n+  /** A Listener notifies cache eviction events. */\n+  interface EvictionListener<K, V> {\n+\n+    /**\n+     * Notifies the listener when any cache entry is evicted. Implementation can assume that this\n+     * method is called serially. Implementation should be non blocking, for long running task\n+     * consider offloading the task to {@link java.util.concurrent.Executor}.\n+     */\n+    void onEviction(K key, V value, EvictionType cause);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3OTE5Ng=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTA5NzI4OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwMjo0NTowM1rOF71fzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjowMzozMVrOF-p5Qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI4Njc5Nw==", "bodyText": "nit: estimatedMaxSizeBytes to be consistent in name.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398286797", "createdAt": "2020-03-26T02:45:03Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0MjQzNQ==", "bodyText": "done.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401242435", "createdAt": "2020-03-31T22:03:31Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI4Njc5Nw=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTE1NjQ3OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwMzoyMTozOVrOF72Duw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxMzoxN1rOF-qJUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI5NTk5NQ==", "bodyText": "optional: you can do\nLinkedHashLruCache<?, ?>.SizedValue that = (LinkedHashLruCache<?, ?>.SizedValue) o;\nto avoid @SuppressWarnings(\"unchecked\")", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r398295995", "createdAt": "2020-03-26T03:21:39Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired\n+    return readInternal(key) != null;\n+  }\n+\n+  /** Returns shallow copied values in the cache. */\n+  public final List<V> values() {\n+    synchronized (lock) {\n+      List<V> list = new ArrayList<>(delegate.size());\n+      for (SizedValue value : delegate.values()) {\n+        list.add(value.value);\n+      }\n+      return Collections.unmodifiableList(list);\n+    }\n+  }\n+\n+  /**\n+   * Resizes cache. If new size is smaller than current estimated size, it will free up space by\n+   * removing expired entries and removing oldest entries by LRU order.\n+   */\n+  public final void resize(int newSizeBytes) {\n+    long now = timeProvider.currentTimeNanos();\n+    synchronized (lock) {\n+      long estimatedSizeBytesCopy = estimatedMaxSizeBytes;\n+      this.estimatedMaxSizeBytes = newSizeBytes;\n+      if (estimatedSizeBytesCopy <= newSizeBytes) {\n+        // new size is larger no need to do cleanup\n+        return;\n+      }\n+      // cleanup expired entries\n+      cleanupExpiredEntries(now);\n+\n+      // cleanup eldest entry until new size limit\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && estimatedMaxSizeBytes > this.estimatedSizeBytes.get()) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        lruIter.remove();\n+        // eviction listener will update the estimatedSizeBytes\n+        evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.SIZE);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final int estimatedSize() {\n+    synchronized (lock) {\n+      return delegate.size();\n+    }\n+  }\n+\n+  private boolean cleanupExpiredEntries(long now) {\n+    return cleanupExpiredEntries(Integer.MAX_VALUE, now);\n+  }\n+\n+  // maxExpiredEntries is by number of entries\n+  private boolean cleanupExpiredEntries(int maxExpiredEntries, long now) {\n+    checkArgument(maxExpiredEntries > 0, \"maxExpiredEntries must be positive\");\n+    boolean removedAny = false;\n+    synchronized (lock) {\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && maxExpiredEntries > 0) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        if (isExpired(entry.getKey(), entry.getValue().value, now)) {\n+          lruIter.remove();\n+          evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.EXPIRED);\n+          removedAny = true;\n+          maxExpiredEntries--;\n+        }\n+      }\n+    }\n+    return removedAny;\n+  }\n+\n+  @Override\n+  public final void close() {\n+    synchronized (lock) {\n+      periodicCleaner.stop();\n+      doClose();\n+      delegate.clear();\n+    }\n+  }\n+\n+  protected void doClose() {}\n+\n+  /** Periodically cleans up the AsyncRequestCache. */\n+  private final class PeriodicCleaner implements Runnable {\n+\n+    private final ScheduledExecutorService ses;\n+    private final int interval;\n+    private final TimeUnit intervalUnit;\n+    private ScheduledFuture<?> scheduledFuture;\n+\n+    PeriodicCleaner(ScheduledExecutorService ses, int interval, TimeUnit intervalUnit) {\n+      this.ses = checkNotNull(ses, \"ses\");\n+      checkState(interval > 0, \"interval must be positive\");\n+      this.interval = interval;\n+      this.intervalUnit = checkNotNull(intervalUnit, \"intervalUnit\");\n+    }\n+\n+    PeriodicCleaner start() {\n+      checkState(scheduledFuture == null, \"cleaning task can be started only once\");\n+      this.scheduledFuture =\n+          ses.scheduleAtFixedRate(this, interval, interval, intervalUnit);\n+      return this;\n+    }\n+\n+    void stop() {\n+      if (scheduledFuture != null) {\n+        scheduledFuture.cancel(false);\n+        scheduledFuture = null;\n+      }\n+    }\n+\n+    @Override\n+    public void run() {\n+      cleanupExpiredEntries(timeProvider.currentTimeNanos());\n+    }\n+  }\n+\n+  /** A {@link EvictionListener} keeps track of size. */\n+  private final class SizeHandlingEvictionListener implements EvictionListener<K, SizedValue> {\n+\n+    private final EvictionListener<K, V> delegate;\n+\n+    SizeHandlingEvictionListener(@Nullable EvictionListener<K, V> delegate) {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public void onEviction(K key, SizedValue value, EvictionType cause) {\n+      estimatedSizeBytes.addAndGet(-1 * estimateSizeOf(key, value.value));\n+      if (delegate != null) {\n+        delegate.onEviction(key, value.value, cause);\n+      }\n+    }\n+  }\n+\n+  private final class SizedValue {\n+    volatile int size;\n+    final V value;\n+\n+    SizedValue(int size, V value) {\n+      this.size = size;\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      // NOTE: the size doesn't affect equality\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      @SuppressWarnings(\"unchecked\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 366}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0NjU0NQ==", "bodyText": "thanks, done.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401246545", "createdAt": "2020-03-31T22:13:17Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired\n+    return readInternal(key) != null;\n+  }\n+\n+  /** Returns shallow copied values in the cache. */\n+  public final List<V> values() {\n+    synchronized (lock) {\n+      List<V> list = new ArrayList<>(delegate.size());\n+      for (SizedValue value : delegate.values()) {\n+        list.add(value.value);\n+      }\n+      return Collections.unmodifiableList(list);\n+    }\n+  }\n+\n+  /**\n+   * Resizes cache. If new size is smaller than current estimated size, it will free up space by\n+   * removing expired entries and removing oldest entries by LRU order.\n+   */\n+  public final void resize(int newSizeBytes) {\n+    long now = timeProvider.currentTimeNanos();\n+    synchronized (lock) {\n+      long estimatedSizeBytesCopy = estimatedMaxSizeBytes;\n+      this.estimatedMaxSizeBytes = newSizeBytes;\n+      if (estimatedSizeBytesCopy <= newSizeBytes) {\n+        // new size is larger no need to do cleanup\n+        return;\n+      }\n+      // cleanup expired entries\n+      cleanupExpiredEntries(now);\n+\n+      // cleanup eldest entry until new size limit\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && estimatedMaxSizeBytes > this.estimatedSizeBytes.get()) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        lruIter.remove();\n+        // eviction listener will update the estimatedSizeBytes\n+        evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.SIZE);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final int estimatedSize() {\n+    synchronized (lock) {\n+      return delegate.size();\n+    }\n+  }\n+\n+  private boolean cleanupExpiredEntries(long now) {\n+    return cleanupExpiredEntries(Integer.MAX_VALUE, now);\n+  }\n+\n+  // maxExpiredEntries is by number of entries\n+  private boolean cleanupExpiredEntries(int maxExpiredEntries, long now) {\n+    checkArgument(maxExpiredEntries > 0, \"maxExpiredEntries must be positive\");\n+    boolean removedAny = false;\n+    synchronized (lock) {\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && maxExpiredEntries > 0) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        if (isExpired(entry.getKey(), entry.getValue().value, now)) {\n+          lruIter.remove();\n+          evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.EXPIRED);\n+          removedAny = true;\n+          maxExpiredEntries--;\n+        }\n+      }\n+    }\n+    return removedAny;\n+  }\n+\n+  @Override\n+  public final void close() {\n+    synchronized (lock) {\n+      periodicCleaner.stop();\n+      doClose();\n+      delegate.clear();\n+    }\n+  }\n+\n+  protected void doClose() {}\n+\n+  /** Periodically cleans up the AsyncRequestCache. */\n+  private final class PeriodicCleaner implements Runnable {\n+\n+    private final ScheduledExecutorService ses;\n+    private final int interval;\n+    private final TimeUnit intervalUnit;\n+    private ScheduledFuture<?> scheduledFuture;\n+\n+    PeriodicCleaner(ScheduledExecutorService ses, int interval, TimeUnit intervalUnit) {\n+      this.ses = checkNotNull(ses, \"ses\");\n+      checkState(interval > 0, \"interval must be positive\");\n+      this.interval = interval;\n+      this.intervalUnit = checkNotNull(intervalUnit, \"intervalUnit\");\n+    }\n+\n+    PeriodicCleaner start() {\n+      checkState(scheduledFuture == null, \"cleaning task can be started only once\");\n+      this.scheduledFuture =\n+          ses.scheduleAtFixedRate(this, interval, interval, intervalUnit);\n+      return this;\n+    }\n+\n+    void stop() {\n+      if (scheduledFuture != null) {\n+        scheduledFuture.cancel(false);\n+        scheduledFuture = null;\n+      }\n+    }\n+\n+    @Override\n+    public void run() {\n+      cleanupExpiredEntries(timeProvider.currentTimeNanos());\n+    }\n+  }\n+\n+  /** A {@link EvictionListener} keeps track of size. */\n+  private final class SizeHandlingEvictionListener implements EvictionListener<K, SizedValue> {\n+\n+    private final EvictionListener<K, V> delegate;\n+\n+    SizeHandlingEvictionListener(@Nullable EvictionListener<K, V> delegate) {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public void onEviction(K key, SizedValue value, EvictionType cause) {\n+      estimatedSizeBytes.addAndGet(-1 * estimateSizeOf(key, value.value));\n+      if (delegate != null) {\n+        delegate.onEviction(key, value.value, cause);\n+      }\n+    }\n+  }\n+\n+  private final class SizedValue {\n+    volatile int size;\n+    final V value;\n+\n+    SizedValue(int size, V value) {\n+      this.size = size;\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      // NOTE: the size doesn't affect equality\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      @SuppressWarnings(\"unchecked\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI5NTk5NQ=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 366}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NzYwMDU1OnYy", "diffSide": "RIGHT", "path": "rls/src/test/java/io/grpc/rls/internal/LinkedHashLruCacheTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxODozMzozNFrOF-i6cA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoyMzoxMFrOF-qYrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTEyODA0OA==", "bodyText": "nit: newline at end to be consistent.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401128048", "createdAt": "2020-03-31T18:33:34Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/test/java/io/grpc/rls/internal/LinkedHashLruCacheTest.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.truth.Truth.assertThat;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.CALLS_REAL_METHODS;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.grpc.internal.TimeProvider;\n+import io.grpc.rls.internal.LruCache.EvictionListener;\n+import io.grpc.rls.internal.LruCache.EvictionType;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnit;\n+import org.mockito.junit.MockitoRule;\n+\n+@RunWith(JUnit4.class)\n+public class LinkedHashLruCacheTest {\n+\n+  private static final int MAX_SIZE = 5;\n+\n+  @Rule\n+  public final MockitoRule mocks = MockitoJUnit.rule();\n+\n+  private final DoNotUseFakeScheduledService fakeScheduledService =\n+      mock(DoNotUseFakeScheduledService.class, CALLS_REAL_METHODS);\n+  private final TimeProvider timeProvider = fakeScheduledService.getFakeTicker();\n+\n+  @Mock\n+  private EvictionListener<Integer, Entry> evictionListener;\n+  private LinkedHashLruCache<Integer, Entry> cache;\n+\n+  @Before\n+  public void setUp() {\n+    this.cache = new LinkedHashLruCache<Integer, Entry>(\n+        MAX_SIZE,\n+        evictionListener,\n+        10,\n+        TimeUnit.NANOSECONDS,\n+        fakeScheduledService,\n+        timeProvider) {\n+      @Override\n+      protected boolean isExpired(Integer key, Entry value, long nowNanos) {\n+        return value.expireTime <= nowNanos;\n+      }\n+    };\n+  }\n+\n+  @Test\n+  public void eviction_size() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      cache.cache(i, new Entry(\"Entry\" + i, Long.MAX_VALUE));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    verify(evictionListener).onEviction(1, new Entry(\"Entry1\", Long.MAX_VALUE), EvictionType.SIZE);\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void size() {\n+    Entry entry1 = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry entry2 = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, entry1);\n+    cache.cache(1, entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(2);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(entry1);\n+    assertThat(cache.estimatedSize()).isEqualTo(1);\n+\n+    assertThat(cache.invalidate(1)).isEqualTo(entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(0);\n+  }\n+\n+  @Test\n+  public void eviction_expire() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPIRED);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(1, survivor, EvictionType.EXPIRED);\n+  }\n+\n+  @Test\n+  public void eviction_explicit() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(toBeEvicted);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPLICIT);\n+  }\n+\n+  @Test\n+  public void eviction_replaced() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(0, survivor);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.REPLACED);\n+  }\n+\n+  @Test\n+  public void eviction_size_shouldEvictAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last two entries are <= current time (already expired)\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i - 1));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    // should remove MAX_SIZE-1 instead of MAX_SIZE because MAX_SIZE is accessed later\n+    verify(evictionListener)\n+        .onEviction(eq(MAX_SIZE - 1), any(Entry.class), eq(EvictionType.EXPIRED));\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void eviction_get_shouldNotReturnAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last entry is already expired when added\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i));\n+    }\n+\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+    assertThat(cache.read(MAX_SIZE)).isNull();\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE - 1);\n+    verify(evictionListener).onEviction(eq(MAX_SIZE), any(Entry.class), eq(EvictionType.EXPIRED));\n+  }\n+\n+  private static final class Entry {\n+    String value;\n+    long expireTime;\n+\n+    Entry(String value, long expireTime) {\n+      this.value = value;\n+      this.expireTime = expireTime;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      Entry entry = (Entry) o;\n+      return expireTime == entry.expireTime && Objects.equals(value, entry.value);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(value, expireTime);\n+    }\n+  }\n+\n+  /**\n+   * A fake minimal implementation of ScheduledExecutorService *only* supports scheduledAtFixedRate\n+   * with a lot of limitation / assumptions. Only intended to be used in this test with\n+   * CALL_REAL_METHODS mock.\n+   */\n+  private abstract static class DoNotUseFakeScheduledService implements ScheduledExecutorService {\n+\n+    private long currTimeNanos;\n+    private long period;\n+    private long nextRun;\n+    private AtomicReference<Runnable> command;\n+\n+    @Override\n+    public ScheduledFuture<?> scheduleAtFixedRate(\n+        Runnable command, long initialDelay, long period, TimeUnit unit) {\n+      // hack to initialize\n+      if (this.command == null) {\n+        this.command = new AtomicReference<>();\n+      }\n+      checkState(this.command.get() == null, \"only can schedule one\");\n+      checkState(period > 0, \"period should be positive\");\n+      checkState(initialDelay >= 0, \"initial delay should be >= 0\");\n+      if (initialDelay == 0) {\n+        initialDelay = period;\n+        command.run();\n+      }\n+      this.command.set(checkNotNull(command, \"command\"));\n+      this.nextRun = checkNotNull(unit, \"unit\").toNanos(initialDelay) + currTimeNanos;\n+      this.period = unit.toNanos(period);\n+      return mock(ScheduledFuture.class);\n+    }\n+\n+    TimeProvider getFakeTicker() {\n+      return new TimeProvider() {\n+        @Override\n+        public long currentTimeNanos() {\n+          return currTimeNanos;\n+        }\n+      };\n+    }\n+\n+    void advance(long delta, TimeUnit unit) {\n+      // if scheduled command, only can advance the ticker to trigger at most 1 event\n+      boolean scheduled = command != null && command.get() != null;\n+      long deltaNanos = unit.toNanos(delta);\n+      if (scheduled) {\n+        checkArgument(\n+            (this.currTimeNanos + deltaNanos) < (nextRun + 2 * period),\n+            \"Cannot advance ticker because more than one repeated tasks will run\");\n+        long finalTime = this.currTimeNanos + deltaNanos;\n+        if (finalTime >= nextRun) {\n+          nextRun += period;\n+          this.currTimeNanos = nextRun;\n+          command.get().run();\n+        }\n+        this.currTimeNanos = finalTime;\n+      } else {\n+        this.currTimeNanos += deltaNanos;\n+      }\n+    }\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI1MDQ3Nw==", "bodyText": "done.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401250477", "createdAt": "2020-03-31T22:23:10Z", "author": {"login": "creamsoup"}, "path": "rls/src/test/java/io/grpc/rls/internal/LinkedHashLruCacheTest.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.truth.Truth.assertThat;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.CALLS_REAL_METHODS;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.grpc.internal.TimeProvider;\n+import io.grpc.rls.internal.LruCache.EvictionListener;\n+import io.grpc.rls.internal.LruCache.EvictionType;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnit;\n+import org.mockito.junit.MockitoRule;\n+\n+@RunWith(JUnit4.class)\n+public class LinkedHashLruCacheTest {\n+\n+  private static final int MAX_SIZE = 5;\n+\n+  @Rule\n+  public final MockitoRule mocks = MockitoJUnit.rule();\n+\n+  private final DoNotUseFakeScheduledService fakeScheduledService =\n+      mock(DoNotUseFakeScheduledService.class, CALLS_REAL_METHODS);\n+  private final TimeProvider timeProvider = fakeScheduledService.getFakeTicker();\n+\n+  @Mock\n+  private EvictionListener<Integer, Entry> evictionListener;\n+  private LinkedHashLruCache<Integer, Entry> cache;\n+\n+  @Before\n+  public void setUp() {\n+    this.cache = new LinkedHashLruCache<Integer, Entry>(\n+        MAX_SIZE,\n+        evictionListener,\n+        10,\n+        TimeUnit.NANOSECONDS,\n+        fakeScheduledService,\n+        timeProvider) {\n+      @Override\n+      protected boolean isExpired(Integer key, Entry value, long nowNanos) {\n+        return value.expireTime <= nowNanos;\n+      }\n+    };\n+  }\n+\n+  @Test\n+  public void eviction_size() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      cache.cache(i, new Entry(\"Entry\" + i, Long.MAX_VALUE));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    verify(evictionListener).onEviction(1, new Entry(\"Entry1\", Long.MAX_VALUE), EvictionType.SIZE);\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void size() {\n+    Entry entry1 = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry entry2 = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, entry1);\n+    cache.cache(1, entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(2);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(entry1);\n+    assertThat(cache.estimatedSize()).isEqualTo(1);\n+\n+    assertThat(cache.invalidate(1)).isEqualTo(entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(0);\n+  }\n+\n+  @Test\n+  public void eviction_expire() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPIRED);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(1, survivor, EvictionType.EXPIRED);\n+  }\n+\n+  @Test\n+  public void eviction_explicit() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(toBeEvicted);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPLICIT);\n+  }\n+\n+  @Test\n+  public void eviction_replaced() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(0, survivor);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.REPLACED);\n+  }\n+\n+  @Test\n+  public void eviction_size_shouldEvictAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last two entries are <= current time (already expired)\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i - 1));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    // should remove MAX_SIZE-1 instead of MAX_SIZE because MAX_SIZE is accessed later\n+    verify(evictionListener)\n+        .onEviction(eq(MAX_SIZE - 1), any(Entry.class), eq(EvictionType.EXPIRED));\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void eviction_get_shouldNotReturnAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last entry is already expired when added\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i));\n+    }\n+\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+    assertThat(cache.read(MAX_SIZE)).isNull();\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE - 1);\n+    verify(evictionListener).onEviction(eq(MAX_SIZE), any(Entry.class), eq(EvictionType.EXPIRED));\n+  }\n+\n+  private static final class Entry {\n+    String value;\n+    long expireTime;\n+\n+    Entry(String value, long expireTime) {\n+      this.value = value;\n+      this.expireTime = expireTime;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      Entry entry = (Entry) o;\n+      return expireTime == entry.expireTime && Objects.equals(value, entry.value);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(value, expireTime);\n+    }\n+  }\n+\n+  /**\n+   * A fake minimal implementation of ScheduledExecutorService *only* supports scheduledAtFixedRate\n+   * with a lot of limitation / assumptions. Only intended to be used in this test with\n+   * CALL_REAL_METHODS mock.\n+   */\n+  private abstract static class DoNotUseFakeScheduledService implements ScheduledExecutorService {\n+\n+    private long currTimeNanos;\n+    private long period;\n+    private long nextRun;\n+    private AtomicReference<Runnable> command;\n+\n+    @Override\n+    public ScheduledFuture<?> scheduleAtFixedRate(\n+        Runnable command, long initialDelay, long period, TimeUnit unit) {\n+      // hack to initialize\n+      if (this.command == null) {\n+        this.command = new AtomicReference<>();\n+      }\n+      checkState(this.command.get() == null, \"only can schedule one\");\n+      checkState(period > 0, \"period should be positive\");\n+      checkState(initialDelay >= 0, \"initial delay should be >= 0\");\n+      if (initialDelay == 0) {\n+        initialDelay = period;\n+        command.run();\n+      }\n+      this.command.set(checkNotNull(command, \"command\"));\n+      this.nextRun = checkNotNull(unit, \"unit\").toNanos(initialDelay) + currTimeNanos;\n+      this.period = unit.toNanos(period);\n+      return mock(ScheduledFuture.class);\n+    }\n+\n+    TimeProvider getFakeTicker() {\n+      return new TimeProvider() {\n+        @Override\n+        public long currentTimeNanos() {\n+          return currTimeNanos;\n+        }\n+      };\n+    }\n+\n+    void advance(long delta, TimeUnit unit) {\n+      // if scheduled command, only can advance the ticker to trigger at most 1 event\n+      boolean scheduled = command != null && command.get() != null;\n+      long deltaNanos = unit.toNanos(delta);\n+      if (scheduled) {\n+        checkArgument(\n+            (this.currTimeNanos + deltaNanos) < (nextRun + 2 * period),\n+            \"Cannot advance ticker because more than one repeated tasks will run\");\n+        long finalTime = this.currTimeNanos + deltaNanos;\n+        if (finalTime >= nextRun) {\n+          nextRun += period;\n+          this.currTimeNanos = nextRun;\n+          command.get().run();\n+        }\n+        this.currTimeNanos = finalTime;\n+      } else {\n+        this.currTimeNanos += deltaNanos;\n+      }\n+    }\n+  }\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTEyODA0OA=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 255}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NzcwNzU5OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTowMToxOFrOF-j9Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjoxMjowM1rOF-qHeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE0NTA5OQ==", "bodyText": "These checkStates/checkNotNull are repeated and redundant.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401145099", "createdAt": "2020-03-31T19:01:18Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired\n+    return readInternal(key) != null;\n+  }\n+\n+  /** Returns shallow copied values in the cache. */\n+  public final List<V> values() {\n+    synchronized (lock) {\n+      List<V> list = new ArrayList<>(delegate.size());\n+      for (SizedValue value : delegate.values()) {\n+        list.add(value.value);\n+      }\n+      return Collections.unmodifiableList(list);\n+    }\n+  }\n+\n+  /**\n+   * Resizes cache. If new size is smaller than current estimated size, it will free up space by\n+   * removing expired entries and removing oldest entries by LRU order.\n+   */\n+  public final void resize(int newSizeBytes) {\n+    long now = timeProvider.currentTimeNanos();\n+    synchronized (lock) {\n+      long estimatedSizeBytesCopy = estimatedMaxSizeBytes;\n+      this.estimatedMaxSizeBytes = newSizeBytes;\n+      if (estimatedSizeBytesCopy <= newSizeBytes) {\n+        // new size is larger no need to do cleanup\n+        return;\n+      }\n+      // cleanup expired entries\n+      cleanupExpiredEntries(now);\n+\n+      // cleanup eldest entry until new size limit\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && estimatedMaxSizeBytes > this.estimatedSizeBytes.get()) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        lruIter.remove();\n+        // eviction listener will update the estimatedSizeBytes\n+        evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.SIZE);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final int estimatedSize() {\n+    synchronized (lock) {\n+      return delegate.size();\n+    }\n+  }\n+\n+  private boolean cleanupExpiredEntries(long now) {\n+    return cleanupExpiredEntries(Integer.MAX_VALUE, now);\n+  }\n+\n+  // maxExpiredEntries is by number of entries\n+  private boolean cleanupExpiredEntries(int maxExpiredEntries, long now) {\n+    checkArgument(maxExpiredEntries > 0, \"maxExpiredEntries must be positive\");\n+    boolean removedAny = false;\n+    synchronized (lock) {\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && maxExpiredEntries > 0) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        if (isExpired(entry.getKey(), entry.getValue().value, now)) {\n+          lruIter.remove();\n+          evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.EXPIRED);\n+          removedAny = true;\n+          maxExpiredEntries--;\n+        }\n+      }\n+    }\n+    return removedAny;\n+  }\n+\n+  @Override\n+  public final void close() {\n+    synchronized (lock) {\n+      periodicCleaner.stop();\n+      doClose();\n+      delegate.clear();\n+    }\n+  }\n+\n+  protected void doClose() {}\n+\n+  /** Periodically cleans up the AsyncRequestCache. */\n+  private final class PeriodicCleaner implements Runnable {\n+\n+    private final ScheduledExecutorService ses;\n+    private final int interval;\n+    private final TimeUnit intervalUnit;\n+    private ScheduledFuture<?> scheduledFuture;\n+\n+    PeriodicCleaner(ScheduledExecutorService ses, int interval, TimeUnit intervalUnit) {\n+      this.ses = checkNotNull(ses, \"ses\");\n+      checkState(interval > 0, \"interval must be positive\");\n+      this.interval = interval;\n+      this.intervalUnit = checkNotNull(intervalUnit, \"intervalUnit\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 307}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0NjA3NA==", "bodyText": "removed one from caller side.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401246074", "createdAt": "2020-03-31T22:12:03Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired\n+    return readInternal(key) != null;\n+  }\n+\n+  /** Returns shallow copied values in the cache. */\n+  public final List<V> values() {\n+    synchronized (lock) {\n+      List<V> list = new ArrayList<>(delegate.size());\n+      for (SizedValue value : delegate.values()) {\n+        list.add(value.value);\n+      }\n+      return Collections.unmodifiableList(list);\n+    }\n+  }\n+\n+  /**\n+   * Resizes cache. If new size is smaller than current estimated size, it will free up space by\n+   * removing expired entries and removing oldest entries by LRU order.\n+   */\n+  public final void resize(int newSizeBytes) {\n+    long now = timeProvider.currentTimeNanos();\n+    synchronized (lock) {\n+      long estimatedSizeBytesCopy = estimatedMaxSizeBytes;\n+      this.estimatedMaxSizeBytes = newSizeBytes;\n+      if (estimatedSizeBytesCopy <= newSizeBytes) {\n+        // new size is larger no need to do cleanup\n+        return;\n+      }\n+      // cleanup expired entries\n+      cleanupExpiredEntries(now);\n+\n+      // cleanup eldest entry until new size limit\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && estimatedMaxSizeBytes > this.estimatedSizeBytes.get()) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        lruIter.remove();\n+        // eviction listener will update the estimatedSizeBytes\n+        evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.SIZE);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final int estimatedSize() {\n+    synchronized (lock) {\n+      return delegate.size();\n+    }\n+  }\n+\n+  private boolean cleanupExpiredEntries(long now) {\n+    return cleanupExpiredEntries(Integer.MAX_VALUE, now);\n+  }\n+\n+  // maxExpiredEntries is by number of entries\n+  private boolean cleanupExpiredEntries(int maxExpiredEntries, long now) {\n+    checkArgument(maxExpiredEntries > 0, \"maxExpiredEntries must be positive\");\n+    boolean removedAny = false;\n+    synchronized (lock) {\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && maxExpiredEntries > 0) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        if (isExpired(entry.getKey(), entry.getValue().value, now)) {\n+          lruIter.remove();\n+          evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.EXPIRED);\n+          removedAny = true;\n+          maxExpiredEntries--;\n+        }\n+      }\n+    }\n+    return removedAny;\n+  }\n+\n+  @Override\n+  public final void close() {\n+    synchronized (lock) {\n+      periodicCleaner.stop();\n+      doClose();\n+      delegate.clear();\n+    }\n+  }\n+\n+  protected void doClose() {}\n+\n+  /** Periodically cleans up the AsyncRequestCache. */\n+  private final class PeriodicCleaner implements Runnable {\n+\n+    private final ScheduledExecutorService ses;\n+    private final int interval;\n+    private final TimeUnit intervalUnit;\n+    private ScheduledFuture<?> scheduledFuture;\n+\n+    PeriodicCleaner(ScheduledExecutorService ses, int interval, TimeUnit intervalUnit) {\n+      this.ses = checkNotNull(ses, \"ses\");\n+      checkState(interval > 0, \"interval must be positive\");\n+      this.interval = interval;\n+      this.intervalUnit = checkNotNull(intervalUnit, \"intervalUnit\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE0NTA5OQ=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 307}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NzgxMzc4OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTozMDoxOFrOF-k-xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMToxMTo0M1rOGABzmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MTkyNg==", "bodyText": "Really need to run the callback inside the lock?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401161926", "createdAt": "2020-03-31T19:30:18Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0NDg5NA==", "bodyText": "it depends on what user expects (in this case me, because it is internal package private api). EXPIRED and REPLACED for the same key in different order can be critical depends on the client's expectation. In order with delayed can be harmful as well.\nI borrowed the behavior from guava cache: https://github.com/google/guava/wiki/CachesExplained#removal-listeners", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401244894", "createdAt": "2020-03-31T22:09:15Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MTkyNg=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjYzNjg3OA==", "bodyText": "My concern is that there is potential risk of deadlock: if onEvicition() callback acquires another lock, and if another thread is calling LinkedHashLruCache's methods while holding the other lock.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402636878", "createdAt": "2020-04-02T22:39:48Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MTkyNg=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY4Mjc3Nw==", "bodyText": "i see the concern, the callback acquiring another lock is violating the contract since it is blocking operation (this is why the javadoc asks non blocking operation). but there is nothing prevents it from doing it. we could have a utility class like guava to offload blocking, but it seems overkill for this helper class (offloading option is mentioned in the javadoc).", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402682777", "createdAt": "2020-04-03T01:11:43Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    checkNotNull(ses, \"ses\");\n+    checkState(cleaningInterval > 0, \"cleaning interval must be positive\");\n+    checkNotNull(cleaningIntervalUnit, \"cleaningIntervalUnit\");\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MTkyNg=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NzgyNTAzOnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxOTozMzo0M1rOF-lFzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjowMzoyMVrOF-p4-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MzcyNQ==", "bodyText": "always return false?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401163725", "createdAt": "2020-03-31T19:33:43Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0MjM2Mg==", "bodyText": "yes, because this code is handling the eviction instead of letting the LinkedHashMap handle it. see LinkedHashMap documentation for more details.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r401242362", "createdAt": "2020-03-31T22:03:21Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A {@link LinkedHashLruCache} implements least recently used caching where it supports access\n+ * order lru cache eviction while allowing entry level expiration time. When the cache reaches max\n+ * capacity, LruCache try to remove up to one already expired entries. If it doesn't find any\n+ * expired entries, it will remove based on access order of entry. On top of this, LruCache also\n+ * proactively removed expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long maxEstimatedSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(maxEstimatedSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = maxEstimatedSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (maxEstimatedSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE2MzcyNQ=="}, "originalCommit": {"oid": "81f0b870710a42b8ed25d3e6acc64ca3882c9b35"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5NzExMzA2OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQyMjozNToxNFrOF_-5TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMToxNzoyMlrOGAB5Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjYzNTA4NA==", "bodyText": "Why not EvictionType.EXPLICIT?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402635084", "createdAt": "2020-04-02T22:35:14Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY4NDE5OQ==", "bodyText": "yes, it need to be EXPLICIT, i even mentioned in the javadoc lol. fixed.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402684199", "createdAt": "2020-04-03T01:17:22Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjYzNTA4NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5NzE3MDEwOnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQyMjo1OToxMVrOF__bmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxMjoxMVrOGEAVIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA==", "bodyText": "The write may have data race for concurrent invocations of updateEntrySize(), causing  estimatedSizeBytes.addAndGet(newSize - prevSize) to yield wrong result.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402643864", "createdAt": "2020-04-02T22:59:11Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY4MDQ5Mg==", "bodyText": "i think the concurrent invocation part is fine. because of the key and value are final. so newSize is constant.\nif esimateSizeOf returns different value it can be wrong. in this case, we can't do anything to correct it. i'll add it to the doc.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402680492", "createdAt": "2020-04-03T01:03:13Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQxNTU1OQ==", "bodyText": "I think it's still racy. An extreme example, if there are multiple concurrent invocations of updateEntrySize() at exactly the same pace line by line, the estimatedSizeBytes will be incremented multiple times.\nWe can do the following instead of estimatedSizeBytes.addAndGet(newSize - prevSize),\nboolean updated = estimatedSizeBytes.compareAndSet(prevSize, newSize);\nif (!updated) {\n  // retry\n  updateEntrySize(key);\n}", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406415559", "createdAt": "2020-04-09T19:03:46Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUzMDE5MQ==", "bodyText": "that's violating the contract that estimateSizeOf should be constant during the call.\neven if we do compare and swap it will still be out of sync. it should update size from last modified size - original size where original is before the concurrent call. CAS will ignore valid size changes unless in a while loop doing everything over. we might as well use lock  in this case, but seems a bit overkill than a good contract.\nyes, so this is not thread safe, i'll remove the thread safe annotation. hmm holding lock doesn't sound that bad than removing the guarantee, WDYT?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406530191", "createdAt": "2020-04-09T23:27:10Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU0MjA4OQ==", "bodyText": "I don't think that's violating the contract that estimateSizeOf should be constant during the call. As long as the value is not mutating during the call, it is constant.\n\neven if we do compare and swap it will still be out of sync\nWhy? If compare and swap fails we start over and retry, until it succeeds. When it succeeds it should be in sync.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406542089", "createdAt": "2020-04-10T00:12:40Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1MDc0Mg==", "bodyText": "okay my bad it is not violating the exact sentence. i really meant it shouldn't be called concurrently and mistakenly use the method. but my intention was the readInternal should be identical (including size). and i also missed the retry =(.\ntechnically it is still not correct because of the eviction. only correct threadsafe way i can see is holding a lock. Even if we ignore the race between eviction/updateSize, lock will be faster than retry because everytime it retries it need to acquire the lock (assuming estimateSizeOf is fast - it should be).", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406550742", "createdAt": "2020-04-10T00:47:07Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MjEyMw==", "bodyText": "Using a lock looks quite simple. What is your concern for using a lock here? It's already used almost everywhere.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406852123", "createdAt": "2020-04-10T17:10:11Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1Mjg5Nw==", "bodyText": "yeah i liked said i think lock is good for this case. i'll make the change.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406852897", "createdAt": "2020-04-10T17:12:11Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0Mzg2NA=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5NzE4OTc3OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQyMzowODowM1rOF__n5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMToxOTo1OVrOGAB7_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0NzAxMw==", "bodyText": "I'm not clear what the comment means. Is it necessary?", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402647013", "createdAt": "2020-04-02T23:08:03Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY4NDkyNA==", "bodyText": "i can improve the comment\nif you get why it call readInternal instead of LinkedHashMap#contains without much thinking i am more than happy to remove it.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402684924", "createdAt": "2020-04-03T01:19:59Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY0NzAxMw=="}, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5NzMxOTU2OnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMDoxMjo1NlrOGAA0zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwMDoxMjo1NlrOGAA0zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY2NjcwMg==", "bodyText": "Favor composition over inheritance (go/ej3e-18)", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r402666702", "createdAt": "2020-04-03T00:12:56Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /** Updates size for given key if entry exists. It is useful if the cache value is mutated. */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {\n+    checkNotNull(key, \"key\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.get(key);\n+      if (existing != null && isExpired(key, existing.value, timeProvider.currentTimeNanos())) {\n+        invalidate(key, EvictionType.EXPIRED);\n+        return null;\n+      }\n+      return existing;\n+    }\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V invalidate(K key) {\n+    return invalidate(key, EvictionType.EXPLICIT);\n+  }\n+\n+  @Nullable\n+  private V invalidate(K key, EvictionType cause) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(cause, \"cause\");\n+    synchronized (lock) {\n+      SizedValue existing = delegate.remove(key);\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, cause);\n+      }\n+      return existing == null ? null : existing.value;\n+    }\n+  }\n+\n+  @Override\n+  public final void invalidateAll(Iterable<K> keys) {\n+    checkNotNull(keys, \"keys\");\n+    synchronized (lock) {\n+      for (K key : keys) {\n+        SizedValue existing = delegate.remove(key);\n+        if (existing != null) {\n+          evictionListener.onEviction(key, existing, EvictionType.EXPIRED);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final boolean hasCacheEntry(K key) {\n+    // call get to handle expired\n+    return readInternal(key) != null;\n+  }\n+\n+  /** Returns shallow copied values in the cache. */\n+  public final List<V> values() {\n+    synchronized (lock) {\n+      List<V> list = new ArrayList<>(delegate.size());\n+      for (SizedValue value : delegate.values()) {\n+        list.add(value.value);\n+      }\n+      return Collections.unmodifiableList(list);\n+    }\n+  }\n+\n+  /**\n+   * Resizes cache. If new size is smaller than current estimated size, it will free up space by\n+   * removing expired entries and removing oldest entries by LRU order.\n+   */\n+  public final void resize(int newSizeBytes) {\n+    long now = timeProvider.currentTimeNanos();\n+    synchronized (lock) {\n+      long estimatedSizeBytesCopy = estimatedMaxSizeBytes;\n+      this.estimatedMaxSizeBytes = newSizeBytes;\n+      if (estimatedSizeBytesCopy <= newSizeBytes) {\n+        // new size is larger no need to do cleanup\n+        return;\n+      }\n+      // cleanup expired entries\n+      cleanupExpiredEntries(now);\n+\n+      // cleanup eldest entry until new size limit\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && estimatedMaxSizeBytes > this.estimatedSizeBytes.get()) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        lruIter.remove();\n+        // eviction listener will update the estimatedSizeBytes\n+        evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.SIZE);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  @CheckReturnValue\n+  public final int estimatedSize() {\n+    synchronized (lock) {\n+      return delegate.size();\n+    }\n+  }\n+\n+  private boolean cleanupExpiredEntries(long now) {\n+    return cleanupExpiredEntries(Integer.MAX_VALUE, now);\n+  }\n+\n+  // maxExpiredEntries is by number of entries\n+  private boolean cleanupExpiredEntries(int maxExpiredEntries, long now) {\n+    checkArgument(maxExpiredEntries > 0, \"maxExpiredEntries must be positive\");\n+    boolean removedAny = false;\n+    synchronized (lock) {\n+      Iterator<Map.Entry<K, SizedValue>> lruIter = delegate.entrySet().iterator();\n+      while (lruIter.hasNext() && maxExpiredEntries > 0) {\n+        Map.Entry<K, SizedValue> entry = lruIter.next();\n+        if (isExpired(entry.getKey(), entry.getValue().value, now)) {\n+          lruIter.remove();\n+          evictionListener.onEviction(entry.getKey(), entry.getValue(), EvictionType.EXPIRED);\n+          removedAny = true;\n+          maxExpiredEntries--;\n+        }\n+      }\n+    }\n+    return removedAny;\n+  }\n+\n+  @Override\n+  public final void close() {\n+    synchronized (lock) {\n+      periodicCleaner.stop();\n+      doClose();\n+      delegate.clear();\n+    }\n+  }\n+\n+  protected void doClose() {}\n+\n+  /** Periodically cleans up the AsyncRequestCache. */\n+  private final class PeriodicCleaner implements Runnable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7bf4a5753b44ac2c736f6b02defa317f19ca079e"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMjc1MTYyOnYy", "diffSide": "RIGHT", "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMjozMTo0MVrOGDrmTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMjo0Mzo0OVrOGDr1_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUxMzIzMQ==", "bodyText": "readSizedValue() might be a better name.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406513231", "createdAt": "2020-04-09T22:31:41Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,386 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /**\n+   * Updates size for given key if entry exists. It is useful if the cache value is mutated.\n+   *\n+   * <p>Note: During this method call, the {@link #estimateSizeOf(Object, Object)} should return\n+   * same value. Otherwise, the estimated size can be out of sync.\n+   */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab48ca88404405530dce489f151e57a6ad0b03bd"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUxNzI0Nw==", "bodyText": "the emphasize is how it is reading the cache not what it reads. e.g. handling expired. SizedValue is also internal representation of the value.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406517247", "createdAt": "2020-04-09T22:43:49Z", "author": {"login": "creamsoup"}, "path": "rls/src/main/java/io/grpc/rls/internal/LinkedHashLruCache.java", "diffHunk": "@@ -0,0 +1,386 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import com.google.common.base.MoreObjects;\n+import io.grpc.internal.TimeProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import javax.annotation.CheckReturnValue;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.GuardedBy;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/**\n+ * A LinkedHashLruCache implements least recently used caching where it supports access order lru\n+ * cache eviction while allowing entry level expiration time. When the cache reaches max capacity,\n+ * LruCache try to remove up to one already expired entries. If it doesn't find any expired entries,\n+ * it will remove based on access order of entry. On top of this, LruCache also proactively removes\n+ * expired entries based on configured time interval.\n+ */\n+@ThreadSafe\n+abstract class LinkedHashLruCache<K, V> implements LruCache<K, V> {\n+\n+  private final Object lock = new Object();\n+\n+  @GuardedBy(\"lock\")\n+  private final LinkedHashMap<K, SizedValue> delegate;\n+  private final PeriodicCleaner periodicCleaner;\n+  private final TimeProvider timeProvider;\n+  private final EvictionListener<K, SizedValue> evictionListener;\n+  private final AtomicLong estimatedSizeBytes = new AtomicLong();\n+  private long estimatedMaxSizeBytes;\n+\n+  LinkedHashLruCache(\n+      final long estimatedMaxSizeBytes,\n+      @Nullable final EvictionListener<K, V> evictionListener,\n+      int cleaningInterval,\n+      TimeUnit cleaningIntervalUnit,\n+      ScheduledExecutorService ses,\n+      final TimeProvider timeProvider) {\n+    checkState(estimatedMaxSizeBytes > 0, \"max estimated cache size should be positive\");\n+    this.estimatedMaxSizeBytes = estimatedMaxSizeBytes;\n+    this.evictionListener = new SizeHandlingEvictionListener(evictionListener);\n+    this.timeProvider = checkNotNull(timeProvider, \"timeProvider\");\n+    delegate = new LinkedHashMap<K, SizedValue>(\n+        // rough estimate or minimum hashmap default\n+        Math.max((int) (estimatedMaxSizeBytes / 1000), 16),\n+        /* loadFactor= */ 0.75f,\n+        /* accessOrder= */ true) {\n+      @Override\n+      protected boolean removeEldestEntry(Map.Entry<K, SizedValue> eldest) {\n+        if (estimatedSizeBytes.get() <= LinkedHashLruCache.this.estimatedMaxSizeBytes) {\n+          return false;\n+        }\n+\n+        // first, remove at most 1 expired entry\n+        boolean removed = cleanupExpiredEntries(1, timeProvider.currentTimeNanos());\n+        // handles size based eviction if necessary no expired entry\n+        boolean shouldRemove =\n+            !removed && shouldInvalidateEldestEntry(eldest.getKey(), eldest.getValue().value);\n+        if (shouldRemove) {\n+          // remove entry by us to make sure lruIterator and cache is in sync\n+          LinkedHashLruCache.this.invalidate(eldest.getKey(), EvictionType.SIZE);\n+        }\n+        return false;\n+      }\n+    };\n+    periodicCleaner = new PeriodicCleaner(ses, cleaningInterval, cleaningIntervalUnit).start();\n+  }\n+\n+  /**\n+   * Determines if the eldest entry should be kept or not when the cache size limit is reached. Note\n+   * that LruCache is access level and the eldest is determined by access pattern.\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected boolean shouldInvalidateEldestEntry(K eldestKey, V eldestValue) {\n+    return true;\n+  }\n+\n+  /** Determines if the entry is already expired or not. */\n+  protected abstract boolean isExpired(K key, V value, long nowNanos);\n+\n+  /**\n+   * Returns estimated size of entry to keep track. If it always returns 1, the max size bytes\n+   * behaves like max number of entry (default behavior).\n+   */\n+  @SuppressWarnings(\"unused\")\n+  protected int estimateSizeOf(K key, V value) {\n+    return 1;\n+  }\n+\n+  /**\n+   * Updates size for given key if entry exists. It is useful if the cache value is mutated.\n+   *\n+   * <p>Note: During this method call, the {@link #estimateSizeOf(Object, Object)} should return\n+   * same value. Otherwise, the estimated size can be out of sync.\n+   */\n+  public void updateEntrySize(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry == null) {\n+      return;\n+    }\n+    int prevSize = entry.size;\n+    int newSize = estimateSizeOf(key, entry.value);\n+    entry.size = newSize;\n+    estimatedSizeBytes.addAndGet(newSize - prevSize);\n+  }\n+\n+  @Override\n+  @Nullable\n+  public final V cache(K key, V value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    SizedValue existing;\n+    int size = estimateSizeOf(key, value);\n+    synchronized (lock) {\n+      estimatedSizeBytes.addAndGet(size);\n+      existing = delegate.put(key, new SizedValue(size, value));\n+      if (existing != null) {\n+        evictionListener.onEviction(key, existing, EvictionType.REPLACED);\n+      }\n+    }\n+    return existing == null ? null : existing.value;\n+  }\n+\n+  @Override\n+  @Nullable\n+  @CheckReturnValue\n+  public final V read(K key) {\n+    SizedValue entry = readInternal(key);\n+    if (entry != null) {\n+      return entry.value;\n+    }\n+    return null;\n+  }\n+\n+  @Nullable\n+  @CheckReturnValue\n+  private SizedValue readInternal(K key) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUxMzIzMQ=="}, "originalCommit": {"oid": "ab48ca88404405530dce489f151e57a6ad0b03bd"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMjc3ODg1OnYy", "diffSide": "RIGHT", "path": "rls/src/test/java/io/grpc/rls/internal/LinkedHashLruCacheTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMjo0NDowM1rOGDr2SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMjo0NDowM1rOGDr2SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUxNzMyMQ==", "bodyText": "If this method is not supposed to be mocked, it can be final.", "url": "https://github.com/grpc/grpc-java/pull/6799#discussion_r406517321", "createdAt": "2020-04-09T22:44:03Z", "author": {"login": "dapengzhang0"}, "path": "rls/src/test/java/io/grpc/rls/internal/LinkedHashLruCacheTest.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright 2020 The gRPC Authors\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.grpc.rls.internal;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.truth.Truth.assertThat;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.CALLS_REAL_METHODS;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.grpc.internal.TimeProvider;\n+import io.grpc.rls.internal.LruCache.EvictionListener;\n+import io.grpc.rls.internal.LruCache.EvictionType;\n+import java.util.Objects;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnit;\n+import org.mockito.junit.MockitoRule;\n+\n+@RunWith(JUnit4.class)\n+public class LinkedHashLruCacheTest {\n+\n+  private static final int MAX_SIZE = 5;\n+\n+  @Rule\n+  public final MockitoRule mocks = MockitoJUnit.rule();\n+\n+  private final DoNotUseFakeScheduledService fakeScheduledService =\n+      mock(DoNotUseFakeScheduledService.class, CALLS_REAL_METHODS);\n+  private final TimeProvider timeProvider = fakeScheduledService.getFakeTicker();\n+\n+  @Mock\n+  private EvictionListener<Integer, Entry> evictionListener;\n+  private LinkedHashLruCache<Integer, Entry> cache;\n+\n+  @Before\n+  public void setUp() {\n+    this.cache = new LinkedHashLruCache<Integer, Entry>(\n+        MAX_SIZE,\n+        evictionListener,\n+        10,\n+        TimeUnit.NANOSECONDS,\n+        fakeScheduledService,\n+        timeProvider) {\n+      @Override\n+      protected boolean isExpired(Integer key, Entry value, long nowNanos) {\n+        return value.expireTime <= nowNanos;\n+      }\n+    };\n+  }\n+\n+  @Test\n+  public void eviction_size() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      cache.cache(i, new Entry(\"Entry\" + i, Long.MAX_VALUE));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    verify(evictionListener).onEviction(1, new Entry(\"Entry1\", Long.MAX_VALUE), EvictionType.SIZE);\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void size() {\n+    Entry entry1 = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry entry2 = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, entry1);\n+    cache.cache(1, entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(2);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(entry1);\n+    assertThat(cache.estimatedSize()).isEqualTo(1);\n+\n+    assertThat(cache.invalidate(1)).isEqualTo(entry2);\n+    assertThat(cache.estimatedSize()).isEqualTo(0);\n+  }\n+\n+  @Test\n+  public void eviction_expire() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPIRED);\n+\n+    fakeScheduledService.advance(10, TimeUnit.NANOSECONDS);\n+    verify(evictionListener).onEviction(1, survivor, EvictionType.EXPIRED);\n+  }\n+\n+  @Test\n+  public void eviction_explicit() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(1, survivor);\n+\n+    assertThat(cache.invalidate(0)).isEqualTo(toBeEvicted);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.EXPLICIT);\n+  }\n+\n+  @Test\n+  public void eviction_replaced() {\n+    Entry toBeEvicted = new Entry(\"Entry0\", timeProvider.currentTimeNanos() + 10);\n+    Entry survivor = new Entry(\"Entry1\", timeProvider.currentTimeNanos() + 20);\n+    cache.cache(0, toBeEvicted);\n+    cache.cache(0, survivor);\n+\n+    verify(evictionListener).onEviction(0, toBeEvicted, EvictionType.REPLACED);\n+  }\n+\n+  @Test\n+  public void eviction_size_shouldEvictAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last two entries are <= current time (already expired)\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i - 1));\n+    }\n+    cache.cache(MAX_SIZE + 1, new Entry(\"should kick the first\", Long.MAX_VALUE));\n+\n+    // should remove MAX_SIZE-1 instead of MAX_SIZE because MAX_SIZE is accessed later\n+    verify(evictionListener)\n+        .onEviction(eq(MAX_SIZE - 1), any(Entry.class), eq(EvictionType.EXPIRED));\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+  }\n+\n+  @Test\n+  public void eviction_get_shouldNotReturnAlreadyExpired() {\n+    for (int i = 1; i <= MAX_SIZE; i++) {\n+      // last entry is already expired when added\n+      cache.cache(i, new Entry(\"Entry\" + i, timeProvider.currentTimeNanos() + MAX_SIZE - i));\n+    }\n+\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE);\n+    assertThat(cache.read(MAX_SIZE)).isNull();\n+    assertThat(cache.estimatedSize()).isEqualTo(MAX_SIZE - 1);\n+    verify(evictionListener).onEviction(eq(MAX_SIZE), any(Entry.class), eq(EvictionType.EXPIRED));\n+  }\n+\n+  private static final class Entry {\n+    String value;\n+    long expireTime;\n+\n+    Entry(String value, long expireTime) {\n+      this.value = value;\n+      this.expireTime = expireTime;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      Entry entry = (Entry) o;\n+      return expireTime == entry.expireTime && Objects.equals(value, entry.value);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(value, expireTime);\n+    }\n+  }\n+\n+  /**\n+   * A fake minimal implementation of ScheduledExecutorService *only* supports scheduledAtFixedRate\n+   * with a lot of limitation / assumptions. Only intended to be used in this test with\n+   * CALL_REAL_METHODS mock.\n+   */\n+  private abstract static class DoNotUseFakeScheduledService implements ScheduledExecutorService {\n+\n+    private long currTimeNanos;\n+    private long period;\n+    private long nextRun;\n+    private AtomicReference<Runnable> command;\n+\n+    @Override\n+    public ScheduledFuture<?> scheduleAtFixedRate(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab48ca88404405530dce489f151e57a6ad0b03bd"}, "originalPosition": 207}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2888, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}