{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyNDIwNzk3", "number": 1251, "title": "SAMZA-2431: Fix the checkpoint and changelog topic auto-creation.", "bodyText": "Symptom: Checkpoint and changelog kafka topics of a samza job may be created with cleanup.policy set to 'delete' instead of 'compact' for certain cases.\nCause:\n\nCheckpoint: The control-flow in KafkaStreamSpec to build checkpoint spec swallows the essential kafka-topic configuration rather it passes empty configuration bag to kafka-broker. This behavior was introduced in SAMZA-2339.\nChangelog: The change-log topic configurations were incorrectly generated when the RocksDB store TTL is set to -1 by the user. This behavior was introduced in SAMZA-1929.\n\nChanges: Fix the topic-creation control-flow for the metadata topics and generate the correct topic-configurations.\nTests: Added unit tests to validate that the expected topic configuration bag was generated for both checkpoint and changelog topics.\nAPI Changes: None\nUpgrade Instructions: None\nUsage Instructions: None", "createdAt": "2020-01-14T03:06:36Z", "url": "https://github.com/apache/samza/pull/1251", "merged": true, "mergeCommit": {"oid": "80c799dcbe64fbfeaeb8c51c855dbaf6c92506cf"}, "closed": true, "closedAt": "2020-01-15T03:02:13Z", "author": {"login": "shanthoosh"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb6VO_ZgFqTM0MjczOTU2NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABb6b4NRABqjI5NDkyNjA3OTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyNzM5NTY0", "url": "https://github.com/apache/samza/pull/1251#pullrequestreview-342739564", "createdAt": "2020-01-14T18:19:08Z", "commit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoxOTowOFrOFdhLbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoyNjo1MVrOFdhZhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA==", "bodyText": "Minor: Fix indentation.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366496620", "createdAt": "2020-01-14T18:19:08Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ==", "bodyText": "Minor: Can use 'rocksDbTtl' here instead of getting it again.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366497129", "createdAt": "2020-01-14T18:20:13Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzM0Nw==", "bodyText": "Set the max.message.bytes property too (line 338)", "url": "https://github.com/apache/samza/pull/1251#discussion_r366497347", "createdAt": "2020-01-14T18:20:44Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5ODY3MA==", "bodyText": "Do you need this containsKey check in the new condition as well?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366498670", "createdAt": "2020-01-14T18:23:37Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {\n+          kafkaChangeLogProperties.setProperty(\"cleanup.policy\", \"compact\")\n+        } else if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUwMDIzMQ==", "bodyText": "Here's how this is done in KafkaCheckpointManager:\n    val checkpointSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(checkpointTopic, checkpointSystemName))\n        .copyWithReplicationFactor(kafkaConfig.getCheckpointReplicationFactor.get.toInt)\n        .copyWithProperties(kafkaConfig.getCheckpointTopicProperties)\n\nCan we do that here for consistency?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366500231", "createdAt": "2020-01-14T18:26:51Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyNzgwMzgy", "url": "https://github.com/apache/samza/pull/1251#pullrequestreview-342780382", "createdAt": "2020-01-14T19:24:24Z", "commit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxOToyNDoyNFrOFdjFUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxOTo1NTowNVrOFdj8pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyNzgyNw==", "bodyText": "Fixed the indentation.\nUpdated the checkpoint spec building to be consistent with KafkaCheckpointManager.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366527827", "createdAt": "2020-01-14T19:24:24Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyODc4MA==", "bodyText": "Done.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366528780", "createdAt": "2020-01-14T19:26:20Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU0MTk4OQ==", "bodyText": "Yes.  At the end of this function, the changelog properties if defined for the store in user-config are picked-up and populated in the resultant change-log-topic-properties. So this cleanup.policy check is still necessary. We could simplify the code, but I prefer not to couple clean-up with a critical-fix.  What do you think?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366541989", "createdAt": "2020-01-14T19:55:05Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {\n+          kafkaChangeLogProperties.setProperty(\"cleanup.policy\", \"compact\")\n+        } else if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5ODY3MA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyODYyNzMy", "url": "https://github.com/apache/samza/pull/1251#pullrequestreview-342862732", "createdAt": "2020-01-14T21:51:19Z", "commit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQyMTo1MToxOVrOFdm9bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQyMTo1MToxOVrOFdm9bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU5MTM0Mg==", "bodyText": "Why not kafkaConfig.getCheckpointTopicProperties?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366591342", "createdAt": "2020-01-14T21:51:19Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyODk2Nzc0", "url": "https://github.com/apache/samza/pull/1251#pullrequestreview-342896774", "createdAt": "2020-01-14T23:01:47Z", "commit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyOTIyMzk4", "url": "https://github.com/apache/samza/pull/1251#pullrequestreview-342922398", "createdAt": "2020-01-15T00:16:47Z", "commit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMDoxNjo0OFrOFdp7KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMDoyMTozOFrOFdqAmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYzOTkxMg==", "bodyText": "We can do it separately, can we add .copyWithConfig or .copyWithMap which takes Config or Map<String, String> ?\ncopyWithProperties is not user friendly, as we are converting a Map/Config to a properties, and this method basically convert it back.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366639912", "createdAt": "2020-01-15T00:16:48Z", "author": {"login": "kw2542"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY0MTMwNA==", "bodyText": "any specific reason to split this line into two? val storeTTLkey does not seem to be used elsewhere.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366641304", "createdAt": "2020-01-15T00:21:38Z", "author": {"login": "kw2542"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,19 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 13}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a38393967b73546ddef2ceef9bc33396381b0767", "author": {"user": {"login": "shanthoosh", "name": null}}, "url": "https://github.com/apache/samza/commit/a38393967b73546ddef2ceef9bc33396381b0767", "committedDate": "2020-01-15T02:11:15Z", "message": "Fix the checkpoint and changelog topic creation configurations."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5df1650f62412c4b8bd068ada0628e6be71bc21", "author": {"user": {"login": "shanthoosh", "name": null}}, "url": "https://github.com/apache/samza/commit/f5df1650f62412c4b8bd068ada0628e6be71bc21", "committedDate": "2020-01-15T02:11:15Z", "message": "Address review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "934f5067903ba406bfe4d29514465d21808613a0", "author": {"user": {"login": "shanthoosh", "name": null}}, "url": "https://github.com/apache/samza/commit/934f5067903ba406bfe4d29514465d21808613a0", "committedDate": "2020-01-15T02:11:15Z", "message": "Address review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c6c36affa4112ce9abe76da7819f64a2bdf054ed", "author": {"user": {"login": "shanthoosh", "name": null}}, "url": "https://github.com/apache/samza/commit/c6c36affa4112ce9abe76da7819f64a2bdf054ed", "committedDate": "2020-01-15T01:20:18Z", "message": "Address review comments."}, "afterCommit": {"oid": "934f5067903ba406bfe4d29514465d21808613a0", "author": {"user": {"login": "shanthoosh", "name": null}}, "url": "https://github.com/apache/samza/commit/934f5067903ba406bfe4d29514465d21808613a0", "committedDate": "2020-01-15T02:11:15Z", "message": "Address review comments."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4751, "cost": 1, "resetAt": "2021-11-01T11:59:11Z"}}}