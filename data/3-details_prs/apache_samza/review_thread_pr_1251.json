{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyNDIwNzk3", "number": 1251, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoxOTowOFrODX8XmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMDoyMTozOFrODYB5tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NDMyOTIwOnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoxOTowOFrOFdhLbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxOToyNDoyNFrOFdjFUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA==", "bodyText": "Minor: Fix indentation.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366496620", "createdAt": "2020-01-14T18:19:08Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUwMDIzMQ==", "bodyText": "Here's how this is done in KafkaCheckpointManager:\n    val checkpointSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(checkpointTopic, checkpointSystemName))\n        .copyWithReplicationFactor(kafkaConfig.getCheckpointReplicationFactor.get.toInt)\n        .copyWithProperties(kafkaConfig.getCheckpointTopicProperties)\n\nCan we do that here for consistency?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366500231", "createdAt": "2020-01-14T18:26:51Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyNzgyNw==", "bodyText": "Fixed the indentation.\nUpdated the checkpoint spec building to be consistent with KafkaCheckpointManager.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366527827", "createdAt": "2020-01-14T19:24:24Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,8 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      kafkaSpec = KafkaStreamSpec.fromSpec(spec)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NjYyMA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NDMzMjI4OnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoyMDoxM1rOFdhNaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxOToyNjoyMFrOFdjJDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ==", "bodyText": "Minor: Can use 'rocksDbTtl' here instead of getting it again.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366497129", "createdAt": "2020-01-14T18:20:13Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzM0Nw==", "bodyText": "Set the max.message.bytes property too (line 338)", "url": "https://github.com/apache/samza/pull/1251#discussion_r366497347", "createdAt": "2020-01-14T18:20:44Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyODc4MA==", "bodyText": "Done.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366528780", "createdAt": "2020-01-14T19:26:20Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5NzEyOQ=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NDM0MTUyOnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxODoyMzozN1rOFdhTbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxOTo1NTowNVrOFdj8pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5ODY3MA==", "bodyText": "Do you need this containsKey check in the new condition as well?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366498670", "createdAt": "2020-01-14T18:23:37Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {\n+          kafkaChangeLogProperties.setProperty(\"cleanup.policy\", \"compact\")\n+        } else if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU0MTk4OQ==", "bodyText": "Yes.  At the end of this function, the changelog properties if defined for the store in user-config are picked-up and populated in the resultant change-log-topic-properties. So this cleanup.policy check is still necessary. We could simplify the code, but I prefer not to couple clean-up with a critical-fix.  What do you think?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366541989", "createdAt": "2020-01-14T19:55:05Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,18 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {\n       case Some(rocksDbTtl) =>\n-        if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {\n+        if (config.getInt(storeTTLkey, 0) < 0) {\n+          kafkaChangeLogProperties.setProperty(\"cleanup.policy\", \"compact\")\n+        } else if (!config.containsKey(\"stores.%s.changelog.kafka.cleanup.policy\" format name)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ5ODY3MA=="}, "originalCommit": {"oid": "23ae3230ab2f0a715cb5b8858d62bedd04d0fba4"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NDkyMDA4OnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQyMTo1MToxOVrOFdm9bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQyMzowMToyOFrOFdonIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU5MTM0Mg==", "bodyText": "Why not kafkaConfig.getCheckpointTopicProperties?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366591342", "createdAt": "2020-01-14T21:51:19Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYwODMyNg==", "bodyText": "The control flow to create the checkpoint stream is the following:\n\nBuild the checkpoint spec in KafkaCheckpointManagerFactory, which uses kafkaConfig.getCheckpointProperties to populate the config in CheckpointSpec.\nKafkaCheckpointManager.createStream uses the checkpoint spec that was built from 1 and KafkaSystemAdmin.createStream in-turn invokes KafkaSystemAdmin.toKafkaSpec.\nKafkaSystemAdmin.toKafkaSpec(CheckpointSpec) above is used to convert the incoming spec of type StreamSpec to KafkaStreamSpec .\n\nThe config in incoming checkpoint StreamSpec is already built using   kafkaConfig.getCheckpointTopicProperties in KafkaCheckpointManagerFactory. So using kafkaConfig.getCheckpointProperties here again would be redundant, unnecessary and might return incorrect config-bag.\nWhat do you think?", "url": "https://github.com/apache/samza/pull/1251#discussion_r366608326", "createdAt": "2020-01-14T22:33:22Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU5MTM0Mg=="}, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYxODQwMQ==", "bodyText": "Got it, so the spec.getConfig here is already the checkpoint topic related subset. In that case, LGTM.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366618401", "createdAt": "2020-01-14T23:01:28Z", "author": {"login": "prateekm"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU5MTM0Mg=="}, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NTIyNjIyOnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMDoxNjo0OFrOFdp7KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMToxODozOFrOFdq3QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYzOTkxMg==", "bodyText": "We can do it separately, can we add .copyWithConfig or .copyWithMap which takes Config or Map<String, String> ?\ncopyWithProperties is not user friendly, as we are converting a Map/Config to a properties, and this method basically convert it back.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366639912", "createdAt": "2020-01-15T00:16:48Z", "author": {"login": "kw2542"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTI5Ng==", "bodyText": "Yes, adding the copyWithConfig to KafkaStreamSpec might be good convenience API. We've additional cleanup planned with this control-flow in the near-future. If acceptable, then I can couple this change with that. Since it's a critical bug-fix, I would prefer to keep the change minimal by containing it to the fix alone and do the clean-up later.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366655296", "createdAt": "2020-01-15T01:18:38Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/java/org/apache/samza/system/kafka/KafkaSystemAdmin.java", "diffHunk": "@@ -541,8 +541,11 @@ public KafkaStreamSpec toKafkaSpec(StreamSpec spec) {\n           new KafkaStreamSpec(spec.getId(), spec.getPhysicalName(), systemName, 1, coordinatorStreamReplicationFactor,\n               coordinatorStreamProperties);\n     } else if (spec.isCheckpointStream()) {\n-      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), systemName))\n-              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()));\n+      Properties checkpointTopicProperties = new Properties();\n+      checkpointTopicProperties.putAll(spec.getConfig());\n+      kafkaSpec = KafkaStreamSpec.fromSpec(StreamSpec.createCheckpointStreamSpec(spec.getPhysicalName(), spec.getSystemName()))\n+              .copyWithReplicationFactor(Integer.parseInt(new KafkaConfig(config).getCheckpointReplicationFactor().get()))\n+              .copyWithProperties(checkpointTopicProperties);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYzOTkxMg=="}, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NTIzNTczOnYy", "diffSide": "RIGHT", "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMDoyMTozOFrOFdqAmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMToxODo0MlrOFdq3TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY0MTMwNA==", "bodyText": "any specific reason to split this line into two? val storeTTLkey does not seem to be used elsewhere.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366641304", "createdAt": "2020-01-15T00:21:38Z", "author": {"login": "kw2542"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,19 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTMwOQ==", "bodyText": "Removed.", "url": "https://github.com/apache/samza/pull/1251#discussion_r366655309", "createdAt": "2020-01-15T01:18:42Z", "author": {"login": "shanthoosh"}, "path": "samza-kafka/src/main/scala/org/apache/samza/config/KafkaConfig.scala", "diffHunk": "@@ -316,16 +316,19 @@ class KafkaConfig(config: Config) extends ScalaMapConfig(config) {\n     val filteredConfigs = config.subset(KafkaConfig.CHANGELOG_STREAM_KAFKA_SETTINGS format name, true)\n     val kafkaChangeLogProperties = new Properties\n \n-    val appConfig = new ApplicationConfig(config)\n     // SAMZA-1600: do not use the combination of \"compact,delete\" as cleanup policy until we pick up Kafka broker 0.11.0.57,\n     // 1.0.2, or 1.1.0 (see KAFKA-6568)\n \n     // Adjust changelog topic setting, when TTL is set on a RocksDB store\n     //  - Disable log compaction on Kafka changelog topic\n     //  - Set topic TTL to be the same as RocksDB TTL\n-    Option(config.get(\"stores.%s.rocksdb.ttl.ms\" format name)) match {\n+    val storeTTLkey = \"stores.%s.rocksdb.ttl.ms\" format name\n+    Option(config.get(storeTTLkey)) match {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY0MTMwNA=="}, "originalCommit": {"oid": "f56966c1b441223cf03f4377af63abec9970a667"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1546, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}