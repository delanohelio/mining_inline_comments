{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA0NDY2MjQy", "number": 2128, "title": "Cluster Awareness: Introduce generic SLS files for Cluster Management and CaaSP Cluster Provider custom Salt module.", "bodyText": "What does this PR change?\nThis PR corresponds to the initial implementation of the Salt related parts from RFC: Cluster Awareness in order to enable \"Cluster Awareness\" in Uyuni/SUSE Manager.\nSummary:\n\n\nNew custom Salt module for abstracting the different Cluster Providers (mgrclusters)\n\n\nNew CaaSP Cluster Provider Salt module based on Skuba (caasp)\n\n\nIntroduce new SLS state files for issuing cluster related actions:\n\nclusters.createcluster\nclusters.listnodes\nclusters.addnode\nclusters.removenode\nclusters.upgradecluster\n\n\n\nEach of those states require some particular pillar data in order to render:\n\ncluster_type: The type of the cluster. This is used to resolve the underlying Cluster Provider Salt module: i.a. caasp, ses, etc\nssh_auth_sock (optional): If defined, the selected SSH_AUTH_SOCK will be loaded into the environment where the cluster provider module runs.\nparams: the actual parameters passed to the resolved cluster provider module.\n\nCaaSP Cluster Provider Salt module\nThis custom Salt module allows to operate skuba using Salt and get the outputs in a Salty way:\n\nAdded create_cluster, list_nodes, add_node, remove_node, upgrade_cluster and upgrade_node methods.\nOptional parameters are supported: i.a. sudo, user, strict_capability_defaults, cloud_provider, etc\nGlobal parameter verbosity is also supported.\nThe default timeout for the skuba process is set to 1200 (20 min) but you can override it using the timeout parameter.\n\nAn example of usage\n\nGiven this custom pillar data:\n\n{\n  \"cluster_type\": \"caasp\",\n  \"ssh_auth_sock\": \"/tmp/ssh-2ype0xSxuC/agent.3070\",\n  \"params\": {\n    \"cluster_path\":\"/root/\",\n    \"cluster_name\": \"my-cluster-salt-v3\",\n    \"first_node_name\": \"master-one\",\n    \"target\": \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\"\n  }\n}\n\nWe schedule the creation of the cluster:\n\next-suma-head-caasp-min-sles15sp1:~ # salt-call state.apply clusters.createcluster pillar='{\"cluster_type\": \"caasp\", \"ssh_auth_sock\": \"/tmp/ssh-2ype0xSxuC/agent.3070\", \"params\": {\"cluster_basedir\":\"/root/\", \"cluster_name\": \"my-cluster-salt-v3\", \"first_node_name\": \"master-one\", \"target\": \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\"}}'\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\nlocal:\n----------\n          ID: sync_modules\n    Function: module.run\n        Name: saltutil.sync_modules\n      Result: True\n     Comment: Module function saltutil.sync_modules executed\n     Started: 16:58:32.113176\n    Duration: 161.89 ms\n     Changes:   \n              ----------\n              ret:\n----------\n          ID: ssh_agent_socket\n    Function: environ.setenv\n        Name: SSH_AUTH_SOCK\n      Result: True\n     Comment: Environ values were already set with the correct values\n     Started: 16:58:32.275613\n    Duration: 0.53 ms\n     Changes:   \n----------\n          ID: mgr_cluster_create_cluster\n    Function: module.run\n        Name: mgrclusters.create_cluster\n      Result: True\n     Comment: Module function mgrclusters.create_cluster executed\n     Started: 16:58:32.276582\n    Duration: 147318.76 ms\n     Changes:   \n              ----------\n              ret:\n                  ----------\n                  retcode:\n                      0\n                  stderr:\n                      W0416 16:58:32.436705    9695 ssh.go:311] \n                      The authenticity of host '192.168.122.177:22' can't be established.\n                      ECDSA key fingerprint is 12:ce:1f:71:a8:34:94:ec:ba:f3:60:2e:33:12:32:a0.\n                      I0416 16:58:32.436767    9695 ssh.go:312] accepting SSH key for \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local:22\"\n                      I0416 16:58:32.436779    9695 ssh.go:313] adding fingerprint for \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local:22\" to \"known_hosts\"\n                      E0416 16:58:35.010527    9695 ssh.go:195] W0416 16:58:26.337726   24196 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\n                      E0416 16:58:35.528951    9695 ssh.go:195] No files found for firewalld.service.\n                      E0416 16:58:37.521406    9695 ssh.go:195] \t[WARNING Hostname]: hostname \"master-one\" could not be reached\n                      E0416 16:58:37.521453    9695 ssh.go:195] \t[WARNING Hostname]: hostname \"master-one\": lookup master-one on 192.168.122.1:53: no such host\n                      E0416 17:00:15.442703    9695 ssh.go:195] Created symlink /etc/systemd/system/timers.target.wants/skuba-update.timer \u2192 /usr/lib/systemd/system/skuba-update.timer.\n                  stdout:\n                      [init] configuration files written to /root/my-cluster-salt-v3\n                      [bootstrap] updating init configuration with target information\n                      [bootstrap] writing init configuration for node\n                      [bootstrap] applying init configuration to node\n                      [bootstrap] successfully bootstrapped core components on node \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\" with Kubernetes: \"1.16.2\"\n                      [bootstrap] downloading secrets from bootstrapped node \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\"\n                      [bootstrap] deploying core add-ons on node \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\"\n                      [bootstrap] successfully bootstrapped core add-ons on node \"ext-suma-head-caasp-min-sles15sp1-v3.tf.local\"\n                  success:\n                      True\n\nSummary for local\n------------\nSucceeded: 3 (changed=2)\nFailed:    0\n------------\nTotal states run:     3\nTotal run time: 147.481 s\n\nWe can now add a worker using the clusters.addnode state and providing the necessary pillar data:\n\next-suma-head-caasp-min-sles15sp1:~ # salt-call state.apply clusters.addnode pillar='{\"cluster_type\": \"caasp\", \"ssh_auth_sock\": \"/tmp/ssh-2ype0xSxuC/agent.3070\", \"params\": {\"skuba_cluster_path\":\"/root/my-cluster-salt-v3\", \"node_name\": \"worker-one\", \"target\": \"ext-suma-head-caasp-min-sles15sp1-v4.tf.local\", \"role\":\"worker\"}}'\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\nlocal:\n----------\n          ID: sync_modules\n    Function: module.run\n        Name: saltutil.sync_modules\n      Result: True\n     Comment: Module function saltutil.sync_modules executed\n     Started: 17:08:44.180391\n    Duration: 173.156 ms\n     Changes:   \n              ----------\n              ret:\n----------\n          ID: ssh_agent_socket\n    Function: environ.setenv\n        Name: SSH_AUTH_SOCK\n      Result: True\n     Comment: Environ values were already set with the correct values\n     Started: 17:08:44.354182\n    Duration: 0.559 ms\n     Changes:   \n----------\n          ID: mgr_cluster_add_node\n    Function: module.run\n        Name: mgrclusters.add_node\n      Result: True\n     Comment: Module function mgrclusters.add_node executed\n     Started: 17:08:44.355198\n    Duration: 19238.299 ms\n     Changes:   \n              ----------\n              ret:\n                  ----------\n                  retcode:\n                      0\n                  stderr:\n                      W0416 17:08:44.473153   10299 ssh.go:311] \n                      The authenticity of host '192.168.122.28:22' can't be established.\n                      ECDSA key fingerprint is 59:11:f9:f0:0c:0f:1e:7e:ac:b6:5a:79:0b:d8:16:25.\n                      I0416 17:08:44.473201   10299 ssh.go:312] accepting SSH key for \"ext-suma-head-caasp-min-sles15sp1-v4.tf.local:22\"\n                      I0416 17:08:44.473208   10299 ssh.go:313] adding fingerprint for \"ext-suma-head-caasp-min-sles15sp1-v4.tf.local:22\" to \"known_hosts\"\n                      E0416 17:08:45.939166   10299 ssh.go:195] W0416 17:08:50.194215    3029 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\n                      E0416 17:08:46.479947   10299 ssh.go:195] No files found for firewalld.service.\n                      E0416 17:08:48.335006   10299 ssh.go:195] Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service \u2192 /usr/lib/systemd/system/kubelet.service.\n                      E0416 17:08:48.770508   10299 ssh.go:195] \t[WARNING Hostname]: hostname \"worker-one\" could not be reached\n                      E0416 17:08:48.770566   10299 ssh.go:195] \t[WARNING Hostname]: hostname \"worker-one\": lookup worker-one on 192.168.122.1:53: no such host\n                  stdout:\n                      [join] applying states to new node\n                      [join] node successfully joined the cluster\n                  success:\n                      True\n\nSummary for local\n------------\nSucceeded: 3 (changed=2)\nFailed:    0\n------------\nTotal states run:     3\nTotal run time:  19.412 s\n\nAlso listing the current nodes of our cluster:\n\next-suma-head-caasp-min-sles15sp1:~ # salt-call state.apply clusters.listnodes pillar='{\"cluster_type\": \"caasp\", \"ssh_auth_sock\": \"/tmp/ssh-2ype0xSxuC/agent.3070\", \"params\": {\"skuba_cluster_path\":\"/root/my-cluster-salt-v3\"}}'\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\nlocal:\n----------\n          ID: sync_modules\n    Function: module.run\n        Name: saltutil.sync_modules\n      Result: True\n     Comment: Module function saltutil.sync_modules executed\n     Started: 17:09:51.496280\n    Duration: 157.485 ms\n     Changes:   \n              ----------\n              ret:\n----------\n          ID: ssh_agent_socket\n    Function: environ.setenv\n        Name: SSH_AUTH_SOCK\n      Result: True\n     Comment: Environ values were already set with the correct values\n     Started: 17:09:51.654342\n    Duration: 0.64 ms\n     Changes:   \n----------\n          ID: mgr_cluster_list_nodes\n    Function: module.run\n        Name: mgrclusters.list_nodes\n      Result: True\n     Comment: Module function mgrclusters.list_nodes executed\n     Started: 17:09:51.655506\n    Duration: 68.206 ms\n     Changes:   \n              ----------\n              ret:\n                  ----------\n                  master-one:\n                      ----------\n                      caasp-release-version:\n                          4.1.2\n                      container-runtime:\n                          cri-o://1.16.1\n                      has-disruptive-updates:\n                          False\n                      has-updates:\n                          False\n                      kernel-version:\n                          4.12.14-193-default\n                      kubelet-version:\n                          v1.16.2\n                      os-image:\n                          SUSE Linux Enterprise Server 15 SP1\n                      role:\n                          master\n                      status:\n                          Ready\n                  worker-one:\n                      ----------\n                      caasp-release-version:\n                          4.1.2\n                      container-runtime:\n                          cri-o://1.16.1\n                      has-disruptive-updates:\n                          False\n                      has-updates:\n                          False\n                      kernel-version:\n                          4.12.14-197.37-default\n                      kubelet-version:\n                          v1.16.2\n                      os-image:\n                          SUSE Linux Enterprise Server 15 SP1\n                      role:\n                          None\n                      status:\n                          Ready\n\nSummary for local\n------------\nSucceeded: 3 (changed=2)\nFailed:    0\n------------\nTotal states run:     3\nTotal run time: 226.331 ms\nOf course, if there is any error, it will be nicely reported:\next-suma-head-caasp-min-sles15sp1:~ # salt-call state.apply clusters.removenode pillar='{\"cluster_type\": \"caasp\", \"ssh_auth_sock\": \"/tmp/ssh-2ype0xSxuC/agent.3070\", \"params\": {\"skuba_cluster_path\":\"/root/my-cluster-salt-v3\", \"node_name\": \"not-existing-node\"}}'\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\n[WARNING ] The function \"module.run\" is using its deprecated version and will expire in version \"Sodium\".\n[ERROR   ] Unexpected error 255 at skuba when removing a node: F0416 17:25:28.992415   10464 remove.go:52] error removing node not-existing-node: [remove-node] could not get node not-existing-node: nodes \"not-existing-node\" not found\n\n[ERROR   ] {'ret': {'stdout': '', 'stderr': 'F0416 17:25:28.992415   10464 remove.go:52] error removing node not-existing-node: [remove-node] could not get node not-existing-node: nodes \"not-existing-node\" not found\\n', 'success': False, 'retcode': 255}}\nlocal:\n----------\n          ID: sync_modules\n    Function: module.run\n        Name: saltutil.sync_modules\n      Result: True\n     Comment: Module function saltutil.sync_modules executed\n     Started: 17:25:28.777045\n    Duration: 159.059 ms\n     Changes:   \n              ----------\n              ret:\n----------\n          ID: ssh_agent_socket\n    Function: environ.setenv\n        Name: SSH_AUTH_SOCK\n      Result: True\n     Comment: Environ values were already set with the correct values\n     Started: 17:25:28.936661\n    Duration: 0.596 ms\n     Changes:   \n----------\n          ID: mgr_cluster_remove_node\n    Function: module.run\n        Name: mgrclusters.remove_node\n      Result: False\n     Comment: Module function mgrclusters.remove_node executed\n     Started: 17:25:28.937772\n    Duration: 57.813 ms\n     Changes:   \n              ----------\n              ret:\n                  ----------\n                  retcode:\n                      255\n                  stderr:\n                      F0416 17:25:28.992415   10464 remove.go:52] error removing node not-existing-node: [remove-node] could not get node not-existing-node: nodes \"not-existing-node\" not found\n                  stdout:\n                  success:\n                      False\n\nSummary for local\n------------\nSucceeded: 2 (changed=2)\nFailed:    1\n------------\nTotal states run:     3\nTotal run time: 217.468 ms\nGUI diff\nNo difference.\n\n DONE\n\nDocumentation\n\n\nNo documentation needed: this is backend part\n\n\n DONE\n\n\nTest coverage\n\n\nNo tests: no tests yet, probably tested at cucumber level\n\n\n DONE\n\n\nLinks\nTracks SUSE/spacewalk#11158\n\n DONE\n\nChangelogs\nIf you don't need a changelog check, please mark this checkbox:\n\n No changelog needed\n\nIf you uncheck the checkbox after the PR is created, you will need to re-run changelog_test (see below)\nRe-run a test\nIf you need to re-run a test, please mark the related checkbox, it will be unchecked automatically once it has re-run:\n\n Re-run test \"changelog_test\"\n Re-run test \"backend_unittests_pgsql\"\n Re-run test \"java_lint_checkstyle\"\n Re-run test \"java_pgsql_tests\"\n Re-run test \"ruby_rubocop\"\n Re-run test \"schema_migration_test_oracle\"\n Re-run test \"schema_migration_test_pgsql\"\n Re-run test \"susemanager_unittests\"\n Re-run test \"javascript_lint\"\n Re-run test \"spacecmd_unittests\"", "createdAt": "2020-04-16T15:29:06Z", "url": "https://github.com/uyuni-project/uyuni/pull/2128", "merged": true, "mergeCommit": {"oid": "755ce41ed64d8822edfdc04947c81e6f9e683be5"}, "closed": true, "closedAt": "2020-04-27T10:36:48Z", "author": {"login": "meaksh"}, "timelineItems": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYOlB_gFqTM5NDc1OTMyMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcaso88AFqTM5OTcxNTI4MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0NzU5MzIx", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-394759321", "createdAt": "2020-04-16T15:38:36Z", "commit": {"oid": "598fcc0bd066618341892dd7b6c71f12bab495ee"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNTozODozNlrOGGrZOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxNTozODozNlrOGGrZOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY1NTYxMA==", "bodyText": "Please start all our ids with mgr_ and use also a unique name.\nIf somebody call addnode and removenode in one state.apply it will fail.\nI think it is better to use always unique ids.", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r409655610", "createdAt": "2020-04-16T15:38:36Z", "author": {"login": "mcalmer"}, "path": "susemanager-utils/susemanager-sls/salt/clusters/addnode.sls", "diffHunk": "@@ -0,0 +1,24 @@\n+{%- if pillar.get('ssh_auth_sock', False) %}\n+ssh_agent_socket:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "598fcc0bd066618341892dd7b6c71f12bab495ee"}, "originalPosition": 2}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86425eac9dad7afb7dd584f88dc84069d65e36e0", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/86425eac9dad7afb7dd584f88dc84069d65e36e0", "committedDate": "2020-04-17T10:53:28Z", "message": "Add mgrclusters meta custom module for Salt"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53f1d770419aeac9dc5ff387d676cde20c31ffeb", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/53f1d770419aeac9dc5ff387d676cde20c31ffeb", "committedDate": "2020-04-17T10:53:36Z", "message": "Add CaaSP Cluster Provider Management module for Salt"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04da7e85aa3fa1042c928ed32fc28c4ec350d6dd", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/04da7e85aa3fa1042c928ed32fc28c4ec350d6dd", "committedDate": "2020-04-17T10:53:36Z", "message": "Add initial SLS states files for managing clusters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "666f4b8a4f31c3365f4d81ed3c8a89fcac3acd4e", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/666f4b8a4f31c3365f4d81ed3c8a89fcac3acd4e", "committedDate": "2020-04-17T10:53:36Z", "message": "Change format for custom Salt 'caasp.list_nodes' output"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65cb3390bcc4d71eeeff27db394e875d8e897161", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/65cb3390bcc4d71eeeff27db394e875d8e897161", "committedDate": "2020-04-17T10:53:37Z", "message": "Sanizite values returned by skuba CLI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16a9f1677b4ecadc487c925ab24f6f6e3c0cc5c9", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/16a9f1677b4ecadc487c925ab24f6f6e3c0cc5c9", "committedDate": "2020-04-17T10:53:37Z", "message": "Implement 'upgrade_cluster' method on CaaSP cluster manager"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "211fc52c83781efed37a0cbb09fc6d2ef1dea242", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/211fc52c83781efed37a0cbb09fc6d2ef1dea242", "committedDate": "2020-04-17T10:53:37Z", "message": "Set default timeout to 20 minutes for Skuba commands"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "295ef14fefdb8eaff4108b9eb31357d858fabe18", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/295ef14fefdb8eaff4108b9eb31357d858fabe18", "committedDate": "2020-04-17T10:53:37Z", "message": "Add SLS file for clusters.createcluster state"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4aba93a5152a0860880d98708d1a2fbfebcbc9fb", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/4aba93a5152a0860880d98708d1a2fbfebcbc9fb", "committedDate": "2020-04-17T10:53:38Z", "message": "Set SSH_AUTH_SOCK env if available for clusters SLS states"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e898f9fdedd58a08616be6e1b92b4d898e525e6", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/9e898f9fdedd58a08616be6e1b92b4d898e525e6", "committedDate": "2020-04-17T10:53:38Z", "message": "Take care of <none> values on skuba output"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aeec39cd29ffafc831973bc5f4796e4daca6da0d", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/aeec39cd29ffafc831973bc5f4796e4daca6da0d", "committedDate": "2020-04-17T10:53:39Z", "message": "Implement cluster creation methods for CaaSP cluster"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2c5951c55fd1fb06bb19955aa20b726820910e2", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/e2c5951c55fd1fb06bb19955aa20b726820910e2", "committedDate": "2020-04-17T10:53:40Z", "message": "Add gateway for mgrclusters.create_cluster module"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "520343dfdf580204a9dfcb789dc1f9483fa59d27", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/520343dfdf580204a9dfcb789dc1f9483fa59d27", "committedDate": "2020-04-17T10:53:40Z", "message": "Fix salt.utils.path.which import in 2016.11 codebase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8dcdba55c9457c60434d71f07bb7bc33a8ea37a", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/c8dcdba55c9457c60434d71f07bb7bc33a8ea37a", "committedDate": "2020-04-17T10:53:40Z", "message": "Add comments and fix identation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e0a311acad93d91d8e372096c5348d451176769", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/5e0a311acad93d91d8e372096c5348d451176769", "committedDate": "2020-04-17T10:53:40Z", "message": "Introduce 'upgrade_node' and optional parameters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3f1625f397140b0813bee4b4d08a2529166f40a", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/a3f1625f397140b0813bee4b4d08a2529166f40a", "committedDate": "2020-04-17T10:53:41Z", "message": "Cosmetic changes at method signature"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "803160f67890bde1697fc2628124979fe9dcc42c", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/803160f67890bde1697fc2628124979fe9dcc42c", "committedDate": "2020-04-17T10:53:41Z", "message": "Fix upgrade_cluster skuba commands args"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "364fe59e2db9b6109860afba822fed7daf95557f", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/364fe59e2db9b6109860afba822fed7daf95557f", "committedDate": "2020-04-17T10:53:41Z", "message": "Add 'mgrclusters' and 'mgr_caasp_manager' Salt modules to spec file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "290f9a29bdc8007a66e21285ad6c842883a232e0", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/290f9a29bdc8007a66e21285ad6c842883a232e0", "committedDate": "2020-04-17T10:54:03Z", "message": "Update changelog for susemanager-sls"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e80e4aa78687a1b1aef85eafec815a37e1385e4", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/8e80e4aa78687a1b1aef85eafec815a37e1385e4", "committedDate": "2020-04-17T10:54:03Z", "message": "Make clusters SLS files to require util.syncmodules"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5120f3176a843dfbef9d643b386d9a7f00c52345", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/5120f3176a843dfbef9d643b386d9a7f00c52345", "committedDate": "2020-04-17T10:54:03Z", "message": "Pass 'cloud_provider' and 'strict_capability_defaults' to cluster_init"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a82810b2e078264a9bb4518412777ea4d4fe22a0", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/a82810b2e078264a9bb4518412777ea4d4fe22a0", "committedDate": "2020-04-17T10:54:03Z", "message": "Use unique state ids between different clusters states"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14c785fa105894990f9fc9e00d70b15a3b093b7a", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/14c785fa105894990f9fc9e00d70b15a3b093b7a", "committedDate": "2020-04-17T10:54:04Z", "message": "Add missing upgradecluster.sls file"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c05c42779a492d7cab84333c867efc6229a9b26c", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/c05c42779a492d7cab84333c867efc6229a9b26c", "committedDate": "2020-04-16T15:52:27Z", "message": "Use unique state ids between different clusters states"}, "afterCommit": {"oid": "14c785fa105894990f9fc9e00d70b15a3b093b7a", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/14c785fa105894990f9fc9e00d70b15a3b093b7a", "committedDate": "2020-04-17T10:54:04Z", "message": "Add missing upgradecluster.sls file"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1MzU4Njkz", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-395358693", "createdAt": "2020-04-17T10:58:53Z", "commit": {"oid": "c05c42779a492d7cab84333c867efc6229a9b26c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMDo1ODo1M1rOGHJjKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMDo1ODo1M1rOGHJjKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDE0OTY3NQ==", "bodyText": "For consistency maybe this parameter should be called skuba_cluster_path.", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r410149675", "createdAt": "2020-04-17T10:58:53Z", "author": {"login": "mateiw"}, "path": "susemanager-utils/susemanager-sls/src/modules/mgr_caasp_manager.py", "diffHunk": "@@ -0,0 +1,347 @@\n+# -*- coding: utf-8 -*-\n+'''\n+SUSE Manager CaaSP Cluster Manager module for Salt\n+\n+'''\n+from __future__ import absolute_import\n+\n+\n+import logging\n+import os\n+import subprocess\n+\n+import salt.utils.stringutils\n+import salt.utils.timed_subprocess\n+\n+try:\n+    from salt.utils.path import which\n+except ImportError:\n+    from salt.utils import which\n+\n+from salt.utils.dictupdate import merge_list\n+from salt.exceptions import CommandExecutionError\n+\n+\n+log = logging.getLogger(__name__)\n+\n+__virtualname__ = 'caasp'\n+\n+DEFAULT_TIMEOUT = 1200\n+\n+\n+def __virtual__():\n+    '''\n+    This module is always enabled while 'skuba' CLI tools is available.\n+    '''\n+    return __virtualname__ if which('skuba') else (False, 'skuba is not available')\n+\n+\n+def _call_skuba(skuba_cluster_path,\n+                cmd_args,\n+                timeout=DEFAULT_TIMEOUT,\n+                **kwargs):\n+\n+    log.debug(\"Calling Skuba CLI: 'skuba {}' - Timeout: {}\".format(cmd_args, timeout))\n+    try:\n+        skuba_proc = salt.utils.timed_subprocess.TimedProc(\n+            [\"skuba\"] + cmd_args.split(),\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            timeout=timeout,\n+            cwd=skuba_cluster_path,\n+        )\n+        skuba_proc.run()\n+        return skuba_proc\n+    except Exception as exc:\n+        error_msg = \"Unexpected error while calling skuba: {}\".format(exc)\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+\n+def _sanitize_skuba_output_values(items):\n+    ret = []\n+    for i in items:\n+        if i.lower() == 'no':\n+            ret.append(False)\n+        elif i.lower() == 'yes':\n+            ret.append(True)\n+        elif i.lower() == '<none>':\n+            ret.append(None)\n+        else:\n+            ret.append(i)\n+    return ret\n+\n+\n+def list_nodes(skuba_cluster_path,\n+               timeout=DEFAULT_TIMEOUT,\n+               **kwargs):\n+    skuba_proc = _call_skuba(skuba_cluster_path, \"cluster status\")\n+    if skuba_proc.process.returncode != 0 or skuba_proc.stderr:\n+        error_msg = \"Unexpected error {} at skuba when listing nodes: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+    skuba_proc_lines = salt.utils.stringutils.to_str(skuba_proc.stdout).splitlines()\n+\n+    ret = {}\n+    try:\n+        # The first line of skuba output are the headers\n+        headers = [x.strip().lower() for x in skuba_proc_lines[0].split('  ') if x]\n+        name_idx = headers.index('name')\n+        headers.remove('name')\n+        for line in skuba_proc_lines[1:]:\n+            items = [x.strip() for x in line.split('  ') if x]\n+            node_name = items.pop(name_idx)\n+            node_zip = zip(headers, _sanitize_skuba_output_values(items))\n+            ret[node_name] = dict(node_zip)\n+    except Exception as exc:\n+        error_msg = \"Unexpected error while parsing skuba output: {}\".format(exc)\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+    return ret\n+\n+\n+def remove_node(skuba_cluster_path,\n+                node_name,\n+                drain_timeout=None,\n+                verbosity=None,\n+                timeout=DEFAULT_TIMEOUT,\n+                **kwargs):\n+\n+    cmd_args = \"node remove {}\".format(node_name)\n+\n+    if drain_timeout:\n+        cmd_args += \" --drain-timeout {}\".format(drain_timeout)\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when removing a node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def add_node(skuba_cluster_path,\n+             node_name,\n+             role,\n+             target,\n+             ignore_preflight_errors=None,\n+             port=None,\n+             sudo=None,\n+             user=None,\n+             verbosity=None,\n+             timeout=DEFAULT_TIMEOUT,\n+             **kwargs):\n+\n+    cmd_args = \"node join --role {} --target {} {}\".format(role, target, node_name)\n+\n+    if ignore_preflight_errors:\n+        cmd_args += \" --ignore-preflight-errors {}\".format(ignore_preflight_errors)\n+    if port:\n+        cmd_args += \" --port {}\".format(port)\n+    if sudo:\n+        cmd_args += \" --sudo\"\n+    if user:\n+        cmd_args += \" --user {}\".format(user)\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when adding a new node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def upgrade_cluster(skuba_cluster_path,\n+                    verbosity=None,\n+                    timeout=DEFAULT_TIMEOUT,\n+                    **kwargs):\n+\n+    cmd_args = \"cluster upgrade plan\"\n+\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when upgrading the cluster: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def upgrade_node(skuba_cluster_path,\n+                 verbosity=None,\n+                 timeout=DEFAULT_TIMEOUT,\n+                 plan=False,\n+                 **kwargs):\n+\n+    cmd_args = \"node upgrade {}\".format(\"plan\" if plan else \"apply\")\n+\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when upgrading the node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def cluster_init(name,\n+                 cluster_path,\n+                 target,\n+                 cloud_provider=None,\n+                 strict_capability_defaults=False,\n+                 verbosity=None,\n+                 timeout=DEFAULT_TIMEOUT,\n+                 **kwargs):\n+\n+    cmd_args = \"cluster init --control-plane {} {}\".format(target, name)\n+\n+    if cloud_provider:\n+        cmd_args += \" --cloud-provider {}\".format(cloud_provider)\n+    if strict_capability_defaults:\n+        cmd_args += \" --strict-capability-defaults\"\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when initializing the cluster: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def master_bootstrap(node_name,\n+                     skuba_cluster_path,\n+                     target,\n+                     ignore_preflight_errors=None,\n+                     port=None,\n+                     sudo=None,\n+                     user=None,\n+                     verbosity=None,\n+                     timeout=DEFAULT_TIMEOUT,\n+                     **kwargs):\n+\n+    cmd_args = \"node bootstrap --target {} {}\".format(target, node_name)\n+\n+    if ignore_preflight_errors:\n+        cmd_args += \" --ignore-preflight-errors {}\".format(ignore_preflight_errors)\n+    if port:\n+        cmd_args += \" --port {}\".format(port)\n+    if sudo:\n+        cmd_args += \" --sudo\"\n+    if user:\n+        cmd_args += \" --user {}\".format(user)\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when bootstrapping the node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def create_cluster(cluster_name,\n+                   cluster_path,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c05c42779a492d7cab84333c867efc6229a9b26c"}, "originalPosition": 306}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/711e983f49d7dab6b5451c5a9b155ae1295295cf", "committedDate": "2020-04-17T11:19:33Z", "message": "Rename some parameters to have better consistency"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NTY2MzIz", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-395566323", "createdAt": "2020-04-17T15:43:36Z", "commit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo0MzozNlrOGHTMsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo0MzozNlrOGHTMsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwNzc2MA==", "bodyText": "@ereslibre : this is the core of what we would be using to call skuba via Salt.\nWould you please review it if it makes sense from the skuba standpoint?", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r410307760", "createdAt": "2020-04-17T15:43:36Z", "author": {"login": "mbologna"}, "path": "susemanager-utils/susemanager-sls/src/modules/mgr_caasp_manager.py", "diffHunk": "@@ -0,0 +1,347 @@\n+# -*- coding: utf-8 -*-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NTY4MDg4", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-395568088", "createdAt": "2020-04-17T15:45:48Z", "commit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo0NTo0OFrOGHTR-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo0NTo0OFrOGHTR-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwOTExNA==", "bodyText": "I assume that if a specific type of cluster does not offer the init workflow, we can skip the initialization and jump directly to other operations, correct?", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r410309114", "createdAt": "2020-04-17T15:45:48Z", "author": {"login": "mbologna"}, "path": "susemanager-utils/susemanager-sls/salt/clusters/createcluster.sls", "diffHunk": "@@ -0,0 +1,24 @@\n+{%- if pillar.get('ssh_auth_sock', False) %}\n+mgr_ssh_agent_socket_clusters_createcluster:\n+  environ.setenv:\n+    - name: SSH_AUTH_SOCK\n+    - value: {{ pillar['ssh_auth_sock'] }}\n+{%- endif %}\n+\n+mgr_cluster_create_cluster:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NTc3NDE0", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-395577414", "createdAt": "2020-04-17T15:57:17Z", "commit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo1NzoxN1rOGHTuag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxNTo1NzoxN1rOGHTuag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxNjM5NA==", "bodyText": "I think timeout  timeout should be passed as arg to every call to _call_skuba, am I assuming correctly?", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r410316394", "createdAt": "2020-04-17T15:57:17Z", "author": {"login": "mbologna"}, "path": "susemanager-utils/susemanager-sls/src/modules/mgr_caasp_manager.py", "diffHunk": "@@ -0,0 +1,347 @@\n+# -*- coding: utf-8 -*-\n+'''\n+SUSE Manager CaaSP Cluster Manager module for Salt\n+\n+'''\n+from __future__ import absolute_import\n+\n+\n+import logging\n+import os\n+import subprocess\n+\n+import salt.utils.stringutils\n+import salt.utils.timed_subprocess\n+\n+try:\n+    from salt.utils.path import which\n+except ImportError:\n+    from salt.utils import which\n+\n+from salt.utils.dictupdate import merge_list\n+from salt.exceptions import CommandExecutionError\n+\n+\n+log = logging.getLogger(__name__)\n+\n+__virtualname__ = 'caasp'\n+\n+DEFAULT_TIMEOUT = 1200\n+\n+\n+def __virtual__():\n+    '''\n+    This module is always enabled while 'skuba' CLI tools is available.\n+    '''\n+    return __virtualname__ if which('skuba') else (False, 'skuba is not available')\n+\n+\n+def _call_skuba(skuba_cluster_path,\n+                cmd_args,\n+                timeout=DEFAULT_TIMEOUT,\n+                **kwargs):\n+\n+    log.debug(\"Calling Skuba CLI: 'skuba {}' - Timeout: {}\".format(cmd_args, timeout))\n+    try:\n+        skuba_proc = salt.utils.timed_subprocess.TimedProc(\n+            [\"skuba\"] + cmd_args.split(),\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            timeout=timeout,\n+            cwd=skuba_cluster_path,\n+        )\n+        skuba_proc.run()\n+        return skuba_proc\n+    except Exception as exc:\n+        error_msg = \"Unexpected error while calling skuba: {}\".format(exc)\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+\n+def _sanitize_skuba_output_values(items):\n+    ret = []\n+    for i in items:\n+        if i.lower() == 'no':\n+            ret.append(False)\n+        elif i.lower() == 'yes':\n+            ret.append(True)\n+        elif i.lower() == '<none>':\n+            ret.append(None)\n+        else:\n+            ret.append(i)\n+    return ret\n+\n+\n+def list_nodes(skuba_cluster_path,\n+               timeout=DEFAULT_TIMEOUT,\n+               **kwargs):\n+    skuba_proc = _call_skuba(skuba_cluster_path, \"cluster status\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1NTc5NDU3", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-395579457", "createdAt": "2020-04-17T15:59:51Z", "commit": {"oid": "711e983f49d7dab6b5451c5a9b155ae1295295cf"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "752181d41f679c1d4d85fe3e1ab7243c1e27db1f", "author": {"user": {"login": "meaksh", "name": "Pablo Su\u00e1rez Hern\u00e1ndez"}}, "url": "https://github.com/uyuni-project/uyuni/commit/752181d41f679c1d4d85fe3e1ab7243c1e27db1f", "committedDate": "2020-04-17T16:15:18Z", "message": "Pass custom timeout value to _skuba_call method"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3MTExMTY4", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-397111168", "createdAt": "2020-04-21T08:38:16Z", "commit": {"oid": "752181d41f679c1d4d85fe3e1ab7243c1e27db1f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5NzE1Mjgw", "url": "https://github.com/uyuni-project/uyuni/pull/2128#pullrequestreview-399715280", "createdAt": "2020-04-24T07:47:54Z", "commit": {"oid": "752181d41f679c1d4d85fe3e1ab7243c1e27db1f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNzo0Nzo1NFrOGLLBkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNzo0Nzo1NFrOGLLBkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2ODE0Ng==", "bodyText": "Just a comment for the near future (non-blocking). When performing upgrades there are different stages when it comes to upgrading addons and nodes, so in general:\n\nUpgrade addons to the latest version of the current platform version (skuba addon upgrade plan/apply)\nUpgrade all nodes normally (skuba node upgrade plan/apply)\nUpgrade addons to the new version of the platform (skuba addon upgrade plan/apply)\n\n1. and 3. are not always required (depends on what changes on each release), but can always be run to ensure the most constraining scenario.", "url": "https://github.com/uyuni-project/uyuni/pull/2128#discussion_r414368146", "createdAt": "2020-04-24T07:47:54Z", "author": {"login": "ereslibre"}, "path": "susemanager-utils/susemanager-sls/src/modules/mgr_caasp_manager.py", "diffHunk": "@@ -0,0 +1,347 @@\n+# -*- coding: utf-8 -*-\n+'''\n+SUSE Manager CaaSP Cluster Manager module for Salt\n+\n+'''\n+from __future__ import absolute_import\n+\n+\n+import logging\n+import os\n+import subprocess\n+\n+import salt.utils.stringutils\n+import salt.utils.timed_subprocess\n+\n+try:\n+    from salt.utils.path import which\n+except ImportError:\n+    from salt.utils import which\n+\n+from salt.utils.dictupdate import merge_list\n+from salt.exceptions import CommandExecutionError\n+\n+\n+log = logging.getLogger(__name__)\n+\n+__virtualname__ = 'caasp'\n+\n+DEFAULT_TIMEOUT = 1200\n+\n+\n+def __virtual__():\n+    '''\n+    This module is always enabled while 'skuba' CLI tools is available.\n+    '''\n+    return __virtualname__ if which('skuba') else (False, 'skuba is not available')\n+\n+\n+def _call_skuba(skuba_cluster_path,\n+                cmd_args,\n+                timeout=DEFAULT_TIMEOUT,\n+                **kwargs):\n+\n+    log.debug(\"Calling Skuba CLI: 'skuba {}' - Timeout: {}\".format(cmd_args, timeout))\n+    try:\n+        skuba_proc = salt.utils.timed_subprocess.TimedProc(\n+            [\"skuba\"] + cmd_args.split(),\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            timeout=timeout,\n+            cwd=skuba_cluster_path,\n+        )\n+        skuba_proc.run()\n+        return skuba_proc\n+    except Exception as exc:\n+        error_msg = \"Unexpected error while calling skuba: {}\".format(exc)\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+\n+def _sanitize_skuba_output_values(items):\n+    ret = []\n+    for i in items:\n+        if i.lower() == 'no':\n+            ret.append(False)\n+        elif i.lower() == 'yes':\n+            ret.append(True)\n+        elif i.lower() == '<none>':\n+            ret.append(None)\n+        else:\n+            ret.append(i)\n+    return ret\n+\n+\n+def list_nodes(skuba_cluster_path,\n+               timeout=DEFAULT_TIMEOUT,\n+               **kwargs):\n+    skuba_proc = _call_skuba(skuba_cluster_path, \"cluster status\", timeout=timeout)\n+    if skuba_proc.process.returncode != 0 or skuba_proc.stderr:\n+        error_msg = \"Unexpected error {} at skuba when listing nodes: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+    skuba_proc_lines = salt.utils.stringutils.to_str(skuba_proc.stdout).splitlines()\n+\n+    ret = {}\n+    try:\n+        # The first line of skuba output are the headers\n+        headers = [x.strip().lower() for x in skuba_proc_lines[0].split('  ') if x]\n+        name_idx = headers.index('name')\n+        headers.remove('name')\n+        for line in skuba_proc_lines[1:]:\n+            items = [x.strip() for x in line.split('  ') if x]\n+            node_name = items.pop(name_idx)\n+            node_zip = zip(headers, _sanitize_skuba_output_values(items))\n+            ret[node_name] = dict(node_zip)\n+    except Exception as exc:\n+        error_msg = \"Unexpected error while parsing skuba output: {}\".format(exc)\n+        log.error(error_msg)\n+        raise CommandExecutionError(error_msg)\n+\n+    return ret\n+\n+\n+def remove_node(skuba_cluster_path,\n+                node_name,\n+                drain_timeout=None,\n+                verbosity=None,\n+                timeout=DEFAULT_TIMEOUT,\n+                **kwargs):\n+\n+    cmd_args = \"node remove {}\".format(node_name)\n+\n+    if drain_timeout:\n+        cmd_args += \" --drain-timeout {}\".format(drain_timeout)\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args, timeout=timeout)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when removing a node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def add_node(skuba_cluster_path,\n+             node_name,\n+             role,\n+             target,\n+             ignore_preflight_errors=None,\n+             port=None,\n+             sudo=None,\n+             user=None,\n+             verbosity=None,\n+             timeout=DEFAULT_TIMEOUT,\n+             **kwargs):\n+\n+    cmd_args = \"node join --role {} --target {} {}\".format(role, target, node_name)\n+\n+    if ignore_preflight_errors:\n+        cmd_args += \" --ignore-preflight-errors {}\".format(ignore_preflight_errors)\n+    if port:\n+        cmd_args += \" --port {}\".format(port)\n+    if sudo:\n+        cmd_args += \" --sudo\"\n+    if user:\n+        cmd_args += \" --user {}\".format(user)\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args, timeout=timeout)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when adding a new node: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def upgrade_cluster(skuba_cluster_path,\n+                    verbosity=None,\n+                    timeout=DEFAULT_TIMEOUT,\n+                    **kwargs):\n+\n+    cmd_args = \"cluster upgrade plan\"\n+\n+    if verbosity:\n+        cmd_args += \" --verbosity {}\".format(verbosity)\n+\n+    skuba_proc = _call_skuba(skuba_cluster_path, cmd_args, timeout=timeout)\n+    if skuba_proc.process.returncode != 0:\n+        error_msg = \"Unexpected error {} at skuba when upgrading the cluster: {}\".format(\n+                skuba_proc.process.returncode,\n+                salt.utils.stringutils.to_str(skuba_proc.stderr))\n+        log.error(error_msg)\n+\n+    ret = {\n+        'stdout': salt.utils.stringutils.to_str(skuba_proc.stdout),\n+        'stderr': salt.utils.stringutils.to_str(skuba_proc.stderr),\n+        'success': not skuba_proc.process.returncode,\n+        'retcode': skuba_proc.process.returncode,\n+    }\n+    return ret\n+\n+\n+def upgrade_node(skuba_cluster_path,\n+                 verbosity=None,\n+                 timeout=DEFAULT_TIMEOUT,\n+                 plan=False,\n+                 **kwargs):\n+\n+    cmd_args = \"node upgrade {}\".format(\"plan\" if plan else \"apply\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "752181d41f679c1d4d85fe3e1ab7243c1e27db1f"}, "originalPosition": 210}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1356, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}