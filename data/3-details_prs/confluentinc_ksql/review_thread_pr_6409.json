{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxODMwNjE4", "number": 6409, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMzo1ODo0MFrOEs0X4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxOTo1NjoyNlrOEv9AUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDMwODgxOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMzo1ODo0MFrOHgQ_VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMzowNDozNlrOHgTwsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NDgzNw==", "bodyText": "The pullQueryContext is the same, the only thing that changes are the keys right?You could avoid creating a new object and just set the keys in each iteration.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503594837", "createdAt": "2020-10-12T23:58:40Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDI0MQ==", "bodyText": "PullQueryContext is threadsafe because it's immutable.  Dealing with small immutable objects is fairly efficient in the JVM and is considered worth doing to make things threadsafe and easy to understand.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503640241", "createdAt": "2020-10-13T03:04:36Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NDgzNw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDMxMjg4OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMDowMDo1OFrOHgRBlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxOTozNzoyOFrOHg2brg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTQxMw==", "bodyText": "Why don't you return besides they keys also the active, standby per group? This way you wouldn't need to do locate twice basically.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503595413", "createdAt": "2020-10-13T00:00:58Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5ODE3MA==", "bodyText": "Actually, I think currently the routing is wrong and you have to return the <active,standby> per group", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503598170", "createdAt": "2020-10-13T00:11:40Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTQxMw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzODY1Nw==", "bodyText": "Yeah, I had thought that a given set of partitions were grouped together at each active and standby, but I think you're right this isn't the case.  I'll change it to groupByActiveStandyList or something similar.  In practice, there aren't too many standbys, so this is likely to be a lot better than grouping by partition or just fetching by key.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503638657", "createdAt": "2020-10-13T02:58:40Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTQxMw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwODMwMg==", "bodyText": "I reworked this so that it now groups key together only if they share the same list of nodes, including active and all standbys.  Most of the time if there's 1 or 2 standbys and lots of keys fetched, this will hopefully reduce unnecessary calls.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r504208302", "createdAt": "2020-10-13T19:37:28Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTQxMw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDMxNTAzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMDowMjoxN1rOHgRCzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMjo1ODo1OFrOHgTq2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTcyNQ==", "bodyText": "It seems that you assume that all keys in the group are routed to the same standby but that's not the case, since they might belong to different partitions.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503595725", "createdAt": "2020-10-13T00:02:17Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -271,8 +308,9 @@ private PullQueryResult handlePullQuery(\n   ) {\n     // Get active and standby nodes for this key\n     final Locator locator = pullQueryContext.mat.locator();\n+    final Struct key = Iterables.getLast(pullQueryContext.keys);\n     final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n+        key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzODc0NA==", "bodyText": "I had thought that a given set of partitions were grouped together at each active and standby, but I think you're right this isn't the case.  I'll rework this.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503638744", "createdAt": "2020-10-13T02:58:58Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -271,8 +308,9 @@ private PullQueryResult handlePullQuery(\n   ) {\n     // Get active and standby nodes for this key\n     final Locator locator = pullQueryContext.mat.locator();\n+    final Struct key = Iterables.getLast(pullQueryContext.keys);\n     final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n+        key,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NTcyNQ=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDMyODgxOnYy", "diffSide": "RIGHT", "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMDowOTo1NlrOHgRKpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMzowNjozNVrOHgTyng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NzczNA==", "bodyText": "Here you group keys based on the node that hosts the active partition. However, the keys may belong to different partitions and hence the standby may be different for keys in the same group. I think you should group them per <active, standby> combination else when we do the routing, we cannot assume that all keys go to the same standby  (when active dead)", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503597734", "createdAt": "2020-10-13T00:09:56Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -118,6 +123,21 @@\n     return filteredHosts;\n   }\n \n+  @Override\n+  public List<List<Struct>> groupByLocation(final List<Struct> keys) {\n+    final Map<String, List<Struct>> groups = new HashMap<>();\n+    for (Struct key : keys) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDczNA==", "bodyText": "Yeah, I think either grouping by partition or <active, standby> list would be best.  I think in the normal case where you don't have many standbys, the latter is a good method.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503640734", "createdAt": "2020-10-13T03:06:35Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -118,6 +123,21 @@\n     return filteredHosts;\n   }\n \n+  @Override\n+  public List<List<Struct>> groupByLocation(final List<Struct> keys) {\n+    final Map<String, List<Struct>> groups = new HashMap<>();\n+    for (Struct key : keys) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5NzczNA=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDM0MzIzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMDoxODozMlrOHgRTMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMzowMjozMlrOHgTupw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5OTkyMw==", "bodyText": "What's the benefit of having this class versus adding one more entry in the ComparisonTarget? Seems a lot of work for achieving the same functionality?", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503599923", "createdAt": "2020-10-13T00:18:32Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -761,19 +827,78 @@ private static Instant asInstant(final Expression other) {\n   }\n \n   private enum ComparisonTarget {\n-    KEYCOL,\n     WINDOWSTART,\n     WINDOWEND\n   }\n \n-  private static Map<ComparisonTarget, List<ComparisonExpression>> extractComparisons(\n+  private static class KeyAndWindowBounds {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzOTcxOQ==", "bodyText": "InPredicate isn't a ComparisonExpression.  Here, I can have a different type for each target type.\nAlternatively, I could have change the map to point to an expression and casted back to the subtype, but that's not a great practice.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503639719", "createdAt": "2020-10-13T03:02:32Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -761,19 +827,78 @@ private static Instant asInstant(final Expression other) {\n   }\n \n   private enum ComparisonTarget {\n-    KEYCOL,\n     WINDOWSTART,\n     WINDOWEND\n   }\n \n-  private static Map<ComparisonTarget, List<ComparisonExpression>> extractComparisons(\n+  private static class KeyAndWindowBounds {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU5OTkyMw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 360}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDM2MjE2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwMDoyOToxN1rOHgRdzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMTowNzo0OFrOHiaxkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYwMjYzNw==", "bodyText": "Shouldn't there be some error handling here? What happens if a thread dies due to an uncaught exception? We would like to fail the entire query and not return partial results, right? Maybe handle something like this:\n        try {\n            future.get();\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        } catch (ExecutionException e) {\n            // Extract the actual exception from its wrapper\n            Throwable t = e.getCause();\n            System.err.println(\"Uncaught exception is detected! \" + t\n                    + \" st: \" + Arrays.toString(t.getStackTrace()));\n            // ... Handle the exception\n        }", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r503602637", "createdAt": "2020-10-13T00:29:17Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            keys,\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            routingOptions\n+        )));\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      }\n+      for (Future<PullQueryResult> future : futures) {\n+        final PullQueryResult result = future.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwNzYwNQ==", "bodyText": "Yeah, I agree that we don't want partial results, so I wouldn't want to catch each individual future.\nAlso, calling interrupt is what causes the InterruptedException and it sends that exception to other threads.  To have the current thread have it thrown, all we have to do is not catch it.\nI pretty much just let the existing handler catch Exception in the wider scope.  I added a small special case to unwrap ExecutionException so we get the same error message.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r504207605", "createdAt": "2020-10-13T19:36:11Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            keys,\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            routingOptions\n+        )));\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      }\n+      for (Future<PullQueryResult> future : futures) {\n+        final PullQueryResult result = future.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYwMjYzNw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA1MzM0Ng==", "bodyText": "calling interrupt is what causes the InterruptedException\n\nI think you probably meant this, but just a small clarification here - Thread.currentThread().interrupt() does not directly cause an InterruptedException. It just sets an interrupt flag on the thread's state, and then any blocking code is expected to check that flag and raise an InterruptedException when it notices that flag. There is no guarantee that calling interupt will actually do anything (especially if there's misbehaving client code). It's a cooperative strategy.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505053346", "createdAt": "2020-10-14T23:10:53Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            keys,\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            routingOptions\n+        )));\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      }\n+      for (Future<PullQueryResult> future : futures) {\n+        final PullQueryResult result = future.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYwMjYzNw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg1MjMwNA==", "bodyText": "Yeah, you're right that a thread may have to explicitly check for being interrupted if it's doing a lot of computation.  If it's sleeping in some manner, there's a good chance that it will be thrown for the thread, e.g. Thread.sleep.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505852304", "createdAt": "2020-10-15T21:07:48Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,26 +230,48 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<List<Struct>> keysByLocation = mat.locator().groupByLocation(\n+          whereInfo.keysBound.stream()\n+              .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+              .collect(Collectors.toList()));\n+\n+      for (List<Struct> keys : keysByLocation) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            keys,\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            routingOptions\n+        )));\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      }\n+      for (Future<PullQueryResult> future : futures) {\n+        final PullQueryResult result = future.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYwMjYzNw=="}, "originalCommit": {"oid": "dadec8e9b9e5a81bbb022aa8bc03634652fcb449"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MzU5ODM0OnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/engine/rewrite/PullQueryKeyUpdater.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzowMDoxOFrOHhpkmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMDo1NTo0NVrOHiaJDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA0NjE3MA==", "bodyText": "very soon we're going to support all expression types as keys (including structs) and we already support DECIMAL keys. I'm not entirely sure that this approach will work then since it's not always possible to take a java object and convert it into an expression as the inverse conversion is lossy. Is it possible to extract the original expression when we construct the List<Struct> keys instead of mapping it to Java objects?", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505046170", "createdAt": "2020-10-14T23:00:18Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/engine/rewrite/PullQueryKeyUpdater.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.engine.rewrite;\n+\n+import com.google.common.collect.Iterables;\n+import io.confluent.ksql.engine.rewrite.ExpressionTreeRewriter.Context;\n+import io.confluent.ksql.execution.expression.tree.DoubleLiteral;\n+import io.confluent.ksql.execution.expression.tree.Expression;\n+import io.confluent.ksql.execution.expression.tree.InListExpression;\n+import io.confluent.ksql.execution.expression.tree.InPredicate;\n+import io.confluent.ksql.execution.expression.tree.IntegerLiteral;\n+import io.confluent.ksql.execution.expression.tree.LongLiteral;\n+import io.confluent.ksql.execution.expression.tree.StringLiteral;\n+import io.confluent.ksql.execution.expression.tree.VisitParentExpressionVisitor;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.util.KsqlException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+\n+/**\n+ * Takes a configured statement and rewrites it to update the keys requested.  This only works if\n+ * the query uses the IN keyword.\n+ * This is required because when fetching multiple keys using the IN keyword, a given host may be\n+ * the active for one partition and standby for another partition, and if you issue a query\n+ * requesting keys from both partitions while using ksql.query.pull.enable.standby.reads=true,\n+ * you'll fetch stale values unintentionally.\n+ */\n+public final class PullQueryKeyUpdater {\n+\n+  private PullQueryKeyUpdater() {}\n+\n+  /**\n+   * Returns a Statement with the IN expression updated to include only the given keys.\n+   * @param statement the original statement.\n+   * @param keys the keys to include\n+   * @return The updated statement\n+   */\n+  public static Statement update(\n+      final Query statement,\n+      final List<Struct> keys) {\n+    final BiFunction<Expression, Void, Expression> expressionRewriter =\n+        (e, v) -> ExpressionTreeRewriter.rewriteWith(\n+            new ExpressionRewriterPlugin(keys)::process, e, v);\n+    return (Statement) new StatementRewriter<>(expressionRewriter, (n, c) -> Optional.empty())\n+        .rewrite(statement, null);\n+  }\n+\n+  private static final class ExpressionRewriterPlugin extends\n+      VisitParentExpressionVisitor<Optional<Expression>, Context<Void>> {\n+\n+    private final List<Struct> keys;\n+\n+    ExpressionRewriterPlugin(final List<Struct> keys) {\n+      super(Optional.empty());\n+      this.keys = keys;\n+    }\n+\n+    @Override\n+    public Optional<Expression> visitInPredicate(final InPredicate node, final Context<Void> ctx) {\n+      final List<Expression> values = keys.stream().map(k -> {\n+        final Field field = Iterables.getOnlyElement(k.schema().fields());\n+        return convertToExpression(field.schema(), k.get(field));\n+      }).collect(Collectors.toList());\n+      final InListExpression inList\n+          = new InListExpression(node.getValueList().getLocation(), values);\n+      return Optional.of(new InPredicate(node.getLocation(), node.getValue(), inList));\n+    }\n+\n+    private Expression convertToExpression(final Schema schema, final Object value) {\n+      switch (schema.type()) {\n+        case STRING:\n+          return new StringLiteral((String) value);\n+        case INT8:\n+        case INT16:\n+        case INT32:\n+          return new IntegerLiteral((int) value);\n+        case INT64:\n+          return new LongLiteral((long) value);\n+        case FLOAT32:\n+        case FLOAT64:\n+          return new DoubleLiteral((double) value);\n+        default:\n+          throw new KsqlException(\"Unknown key type \" + schema.type());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MTkzMg==", "bodyText": "As we mentioned elsewhere, I entirely got rid of this rewriter and instead use partitions to ensure I only read the correct data rather than the keys only.  I also added partition to the read calls on streams so that it enforces this both for this change and when range queries are soon implemented.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505841932", "createdAt": "2020-10-15T20:55:45Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/engine/rewrite/PullQueryKeyUpdater.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.engine.rewrite;\n+\n+import com.google.common.collect.Iterables;\n+import io.confluent.ksql.engine.rewrite.ExpressionTreeRewriter.Context;\n+import io.confluent.ksql.execution.expression.tree.DoubleLiteral;\n+import io.confluent.ksql.execution.expression.tree.Expression;\n+import io.confluent.ksql.execution.expression.tree.InListExpression;\n+import io.confluent.ksql.execution.expression.tree.InPredicate;\n+import io.confluent.ksql.execution.expression.tree.IntegerLiteral;\n+import io.confluent.ksql.execution.expression.tree.LongLiteral;\n+import io.confluent.ksql.execution.expression.tree.StringLiteral;\n+import io.confluent.ksql.execution.expression.tree.VisitParentExpressionVisitor;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.util.KsqlException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+\n+/**\n+ * Takes a configured statement and rewrites it to update the keys requested.  This only works if\n+ * the query uses the IN keyword.\n+ * This is required because when fetching multiple keys using the IN keyword, a given host may be\n+ * the active for one partition and standby for another partition, and if you issue a query\n+ * requesting keys from both partitions while using ksql.query.pull.enable.standby.reads=true,\n+ * you'll fetch stale values unintentionally.\n+ */\n+public final class PullQueryKeyUpdater {\n+\n+  private PullQueryKeyUpdater() {}\n+\n+  /**\n+   * Returns a Statement with the IN expression updated to include only the given keys.\n+   * @param statement the original statement.\n+   * @param keys the keys to include\n+   * @return The updated statement\n+   */\n+  public static Statement update(\n+      final Query statement,\n+      final List<Struct> keys) {\n+    final BiFunction<Expression, Void, Expression> expressionRewriter =\n+        (e, v) -> ExpressionTreeRewriter.rewriteWith(\n+            new ExpressionRewriterPlugin(keys)::process, e, v);\n+    return (Statement) new StatementRewriter<>(expressionRewriter, (n, c) -> Optional.empty())\n+        .rewrite(statement, null);\n+  }\n+\n+  private static final class ExpressionRewriterPlugin extends\n+      VisitParentExpressionVisitor<Optional<Expression>, Context<Void>> {\n+\n+    private final List<Struct> keys;\n+\n+    ExpressionRewriterPlugin(final List<Struct> keys) {\n+      super(Optional.empty());\n+      this.keys = keys;\n+    }\n+\n+    @Override\n+    public Optional<Expression> visitInPredicate(final InPredicate node, final Context<Void> ctx) {\n+      final List<Expression> values = keys.stream().map(k -> {\n+        final Field field = Iterables.getOnlyElement(k.schema().fields());\n+        return convertToExpression(field.schema(), k.get(field));\n+      }).collect(Collectors.toList());\n+      final InListExpression inList\n+          = new InListExpression(node.getValueList().getLocation(), values);\n+      return Optional.of(new InPredicate(node.getLocation(), node.getValue(), inList));\n+    }\n+\n+    private Expression convertToExpression(final Schema schema, final Object value) {\n+      switch (schema.type()) {\n+        case STRING:\n+          return new StringLiteral((String) value);\n+        case INT8:\n+        case INT16:\n+        case INT32:\n+          return new IntegerLiteral((int) value);\n+        case INT64:\n+          return new LongLiteral((long) value);\n+        case FLOAT32:\n+        case FLOAT64:\n+          return new DoubleLiteral((double) value);\n+        default:\n+          throw new KsqlException(\"Unknown key type \" + schema.type());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA0NjE3MA=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MzYyNDM1OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzowNzowM1rOHhp2QA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMTowMDoyM1rOHiaYkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA1MDY4OA==", "bodyText": "is there any requirement that the pull query result is reproducible? it seems like we add rows to the result based on the order of the nodes that we request, but those nodes can host different partitions at different times. I think it's reasonable to not guarantee ordering, but food for thought (and probably should be documented)", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505050688", "createdAt": "2020-10-14T23:07:03Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,36 +234,73 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<Struct> keys = whereInfo.keysBound.stream()\n+          .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+          .collect(ImmutableList.toImmutableList());\n+      final List<KsqlNodeList> nodeLists = mat.locator().locate(\n+          keys,\n+          routingOptions,\n+          routingFilterFactory\n+      );\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      for (KsqlNodeList ksqlNodeList : nodeLists) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            ksqlNodeList.getKeys(),\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            ksqlNodeList.getNodes(),\n+            routingOptions\n+        )));\n \n-      final PullQueryResult result = handlePullQuery(\n-          statement,\n-          executionContext,\n-          serviceContext,\n-          pullQueryContext,\n-          routingOptions\n-      );\n+      }\n+      for (Future<PullQueryResult> future : futures) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0NTkwNw==", "bodyText": "Yes, you're right that there are no guarantees about ordering.  I'll document that in the code and consider documenting elsewhere as well.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505845907", "createdAt": "2020-10-15T21:00:23Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -224,36 +234,73 @@ public PullQueryResult execute(\n           .getMaterialization(queryId, contextStacker)\n           .orElseThrow(() -> notMaterializedException(getSourceName(analysis)));\n \n-      final Struct key = asKeyStruct(whereInfo.keyBound, query.getPhysicalSchema());\n+      final List<Optional<KsqlNode>> sourceNodes = new ArrayList<>();\n+      final List<List<?>> tableRows = new ArrayList<>();\n+      final List<LogicalSchema> schemas = new ArrayList<>();\n+      final List<Future<PullQueryResult>> futures = new ArrayList<>();\n+      final List<Struct> keys = whereInfo.keysBound.stream()\n+          .map(keyBound -> asKeyStruct(keyBound, query.getPhysicalSchema()))\n+          .collect(ImmutableList.toImmutableList());\n+      final List<KsqlNodeList> nodeLists = mat.locator().locate(\n+          keys,\n+          routingOptions,\n+          routingFilterFactory\n+      );\n \n-      final PullQueryContext pullQueryContext = new PullQueryContext(\n-          key,\n-          mat,\n-          analysis,\n-          whereInfo,\n-          queryId,\n-          contextStacker,\n-          pullQueryMetrics);\n+      for (KsqlNodeList ksqlNodeList : nodeLists) {\n+        final PullQueryContext pullQueryContext = new PullQueryContext(\n+            ksqlNodeList.getKeys(),\n+            mat,\n+            analysis,\n+            whereInfo,\n+            queryId,\n+            contextStacker,\n+            pullQueryMetrics);\n+\n+        futures.add(executorService.submit(() -> handlePullQuery(\n+            statement,\n+            executionContext,\n+            serviceContext,\n+            pullQueryContext,\n+            ksqlNodeList.getNodes(),\n+            routingOptions\n+        )));\n \n-      final PullQueryResult result = handlePullQuery(\n-          statement,\n-          executionContext,\n-          serviceContext,\n-          pullQueryContext,\n-          routingOptions\n-      );\n+      }\n+      for (Future<PullQueryResult> future : futures) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA1MDY4OA=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MzY2MjM3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzoxNjoyOFrOHhqP3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMToxMjowMVrOHia-ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA1NzI0Ng==", "bodyText": "nit: probably makes sense to make this Optional<List<KsqlNode>> instead of List<Optional<KsqlNode>> to prevent creating a list of empty optionals in the non-debug case. And then you can also use Collections#nCopies to make it a little more readable", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505057246", "createdAt": "2020-10-14T23:16:28Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -289,11 +329,14 @@ private PullQueryResult handlePullQuery(\n     // increasing order of lag.\n     for (KsqlNode node : filteredAndOrderedNodes) {\n       try {\n+        final TableRows rows\n+            = routeQuery(node, statement, executionContext, serviceContext, pullQueryContext);\n         final Optional<KsqlNode> debugNode = Optional.ofNullable(\n             routingOptions.isDebugRequest() ? node : null);\n-        return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n+        final List<Optional<KsqlNode>> debugNodes = rows.getRows().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg1NTU4OQ==", "bodyText": "I had been thinking that maybe this was necessary because different machines can have different configs, but I now remember that this is a request config, so it should always work.  Ok, made it Optional<List<KsqlNode>>", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505855589", "createdAt": "2020-10-15T21:12:01Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -289,11 +329,14 @@ private PullQueryResult handlePullQuery(\n     // increasing order of lag.\n     for (KsqlNode node : filteredAndOrderedNodes) {\n       try {\n+        final TableRows rows\n+            = routeQuery(node, statement, executionContext, serviceContext, pullQueryContext);\n         final Optional<KsqlNode> debugNode = Optional.ofNullable(\n             routingOptions.isDebugRequest() ? node : null);\n-        return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n+        final List<Optional<KsqlNode>> debugNodes = rows.getRows().stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA1NzI0Ng=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MzY5MDc5OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzoyMzo1M1rOHhqjdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMToyNzo0NVrOHibwjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA2MjI2MA==", "bodyText": "I'm worried this approach doesn't scale well for things other than IN queries (not to mention that it feels hacky). Instead, it probably makes sense to have a way to specify that a pull query (internally routed only) should only read from certain partitions. Otherwise, how would we handle things like range queries? I can't \"rewrite\" the range query and avoid the risk of reading standby data.\nGenerally, I think it might make sense to think about communicating pull queries internally using something other than just the SQL statement.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505062260", "createdAt": "2020-10-14T23:23:53Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -389,8 +438,14 @@ private static TableRows queryRowsLocally(\n   private static TableRows forwardTo(\n       final KsqlNode owner,\n       final ConfiguredStatement<Query> statement,\n-      final ServiceContext serviceContext\n+      final ServiceContext serviceContext,\n+      final PullQueryContext pullQueryContext\n   ) {\n+    // Rewrite the expression to only query for the particular keys we care about for this node.\n+    // Otherwise, we'll risk reading standby data for other partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MzE1OQ==", "bodyText": "Yeah, that's fair.  We actually were discussing range queries as well and it's probably easier to use a unified approach.  I think that it should be sufficient to specify partition when reading from the table (to ensure you're only reading from the actives and standbys you intend) and also on the second hop, avoid reading keys that are not hosted on this machine or that don't agree with those specified partitions.  That same approach will work on range queries as well.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505093159", "createdAt": "2020-10-15T00:19:08Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -389,8 +438,14 @@ private static TableRows queryRowsLocally(\n   private static TableRows forwardTo(\n       final KsqlNode owner,\n       final ConfiguredStatement<Query> statement,\n-      final ServiceContext serviceContext\n+      final ServiceContext serviceContext,\n+      final PullQueryContext pullQueryContext\n   ) {\n+    // Rewrite the expression to only query for the particular keys we care about for this node.\n+    // Otherwise, we'll risk reading standby data for other partitions.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA2MjI2MA=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2ODQyOQ==", "bodyText": "Ok, implemented that other approach where partition is specified.  No rewriting necessary.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505868429", "createdAt": "2020-10-15T21:27:45Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -389,8 +438,14 @@ private static TableRows queryRowsLocally(\n   private static TableRows forwardTo(\n       final KsqlNode owner,\n       final ConfiguredStatement<Query> statement,\n-      final ServiceContext serviceContext\n+      final ServiceContext serviceContext,\n+      final PullQueryContext pullQueryContext\n   ) {\n+    // Rewrite the expression to only query for the particular keys we care about for this node.\n+    // Otherwise, we'll risk reading standby data for other partitions.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA2MjI2MA=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MzcwNjM3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzoyNzo1OFrOHhqt_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMDo1OToyNVrOHiaVIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA2NDk1OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    throw new KsqlException(\"Ony comparison to literals is currently supported: \"\n          \n          \n            \n                    throw new KsqlException(\"Only comparison to literals is currently supported: \"", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505064959", "createdAt": "2020-10-14T23:27:58Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -535,44 +590,65 @@ private static WhereInfo extractWhereInfo(\n     final Expression where = analysis.getWhereExpression()\n         .orElseThrow(() -> invalidWhereClauseException(\"Missing WHERE clause\", windowed));\n \n-    final Map<ComparisonTarget, List<ComparisonExpression>> comparisons =\n-        extractComparisons(where, query);\n-\n-    final List<ComparisonExpression> keyComparison = comparisons.get(ComparisonTarget.KEYCOL);\n-    if (keyComparison == null) {\n+    final KeyAndWindowBounds keyAndWindowBounds = extractComparisons(where, query);\n+    final List<ComparisonExpression> keyComparison = keyAndWindowBounds.getKeyColExpression();\n+    final List<InPredicate> inPredicate = keyAndWindowBounds.getInPredicate();\n+    if (keyComparison.size() == 0 && inPredicate.size() == 0) {\n       throw invalidWhereClauseException(\"WHERE clause missing key column\", windowed);\n+    } else if ((keyComparison.size() + inPredicate.size()) > 1) {\n+      throw invalidWhereClauseException(\"Multiple bounds on key column\", windowed);\n     }\n \n-    final Object key = extractKeyWhereClause(\n-        keyComparison,\n-        windowed,\n-        query.getLogicalSchema()\n-    );\n+    final List<Object> keys;\n+    if (keyComparison.size() > 0) {\n+      keys = ImmutableList.of(\n+          extractKeyWhereClause(keyComparison, windowed, query.getLogicalSchema()));\n+    } else {\n+      keys = extractKeysFromInPredicate(inPredicate, windowed, query.getLogicalSchema());\n+    }\n \n     if (!windowed) {\n-      if (comparisons.size() > 1) {\n+      if (keyAndWindowBounds.getWindowStartExpression().size() > 0\n+          || keyAndWindowBounds.getWindowEndExpression().size() > 0) {\n         throw invalidWhereClauseException(\"Unsupported WHERE clause\", false);\n       }\n \n-      return new WhereInfo(key, Optional.empty());\n+      return new WhereInfo(keys, Optional.empty());\n     }\n \n     final WindowBounds windowBounds =\n-        extractWhereClauseWindowBounds(comparisons);\n+        extractWhereClauseWindowBounds(keyAndWindowBounds);\n \n-    return new WhereInfo(key, Optional.of(windowBounds));\n+    return new WhereInfo(keys, Optional.of(windowBounds));\n   }\n \n-  private static Object extractKeyWhereClause(\n-      final List<ComparisonExpression> comparisons,\n+  private static List<Object> extractKeysFromInPredicate(\n+      final List<InPredicate> inPredicates,\n       final boolean windowed,\n       final LogicalSchema schema\n   ) {\n-    if (comparisons.size() != 1) {\n-      throw invalidWhereClauseException(\"Multiple bounds on key column\", windowed);\n+    final InPredicate inPredicate = Iterables.getLast(inPredicates);\n+    final List<Object> result = new ArrayList<>();\n+    for (Expression expression : inPredicate.getValueList().getValues()) {\n+      if (!(expression instanceof Literal)) {\n+        throw new KsqlException(\"Ony comparison to literals is currently supported: \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 383}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0NTAyNw==", "bodyText": "Done", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505845027", "createdAt": "2020-10-15T20:59:25Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -535,44 +590,65 @@ private static WhereInfo extractWhereInfo(\n     final Expression where = analysis.getWhereExpression()\n         .orElseThrow(() -> invalidWhereClauseException(\"Missing WHERE clause\", windowed));\n \n-    final Map<ComparisonTarget, List<ComparisonExpression>> comparisons =\n-        extractComparisons(where, query);\n-\n-    final List<ComparisonExpression> keyComparison = comparisons.get(ComparisonTarget.KEYCOL);\n-    if (keyComparison == null) {\n+    final KeyAndWindowBounds keyAndWindowBounds = extractComparisons(where, query);\n+    final List<ComparisonExpression> keyComparison = keyAndWindowBounds.getKeyColExpression();\n+    final List<InPredicate> inPredicate = keyAndWindowBounds.getInPredicate();\n+    if (keyComparison.size() == 0 && inPredicate.size() == 0) {\n       throw invalidWhereClauseException(\"WHERE clause missing key column\", windowed);\n+    } else if ((keyComparison.size() + inPredicate.size()) > 1) {\n+      throw invalidWhereClauseException(\"Multiple bounds on key column\", windowed);\n     }\n \n-    final Object key = extractKeyWhereClause(\n-        keyComparison,\n-        windowed,\n-        query.getLogicalSchema()\n-    );\n+    final List<Object> keys;\n+    if (keyComparison.size() > 0) {\n+      keys = ImmutableList.of(\n+          extractKeyWhereClause(keyComparison, windowed, query.getLogicalSchema()));\n+    } else {\n+      keys = extractKeysFromInPredicate(inPredicate, windowed, query.getLogicalSchema());\n+    }\n \n     if (!windowed) {\n-      if (comparisons.size() > 1) {\n+      if (keyAndWindowBounds.getWindowStartExpression().size() > 0\n+          || keyAndWindowBounds.getWindowEndExpression().size() > 0) {\n         throw invalidWhereClauseException(\"Unsupported WHERE clause\", false);\n       }\n \n-      return new WhereInfo(key, Optional.empty());\n+      return new WhereInfo(keys, Optional.empty());\n     }\n \n     final WindowBounds windowBounds =\n-        extractWhereClauseWindowBounds(comparisons);\n+        extractWhereClauseWindowBounds(keyAndWindowBounds);\n \n-    return new WhereInfo(key, Optional.of(windowBounds));\n+    return new WhereInfo(keys, Optional.of(windowBounds));\n   }\n \n-  private static Object extractKeyWhereClause(\n-      final List<ComparisonExpression> comparisons,\n+  private static List<Object> extractKeysFromInPredicate(\n+      final List<InPredicate> inPredicates,\n       final boolean windowed,\n       final LogicalSchema schema\n   ) {\n-    if (comparisons.size() != 1) {\n-      throw invalidWhereClauseException(\"Multiple bounds on key column\", windowed);\n+    final InPredicate inPredicate = Iterables.getLast(inPredicates);\n+    final List<Object> result = new ArrayList<>();\n+    for (Expression expression : inPredicate.getValueList().getValues()) {\n+      if (!(expression instanceof Literal)) {\n+        throw new KsqlException(\"Ony comparison to literals is currently supported: \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA2NDk1OQ=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 383}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2Mzc3Mjc2OnYy", "diffSide": "RIGHT", "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQyMzo0NTowMlrOHhrbZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMDo1ODoxN1rOHiaRdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA3NjU4Mg==", "bodyText": "nit: computeIfAbsent returns the result of the compute, so you could just\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  groups.computeIfAbsent(filteredHosts, nl -> new ArrayList<>());\n          \n          \n            \n                  groups.get(filteredHosts).add(key);\n          \n          \n            \n                  groups.computeIfAbsent(filteredHosts, nl -> new ArrayList<>()).add(key);", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505076582", "createdAt": "2020-10-14T23:45:02Z", "author": {"login": "agavra"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,54 +74,61 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlNodeList> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    final Map<ImmutableList<KsqlNode>, List<Struct>> groups = new HashMap<>();\n+    for (Struct key : keys) {\n+      final KeyQueryMetadata metadata = kafkaStreams\n+          .queryMetadataForKey(stateStoreName, key, keySerializer);\n+\n+      // Fail fast if Streams not ready. Let client handle it\n+      if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n+        LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n+            stateStoreName, key);\n+        throw new MaterializationException(String.format(\n+            \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n+      }\n \n-    LOG.debug(\"Handling pull query for key {} in partition {} of state store {}.\",\n-              key, metadata.partition(), stateStoreName);\n-    \n-    final HostInfo activeHost = metadata.activeHost();\n-    final Set<HostInfo> standByHosts = metadata.standbyHosts();\n-\n-    // If the lookup is for a forwarded request, only filter localhost\n-    List<KsqlHostInfo> allHosts = null;\n-    if (routingOptions.skipForwardRequest()) {\n-      LOG.debug(\"Before filtering: Local host {} \", localHost);\n-      allHosts = ImmutableList.of(new KsqlHostInfo(localHost.getHost(), localHost.getPort()));\n-    } else {\n-      LOG.debug(\"Before filtering: Active host {} , standby hosts {}\", activeHost, standByHosts);\n-      allHosts = Stream.concat(Stream.of(activeHost), standByHosts.stream())\n-          .map(this::asKsqlHost)\n-          .collect(Collectors.toList());\n+      LOG.debug(\"Handling pull query for key {} in partition {} of state store {}.\",\n+          key, metadata.partition(), stateStoreName);\n+      final HostInfo activeHost = metadata.activeHost();\n+      final Set<HostInfo> standByHosts = metadata.standbyHosts();\n+\n+      // If the lookup is for a forwarded request, only filter localhost\n+      List<KsqlHostInfo> allHosts = null;\n+      if (routingOptions.skipForwardRequest()) {\n+        LOG.debug(\"Before filtering: Local host {} \", localHost);\n+        allHosts = ImmutableList.of(new KsqlHostInfo(localHost.getHost(), localHost.getPort()));\n+      } else {\n+        LOG.debug(\"Before filtering: Active host {} , standby hosts {}\", activeHost, standByHosts);\n+        allHosts = Stream.concat(Stream.of(activeHost), standByHosts.stream())\n+            .map(this::asKsqlHost)\n+            .collect(Collectors.toList());\n+      }\n+      final RoutingFilter routingFilter = routingFilterFactory.createRoutingFilter(routingOptions,\n+          allHosts, activeHost, applicationId, stateStoreName, metadata.partition());\n+\n+      // Filter out hosts based on active, liveness and max lag filters.\n+      // The list is ordered by routing preference: active node is first, then standby nodes.\n+      // If heartbeat is not enabled, all hosts are considered alive.\n+      // If the request is forwarded internally from another ksql server, only the max lag filter\n+      // is applied.\n+      final ImmutableList<KsqlNode> filteredHosts = allHosts.stream()\n+          .filter(routingFilter::filter)\n+          .map(this::asNode)\n+          .collect(ImmutableList.toImmutableList());\n+\n+      LOG.debug(\"Filtered and ordered hosts: {}\", filteredHosts);\n+\n+      groups.computeIfAbsent(filteredHosts, nl -> new ArrayList<>());\n+      groups.get(filteredHosts).add(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0NDA4NQ==", "bodyText": "This code hanged a lot, but I used this trick elsewhere.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r505844085", "createdAt": "2020-10-15T20:58:17Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,54 +74,61 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlNodeList> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    final Map<ImmutableList<KsqlNode>, List<Struct>> groups = new HashMap<>();\n+    for (Struct key : keys) {\n+      final KeyQueryMetadata metadata = kafkaStreams\n+          .queryMetadataForKey(stateStoreName, key, keySerializer);\n+\n+      // Fail fast if Streams not ready. Let client handle it\n+      if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n+        LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n+            stateStoreName, key);\n+        throw new MaterializationException(String.format(\n+            \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n+      }\n \n-    LOG.debug(\"Handling pull query for key {} in partition {} of state store {}.\",\n-              key, metadata.partition(), stateStoreName);\n-    \n-    final HostInfo activeHost = metadata.activeHost();\n-    final Set<HostInfo> standByHosts = metadata.standbyHosts();\n-\n-    // If the lookup is for a forwarded request, only filter localhost\n-    List<KsqlHostInfo> allHosts = null;\n-    if (routingOptions.skipForwardRequest()) {\n-      LOG.debug(\"Before filtering: Local host {} \", localHost);\n-      allHosts = ImmutableList.of(new KsqlHostInfo(localHost.getHost(), localHost.getPort()));\n-    } else {\n-      LOG.debug(\"Before filtering: Active host {} , standby hosts {}\", activeHost, standByHosts);\n-      allHosts = Stream.concat(Stream.of(activeHost), standByHosts.stream())\n-          .map(this::asKsqlHost)\n-          .collect(Collectors.toList());\n+      LOG.debug(\"Handling pull query for key {} in partition {} of state store {}.\",\n+          key, metadata.partition(), stateStoreName);\n+      final HostInfo activeHost = metadata.activeHost();\n+      final Set<HostInfo> standByHosts = metadata.standbyHosts();\n+\n+      // If the lookup is for a forwarded request, only filter localhost\n+      List<KsqlHostInfo> allHosts = null;\n+      if (routingOptions.skipForwardRequest()) {\n+        LOG.debug(\"Before filtering: Local host {} \", localHost);\n+        allHosts = ImmutableList.of(new KsqlHostInfo(localHost.getHost(), localHost.getPort()));\n+      } else {\n+        LOG.debug(\"Before filtering: Active host {} , standby hosts {}\", activeHost, standByHosts);\n+        allHosts = Stream.concat(Stream.of(activeHost), standByHosts.stream())\n+            .map(this::asKsqlHost)\n+            .collect(Collectors.toList());\n+      }\n+      final RoutingFilter routingFilter = routingFilterFactory.createRoutingFilter(routingOptions,\n+          allHosts, activeHost, applicationId, stateStoreName, metadata.partition());\n+\n+      // Filter out hosts based on active, liveness and max lag filters.\n+      // The list is ordered by routing preference: active node is first, then standby nodes.\n+      // If heartbeat is not enabled, all hosts are considered alive.\n+      // If the request is forwarded internally from another ksql server, only the max lag filter\n+      // is applied.\n+      final ImmutableList<KsqlNode> filteredHosts = allHosts.stream()\n+          .filter(routingFilter::filter)\n+          .map(this::asNode)\n+          .collect(ImmutableList.toImmutableList());\n+\n+      LOG.debug(\"Filtered and ordered hosts: {}\", filteredHosts);\n+\n+      groups.computeIfAbsent(filteredHosts, nl -> new ArrayList<>());\n+      groups.get(filteredHosts).add(key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA3NjU4Mg=="}, "originalCommit": {"oid": "beb105b8f097b92482913ba27b540d137ed01a33"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4Njc1MTk0OnYy", "diffSide": "RIGHT", "path": "ksqldb-functional-tests/src/test/resources/rest-query-validation-tests/pull-queries-against-materialized-aggregates.json", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxODowNDowMlrOHlKj5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwMTowMjowMFrOHlWgWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODczMjM5MQ==", "bodyText": "It's normal for the IN operator to support some level of coercion of types. I see you've handled INT -> BIGINT. But, strangely, it's also normal to support STRING -> .  Of course, we don't have to support this. But those used to sql may expect it...", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508732391", "createdAt": "2020-10-20T18:04:02Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-functional-tests/src/test/resources/rest-query-validation-tests/pull-queries-against-materialized-aggregates.json", "diffHunk": "@@ -1540,6 +1540,404 @@\n           {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}}\n         ]}\n       ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - STRING\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID STRING KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('10', '8');\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('missing', 'missing again');\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": \"11\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": \"10\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": \"9\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": \"8\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": \"12\", \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` STRING KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[\"10\", 1]}},\n+          {\"row\":{\"columns\":[\"8\", 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` STRING KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - INT\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID INT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10, 1]}},\n+          {\"row\":{\"columns\":[8, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - BIGINT\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID BIGINT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` BIGINT KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10, 1]}},\n+          {\"row\":{\"columns\":[8, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` BIGINT KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - DOUBLE\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID DOUBLE KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8.0);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (1.0E1, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12.0, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10.0, 1]}},\n+          {\"row\":{\"columns\":[8.0, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10.0, 1]}},\n+          {\"row\":{\"columns\":[8.0, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN lookup on wrong type\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID INT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('10', 8);\"\n+      ],\n+      \"expectedError\": {\n+        \"type\": \"io.confluent.ksql.rest.entity.KsqlStatementErrorMessage\",\n+        \"message\": \"'10' can not be converted to the type of the key column: ID INTEGER KEY\",\n+        \"status\": 400\n+      }\n+    },", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyODA5MQ==", "bodyText": "Interesting.  I find it a bit funny that a IN (v1) isn't just syntactic sugar for a = v1.  I may add that as a followup to try to keep from adding much more to this PR.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508928091", "createdAt": "2020-10-21T01:02:00Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-functional-tests/src/test/resources/rest-query-validation-tests/pull-queries-against-materialized-aggregates.json", "diffHunk": "@@ -1540,6 +1540,404 @@\n           {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}}\n         ]}\n       ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - STRING\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID STRING KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('10', '8');\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('missing', 'missing again');\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": \"11\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": \"10\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": \"9\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": \"8\", \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": \"12\", \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` STRING KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[\"10\", 1]}},\n+          {\"row\":{\"columns\":[\"8\", 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` STRING KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - INT\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID INT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10, 1]}},\n+          {\"row\":{\"columns\":[8, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` INTEGER KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - BIGINT\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID BIGINT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` BIGINT KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10, 1]}},\n+          {\"row\":{\"columns\":[8, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` BIGINT KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN key lookup - DOUBLE\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID DOUBLE KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10, 8.0);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (1.0E1, 8);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (123369, 123370);\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"timestamp\": 12345, \"key\": 11.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12365, \"key\": 10.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12366, \"key\": 9.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12367, \"key\": 8.0, \"value\": {}},\n+        {\"topic\": \"test_topic\", \"timestamp\": 12368, \"key\": 12.0, \"value\": {}}\n+      ],\n+      \"responses\": [\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"admin\": {\"@type\": \"currentStatus\"}},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10.0, 1]}},\n+          {\"row\":{\"columns\":[8.0, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}},\n+          {\"row\":{\"columns\":[10.0, 1]}},\n+          {\"row\":{\"columns\":[8.0, 1]}}\n+        ]},\n+        {\"query\": [\n+          {\"header\":{\"schema\":\"`ID` DOUBLE KEY, `COUNT` BIGINT\"}}\n+        ]}\n+      ]\n+    },\n+    {\n+      \"name\": \"non-windowed IN lookup on wrong type\",\n+      \"statements\": [\n+        \"CREATE STREAM INPUT (ID INT KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n+        \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT GROUP BY ID;\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN ('10', 8);\"\n+      ],\n+      \"expectedError\": {\n+        \"type\": \"io.confluent.ksql.rest.entity.KsqlStatementErrorMessage\",\n+        \"message\": \"'10' can not be converted to the type of the key column: ID INTEGER KEY\",\n+        \"status\": 400\n+      }\n+    },", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODczMjM5MQ=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjgxNjQ4OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxODoyMDozMlrOHlLL8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDoxMjozMlrOHlPObQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc0MjY0Mw==", "bodyText": "We really need to add some comments in the code. I am having a hard time following and trying to understand what it does, and I already know what it is supposed to do :-P", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508742643", "createdAt": "2020-10-20T18:20:32Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwODgxMw==", "bodyText": "I agree.  I added a lot of comments for the newer code I wrote.  Tell me if anything is unclear.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508808813", "createdAt": "2020-10-20T20:12:32Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc0MjY0Mw=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjgyMDEzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxODoyMTozMlrOHlLOVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDoyNTo1NFrOHlPsLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc0MzI1NQ==", "bodyText": "Consider adding a more descriptive error message so that we know at which point in the code the query failed and why.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508743255", "createdAt": "2020-10-20T18:21:32Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();\n+    final List<List<?>> tableRows = new ArrayList<>();\n+    final List<LogicalSchema> schemas = new ArrayList<>();\n+    List<KsqlLocation> remainingLocations = ImmutableList.copyOf(locations);\n+    for (int round = 0; ; round++) {\n+      final Map<KsqlNode, List<KsqlLocation>> groupedByHost\n+          = groupByHost(statement, remainingLocations, round);\n+\n+      final Map<KsqlNode, Future<PullQueryResult>> futures = new LinkedHashMap<>();\n+      for (Map.Entry<KsqlNode, List<KsqlLocation>> entry : groupedByHost.entrySet()) {\n+        final KsqlNode node = entry.getKey();\n+        final PullQueryContext pullQueryContext = contextFactory.apply(entry.getValue());\n+\n+        futures.put(node, executorService.submit(() ->  {\n+          final TableRows rows = routeQuery.routeQuery(\n+              node, statement, executionContext, serviceContext, pullQueryContext);\n+          final Optional<List<KsqlNode>> debugNodes = Optional.ofNullable(\n+              routingOptions.isDebugRequest()\n+                  ? Collections.nCopies(rows.getRows().size(), node) : null);\n+          return new PullQueryResult(rows, debugNodes);\n+        }));\n+      }\n+\n+      final ImmutableList.Builder<KsqlLocation> nextRoundRemaining = ImmutableList.builder();\n+      for (Map.Entry<KsqlNode, Future<PullQueryResult>>  entry : futures.entrySet()) {\n+        final Future<PullQueryResult> future = entry.getValue();\n+        final KsqlNode node = entry.getKey();\n+        try {\n+          final PullQueryResult result = future.get();\n+          result.getSourceNodes().ifPresent(sourceNodes::addAll);\n+          schemas.add(result.getTableRows().getSchema());\n+          tableRows.addAll(result.getTableRows().getRows());\n+        } catch (ExecutionException e) {\n+          LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n+              statement.getStatementText(), node, System.currentTimeMillis(), e.getCause());\n+          nextRoundRemaining.addAll(groupedByHost.get(node));\n+        }\n+      }\n+      remainingLocations = nextRoundRemaining.build();\n+\n+      if (remainingLocations.size() == 0) {\n+        validateSchemas(schemas);\n         return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n-      } catch (Exception t) {\n-        LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n-                  statement.getStatementText(), node, System.currentTimeMillis(), t);\n+            new TableRows(statement.getStatementText(), queryId, Iterables.getLast(schemas),\n+                tableRows),\n+            sourceNodes.isEmpty() ? Optional.empty() : Optional.of(sourceNodes));\n       }\n     }\n-    throw new MaterializationException(String.format(\n-        \"Unable to execute pull query: %s\", statement.getStatementText()));\n+  }\n+\n+  private static Map<KsqlNode, List<KsqlLocation>> groupByHost(\n+      final ConfiguredStatement<Query> statement,\n+      final List<KsqlLocation> locations,\n+      final int round) {\n+    final Map<KsqlNode, List<KsqlLocation>> groupedByHost = new LinkedHashMap<>();\n+    for (KsqlLocation location : locations) {\n+      // If one of the partitions required is out of nodes, then we cannot continue.\n+      if (round >= location.getNodes().size()) {\n+        throw new MaterializationException(String.format(\n+            \"Unable to execute pull query: %s\", statement.getStatementText()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgxNjQyOA==", "bodyText": "Ok, changed it to \"Unable to execute pull query: %s. Exhausted standby hosts to try.\"", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508816428", "createdAt": "2020-10-20T20:25:54Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();\n+    final List<List<?>> tableRows = new ArrayList<>();\n+    final List<LogicalSchema> schemas = new ArrayList<>();\n+    List<KsqlLocation> remainingLocations = ImmutableList.copyOf(locations);\n+    for (int round = 0; ; round++) {\n+      final Map<KsqlNode, List<KsqlLocation>> groupedByHost\n+          = groupByHost(statement, remainingLocations, round);\n+\n+      final Map<KsqlNode, Future<PullQueryResult>> futures = new LinkedHashMap<>();\n+      for (Map.Entry<KsqlNode, List<KsqlLocation>> entry : groupedByHost.entrySet()) {\n+        final KsqlNode node = entry.getKey();\n+        final PullQueryContext pullQueryContext = contextFactory.apply(entry.getValue());\n+\n+        futures.put(node, executorService.submit(() ->  {\n+          final TableRows rows = routeQuery.routeQuery(\n+              node, statement, executionContext, serviceContext, pullQueryContext);\n+          final Optional<List<KsqlNode>> debugNodes = Optional.ofNullable(\n+              routingOptions.isDebugRequest()\n+                  ? Collections.nCopies(rows.getRows().size(), node) : null);\n+          return new PullQueryResult(rows, debugNodes);\n+        }));\n+      }\n+\n+      final ImmutableList.Builder<KsqlLocation> nextRoundRemaining = ImmutableList.builder();\n+      for (Map.Entry<KsqlNode, Future<PullQueryResult>>  entry : futures.entrySet()) {\n+        final Future<PullQueryResult> future = entry.getValue();\n+        final KsqlNode node = entry.getKey();\n+        try {\n+          final PullQueryResult result = future.get();\n+          result.getSourceNodes().ifPresent(sourceNodes::addAll);\n+          schemas.add(result.getTableRows().getSchema());\n+          tableRows.addAll(result.getTableRows().getRows());\n+        } catch (ExecutionException e) {\n+          LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n+              statement.getStatementText(), node, System.currentTimeMillis(), e.getCause());\n+          nextRoundRemaining.addAll(groupedByHost.get(node));\n+        }\n+      }\n+      remainingLocations = nextRoundRemaining.build();\n+\n+      if (remainingLocations.size() == 0) {\n+        validateSchemas(schemas);\n         return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n-      } catch (Exception t) {\n-        LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n-                  statement.getStatementText(), node, System.currentTimeMillis(), t);\n+            new TableRows(statement.getStatementText(), queryId, Iterables.getLast(schemas),\n+                tableRows),\n+            sourceNodes.isEmpty() ? Optional.empty() : Optional.of(sourceNodes));\n       }\n     }\n-    throw new MaterializationException(String.format(\n-        \"Unable to execute pull query: %s\", statement.getStatementText()));\n+  }\n+\n+  private static Map<KsqlNode, List<KsqlLocation>> groupByHost(\n+      final ConfiguredStatement<Query> statement,\n+      final List<KsqlLocation> locations,\n+      final int round) {\n+    final Map<KsqlNode, List<KsqlLocation>> groupedByHost = new LinkedHashMap<>();\n+    for (KsqlLocation location : locations) {\n+      // If one of the partitions required is out of nodes, then we cannot continue.\n+      if (round >= location.getNodes().size()) {\n+        throw new MaterializationException(String.format(\n+            \"Unable to execute pull query: %s\", statement.getStatementText()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc0MzI1NQ=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 249}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NzAwNjg0OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxOTowNzoxMlrOHlNCJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDoyMzowNFrOHlPkUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc3MjkwMA==", "bodyText": "What this method does, is group all locations per the same host, which if round=0, will be the active. So, all locations (all keys) that have the same host as active will we grouped together. Then, in the second round, for any keys that the active failed, we will get the standby that is second in ordering.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508772900", "createdAt": "2020-10-20T19:07:12Z", "author": {"login": "vpapavas"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();\n+    final List<List<?>> tableRows = new ArrayList<>();\n+    final List<LogicalSchema> schemas = new ArrayList<>();\n+    List<KsqlLocation> remainingLocations = ImmutableList.copyOf(locations);\n+    for (int round = 0; ; round++) {\n+      final Map<KsqlNode, List<KsqlLocation>> groupedByHost\n+          = groupByHost(statement, remainingLocations, round);\n+\n+      final Map<KsqlNode, Future<PullQueryResult>> futures = new LinkedHashMap<>();\n+      for (Map.Entry<KsqlNode, List<KsqlLocation>> entry : groupedByHost.entrySet()) {\n+        final KsqlNode node = entry.getKey();\n+        final PullQueryContext pullQueryContext = contextFactory.apply(entry.getValue());\n+\n+        futures.put(node, executorService.submit(() ->  {\n+          final TableRows rows = routeQuery.routeQuery(\n+              node, statement, executionContext, serviceContext, pullQueryContext);\n+          final Optional<List<KsqlNode>> debugNodes = Optional.ofNullable(\n+              routingOptions.isDebugRequest()\n+                  ? Collections.nCopies(rows.getRows().size(), node) : null);\n+          return new PullQueryResult(rows, debugNodes);\n+        }));\n+      }\n+\n+      final ImmutableList.Builder<KsqlLocation> nextRoundRemaining = ImmutableList.builder();\n+      for (Map.Entry<KsqlNode, Future<PullQueryResult>>  entry : futures.entrySet()) {\n+        final Future<PullQueryResult> future = entry.getValue();\n+        final KsqlNode node = entry.getKey();\n+        try {\n+          final PullQueryResult result = future.get();\n+          result.getSourceNodes().ifPresent(sourceNodes::addAll);\n+          schemas.add(result.getTableRows().getSchema());\n+          tableRows.addAll(result.getTableRows().getRows());\n+        } catch (ExecutionException e) {\n+          LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n+              statement.getStatementText(), node, System.currentTimeMillis(), e.getCause());\n+          nextRoundRemaining.addAll(groupedByHost.get(node));\n+        }\n+      }\n+      remainingLocations = nextRoundRemaining.build();\n+\n+      if (remainingLocations.size() == 0) {\n+        validateSchemas(schemas);\n         return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n-      } catch (Exception t) {\n-        LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n-                  statement.getStatementText(), node, System.currentTimeMillis(), t);\n+            new TableRows(statement.getStatementText(), queryId, Iterables.getLast(schemas),\n+                tableRows),\n+            sourceNodes.isEmpty() ? Optional.empty() : Optional.of(sourceNodes));\n       }\n     }\n-    throw new MaterializationException(String.format(\n-        \"Unable to execute pull query: %s\", statement.getStatementText()));\n+  }\n+\n+  private static Map<KsqlNode, List<KsqlLocation>> groupByHost(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgxNDQxNg==", "bodyText": "That's correct. I added this example to make it clearer:\n    // For example, locations might be:\n    // [ Partition 0 <Host 1, Host 2>,\n    //   Partition 1 <Host 2, Host 1>,\n    //   Partition 2 <Host 1, Host 2> ]\n    // In Round 0, fetch from Host 1: [Partition 0, Partition 2], from Host 2: [Partition 1]\n    // If everything succeeds, we're done.  If Host 1 failed, then we'd have a Round 1:\n    // In Round 1, fetch from Host 2: [Partition 0, Partition 2].", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508814416", "createdAt": "2020-10-20T20:23:04Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -265,45 +294,104 @@ void checkRateLimit() {\n     }\n   }\n \n-  private PullQueryResult handlePullQuery(\n+  @VisibleForTesting\n+  interface RouteQuery {\n+    TableRows routeQuery(\n+        KsqlNode node,\n+        ConfiguredStatement<Query> statement,\n+        KsqlExecutionContext executionContext,\n+        ServiceContext serviceContext,\n+        PullQueryContext pullQueryContext\n+    );\n+  }\n+\n+  @VisibleForTesting\n+  static PullQueryResult handlePullQuery(\n       final ConfiguredStatement<Query> statement,\n       final KsqlExecutionContext executionContext,\n       final ServiceContext serviceContext,\n-      final PullQueryContext pullQueryContext,\n-      final RoutingOptions routingOptions\n-  ) {\n-    // Get active and standby nodes for this key\n-    final Locator locator = pullQueryContext.mat.locator();\n-    final List<KsqlNode> filteredAndOrderedNodes = locator.locate(\n-        pullQueryContext.key,\n-        routingOptions,\n-        routingFilterFactory\n-    );\n-\n-    if (filteredAndOrderedNodes.isEmpty()) {\n+      final RoutingOptions routingOptions,\n+      final Function<List<KsqlLocation>, PullQueryContext> contextFactory,\n+      final QueryId queryId,\n+      final List<KsqlLocation> locations,\n+      final ExecutorService executorService,\n+      final RouteQuery routeQuery\n+  ) throws InterruptedException {\n+    final boolean anyPartitionsEmpty = locations.stream()\n+        .anyMatch(location -> location.getNodes().isEmpty());\n+    if (anyPartitionsEmpty) {\n       LOG.debug(\"Unable to execute pull query: {}. All nodes are dead or exceed max allowed lag.\",\n-                statement.getStatementText());\n+          statement.getStatementText());\n       throw new MaterializationException(String.format(\n           \"Unable to execute pull query %s. All nodes are dead or exceed max allowed lag.\",\n           statement.getStatementText()));\n     }\n \n-    // Nodes are ordered by preference: active is first if alive then standby nodes in\n-    // increasing order of lag.\n-    for (KsqlNode node : filteredAndOrderedNodes) {\n-      try {\n-        final Optional<KsqlNode> debugNode = Optional.ofNullable(\n-            routingOptions.isDebugRequest() ? node : null);\n+    final List<KsqlNode> sourceNodes = new ArrayList<>();\n+    final List<List<?>> tableRows = new ArrayList<>();\n+    final List<LogicalSchema> schemas = new ArrayList<>();\n+    List<KsqlLocation> remainingLocations = ImmutableList.copyOf(locations);\n+    for (int round = 0; ; round++) {\n+      final Map<KsqlNode, List<KsqlLocation>> groupedByHost\n+          = groupByHost(statement, remainingLocations, round);\n+\n+      final Map<KsqlNode, Future<PullQueryResult>> futures = new LinkedHashMap<>();\n+      for (Map.Entry<KsqlNode, List<KsqlLocation>> entry : groupedByHost.entrySet()) {\n+        final KsqlNode node = entry.getKey();\n+        final PullQueryContext pullQueryContext = contextFactory.apply(entry.getValue());\n+\n+        futures.put(node, executorService.submit(() ->  {\n+          final TableRows rows = routeQuery.routeQuery(\n+              node, statement, executionContext, serviceContext, pullQueryContext);\n+          final Optional<List<KsqlNode>> debugNodes = Optional.ofNullable(\n+              routingOptions.isDebugRequest()\n+                  ? Collections.nCopies(rows.getRows().size(), node) : null);\n+          return new PullQueryResult(rows, debugNodes);\n+        }));\n+      }\n+\n+      final ImmutableList.Builder<KsqlLocation> nextRoundRemaining = ImmutableList.builder();\n+      for (Map.Entry<KsqlNode, Future<PullQueryResult>>  entry : futures.entrySet()) {\n+        final Future<PullQueryResult> future = entry.getValue();\n+        final KsqlNode node = entry.getKey();\n+        try {\n+          final PullQueryResult result = future.get();\n+          result.getSourceNodes().ifPresent(sourceNodes::addAll);\n+          schemas.add(result.getTableRows().getSchema());\n+          tableRows.addAll(result.getTableRows().getRows());\n+        } catch (ExecutionException e) {\n+          LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n+              statement.getStatementText(), node, System.currentTimeMillis(), e.getCause());\n+          nextRoundRemaining.addAll(groupedByHost.get(node));\n+        }\n+      }\n+      remainingLocations = nextRoundRemaining.build();\n+\n+      if (remainingLocations.size() == 0) {\n+        validateSchemas(schemas);\n         return new PullQueryResult(\n-            routeQuery(node, statement, executionContext, serviceContext, pullQueryContext),\n-            debugNode);\n-      } catch (Exception t) {\n-        LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n-                  statement.getStatementText(), node, System.currentTimeMillis(), t);\n+            new TableRows(statement.getStatementText(), queryId, Iterables.getLast(schemas),\n+                tableRows),\n+            sourceNodes.isEmpty() ? Optional.empty() : Optional.of(sourceNodes));\n       }\n     }\n-    throw new MaterializationException(String.format(\n-        \"Unable to execute pull query: %s\", statement.getStatementText()));\n+  }\n+\n+  private static Map<KsqlNode, List<KsqlLocation>> groupByHost(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc3MjkwMA=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 240}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NzEyNDI4OnYy", "diffSide": "RIGHT", "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxOTo0MDo0NlrOHlOJQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMTo0MzowM1rOHmHI-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5MTEwNg==", "bodyText": "Why do you need to know the partitions here? Is it because you don't know which partitions are active and which standbys? If so, we can get this information with KafkaStreams.allMetadata().  If we know what each partition is, then for a forwarded request, we know we need to access the standby partitions whereas for a non-forwarded only the active ones. This could avoid sending the partitions over the network causing overhead.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508791106", "createdAt": "2020-10-20T19:40:46Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg1Mzk5NA==", "bodyText": "We need to know the partitions that are intended to be fetched because the forwarding node is orchestrating this round by round traversal through the prioritized node lists.  The real issue is that I'm always sending all keys to all nodes, regardless of whether I'm intending on fetching just the active partitions or standby ones.\nAs an example, let's say we have a Host 0 receiving the request:\n Key A -> Partition 0 <Host 1, Host 2>\n Key B -> Partition 1 <Host 2, Host 1>\n Key C -> Partition 2 <Host 1, Host 2>\n\nIn round 0, we go to Host 2, sending Keys A, B, and C.  Host 2 is the active for Partition 1, but the standby for Partitions 0 and 2.  Should Host 2 return Key B (as if it's being called for round 0) or Keys A and C (as if it's being called for round 1)?  If we know nothing else, we can't know.  Returning both would intermix the rounds and create duplicates of active and standby data.\nOne method I used to disambiguate things was to rewrite the query to modify the keys requested to indicate which it was interested in.  This was frowned upon, so I instead specified which partitions it was interested in, which accomplishes the same thing. We could specify \"round\" instead and rely on the local host's metadata to figure out what it should return, but I wanted as consistent a snapshot dictated by the orchestrating node.\nIn the above example, Host 0 receives the request and forwards it on to all others, both active and standby, so we can't rely on the status of forwarding to know which partitions we should go for.  Also, if there are multiple standbys, it matters whether we forward it on to the first or second in the list because the first might have lower lag.  So, even among local standby partitions, the orchestrator really may want some subset at a given time.\nAlso, as a last note, the partitions fetched are a comma-separated list of unique partition numbers, so even for 128 partitions, it's not a lot of data.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508853994", "createdAt": "2020-10-20T21:34:27Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5MTEwNg=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyOTUxNA==", "bodyText": "I was thinking that you can be sure if it is a forwarded request, you need to access the standby partitions. And if you can know which partitions are active and which standby, then you can answer the question which partitions should be used to lookup keys. A request that is not forwarded always looksup active partitions whereas a forwarded request always looks up standby partitions.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508929514", "createdAt": "2020-10-21T01:07:17Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5MTEwNg=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcyNDkyMg==", "bodyText": "Discussed offline. The reason we need the partitions is that we need to know also which standby we want since they are ordered so we cannot rely only on the round number", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r509724922", "createdAt": "2020-10-21T21:43:03Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5MTEwNg=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NzE4MDMyOnYy", "diffSide": "RIGHT", "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxOTo1NjoyNlrOHlOr6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwMTowODoyN1rOHlWnEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5OTk3OA==", "bodyText": "Can we make this loop cheaper for forwarded requests if we do send the partitions in the request?", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508799978", "createdAt": "2020-10-20T19:56:26Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();\n+    for (Struct key : keys) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg5MTcwMw==", "bodyText": "The only issue is that we still need to find which keys belong to which partitions to figure out which we should query for locally so we don't try them unnecessarily on rocksdb, which would also be expensive.  I could try to expose the info necessary to create a DefaultStreamPartitioner, but this seems a bit complex.  This may be a little wasteful, but it's not too bad.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508891703", "createdAt": "2020-10-20T23:07:22Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();\n+    for (Struct key : keys) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5OTk3OA=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyOTgwOQ==", "bodyText": "No it's ok. I also realized that there is not much to do that is a low hanging fruit. Just wanted to pick your brain if you had anything in mind. But it's not necessary to do now.", "url": "https://github.com/confluentinc/ksql/pull/6409#discussion_r508929809", "createdAt": "2020-10-21T01:08:27Z", "author": {"login": "vpapavas"}, "path": "ksqldb-streams/src/main/java/io/confluent/ksql/execution/streams/materialization/ks/KsLocator.java", "diffHunk": "@@ -68,28 +73,64 @@\n   }\n \n   @Override\n-  public List<KsqlNode> locate(\n-      final Struct key,\n+  public List<KsqlLocation> locate(\n+      final List<Struct> keys,\n       final RoutingOptions routingOptions,\n       final RoutingFilterFactory routingFilterFactory\n   ) {\n-    final KeyQueryMetadata metadata = kafkaStreams\n-        .queryMetadataForKey(stateStoreName, key, keySerializer);\n-\n-    // Fail fast if Streams not ready. Let client handle it\n-    if (metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n-      LOG.debug(\"KeyQueryMetadata not available for state store {} and key {}\",\n-                stateStoreName, key);\n-      throw new MaterializationException(String.format(\n-          \"KeyQueryMetadata not available for state store %s and key %s\", stateStoreName, key));\n-    }\n+    // Maintain request order for reproducibility by using a LinkedHashMap, even though it's\n+    // not a guarantee of the API.\n+    final Map<Integer, List<KsqlNode>> locationsByPartition = new LinkedHashMap<>();\n+    final Map<Integer, Set<Struct>> keysByPartition = new HashMap<>();\n+    final Set<Integer> filterPartitions = routingOptions.getPartitions();\n+    for (Struct key : keys) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc5OTk3OA=="}, "originalCommit": {"oid": "f721ccbe4f514c0499424ad7a05d58b8d9cf91a2"}, "originalPosition": 40}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2698, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}