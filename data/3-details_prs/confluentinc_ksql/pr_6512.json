{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4OTM2NTI4", "number": 6512, "title": "chore: minor RQTT debug improvements", "bodyText": "Description\nSome minor tweaks that make it easier to debug RQTT failures and production issues.", "createdAt": "2020-10-23T12:34:53Z", "url": "https://github.com/confluentinc/ksql/pull/6512", "merged": true, "mergeCommit": {"oid": "6a0c29014a0d521d21a85456eb42f751d4ee21e3"}, "closed": true, "closedAt": "2020-10-23T20:09:10Z", "author": {"login": "big-andy-coates"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdVVz0VgH2gAyNTA4OTM2NTI4OjJhMjk4YmE4MGYyNDBhMDlmNmIwNWM1MjYxMTBjNWJmNzAyZTFjYjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdVa1E4gFqTUxNTkwNTk5NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2a298ba80f240a09f6b05c526110c5bf702e1cb7", "author": {"user": {"login": "big-andy-coates", "name": "Andy Coates"}}, "url": "https://github.com/confluentinc/ksql/commit/2a298ba80f240a09f6b05c526110c5bf702e1cb7", "committedDate": "2020-10-23T12:34:15Z", "message": "chore: minor RQTT debug improvements\n\nSome minor tweaks that make it easier to debug RQTT failures"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1NjI1ODM5", "url": "https://github.com/confluentinc/ksql/pull/6512#pullrequestreview-515625839", "createdAt": "2020-10-23T12:37:46Z", "commit": {"oid": "2a298ba80f240a09f6b05c526110c5bf702e1cb7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzo0NlrOHnMBrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzo0NlrOHnMBrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzU1MQ==", "bodyText": "I'd argue that this should be a WARN because in normal production steady state operation this shouldn't happen, right?\nIf it is happening, e.g. because the consumer group is unstable, then people will likely be seeing poor pull query performance and may be wondering why.   Logging this will help highlight the cause.\nDuring a cluster resize or bounce this can be logged, but again, if someone is investigating spikes in pull query latency, such a log message may be useful.\nThoughts? cc @AlanConfluent @vpapavas", "url": "https://github.com/confluentinc/ksql/pull/6512#discussion_r510853551", "createdAt": "2020-10-23T12:37:46Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/PullQueryExecutor.java", "diffHunk": "@@ -381,7 +381,7 @@ static PullQueryResult handlePullQuery(\n           schemas.add(result.getTableRows().getSchema());\n           tableRows.addAll(result.getTableRows().getRows());\n         } catch (ExecutionException e) {\n-          LOG.debug(\"Error routing query {} to host {} at timestamp {} with exception {}\",\n+          LOG.warn(\"Error routing query {} to host {} at timestamp {} with exception {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a298ba80f240a09f6b05c526110c5bf702e1cb7"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTA1OTk0", "url": "https://github.com/confluentinc/ksql/pull/6512#pullrequestreview-515905994", "createdAt": "2020-10-23T18:25:09Z", "commit": {"oid": "2a298ba80f240a09f6b05c526110c5bf702e1cb7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4589, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}