{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0MzQ2MzYz", "number": 6649, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxOTozOToyOVrOE7sAew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo1MDoxNVrOFGQzkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDIyNDU5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxOTozOToyOVrOH3eWHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxOTozOToyOVrOH3eWHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzkzMDkxMA==", "bodyText": "This sentence makes me wonder what's wrong with long values. Aren't timestamps just milliseconds values (long value) used anyways? Would the proposal use a different way to store the timestamp in the topic, like bytes?\nThis being the TLDR, perhaps would be good to introduce the feature and why users would get advantage of it instead.\nFor instance, one problem I see not having a TIMESTAMP data type is that I need to always use UDF timestamp functions to convert my columns to/from timestamps; I cannot correctly migrate other DBs tables with Timestamp values to ksqlDB; and my queries (without UDF) would display only long values.\nIf I take the above issues, and the benefits of a TIMESTAMP data type, then I can write something like:\nAdd support for TIMESTAMP column types in ksqlDB. TIMESTAMP column types will allow users to easily migrate DBs tables with timestamps and work with timestamp columns with no column type conversion.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527930910", "createdAt": "2020-11-20T19:39:29Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDMwMTczOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowNToyNVrOH3fFgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowNToyNVrOH3fFgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MzA0MQ==", "bodyText": "Let's extend a little bit what the current problems exist and the benefits users would get when TIMESTAMP column types exist. Seems useful to differentiate between TIMESTAMP column types vs data types. As you mentioned at some point, delimited and JSON will use long values as the data format type, and AVRO will use its own data format type (with ending being a long value too).\nA few comments from above:\nWith Connect integration, KSQL processes data from various sources and sinks.. This seems not in the right context. sources and sinks are also a concept for CREATE_AS queries, not related to Connect. What's the real problem when users use Connect and are migrating tables with timestamps? I don't know about them, so it would be good to know what's wrong. Errors during the Connect migration? Users require UDF to migrate them? What's wrong?\nwhich KSQL currently converts into long values. I'm not sure about this one. As I mentioned in the TLDR section, long values are used in the final format written to the topic anyway, right? Is the intention here that ksqlDB always use long column types to reference to timestamps?\nThis sometimes causes problems when sinking into a database. What's the problem when sinking? If I store a timestamp as long value, then use UDF to read from that sink, then it should be read fine, right? Or is there an actual problem trying to store the timestamp as long value?\nSupporting TIMESTAMP types would make moving time data between KSQL and other data sources/sinks smoother  and less error-prone. This is good. Are there any other reasons? Look at the problems I mentioned in the TLDR section.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527943041", "createdAt": "2020-11-20T20:05:25Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDMwNzk5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowNzo0MVrOH3fJSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowNzo0MVrOH3fJSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0NDAxMQ==", "bodyText": "What about Protobuf? We support that serde too.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527944011", "createdAt": "2020-11-20T20:07:41Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDMwOTE2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowODoxM1rOH3fKEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDowODoxM1rOH3fKEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0NDIwOQ==", "bodyText": "Will this include new built-in functions or modify the existing ones to support TIMESTAMP?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527944209", "createdAt": "2020-11-20T20:08:13Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDMyMjU2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoxMjo0OVrOH3fSPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoxMjo0OVrOH3fSPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0NjMwMA==", "bodyText": "Aren't this out of scope for this KLIP? Or are there plans to work on them? Like DATE and TIME, those seem different data types than TIMESTAMP, so they're not in this scope, right?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527946300", "createdAt": "2020-11-20T20:12:49Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM0NTM2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyMDo0N1rOH3fgJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyMDo0N1rOH3fgJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0OTg2Mw==", "bodyText": "What about:\n\nShould we support arithmetic functions too? casting? any other basic feature should be listed here.\nShould you support STRUCT, ARRAY, MAP as well?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527949863", "createdAt": "2020-11-20T20:20:47Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM1NDIzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyMzo1NlrOH3flmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyMzo1NlrOH3flmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1MTI1OA==", "bodyText": "It will be useable in any place STRING or INTEGER could be used: Will you able to concatenate or do arithmetic operations with timestamps?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527951258", "createdAt": "2020-11-20T20:23:56Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM1ODMzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyNToxM1rOH3foMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyNToxM1rOH3foMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1MTkyMA==", "bodyText": "I know this is trivial, but could you add the syntax how this would work? It's part of the design.\nAlso, what is a timestamp value? Is it a long value? So far, I read that ksqldB treats timestamps as long values, and that we don't what that (even if we store long values), so is there a different value to refer to timestamps?\nSome SQL use this:\ncreate or replace table time (ltz timestamp);\ninsert into time values ('2016-05-01 00:00:00.000');\n\nWe should clarify how a timestamp value will be defined.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527951920", "createdAt": "2020-11-20T20:25:13Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM2NjYwOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyODowOVrOH3ftQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDoyODowOVrOH3ftQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1MzIxNg==", "bodyText": "What about Protobuf serde?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527953216", "createdAt": "2020-11-20T20:28:09Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM3NTQ2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozMDo1NlrOH3fybA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNDoyOTozNFrOH3m27A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NDU0MA==", "bodyText": "Will BIGINT be discontinued? If so, that might break compatibility, right? Or will there an implicity casting of BIGINT -> TIMESTAMP?\nFor instance, I wonder what would happen if these scenarios:\n\nTIMESTAMPTOSTRING(timestamp_col, 'yyyy-MM-dd HH:mm:ss zzz','America/Los_Angeles')\nTIMESTAMPTOSTRING(bigint_col, 'yyyy-MM-dd HH:mm:ss zzz','America/Los_Angeles')\nTIMESTAMPTOSTRING(cast(int_col as bigint), 'yyyy-MM-dd HH:mm:ss zzz','America/Los_Angeles')\n^ Will this fail because we're casting to BIGINT?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527954540", "createdAt": "2020-11-20T20:30:56Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3MDM4MA==", "bodyText": "BIGINT/TIMESTAMP implicit casting makes sense. In that case, all three of those commands would work", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528070380", "createdAt": "2020-11-21T04:29:34Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NDU0MA=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQwMjI1OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0MDo0MVrOH3gCwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDozMzoxOFrOH8TZrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODcyMQ==", "bodyText": "Can't this be an implicit cast? Like:\ninsert into (ts) values ('2020-11-20 14:34:00');\n\nThat above doesn't require a UDF to convert to timestamps.\nOr is the function used for string columns instead of literals? For this, other SQL systems use TO_TIMESTAMP, which converts a String to a Timestamp.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527958721", "createdAt": "2020-11-20T20:40:41Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA2OTk3Mw==", "bodyText": "The reason for this instead of a cast is to support different string formats. Perhaps having a date format property in KSQL could make supporting TIMESTAMP/STRING casting possible?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528069973", "createdAt": "2020-11-21T04:25:16Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODcyMQ=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjcxNDM1Nw==", "bodyText": "we could copy what mySQL does here: https://dev.mysql.com/doc/refman/8.0/en/datetime.html but it would require implementing a local time zone to do the conversion as well", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532714357", "createdAt": "2020-11-30T16:10:49Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODcyMQ=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5NDQ3Nw==", "bodyText": "Yeah supporting string/timestamp casting should work. I think we should keep these functions though, in case a user wants to convert strings in a format that isn't recognized by standard formatters. .", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532994477", "createdAt": "2020-12-01T00:33:18Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODcyMQ=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQwMzA1OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0MDo1OVrOH3gDTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNDoyNToyM1rOH3m1Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODg2MQ==", "bodyText": "What does this do? How is it used?\nFROM_UNIXTIME is a good UDF to support too. Though, this converts the unix in seconds to a String format (no a timestamp column).", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527958861", "createdAt": "2020-11-20T20:40:59Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA2OTk4Mw==", "bodyText": "UNIX_TIMESTAMP returns the current millis in BIGINT. KSQL doesn't use this for conversions.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528069983", "createdAt": "2020-11-21T04:25:23Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1ODg2MQ=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQwNDg1OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0MTo0M1rOH3gEeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0MTo0M1rOH3gEeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1OTE2MQ==", "bodyText": "This can be done with unix_timestamp, can't it?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527959161", "createdAt": "2020-11-20T20:41:43Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQyMDMyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0Njo1OVrOH3gNZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0Njo1OVrOH3gNZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MTQ0Ng==", "bodyText": "Isn't this done as a cast? Like cast(ts as string) ?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527961446", "createdAt": "2020-11-20T20:46:59Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQyNDM3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0ODozNFrOH3gQFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0ODozNFrOH3gQFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MjEzNA==", "bodyText": "If DATE and TIME are out of scope, should we remove these casting for the doc?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527962134", "createdAt": "2020-11-20T20:48:34Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQyNTM1OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0ODo1OFrOH3gQtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNDoyNTozOFrOH3m1dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MjI5NA==", "bodyText": "Casting to/from timestamp is a valid casting in other DBs. Any reason why we're not supporting them?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527962294", "createdAt": "2020-11-20T20:48:58Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3MDAwNQ==", "bodyText": "Casting to/from BIGINT makes sense for KSQL. I have some reservations about casting to strings, as explained above.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528070005", "createdAt": "2020-11-21T04:25:38Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MjI5NA=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQzNDU5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo1MjowMlrOH3gWIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNDo1MToyOFrOH3m-UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MzY4Mg==", "bodyText": "What about QTT tests with different serdes? and the supported UDFs?\nDo we need a performance test plan for TIMESTAMP serdes? We have one for some serde formats, but I don't know if we need one for Timestamps (your call).", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527963682", "createdAt": "2020-11-20T20:52:02Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* ADDTIME(timestamp1, timestamp2)\n+* SUBTIME(timestamp1, timestamp2)\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPS and other data types should not be allowed.\n+\n+### Window units\n+\n+It might make sense to give window units a timestamp representation so that they could be used as time intervals in TIMESTAMP arithmetic. For example,\n+\n+```roomsql\n+SELECT ADDTIME(TIME, 1 DAY) FROM FOO;\n+```\n+\n+Because windowing is handled separately from expression evaluation, doing this will not have any\n+impact on windows.\n+\n+## Test plan\n+\n+There should be an integration test with Kafka Connect and Schema Registry.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3MjI3Mg==", "bodyText": "I don't think having the TIMESTAMP type in a schema would have any effect on performance, and java.time.Instant does not seem to have any performance issues. It might be good to check at some point, but I don't think we will need to benchmark it.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528072272", "createdAt": "2020-11-21T04:51:28Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* ADDTIME(timestamp1, timestamp2)\n+* SUBTIME(timestamp1, timestamp2)\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPS and other data types should not be allowed.\n+\n+### Window units\n+\n+It might make sense to give window units a timestamp representation so that they could be used as time intervals in TIMESTAMP arithmetic. For example,\n+\n+```roomsql\n+SELECT ADDTIME(TIME, 1 DAY) FROM FOO;\n+```\n+\n+Because windowing is handled separately from expression evaluation, doing this will not have any\n+impact on windows.\n+\n+## Test plan\n+\n+There should be an integration test with Kafka Connect and Schema Registry.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MzY4Mg=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQzNTcyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo1MjoyOFrOH3gW1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo1MjoyOFrOH3gW1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2Mzg2MA==", "bodyText": "What about the milestones for docs? We need to add this as a new syntax.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527963860", "createdAt": "2020-11-20T20:52:28Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* ADDTIME(timestamp1, timestamp2)\n+* SUBTIME(timestamp1, timestamp2)\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPS and other data types should not be allowed.\n+\n+### Window units\n+\n+It might make sense to give window units a timestamp representation so that they could be used as time intervals in TIMESTAMP arithmetic. For example,\n+\n+```roomsql\n+SELECT ADDTIME(TIME, 1 DAY) FROM FOO;\n+```\n+\n+Because windowing is handled separately from expression evaluation, doing this will not have any\n+impact on windows.\n+\n+## Test plan\n+\n+There should be an integration test with Kafka Connect and Schema Registry.\n+\n+## LOEs and Delivery Milestones\n+\n+This feature can be broken down into two milestones: implementing the TIMESTAMP type (1-2 weeks) and\n+adding all of the supporting UDFs (~1 week).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDQzOTUzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo1Mzo1M1rOH3gZLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMjo1NTozMVrOH3jYuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2NDQ2Mg==", "bodyText": "Does it mean a value written as TIMESTAMP in 0.15 won't be deserialized in 0.14? Why not? If timestamps will be stored as long values in Delimited, Json, and Avro too; why will they fail?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r527964462", "createdAt": "2020-11-20T20:53:53Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* ADDTIME(timestamp1, timestamp2)\n+* SUBTIME(timestamp1, timestamp2)\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPS and other data types should not be allowed.\n+\n+### Window units\n+\n+It might make sense to give window units a timestamp representation so that they could be used as time intervals in TIMESTAMP arithmetic. For example,\n+\n+```roomsql\n+SELECT ADDTIME(TIME, 1 DAY) FROM FOO;\n+```\n+\n+Because windowing is handled separately from expression evaluation, doing this will not have any\n+impact on windows.\n+\n+## Test plan\n+\n+There should be an integration test with Kafka Connect and Schema Registry.\n+\n+## LOEs and Delivery Milestones\n+\n+This feature can be broken down into two milestones: implementing the TIMESTAMP type (1-2 weeks) and\n+adding all of the supporting UDFs (~1 week).\n+\n+## Documentation Updates\n+\n+There will need to be documentation on the following:\n+\n+* Description of the TIMESTAMP data type\n+* TIMESTAMP usage in WHERE/GROUP/PARTITION clauses\n+* New and updated UDFs\n+* We might want to add this into one of the quick-starts\n+\n+## Compatibility Implications\n+\n+If a user creates a stream or table with a TIMESTAMP typed column, then that column will not be\n+(de)serializable in earlier versions. Besides that, there should be no other compatibility issues. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAxMzQ5OQ==", "bodyText": "The situation I had in mind was if there was a command like CREATE STREAM stream_name (col TIMESTAMP) AS .... KSQL would not recognize TIMESTAMP as a type. That was worded very inaccurately, I'll rewrite that part.\nTimestamps in Avro/JSON etc will be stored and deserialized as long values in older versions.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r528013499", "createdAt": "2020-11-20T22:55:31Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,159 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _KSQL currently converts time data into long values. This KLIP introduces a TIMESTAMP\n+data type to handle time data instead._\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various sources and sinks. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts into long values. This sometimes\n+causes problems when sinking into a database. Supporting TIMESTAMP types would make moving time data\n+between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+## What is in scope\n+### Required\n+\n+* Add TIMESTAMP type to KSQL\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON and Delimited formats\n+* Built-in functions to support TIMESTAMP usage\n+\n+### Future enhancements\n+\n+* Give window units (HOUR, DAY etc) a TIMESTAMP value\n+* Add DATE and TIME types\n+* UDFs such as DAY and HOUR\n+\n+## What is not in scope\n+Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate discussion.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a time without timezone information. It will be useable in any\n+place STRING or INTEGER could be used:\n+```roomsql\n+CREATE STREAM stream_name (col1 TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the Java Date type within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+* UNIX_TIMESTAMP\n+\n+The following UDFs should also be added to support conversions between TIMESTAMP and BIGINT representations of time:\n+\n+* TIMESTAMPTOBIGINT\n+* BIGINTTOTIMESTAMP\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting/Converting\n+\n+Since the above UDFs already handle conversions, casting between TIMESTAMP and STRING or BIGINT is\n+unnecessary. If DATE and TIME types are implemented, then the conversions between them and TIMESTAMP\n+will be as follows:\n+\n+| From      | To        | Result                                                |\n+|---------- |:----------|:------------------------------------------------------|\n+| TIMESTAMP | DATE      | DATE type representing date portion of the TIMESTAMP. |\n+| TIMESTAMP | TIME      | TIME type representing time portion of the TIMESTAMP. |\n+| DATE      | TIMESTAMP | TIMESTAMP with time portion set to 00:00:00.          |\n+| TIME      | TIMESTAMP | TIMESTAMP with date portion set to Unix epoch.        |\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* ADDTIME(timestamp1, timestamp2)\n+* SUBTIME(timestamp1, timestamp2)\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPS and other data types should not be allowed.\n+\n+### Window units\n+\n+It might make sense to give window units a timestamp representation so that they could be used as time intervals in TIMESTAMP arithmetic. For example,\n+\n+```roomsql\n+SELECT ADDTIME(TIME, 1 DAY) FROM FOO;\n+```\n+\n+Because windowing is handled separately from expression evaluation, doing this will not have any\n+impact on windows.\n+\n+## Test plan\n+\n+There should be an integration test with Kafka Connect and Schema Registry.\n+\n+## LOEs and Delivery Milestones\n+\n+This feature can be broken down into two milestones: implementing the TIMESTAMP type (1-2 weeks) and\n+adding all of the supporting UDFs (~1 week).\n+\n+## Documentation Updates\n+\n+There will need to be documentation on the following:\n+\n+* Description of the TIMESTAMP data type\n+* TIMESTAMP usage in WHERE/GROUP/PARTITION clauses\n+* New and updated UDFs\n+* We might want to add this into one of the quick-starts\n+\n+## Compatibility Implications\n+\n+If a user creates a stream or table with a TIMESTAMP typed column, then that column will not be\n+(de)serializable in earlier versions. Besides that, there should be no other compatibility issues. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2NDQ2Mg=="}, "originalCommit": {"oid": "ca5d1c360a0c4105d5a266f6edea6e1157ff4ccd"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzODMwODQ0OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOVQwODoxNzo0M1rOH7hSPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1NDo1NlrOH8SkoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjE3MzM3Mg==", "bodyText": "Time is tricky in Kafka, may be beneficial to clarify which 'now' this would be. Is it the servers time or stream-time? If it is stream-time, will each partition have a different time? Will it be evaluated when the query is issued or when the function is called (important for persistent queries)?\nAll of those variants are useful in different scenarios and unfortunately all of them could be called NOW", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532173372", "createdAt": "2020-11-29T08:17:43Z", "author": {"login": "PeterLindner"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+Because BIGINTs will be implicitly cast into TIMESTAMPs and vice versa, queries using these functions\n+with BIGINT will still work when TIMESTAMP is introduced.\n+\n+There will also be a new function, `NOW()` that returns the current timestamp.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDg5Ng==", "bodyText": "I agree with Andy's suggestion on this. I would also like to note that there is an existing function in KSQL called UNIX_TIMESTAMP that returns the time in ms when the function is called, so the distinction should be clearly defined in the docs.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532980896", "createdAt": "2020-11-30T23:54:56Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+Because BIGINTs will be implicitly cast into TIMESTAMPs and vice versa, queries using these functions\n+with BIGINT will still work when TIMESTAMP is introduced.\n+\n+There will also be a new function, `NOW()` that returns the current timestamp.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjE3MzM3Mg=="}, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MjA3MTM0OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNjowODoxMVrOH8CLfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1NTowMlrOH8Skvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjcxMjMxNg==", "bodyText": "I think it's important to have a discussion on time zone. Will all timestamps be treated by default in UTC? how could a user convert timestamps between zones?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532712316", "createdAt": "2020-11-30T16:08:11Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDkyNw==", "bodyText": "I was thinking that the timestamps here would reflect the timestamps in Kafka, which assumes UTC. MySQL and Snowflake have the convert_tz function, so we could do something similar.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532980927", "createdAt": "2020-11-30T23:55:02Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjcxMjMxNg=="}, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzA0NTM3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOTo1NDowN1rOH8LcqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOTo1NDowN1rOH8LcqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2NDE2OA==", "bodyText": "Mysql removes the T and Z literals from the output. Other big data systems (hive, impala, presto) use the same format.\nIsn't it more readable this way?\nmysql> insert into t1 values ('2020-11-30 13:25:00');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from t1;\n+---------------------+\n| ts                  |\n+---------------------+\n| 2020-11-30 13:25:00 |\n+---------------------+\n1 row in set (0.00 sec)", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532864168", "createdAt": "2020-11-30T19:54:07Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzA1NDQzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOTo1NjoxNFrOH8Likw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOTo1NjoxNFrOH8Likw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2NTY4Mw==", "bodyText": "A string in the format '2020-11-30 13:25:00' is easily interpreted by the java.sql.Timestamp, and you can get the time in millis from epoc quickly. Why not accepting the string format? Mysql and other Big data systems support the String too.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532865683", "createdAt": "2020-11-30T19:56:14Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzA3NDMxOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMDowMToyNlrOH8LugA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTo1MTowMVrOIAtXLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2ODczNg==", "bodyText": "There's a concern I have about the implicit cast here. Some systems (including Mysql, Hive, and Impala) use timestamp as seconds, but Java and other internal formats (i.e. Avro) use timestamp as milliseconds. Why are we choosing milliseconds instead of the common seconds approach? Could users get confused about the value here? Should we prevent this implicit cast instead?\nmysql> select unix_timestamp('2020-11-30 13:25:00');\n+---------------------------------------+\n| unix_timestamp('2020-11-30 13:25:00') |\n+---------------------------------------+\n|                            1606764300 |\n+---------------------------------------+\n1 row in set (0.00 sec)", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532868736", "createdAt": "2020-11-30T20:01:26Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5MTE4Mg==", "bodyText": "The main reason is because Kafka represents timestamps as milliseconds, and a lot of what we have now is based off that (ROWTIME representation as BIGINT, timestamps being converted to long types) so implicitly casting to milliseconds would help with compatibility. However, it seems that most other DBs use a seconds representation, so a new user could get confused by this. We should explain this clearly in the docs if we decide to implicitly cast.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532991182", "createdAt": "2020-12-01T00:23:46Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2ODczNg=="}, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYxNDEyNg==", "bodyText": "Seems we need more discussion about this implicit casting. From the user point of view coming from another DB, they will expect seconds as well. Kafka is new for them, so probably they will assume seconds is used.\nI would say to hold off on this casting until the team agrees what's better.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r537614126", "createdAt": "2020-12-07T15:51:01Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2ODczNg=="}, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzA5NDY4OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMDowNzoyM1rOH8L7Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1NDo1MVrOH8Skgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg3MTk0Nw==", "bodyText": "Mysql and Snowflake uses the TIMESTAMPADD(unit, interval, datetime_expr) format. I don't know about other DBs yet, but should we align with those systems? If so, that means we cannot handle this timestampadd and timestampsub in this klip because time units are out of scope, right?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532871947", "createdAt": "2020-11-30T20:07:23Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+Because BIGINTs will be implicitly cast into TIMESTAMPs and vice versa, queries using these functions\n+with BIGINT will still work when TIMESTAMP is introduced.\n+\n+There will also be a new function, `NOW()` that returns the current timestamp.\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting\n+\n+Casting from TIMESTAMP to BIGINT will return the millisecond representation of the timestamp, and\n+casting from BIGINT to TIMESTAMP will return a TIMESTAMP that is the BIGINT number of milliseconds\n+from Unix epoch. \n+\n+If a user attempts to cast a negative number or a timestamp before Unix epoch, the CAST will throw\n+an error.\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* `TIMESTAMPADD(time_stamp, big_int)` - adds `big_int` milliseconds to `time_stamp` and returns the result as a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDg2Nw==", "bodyText": "Yeah those syntaxes make a lot more sense. I do think that supporting timestamp arithmetic is valuable, so I might include this and time units in scope as another milestone for a later release.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r532980867", "createdAt": "2020-11-30T23:54:51Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,194 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting between TIMESTAMP and BIGINT\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 format:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05T13:15:30Z    |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by using conversion functions:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (STRINGTOTIMESTAMP(\"2020-11-20\", \"YYYY-MM-DD\"));\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+Because BIGINTs will be implicitly cast into TIMESTAMPs and vice versa, queries using these functions\n+with BIGINT will still work when TIMESTAMP is introduced.\n+\n+There will also be a new function, `NOW()` that returns the current timestamp.\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting\n+\n+Casting from TIMESTAMP to BIGINT will return the millisecond representation of the timestamp, and\n+casting from BIGINT to TIMESTAMP will return a TIMESTAMP that is the BIGINT number of milliseconds\n+from Unix epoch. \n+\n+If a user attempts to cast a negative number or a timestamp before Unix epoch, the CAST will throw\n+an error.\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* `TIMESTAMPADD(time_stamp, big_int)` - adds `big_int` milliseconds to `time_stamp` and returns the result as a", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg3MTk0Nw=="}, "originalCommit": {"oid": "7a3b006511c068903be5f10aadb9e5f18782be0a"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDgxMDQwOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTo0ODozNlrOIAtPjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxOTozOTozN1rOIDaKNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYxMjE3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |1994-11-05 13:15:30:112 |\n          \n          \n            \n            |1994-11-05T13:15:30.112 |\n          \n      \n    \n    \n  \n\nLet's go back to the original string format you proposed 'cause the other was using the ISO 8601 standard. This is the format:", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r537612172", "createdAt": "2020-12-07T15:48:36Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ0NTIzOA==", "bodyText": "Now that ksql is moving as a DB, I think we should design timestamps in a same way as they do (in seconds)\n\nI think we should decouple the presentation of timestamps from how they're actually stored internally. We can store timestamps as milliseconds internally and then present them with whatever default precision we feel is best. For example, PostgreSQL stores timestamps internally as 8-byte integers in nanoseconds, but will present their default epoch representation as fractional seconds.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r540445238", "createdAt": "2020-12-10T19:39:37Z", "author": {"login": "derekjn"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYxMjE3Mg=="}, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDkwNjk2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjowNjo0M1rOIAuJQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo1MDo1OFrOIGdNzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYyNjk0NA==", "bodyText": "Will this be stored as milliseconds or seconds? Same concern I had with implicit casting.\nBtw, there's another thing to discuss here. What if users have a different timestamp format? Say a String format 2020-12-07 10:02:05? There are no rules of how JSON stores timestamps, so users may have a different format. The may have CSV topics (comma delimited), which might have this string format as well. But others may long values.\nIs there a way we can define the timestamp format for the text (delimited/json) serdes? Should we define a syntax for this?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r537626944", "createdAt": "2020-12-07T16:06:43Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU3NTM5MA==", "bodyText": "Here's an example of a JSON format I saw in some logs\n\"modified_at\": \"2020-12-09T01:11:45.194487+00:00\"\n\nIf users want to analyze JSON logs with the above timestamp, then the current proposal for Long values won't work. I've seen that a common thing in logs.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r539575390", "createdAt": "2020-12-09T19:13:28Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYyNjk0NA=="}, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQyNjkxNw==", "bodyText": "This is supported by implicit casting. If the user is writing these into a timestamp valued column, then they'll be automatically cast into a timestamp.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r540426917", "createdAt": "2020-12-10T19:10:25Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYyNjk0NA=="}, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjg0MDUwMA==", "bodyText": "We should follow the lead of connect serializers for how they store JSON timestamp data by default", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r542840500", "createdAt": "2020-12-14T21:56:51Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYyNjk0NA=="}, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0MTAzOA==", "bodyText": "Which is milliseconds, right? Update the comment here to mention that timestamps will be stored as milliseconds in JSON and Delimited formats.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543641038", "createdAt": "2020-12-15T19:50:58Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYyNjk0NA=="}, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTAzMDIyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjozMDozOFrOIAvS1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjozMDozOFrOIAvS1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY0NTc4Mg==", "bodyText": "This is gonna cause some confusion between millis and seconds. We really need to discuss this with the rest of the team, how should we handle timestamps, as millis or seconds? If seconds, then we don't want to break compatibility with TIMESTAMPTOSTRING and STRINGTOTIMESTAMP, which currently support milliseconds.\nIf we do milliseconds, then we're good with the above UDF.\nIf we do seconds, we perhaps want to add a to_timestamp(string, format) function that returns the UNIX epoch in seconds. to_timestamp is used in other DB too in the same format. Postgres adds to_timestamp(double) which converts seconds to timestamp.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r537645782", "createdAt": "2020-12-07T16:30:38Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,206 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from BIGINT and STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by milliseconds from Unix epoch or by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (1605927509166);\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.time.Instant` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be updated to use the TIMESTAMP type instead of BIGINT:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "951617055362c4fe697480037855df025372247f"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNjAxMzQ0OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo0ODo1M1rOIGdInw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxOTo0ODo1M1rOIGdInw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzYzOTcxMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n          \n          \n            \n            INSERT INTO stream_name VALUES (\"1994-11-05T13:15:30\");\n          \n      \n    \n    \n  \n\nSame here, let's go back to the original string format you proposed 'cause the other was using the ISO 8601 standard. This is the format:", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543639711", "createdAt": "2020-12-15T19:48:53Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNjA3NTY2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowMzozN1rOIGdtsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowMzozN1rOIGdtsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY0OTIwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            precision (yyyy-mm-dd HH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the\n          \n          \n            \n            precision (yyyy-mm-ddTHH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the\n          \n      \n    \n    \n  \n\nBased ISO 8601", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543649201", "createdAt": "2020-12-15T20:03:37Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.sql.Timestamp` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be deprecated:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+These functions will still be available so that existing queries using these functions won't break,\n+but the documentation will state that they are deprecated and will direct users towards using the\n+TIMESTAMP type and the new functions.\n+\n+The following functions will be added:\n+* `PARSE_TIMESTAMP(format, timestamp_string)` - converts a string into a TIMESTAMP \n+* `FORMAT_TIMESTAMP(format, timestamp)` - returns a string in the specified format\n+* `NOW()` - returns the time after the issuing query is done executing\n+* `CONVERT_TZ(timestamp, from_tz ,to_tz)` - converts a timestamp from one timezone to another\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting\n+\n+Casting from TIMESTAMP to STRING will return the timestamp in ODBC canonical form with millisecond\n+precision (yyyy-mm-dd HH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNjA4MDg4OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowNTowMVrOIGdw0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowNTowMVrOIGdw0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY1MDAwMA==", "bodyText": "Could you add the example for casting? Just to avoid  confusion between implicit and explicit casting. I assume this is explicit casting, right?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543650000", "createdAt": "2020-12-15T20:05:01Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.sql.Timestamp` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be deprecated:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+These functions will still be available so that existing queries using these functions won't break,\n+but the documentation will state that they are deprecated and will direct users towards using the\n+TIMESTAMP type and the new functions.\n+\n+The following functions will be added:\n+* `PARSE_TIMESTAMP(format, timestamp_string)` - converts a string into a TIMESTAMP \n+* `FORMAT_TIMESTAMP(format, timestamp)` - returns a string in the specified format\n+* `NOW()` - returns the time after the issuing query is done executing\n+* `CONVERT_TZ(timestamp, from_tz ,to_tz)` - converts a timestamp from one timezone to another\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNjA4NTEyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowNjowMFrOIGdzPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDowNjowMFrOIGdzPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY1MDYyMg==", "bodyText": "What units will we support? Only DAY?\nCould you add a list of units to support?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543650622", "createdAt": "2020-12-15T20:06:00Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.sql.Timestamp` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be deprecated:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+These functions will still be available so that existing queries using these functions won't break,\n+but the documentation will state that they are deprecated and will direct users towards using the\n+TIMESTAMP type and the new functions.\n+\n+The following functions will be added:\n+* `PARSE_TIMESTAMP(format, timestamp_string)` - converts a string into a TIMESTAMP \n+* `FORMAT_TIMESTAMP(format, timestamp)` - returns a string in the specified format\n+* `NOW()` - returns the time after the issuing query is done executing\n+* `CONVERT_TZ(timestamp, from_tz ,to_tz)` - converts a timestamp from one timezone to another\n+\n+There are a few existing UDFs that deal with dates. These should be left as is until a DATE type is implemented:\n+\n+* UNIX_DATE\n+* DATETOSTRING\n+* STRINGTODATE\n+\n+### Casting\n+\n+Casting from TIMESTAMP to STRING will return the timestamp in ODBC canonical form with millisecond\n+precision (yyyy-mm-dd HH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the\n+string into a TIMESTAMP.\n+\n+### Arithmetic operations and comparisons\n+\n+At the very least, we would want KSQL to be able to compare, add and subtract TIMESTAMPS.\n+\n+[MySQL](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html) and [MariaDB](https://mariadb.com/kb/en/date-time-functions/)\n+have very complete lists of built-in arithmetic functions, while [PostgreSQL](https://www.postgresql.org/docs/9.0/functions-datetime.html)\n+defines behaviors for arithmetic operators.\n+\n+Because functions are more well-defined, we should opt for built-in arithmetic functions\n+instead of using operators. The following functions are necessary:\n+\n+* `TIMESTAMP_ADD(time_stamp, duration)` - adds a duration to `time_stamp` and returns the result as a\n+TIMESTAMP\n+* `TIMESTAMP_SUB(time_stamp1, duration)` - subtracts a duration from `time_stamp` and returns the\n+result as a TIMESTAMP\n+\n+Durations will be expressed in the form, `<integer_value> <unit>`. This is discussed further in the\n+next section.\n+\n+As for comparisons, the following expressions should be supported:\n+```\n+time_stamp1 < time_stamp2\n+time_stamp1 > time_stamp2\n+time_stamp1 = time_stamp2\n+time_stamp1 BETWEEN time_stamp2 AND time_stamp3\n+```\n+\n+Comparisons between TIMESTAMPs and other data types should not be allowed.\n+\n+### Durations / Time units", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNjE1NTU0OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMDoyMzoyMlrOIGebvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMjoxODo0NVrOIGihyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY2MDk4OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * `NOW()` - returns the time after the issuing query is done executing\n          \n          \n            \n            * `NOW()` - returns the timestamp after the issuing query is done executing. All calls of now() within the same query return the same value.\n          \n      \n    \n    \n  \n\nBtw, I'm not sure about getting the time after the query is done executing. What if a user wants to use NOW() in the where clause to get the current day?\nI.e.\nselect * from stream where completed_date = format_timestamp(now(), 'YYYY-mm-dd');\n\nPerhaps the now() needs to be obtained after the query is evaluated or planned? During the logical or physical plan? What if we use a push query (execution does not end until manually terminate it)?", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543660988", "createdAt": "2020-12-15T20:23:22Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.sql.Timestamp` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be deprecated:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+These functions will still be available so that existing queries using these functions won't break,\n+but the documentation will state that they are deprecated and will direct users towards using the\n+TIMESTAMP type and the new functions.\n+\n+The following functions will be added:\n+* `PARSE_TIMESTAMP(format, timestamp_string)` - converts a string into a TIMESTAMP \n+* `FORMAT_TIMESTAMP(format, timestamp)` - returns a string in the specified format\n+* `NOW()` - returns the time after the issuing query is done executing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzcyODA3NQ==", "bodyText": "Updated to clarify this. Obtaining the NOW value seems to make the most sense in the logical plan.", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r543728075", "createdAt": "2020-12-15T22:18:45Z", "author": {"login": "jzaralim"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -0,0 +1,204 @@\n+# KLIP-43: TIMESTAMP Data Type Support\n+\n+**Author**: @jzaralim | \n+**Release Target**: 0.15, 0.16 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6649\n+\n+**tl;dr:** _Add support for TIMESTAMP column types in ksqlDB. This will allow users to easily migrate\n+time data from other databases without having to convert column types, as well as simplify time data\n+manipulation_\n+\n+## Motivation and background\n+\n+With Connect integration, KSQL processes data from various databases. Most other databases\n+have a TIMESTAMP or DATETIME type, which KSQL currently converts to BIGINT. This makes sinking to\n+databases that don't implicitly cast long values to timestamps much more complicated - the user would\n+have to set up their connector to convert these columns back to TIMESTAMP. Supporting TIMESTAMP types\n+would make moving time data between KSQL and other data sources/sinks smoother and less error-prone.\n+\n+Adding a TIMESTAMP type also simplifies time data manipulation. For example, currently\n+if a user wants to extract the month of a timestamp, they would either have to parse a string or do a\n+lot of math. Having time data in a dedicated data type allows for a lot of new UDFs.\n+\n+## What is in scope\n+\n+* Add TIMESTAMP type to KSQL\n+* Support TIMESTAMP arithmetic and comparisons\n+* Allow window units (HOUR, DAY etc) to be used in timestamp functions\n+* Support TIMESTAMP usage in STRUCT, MAP and ARRAY\n+* Serialization and de-serialization of TIMESTAMPs to Avro, JSON, Protobuf and Delimited formats\n+* Update existing built-in functions to use the TIMESTAMP data type\n+* Casting TIMESTAMP to and from STRING\n+\n+## What is not in scope\n+* Changing the ROWTIME data type. We will eventually want this to happen, but that is a separate\n+discussion.\n+* Support for dates before Unix epoch - this is not supported by Kafka. There is a [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-228+Negative+record+timestamp+support)\n+for negative timestamps, but until that is implemented, KSQL can only support positive timestamps.\n+* DATE and TIME types - because TIMESTAMPs represent a point in time, DATE and TIME types\n+would be useful if a user wants to represent a less specific time. These would be added after\n+TIMESTAMP is implemented though.\n+* TIMESTAMP types that store timezone information (TIMESTAMP_TZ) - Several other databases support\n+this. This is something that's useful to add in the future. \n+\n+## Public APIS\n+\n+The TIMESTAMP data type will store a point in time after Unix epoch without timezone information.\n+The syntax is as follows:\n+\n+```roomsql\n+CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n+CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n+```\n+\n+TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+\n+```roomsql\n+> SELECT time FROM stream_name EMIT CHANGES;\n+\n++------------------------+\n+|time                    |\n++------------------------+\n+|1994-11-05 13:15:30:112 |\n+```\n+\n+TIMESTAMPS can be represented by date strings:\n+\n+```roomsql\n+INSERT INTO stream_name VALUES (\"1994-11-05 13:15:30\");\n+```\n+\n+## Design\n+\n+### Serialization/Deserialization\n+\n+TIMESTAMPs will be handled by the `java.sql.Timestamp` class in UTC within KSQL. The corresponding Kafka Connect type is\n+[org.apache.kafka.connect.data.Timestamp](https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Timestamp.html).\n+They are represented as long types in Schema Registry, but also come with a tag indicating that it\n+is a timestamp, so they should be distinguishable from long types when handling serialized values in KSQL.\n+\n+#### Avro\n+\n+Avro schemas represent timestamps as\n+```json\n+{\n+  \"type\": \"long\",\n+  \"logicalType\": \"timestamp-millis\"\n+}\n+```\n+The Avro deserializer KSQL uses supports this.\n+\n+#### Protobuf\n+Protobuf 3 has a Timestamp type. The Protouf deserializer KSQL uses supports this.\n+\n+#### JSON/Delimited\n+\n+Timestamps will get stored in JSON and CSV files as long values. The KSQL JSON and delimited deserializers\n+will be updated to parse timestamps.\n+\n+### UDFs\n+\n+The following UDFs should be deprecated:\n+\n+* TIMESTAMPTOSTRING\n+* STRINGTOTIMESTAMP\n+\n+These functions will still be available so that existing queries using these functions won't break,\n+but the documentation will state that they are deprecated and will direct users towards using the\n+TIMESTAMP type and the new functions.\n+\n+The following functions will be added:\n+* `PARSE_TIMESTAMP(format, timestamp_string)` - converts a string into a TIMESTAMP \n+* `FORMAT_TIMESTAMP(format, timestamp)` - returns a string in the specified format\n+* `NOW()` - returns the time after the issuing query is done executing", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY2MDk4OA=="}, "originalCommit": {"oid": "342ba5974763c3a03ef07d89f04e747dd287d2c4"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTEwNjgyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo0OToyNFrOIHMERA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo0OToyNFrOIHMERA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQwODY0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |1994-11-05T13:15:30:112 |\n          \n          \n            \n            |1994-11-05T13:15:30.112 |\n          \n      \n    \n    \n  \n\nThere is a fractional part separated by a dot mentioned in the standard: \"yyyy-mm-ddThh:mm:ss[.mmm].", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r544408644", "createdAt": "2020-12-16T15:49:24Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -52,21 +52,21 @@ CREATE STREAM stream_name (time TIMESTAMP, COL2 STRING) AS ...\n CREATE TABLE table_name (col1 STRUCT<field TIMESTAMP>) AS ...\n ```\n \n-TIMESTAMPS will be displayed in console as strings in ODBC canonical format with millisecond precision:\n+TIMESTAMPS will be displayed in console as strings in ISO-8601 form with millisecond precision:\n \n ```roomsql\n > SELECT time FROM stream_name EMIT CHANGES;\n \n +------------------------+\n |time                    |\n +------------------------+\n-|1994-11-05 13:15:30:112 |\n+|1994-11-05T13:15:30:112 |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82c6b943b62bb88f2de63ba8a7fc2d1a8386d617"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTExMTIwOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-43-timestamp-data-type-support.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo1MDoxNVrOIHMG-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNTo1MDoxNVrOIHMG-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQwOTMzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            precision (yyyy-mm-ddTHH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the\n          \n          \n            \n            precision (yyyy-mm-ddTHH:mm:ss.fff), and  casting from STRING to TIMESTAMP will attempt to parse the", "url": "https://github.com/confluentinc/ksql/pull/6649#discussion_r544409336", "createdAt": "2020-12-16T15:50:15Z", "author": {"login": "spena"}, "path": "design-proposals/klip-43-timestamp-data-type-support.md", "diffHunk": "@@ -122,10 +122,17 @@ There are a few existing UDFs that deal with dates. These should be left as is u\n \n ### Casting\n \n-Casting from TIMESTAMP to STRING will return the timestamp in ODBC canonical form with millisecond\n-precision (yyyy-mm-dd HH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the\n+Casting from TIMESTAMP to STRING will return the timestamp in ISO-8601 form with millisecond\n+precision (yyyy-mm-ddTHH:mm:ss:fff), and  casting from STRING to TIMESTAMP will attempt to parse the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82c6b943b62bb88f2de63ba8a7fc2d1a8386d617"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2444, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}