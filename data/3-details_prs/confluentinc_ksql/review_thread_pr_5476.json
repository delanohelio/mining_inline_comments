{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyOTE3NTMw", "number": 5476, "reviewThreads": {"totalCount": 44, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyNjowNlrOD_2lJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjo0NDo0MVrOEVz9uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MjgxMTI2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyNjowNlrOGaw42w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDo0NzozOFrOGcrmKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw==", "bodyText": "This is the only naive way I've found so far to get the consumer group id. Would love some feedback on how to get this in a proper/better way.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430717147", "createdAt": "2020-05-26T21:26:06Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTMxNw==", "bodyText": "I think this is the same as StreamsConfig.APPLICATION_ID_CONFIG in the QueryMetadata.getStreamsProperties, but I'd need to double check that", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432725317", "createdAt": "2020-05-29T20:42:20Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNzU5Mg==", "bodyText": "You can see how we build it in QueryExecutor#getApplicationId", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432727592", "createdAt": "2020-05-29T20:47:38Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MjgxMzYwOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyNjo1OFrOGaw6Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1NzowM1rOGexXdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA==", "bodyText": "Would be possible to get more than one source query?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430717518", "createdAt": "2020-05-26T21:26:58Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMzg3MA==", "bodyText": "yes, this is in the case of a JOIN you could possibly have more than one source query - it would be good to test a JOIN scenario end to end", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432723870", "createdAt": "2020-05-29T20:38:52Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxOTI4NQ==", "bodyText": "I'm wondering how it will have more than one source when there is only one topic:\n        final TopicDescription topicDescription = serviceContext.getTopicClient()\n            .describeTopic(dataSource.getKafkaTopicName());\n\nThinking out loud: When a query is joining 2 streams, is acting on top of the streams, not a topic itself, therefore no consumer group.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r434919285", "createdAt": "2020-06-03T23:57:03Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MjgyMDE5OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyOToxM1rOGaw-cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyOToxM1rOGaw-cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODU3Nw==", "bodyText": "Currently using blocking get(). Probably should use a timeout, just not sure what's the best way to pass through the value from config or somewhere else", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430718577", "createdAt": "2020-05-26T21:29:13Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MjgyMTc2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMToyOTo1MFrOGaw_fA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDozODowMlrOGcrWZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODg0NA==", "bodyText": "Not sure how to get this from config.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430718844", "createdAt": "2020-05-26T21:29:50Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMzU1Nw==", "bodyText": "I think you're looking for KsqlConfig.KSQL_SERVICE_ID_CONFIG the config is availabile in the ConfigureStatement class", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432723557", "createdAt": "2020-05-29T20:38:02Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODg0NA=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTEzNjAyOnYy", "diffSide": "RIGHT", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDoyODo1NFrOGcrHlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDoyODo1NFrOGcrHlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcxOTc2Nw==", "bodyText": "nit: our checkstyle should enforce some things like local variables being final whenever possible, you can run mvn checkstyle:checkstyle to make sure that your code passes all checkstyle checks", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432719767", "createdAt": "2020-05-29T20:28:54Z", "author": {"login": "agavra"}, "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -618,6 +619,27 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    Optional<SourceConsumerOffsets> consumerGroupOffsetsOptional = source.getConsumerGroupOffsets();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTE0NjUwOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDozMjo1N1rOGcrOMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1ODo0MVrOGfT1CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMTQ1Ng==", "bodyText": "we should not be passing the real admin client to the sandbox - otherwise it becomes possible for the sandbox to make real changes to the kafka cluster. I recommend instead following the pattern of the above Sandboxes and create a proxy that proxies the admin client for read-only commands.\nBetter yet, I think it might make sense to bake this into the KafkaTopicClient, which already has an api for describeTopic", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432721456", "createdAt": "2020-05-29T20:32:57Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java", "diffHunk": "@@ -46,29 +48,33 @@ public static SandboxedServiceContext create(final ServiceContext serviceContext\n     final SchemaRegistryClient schemaRegistryClient =\n         SandboxedSchemaRegistryClient.createProxy(serviceContext.getSchemaRegistryClient());\n     final ConnectClient connectClient = SandboxConnectClient.createProxy();\n+    final Admin adminClient = serviceContext.getAdminClient();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MzkxMw==", "bodyText": "fixed.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r435483913", "createdAt": "2020-06-04T18:58:41Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java", "diffHunk": "@@ -46,29 +48,33 @@ public static SandboxedServiceContext create(final ServiceContext serviceContext\n     final SchemaRegistryClient schemaRegistryClient =\n         SandboxedSchemaRegistryClient.createProxy(serviceContext.getSchemaRegistryClient());\n     final ConnectClient connectClient = SandboxConnectClient.createProxy();\n+    final Admin adminClient = serviceContext.getAdminClient();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMTQ1Ng=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTE1MTk4OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDozNTowMVrOGcrRbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDozNTowMVrOGcrRbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMjI4NQ==", "bodyText": "I think it would make sense to encapsulate all of this into a single class and have one Map<TopicPartition, SourceConsumerOffsets>", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432722285", "createdAt": "2020-05-29T20:35:01Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java", "diffHunk": "@@ -34,7 +41,11 @@ public static SourceDescription create(\n       final boolean extended,\n       final List<RunningQuery> readQueries,\n       final List<RunningQuery> writeQueries,\n-      final Optional<TopicDescription> topicDescription\n+      final Optional<TopicDescription> topicDescription,\n+      final Optional<ConsumerGroupDescription> consumerGroupDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTE3NDE1OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMDo0MzoyM1rOGcrfFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMToyOFrOGfT7Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTc4Mg==", "bodyText": "as mentioned above, it would be nice to encapsulate this into the KafkaTopicClient (I think, I'm not 100% sure anymore - cc @big-andy-coates for his thoughts on this one)", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432725782", "createdAt": "2020-05-29T20:43:23Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NTQ0Mw==", "bodyText": "@agavra I went by adding methods to KafkaTopicClient, and elevate KafkaConsumerGroupClient\u2014that was hidden in utils\u2014 to handle consumer group offsets query and add it to ServiceContext. Hope this looks better now.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r435485443", "createdAt": "2020-06-04T19:01:28Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTc4Mg=="}, "originalCommit": {"oid": "257c4d061db59030377aec97793cc6389efa935c"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjE5MDYwOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNzoyNDoxMFrOGf28Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTowOToxMlrOGo1-_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ==", "bodyText": "what was the motivation behind passing in a supplier? It looks like we open ourselves to accidentally creating a new admin client each time we make any describe/list request, which could potentially be expensive (I'm not sure how it manages handshakes/connection management with the Kafka broker)\nin most places, it looks like we're just passing in () -> adminClient anyway - but there are a few places we aren't. would be good to audit and understand if this is necessary", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436059159", "createdAt": "2020-06-05T17:24:10Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -52,7 +56,10 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n     try {\n       final Map<String, ConsumerGroupDescription> groups = ExecutorUtil\n           .executeWithRetries(\n-              () -> adminClient.describeConsumerGroups(Collections.singleton(group)).all().get(),\n+              () -> adminClient.get()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NTg3MA==", "bodyText": "In this case I followed KafkaTopicClient design as it is the most similar in terms of behavior and instance creatin.\nAFAIK, as well as Consumer and Producer, Admin client does not do any handshake when instantiated; though it make sense to reduce unnecessary recreations.\nFollowing creation path, KafkaConsumerGroupClient and KafkaTopicClient both receive admin client supplier from ServiceContext which has all instances memoized to avoid recreation, e.g.:\nprivate final MemoizedSupplier<Admin> adminClientSupplier;\nWith this design I don't see a need to change the current behavior. wdyt?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441175870", "createdAt": "2020-06-16T22:23:51Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -52,7 +56,10 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n     try {\n       final Map<String, ConsumerGroupDescription> groups = ExecutorUtil\n           .executeWithRetries(\n-              () -> adminClient.describeConsumerGroups(Collections.singleton(group)).all().get(),\n+              () -> adminClient.get()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4MDcwMQ==", "bodyText": "Yeah, this follows the same patterns as KafkaTopicClient.  The Memorized supplier pattern is used to avoid Admin Clients being instantiated, potentially per request due to different user credentials, during service context creation.  Admin client instantiation is expensive and should be done lazily, as is the case here.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445480701", "createdAt": "2020-06-25T11:09:12Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -52,7 +56,10 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n     try {\n       final Map<String, ConsumerGroupDescription> groups = ExecutorUtil\n           .executeWithRetries(\n-              () -> adminClient.describeConsumerGroups(Collections.singleton(group)).all().get(),\n+              () -> adminClient.get()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjE5Njk1OnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNzoyNjoyM1rOGf3ASg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMjoyNzoxN1rOGkvT8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA2MDIzNA==", "bodyText": "nit: any reason we need a LinkedHashMap? is ordering important?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436060234", "createdAt": "2020-06-05T17:26:23Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "diffHunk": "@@ -311,6 +316,27 @@ public void deleteInternalTopics(final String applicationId) {\n     }\n   }\n \n+  @Override\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n+      final String topicName,\n+      final OffsetSpec offsetSpec\n+  ) {\n+    final TopicDescription topicDescription = describeTopic(topicName);\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new LinkedHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzA3NA==", "bodyText": "Not anymore. Initially I was using this map to iterate and show results in the right order. This has changed. Will update it.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441177074", "createdAt": "2020-06-16T22:27:17Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "diffHunk": "@@ -311,6 +316,27 @@ public void deleteInternalTopics(final String applicationId) {\n     }\n   }\n \n+  @Override\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n+      final String topicName,\n+      final OffsetSpec offsetSpec\n+  ) {\n+    final TopicDescription topicDescription = describeTopic(topicName);\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new LinkedHashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA2MDIzNA=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjQzNjU5OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxODozNzo1NFrOGf5XvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwODo0Mjo1MlrOGk7z9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5OTAwNA==", "bodyText": "A single source can have multiple read-from and write-to queries:\nQueries that read from this STREAM\n-----------------------------------\nCSAS_BAZ_7 (RUNNING) : CREATE STREAM BAZ WITH (KAFKA_TOPIC='BAZ', PARTITIONS=1, REPLICAS=1) AS SELECT * FROM FOO FOO INNER JOIN BAR BAR WITHIN 10 SECONDS ON ((FOO.ID = BAR.ID)) EMIT CHANGES;\nINSERTQUERY_0 (RUNNING) : INSERT INTO foo SELECT * FROM bar;\n\nFor query topology and execution plan please run: EXPLAIN <QueryId>\n\nQueries that write from this STREAM\n-----------------------------------\nINSERTQUERY_9 (RUNNING) : INSERT INTO bar SELECT * FROM foo;\nINSERTQUERY_13 (RUNNING) : INSERT INTO bar SELECT id from bob;\nBasically you can have insert into statements running left and right. In this case, there will be different consumer IDs fro each of the queries.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436099004", "createdAt": "2020-06-05T18:37:54Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +210,49 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    Optional<SourceConsumerGroupOffsets> sourceConsumerOffsets = Optional.empty();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+            .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        if (!sourceQueries.isEmpty()) {\n+          final QueryId queryId = sourceQueries.get(0).getId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MTg3Nw==", "bodyText": "Got it. I have changed impl from Optional to List depending on the input sourceQueries.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441381877", "createdAt": "2020-06-17T08:42:52Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +210,49 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    Optional<SourceConsumerGroupOffsets> sourceConsumerOffsets = Optional.empty();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+            .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        if (!sourceQueries.isEmpty()) {\n+          final QueryId queryId = sourceQueries.get(0).getId();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5OTAwNA=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODIzNTg0OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozNDowN1rOGjMRtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMzoxOTowN1rOGkwXcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NDQ4Nw==", "bodyText": "when will these be null and is 0 a good default?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439554487", "createdAt": "2020-06-12T17:34:07Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE5NDM1NA==", "bodyText": "It will be null when no offsets has been stored yet. e.g. stream created a new topic but topic does not have any data yet.\n0 seemed like a good default, though it might not be the right value: it gives the impression that offset 0 has been committed.\nWould -1 or just a hyphen - be better representation for this scenario?\nAlso, instead of dealing with this here, I can wrap this as an optional an let the client decide how to print. wdyt?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441194354", "createdAt": "2020-06-16T23:19:07Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NDQ4Nw=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODIzOTA2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozNToxNFrOGjMTvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwODo1Njo0NFrOGk8YEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NTAwNA==", "bodyText": "we should extract this to a utility class and reuse it in QueryExecutor (and ditto below)", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439555004", "createdAt": "2020-06-12T17:35:14Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0\n+          ));\n+    }\n+    return sourceConsumerGroupOffsets;\n+  }\n+\n+  private static String getServiceId(final KsqlConfig ksqlConfig) {\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM5MTEyMg==", "bodyText": "I've move it into KsqlConfig itself. Let me know if this is a better place.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441391122", "createdAt": "2020-06-17T08:56:44Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0\n+          ));\n+    }\n+    return sourceConsumerGroupOffsets;\n+  }\n+\n+  private static String getServiceId(final KsqlConfig ksqlConfig) {\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NTAwNA=="}, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODI0ODA0OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozODoyOFrOGjMZdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozODoyOFrOGjMZdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NjQ3MA==", "bodyText": "we should add at least one test where this isn't empty to make sure equals/hashcode properly include it", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439556470", "createdAt": "2020-06-12T17:38:28Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "diffHunk": "@@ -53,117 +54,117 @@ public void shouldImplementHashCodeAndEqualsProperty() {\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING),\n+                    SOME_STRING, Optional.empty()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTcyMDI0OnYy", "diffSide": "RIGHT", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTowNDo0NFrOGo12mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTowNDo0NFrOGo12mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ3ODU1NQ==", "bodyText": "This method is not returning what the name suggests.  It's not returning the service id, (which would be just ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG).\nInstead, its returning the service id prefixed with KSQL_INTERNAL_TOPIC_PREFIX.\nI see you've moved this from another class, but I still think this isn't what we want.  KsqlConfig doesn't need to know about ReservedInternalTopics.\nThis is only needed to construct the query application id. Would you mind moving code into a QueryApplicationId util class? e.g.\n/**\n * Util for creating query application ids.\n */\npublic final class QueryApplicationId {\n\n  private QueryApplicationId() {\n  }\n\n  public static String getQueryApplicationId(\n      final KsqlConfig config,\n      final boolean persistent,\n      final QueryId queryId\n  ) {\n    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n\n    final String configName = persistent\n        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n    \n    final String queryPrefix = config.getString(configName);\n    \n    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n        + serviceId\n        + queryPrefix\n        + queryId;\n  }\n}\nThereby decoupling KqlConfig from any notion of internal topics, and also streamlining the existing code.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445478555", "createdAt": "2020-06-25T11:04:44Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -949,4 +949,9 @@ public KsqlConfig overrideBreakingConfigsWithOriginalValues(final Map<String, ?>\n     SslConfigs.addClientSslSupport(sslConfig);\n     return sslConfig.names();\n   }\n+\n+  public static String getServiceId(KsqlConfig ksqlConfig) {\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc0MzcxOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxMjo1MFrOGo2FiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxMjo1MFrOGo2FiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4MjM3Ng==", "bodyText": "I think you need something move like to give a better error message to the user on failure:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                } catch (final Exception e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get offsets for Kafka Topic \" + topicName, e);\n          \n          \n            \n                }\n          \n          \n            \n               } catch (final TopicAuthorizationException e) {\n          \n          \n            \n                  final Set<String> topics = partitions.stream()\n          \n          \n            \n                      .map(TopicPartition::topic)\n          \n          \n            \n                      .collect(Collectors.toSet());\n          \n          \n            \n            \n          \n          \n            \n                  throw new KsqlTopicAuthorizationException(\n          \n          \n            \n                      AclOperation.DESCRIBE, topics);\n          \n          \n            \n                } catch (final ExecutionException e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get topic offsets. partitions: \" + partitions, e.getCause());\n          \n          \n            \n                } catch (final Exception e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get topic offsets. partitions: \" + partitions, e);\n          \n          \n            \n                }", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445482376", "createdAt": "2020-06-25T11:12:50Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "diffHunk": "@@ -311,6 +316,27 @@ public void deleteInternalTopics(final String applicationId) {\n     }\n   }\n \n+  @Override\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n+      final String topicName,\n+      final OffsetSpec offsetSpec\n+  ) {\n+    final TopicDescription topicDescription = describeTopic(topicName);\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new HashMap<>();\n+    for (TopicPartitionInfo tpInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicName, tpInfo.partition());\n+      offsetsRequest.put(tp, offsetSpec);\n+    }\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get().listOffsets(offsetsRequest).all().get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to get offsets for Kafka Topic \" + topicName, e);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc1NzcxOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxNzozOVrOGo2OIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxNzozOVrOGo2OIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NDU3Ng==", "bodyText": "don't return null.  The contract is to throw an exception, e.g. KafkaResponseGetFailedException, on an unknown group.  (Not that the contract is documented!).", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445484576", "createdAt": "2020-06-25T11:17:39Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package io.confluent.ksql.services;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class FakeKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(String group) {\n+    if (groups.contains(group)) {\n+      Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc1OTIxOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODowNlrOGo2PCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODowNlrOGo2PCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NDgwOQ==", "bodyText": "Likewise.... throw on unknown group.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445484809", "createdAt": "2020-06-25T11:18:06Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package io.confluent.ksql.services;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class FakeKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(String group) {\n+    if (groups.contains(group)) {\n+      Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(String group) {\n+    if (groups.contains(group)) {\n+      Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+      offsets.put(new TopicPartition(\"topic1\", 0), new OffsetAndMetadata(10));\n+      offsets.put(new TopicPartition(\"topic1\", 1), new OffsetAndMetadata(11));\n+      return offsets;\n+    } else {\n+      return Collections.emptyMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc2MDkyOnYy", "diffSide": "RIGHT", "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODozOFrOGo2QCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODozOFrOGo2QCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTA2NQ==", "bodyText": "As before - throw on unknown group", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485065", "createdAt": "2020-06-25T11:18:38Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.test.tools.stubs;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.services.KafkaConsumerGroupClient;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class StubKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(final String group) {\n+    if (groups.contains(group)) {\n+      final Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc2MTA0OnYy", "diffSide": "RIGHT", "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODo0MlrOGo2QIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxODo0MlrOGo2QIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTA4OA==", "bodyText": "As before - throw on unknown group", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485088", "createdAt": "2020-06-25T11:18:42Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.test.tools.stubs;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.services.KafkaConsumerGroupClient;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class StubKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(final String group) {\n+    if (groups.contains(group)) {\n+      final Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    if (groups.contains(group)) {\n+      final Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+      offsets.put(new TopicPartition(\"topic1\", 0), new OffsetAndMetadata(10));\n+      offsets.put(new TopicPartition(\"topic1\", 1), new OffsetAndMetadata(11));\n+      return offsets;\n+    } else {\n+      return Collections.emptyMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc2NDkwOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxOTo1NlrOGo2SZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToxOTo1NlrOGo2SZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTY3MQ==", "bodyText": "Can you extend this error handling, as suggestted above for another function, to return more helpful error messages to the user please?\n\nadd catch block for auth errors\nadd catch block to unwrap execution errors.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485671", "createdAt": "2020-06-25T11:19:56Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -73,4 +80,18 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n       throw new KafkaResponseGetFailedException(\"Failed to describe Kafka consumer groups\", e);\n     }\n   }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get()\n+              .listConsumerGroupOffsets(group)\n+              .partitionsToOffsetAndMetadata()\n+              .get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc3MzU3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyMjo0M1rOGo2Xdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyMjo0M1rOGo2Xdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4Njk2Ng==", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. Objects.requireNonNull", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445486966", "createdAt": "2020-06-25T11:22:43Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffsets {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<SourceConsumerGroupOffset> offsets;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffsets(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;\n+    this.offsets = offsets;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc3ODg3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyNDo0NFrOGo2a-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyNDo0NFrOGo2a-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4Nzg2Ng==", "bodyText": "Feels like a good candidate to move into its own function.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445487866", "createdAt": "2020-06-25T11:24:44Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        for (RunningQuery sourceQuery : sourceQueries) {\n+          final QueryId queryId = sourceQuery.getId();\n+          final String persistenceQueryPrefix =\n+              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n+          final String applicationId = getQueryApplicationId(\n+              KsqlConfig.getServiceId(ksqlConfig),\n+              persistenceQueryPrefix,\n+              queryId\n+          );\n+          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n+              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n+              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n+          sourceConsumerOffsets.add(\n+              new SourceConsumerGroupOffsets(\n+                  applicationId,\n+                  topicDescription.name(),\n+                  consumerOffsets(\n+                      topicDescription,\n+                      topicAndStartOffsets,\n+                      topicAndEndOffsets,\n+                      topicAndConsumerOffsets)));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc5MTAxOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyODo1MlrOGo2irQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMToyODo1MlrOGo2irQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4OTgzNw==", "bodyText": "I think this is a bug: this is getting the offsets of the data source's sink topic, i.e. kafkaTopicName and comparing this to the consumer group offsets of queries writing into the sink topic.  Those queries won't be consuming kafkaTopicName, they'll be producing to it.\nOr am I missing somethinng?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445489837", "createdAt": "2020-06-25T11:28:52Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        for (RunningQuery sourceQuery : sourceQueries) {\n+          final QueryId queryId = sourceQuery.getId();\n+          final String persistenceQueryPrefix =\n+              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n+          final String applicationId = getQueryApplicationId(\n+              KsqlConfig.getServiceId(ksqlConfig),\n+              persistenceQueryPrefix,\n+              queryId\n+          );\n+          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n+              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n+              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTc5NDQ2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozMDowNVrOGo2lBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozMDowNVrOGo2lBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MDQzOQ==", "bodyText": "nit: inline this.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445490439", "createdAt": "2020-06-25T11:30:05Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTgwNzUzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNDo0M1rOGo2tXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNDo0M1rOGo2tXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MjU3NA==", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. ensure none of these are negative:\nPreconditions.checkArgument(partition <= 0, \"invalid partition: \" +. partition);\n // etc", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445492574", "createdAt": "2020-06-25T11:34:43Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffset {\n+\n+  private final int partition;\n+  private final long logStartOffset;\n+  private final long logEndOffset;\n+  private final long consumerOffset;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffset(\n+      @JsonProperty(\"partition\") final int partition,\n+      @JsonProperty(\"logStartOffset\") final long logStartOffset,\n+      @JsonProperty(\"logEndOffset\") final long logEndOffset,\n+      @JsonProperty(\"consumerOffset\") final long consumerOffset\n+  ) {\n+    this.partition = partition;\n+    this.logStartOffset = logStartOffset;\n+    this.logEndOffset = logEndOffset;\n+    this.consumerOffset = consumerOffset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTgxMjc3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNjoyOFrOGo2wtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNjoyOFrOGo2wtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MzQyOQ==", "bodyText": "nit: Prefer ImmutableList.copyOf to Collections.unmodifiableList.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445493429", "createdAt": "2020-06-25T11:36:28Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "diffHunk": "@@ -88,6 +90,8 @@ public SourceDescription(\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n+    this.consumerGroupsOffsets = Collections.unmodifiableList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NTgxNDYzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNzowNlrOGo2xyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxMTozNzowNlrOGo2xyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MzcwNA==", "bodyText": "This exposes mutable state of the object, which breaks encapsulation.  Please use ImmutableList.copyOf in the constructor.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445493704", "createdAt": "2020-06-25T11:37:06Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffsets {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<SourceConsumerGroupOffset> offsets;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffsets(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;\n+    this.offsets = offsets;\n+  }\n+\n+  public String getGroupId() {\n+    return groupId;\n+  }\n+\n+  public String getKafkaTopic() {\n+    return kafkaTopic;\n+  }\n+\n+  public List<SourceConsumerGroupOffset> getOffsets() {\n+    return offsets;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7de0def3b0a72ffff8a6916978e3d588facf0cf"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjAwMTYzOnYy", "diffSide": "RIGHT", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNToxOTo0N1rOGwNbaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNzoxOFrOG5l_Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDg0MQ==", "bodyText": "@big-andy-coates should we also add time suffix_ as in:\n  private static String addTimeSuffix(final String original) {\n    return String.format(\"%s_%d\", original, System.currentTimeMillis());\n  }", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r453204841", "createdAt": "2020-07-11T15:19:47Z", "author": {"login": "jeqo"}, "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {\n+\n+  private QueryApplicationId() {\n+  }\n+\n+  public static String build(\n+      final KsqlConfig config,\n+      final boolean persistent,\n+      final QueryId queryId\n+  ) {\n+    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+\n+    final String configName = persistent\n+        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n+        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n+\n+    final String queryPrefix = config.getString(configName);\n+\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + serviceId\n+        + queryPrefix\n+        + queryId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d795becb3948f8388cb74d75cb7985b694ef606c"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM1OQ==", "bodyText": "Good call. Makes sense to have this method append the time suffix for transient queries, i.e. when !persistent.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463044359", "createdAt": "2020-07-30T14:37:18Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {\n+\n+  private QueryApplicationId() {\n+  }\n+\n+  public static String build(\n+      final KsqlConfig config,\n+      final boolean persistent,\n+      final QueryId queryId\n+  ) {\n+    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+\n+    final String configName = persistent\n+        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n+        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n+\n+    final String queryPrefix = config.getString(configName);\n+\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + serviceId\n+        + queryPrefix\n+        + queryId;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDg0MQ=="}, "originalCommit": {"oid": "d795becb3948f8388cb74d75cb7985b694ef606c"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjAwMzQzOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNToyMTo1MFrOGwNcSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNToyMTo1MFrOGwNcSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNTA2Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\"Failed to list Kafka consumer groups offsets\", e);", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r453205066", "createdAt": "2020-07-11T15:21:50Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -68,9 +77,27 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n                   })).collect(Collectors.toSet());\n \n       return new ConsumerGroupSummary(results);\n+    } catch (final GroupAuthorizationException e) {\n+      throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to describe Kafka consumer groups: \" + group, e);\n+    }\n+  }\n \n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get()\n+              .listConsumerGroupOffsets(group)\n+              .partitionsToOffsetAndMetadata()\n+              .get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final GroupAuthorizationException e) {\n+      throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n     } catch (final Exception e) {\n-      throw new KafkaResponseGetFailedException(\"Failed to describe Kafka consumer groups\", e);\n+      throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d795becb3948f8388cb74d75cb7985b694ef606c"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTAxMDE0OnYy", "diffSide": "RIGHT", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDozNjo1OVrOG5l-Ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMzo1OTo1MlrOG8K-aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDExNQ==", "bodyText": "Class would benefit from a couple of tests!  Maybe one test for persistent and one for transient.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463044115", "createdAt": "2020-07-30T14:36:59Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NzU2MQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465747561", "createdAt": "2020-08-05T13:59:52Z", "author": {"login": "jeqo"}, "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDExNQ=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA0MjQ4OnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0Mzo1OVrOG5mSIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0Mzo1OVrOG5mSIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0OTI0OQ==", "bodyText": "nit: commented out code.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463049249", "createdAt": "2020-07-30T14:43:59Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java", "diffHunk": "@@ -747,6 +817,29 @@ private void givenTopicConfigs(\n     };\n   }\n \n+  private Answer<ListOffsetsResult> listTopicOffsets() {\n+    return inv -> {\n+      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n+      when(result.all()).thenReturn(KafkaFuture.completedFuture(ImmutableMap.of(\n+          new TopicPartition(\"topicA\", 0),\n+          new ListOffsetsResultInfo(100L, 0L, Optional.empty()))));\n+      return result;\n+    };\n+  }\n+\n+  private Answer<ListOffsetsResult> listTopicOffsets(final Exception e) {\n+    return inv -> {\n+      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n+      final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> f = failedFuture(e);\n+      when(result.all()).thenReturn(f);\n+//      return new ListOffsetsResult(\n+//          ImmutableMap.of(\n+//              new TopicPartition(\"topicA\", 0),\n+//              new ListOffsetsResultInfo(100L, 0L, Optional.empty())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA1MDkwOnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0NTozM1rOG5mXBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNDowOToyOVrOG8LZOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MDUwMw==", "bodyText": "Can you add tests to SupportedMethods for these new methods?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463050503", "createdAt": "2020-07-30T14:45:33Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java", "diffHunk": "@@ -78,6 +78,8 @@ private SandboxedKafkaTopicClientTest() {\n           .ignore(\"describeTopic\", String.class)\n           .ignore(\"describeTopics\", Collection.class)\n           .ignore(\"deleteTopics\", Collection.class)\n+          .ignore(\"listTopicsStartOffsets\", Collection.class)\n+          .ignore(\"listTopicsEndOffsets\", Collection.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1NDQyNw==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465754427", "createdAt": "2020-08-05T14:09:29Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java", "diffHunk": "@@ -78,6 +78,8 @@ private SandboxedKafkaTopicClientTest() {\n           .ignore(\"describeTopic\", String.class)\n           .ignore(\"describeTopics\", Collection.class)\n           .ignore(\"deleteTopics\", Collection.class)\n+          .ignore(\"listTopicsStartOffsets\", Collection.class)\n+          .ignore(\"listTopicsEndOffsets\", Collection.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MDUwMw=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA2OTMzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo0OToyOFrOG5miWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNDowOTo0M1rOG8LZ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MzQwMQ==", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. Objects.requireNonNull", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463053401", "createdAt": "2020-07-30T14:49:28Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class QueryOffsetSummary {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<ConsumerPartitionOffsets> offsets;\n+\n+  @JsonCreator\n+  public QueryOffsetSummary(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1NDYwMA==", "bodyText": "ack/", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465754600", "createdAt": "2020-08-05T14:09:43Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class QueryOffsetSummary {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<ConsumerPartitionOffsets> offsets;\n+\n+  @JsonCreator\n+  public QueryOffsetSummary(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MzQwMQ=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA3NzMwOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo1MToxM1rOG5mnYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo1MToxM1rOG5mnYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NDY5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                this.queryOffsetSummaries = Collections.unmodifiableList(\n          \n          \n            \n                this.queryOffsetSummaries = ImmutableList.copyOf(", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463054690", "createdAt": "2020-07-30T14:51:13Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "diffHunk": "@@ -88,6 +90,8 @@ public SourceDescription(\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n+    this.queryOffsetSummaries = Collections.unmodifiableList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTA4MzgzOnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo1Mjo0MVrOG5mrgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNTo1Mjo0MlrOG8P7eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NTc0NQ==", "bodyText": "nit: Can you add a new equality group where the summaries are different please?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463055745", "createdAt": "2020-07-30T14:52:41Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "diffHunk": "@@ -39,131 +39,141 @@\n     private RunningQuery query2;\n     @Mock\n     private FieldInfo fieldInfo;\n+    @Mock\n+    private QueryOffsetSummary summary;\n \n     @SuppressWarnings(\"UnstableApiUsage\")\n     @Test\n     public void shouldImplementHashCodeAndEqualsProperty() {\n         final List<RunningQuery> readQueries = Collections.singletonList(query1);\n         final List<RunningQuery> writeQueries = Collections.singletonList(query2);\n         final List<FieldInfo> fields = Collections.singletonList(fieldInfo);\n+        final List<QueryOffsetSummary> summaries = Collections.singletonList(summary);\n \n         new EqualsTester()\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING),\n+                    SOME_STRING, summaries),\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     \"diff\", Optional.of(WindowType.SESSION), readQueries, writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), ImmutableList.of(), writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, ImmutableList.of(), fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, ImmutableList.of(),\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n+            )\n+            .addEqualityGroup(\n+                new SourceDescription(\n+                    SOME_STRING, Optional.empty(), readQueries, writeQueries, fields,\n+                    SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n+                    SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n+                    SOME_STRING, ImmutableList.of())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, \"diff\",\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                      \"diff\", SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, \"diff\", SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, \"diff\",\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, \"diff\", SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     !SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, \"diff\", SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, \"diff\", SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT + 1, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT + 1,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    \"diff\")\n+                    \"diff\", summaries)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyODcyOA==", "bodyText": "ack.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465828728", "createdAt": "2020-08-05T15:52:42Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "diffHunk": "@@ -39,131 +39,141 @@\n     private RunningQuery query2;\n     @Mock\n     private FieldInfo fieldInfo;\n+    @Mock\n+    private QueryOffsetSummary summary;\n \n     @SuppressWarnings(\"UnstableApiUsage\")\n     @Test\n     public void shouldImplementHashCodeAndEqualsProperty() {\n         final List<RunningQuery> readQueries = Collections.singletonList(query1);\n         final List<RunningQuery> writeQueries = Collections.singletonList(query2);\n         final List<FieldInfo> fields = Collections.singletonList(fieldInfo);\n+        final List<QueryOffsetSummary> summaries = Collections.singletonList(summary);\n \n         new EqualsTester()\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING),\n+                    SOME_STRING, summaries),\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     \"diff\", Optional.of(WindowType.SESSION), readQueries, writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), ImmutableList.of(), writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, ImmutableList.of(), fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, ImmutableList.of(),\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n+            )\n+            .addEqualityGroup(\n+                new SourceDescription(\n+                    SOME_STRING, Optional.empty(), readQueries, writeQueries, fields,\n+                    SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n+                    SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n+                    SOME_STRING, ImmutableList.of())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, \"diff\",\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                      \"diff\", SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, \"diff\", SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, \"diff\",\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, \"diff\", SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     !SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, \"diff\", SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, \"diff\", SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT + 1, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT + 1,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    \"diff\")\n+                    \"diff\", summaries)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NTc0NQ=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTExNzQ2OnYy", "diffSide": "RIGHT", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo1OTo1OVrOG5nAdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNDo1OTo1OVrOG5nAdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2MTEwOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .orElse(-1)));\n          \n          \n            \n                          .orElse(0)));\n          \n      \n    \n    \n  \n\n??", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463061108", "createdAt": "2020-07-30T14:59:59Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -614,6 +616,33 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n+      writer().println();\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Consumer Group\", queryOffsetSummary.getGroupId()));\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Kafka topic\", queryOffsetSummary.getKafkaTopic()));\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Max lag\", queryOffsetSummary.getOffsets().stream()\n+              .mapToLong(s -> s.getLogEndOffset() - s.getConsumerOffset())\n+              .max()\n+              .orElse(-1)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTEzNTY3OnYy", "diffSide": "RIGHT", "path": "ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNTowMzo1NFrOG5nL6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNTo1MTo0MFrOG8P4vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NDA0MQ==", "bodyText": "Rather than adding a whole new test, just enhance shouldPrintTopicDescribeExtended a non-empty list in as the last parameter of the SourceDescriptionEntity constructor.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463064041", "createdAt": "2020-07-30T15:03:54Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java", "diffHunk": "@@ -1135,6 +1139,175 @@ public void testPrintExecuptionPlan() {\n     }\n   }\n \n+  @Test\n+  public void shouldPrintTopicDescribeExtendedWithConsumerOffsets() {\n+    // Given:\n+    final List<RunningQuery> readQueries = ImmutableList.of(\n+        new RunningQuery(\"read query\", ImmutableSet.of(\"sink1\"), ImmutableSet.of(\"sink1 topic\"), new QueryId(\"readId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+    final List<RunningQuery> writeQueries = ImmutableList.of(\n+        new RunningQuery(\"write query\", ImmutableSet.of(\"sink2\"), ImmutableSet.of(\"sink2 topic\"), new QueryId(\"writeId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+\n+    final KsqlEntityList entityList = new KsqlEntityList(ImmutableList.of(\n+        new SourceDescriptionEntity(\n+            \"e\",\n+            new SourceDescription(\n+                \"TestSource\",\n+                Optional.empty(),\n+                readQueries,\n+                writeQueries,\n+                buildTestSchema(SqlTypes.STRING),\n+                DataSourceType.KTABLE.getKsqlType(),\n+                \"2000-01-01\",\n+                \"stats\",\n+                \"errors\",\n+                true,\n+                \"kafka\",\n+                \"avro\",\n+                \"kadka-topic\",\n+                2, 1,\n+                \"sql statement text\",\n+                ImmutableList.of(\n+                    new QueryOffsetSummary(\n+                        \"consumer1\",\n+                        \"kadka-topic\",\n+                        ImmutableList.of(\n+                            new ConsumerPartitionOffsets(0, 100, 900, 800),\n+                            new ConsumerPartitionOffsets(1, 50, 900, 900)\n+                        ))\n+                )),\n+            Collections.emptyList()\n+        ))\n+    );\n+\n+    // When:\n+    console.printKsqlEntityList(entityList);\n+\n+    // Then:\n+    final String output = terminal.getOutputString();\n+    if (console.getOutputFormat() == OutputFormat.JSON) {\n+      assertThat(output, is(\"[ {\" + NEWLINE\n+          + \"  \\\"@type\\\" : \\\"sourceDescription\\\",\" + NEWLINE\n+          + \"  \\\"statementText\\\" : \\\"e\\\",\" + NEWLINE\n+          + \"  \\\"sourceDescription\\\" : {\" + NEWLINE\n+          + \"    \\\"name\\\" : \\\"TestSource\\\",\" + NEWLINE\n+          + \"    \\\"windowType\\\" : null,\" + NEWLINE\n+          + \"    \\\"readQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"read query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink1\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink1 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"readId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"writeQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"write query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink2\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink2 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"writeId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"fields\\\" : [ {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"ROWKEY\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"type\\\" : \\\"KEY\\\"\" + NEWLINE\n+          + \"    }, {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"f_0\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      }\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"type\\\" : \\\"TABLE\\\",\" + NEWLINE\n+          + \"    \\\"timestamp\\\" : \\\"2000-01-01\\\",\" + NEWLINE\n+          + \"    \\\"statistics\\\" : \\\"stats\\\",\" + NEWLINE\n+          + \"    \\\"errorStats\\\" : \\\"errors\\\",\" + NEWLINE\n+          + \"    \\\"extended\\\" : true,\" + NEWLINE\n+          + \"    \\\"keyFormat\\\" : \\\"kafka\\\",\" + NEWLINE\n+          + \"    \\\"valueFormat\\\" : \\\"avro\\\",\" + NEWLINE\n+          + \"    \\\"topic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"    \\\"partitions\\\" : 2,\" + NEWLINE\n+          + \"    \\\"replication\\\" : 1,\" + NEWLINE\n+          + \"    \\\"statement\\\" : \\\"sql statement text\\\",\" + NEWLINE\n+          + \"    \\\"queryOffsetSummaries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"groupId\\\" : \\\"consumer1\\\",\" + NEWLINE\n+          + \"      \\\"kafkaTopic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"      \\\"offsets\\\" : [ {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 0,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 100,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 800\" + NEWLINE\n+          + \"      }, {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 1,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 50,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 900\" + NEWLINE\n+          + \"      } ]\" + NEWLINE\n+          + \"    } ]\" + NEWLINE\n+          + \"  },\" + NEWLINE\n+          + \"  \\\"warnings\\\" : [ ]\" + NEWLINE\n+          + \"} ]\" + NEWLINE));\n+    } else {\n+      assertThat(output, is(\"\" + NEWLINE\n+          + \"Name                 : TestSource\" + NEWLINE\n+          + \"Type                 : TABLE\" + NEWLINE\n+          + \"Timestamp field      : 2000-01-01\" + NEWLINE\n+          + \"Key format           : kafka\" + NEWLINE\n+          + \"Value format         : avro\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic (partitions: 2, replication: 1)\" + NEWLINE\n+          + \"Statement            : sql statement text\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \" Field  | Type                           \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \" ROWKEY | VARCHAR(STRING)  (primary key) \" + NEWLINE\n+          + \" f_0    | VARCHAR(STRING)                \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that read from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"readId (\" + AGGREGATE_STATUS +\") : read query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that write from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"writeId (\" + AGGREGATE_STATUS + \") : write query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Local runtime statistics\" + NEWLINE\n+          + \"------------------------\" + NEWLINE\n+          + \"stats\" + NEWLINE\n+          + \"errors\" + NEWLINE\n+          + \"(Statistics of the local KSQL server interaction with the Kafka topic kadka-topic)\" + NEWLINE\n+          + NEWLINE\n+          + \"Consumer Group       : consumer1\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic\" + NEWLINE\n+          + \"Max lag              : 100\" + NEWLINE\n+          + NEWLINE\n+          + \" Partition | Start Offset | End Offset | Offset | Lag \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE\n+          + \" 0         | 100          | 900        | 800    | 100 \" + NEWLINE\n+          + \" 1         | 50           | 900        | 900    | 0   \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyODAyOQ==", "bodyText": "ack.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465828029", "createdAt": "2020-08-05T15:51:40Z", "author": {"login": "jeqo"}, "path": "ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java", "diffHunk": "@@ -1135,6 +1139,175 @@ public void testPrintExecuptionPlan() {\n     }\n   }\n \n+  @Test\n+  public void shouldPrintTopicDescribeExtendedWithConsumerOffsets() {\n+    // Given:\n+    final List<RunningQuery> readQueries = ImmutableList.of(\n+        new RunningQuery(\"read query\", ImmutableSet.of(\"sink1\"), ImmutableSet.of(\"sink1 topic\"), new QueryId(\"readId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+    final List<RunningQuery> writeQueries = ImmutableList.of(\n+        new RunningQuery(\"write query\", ImmutableSet.of(\"sink2\"), ImmutableSet.of(\"sink2 topic\"), new QueryId(\"writeId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+\n+    final KsqlEntityList entityList = new KsqlEntityList(ImmutableList.of(\n+        new SourceDescriptionEntity(\n+            \"e\",\n+            new SourceDescription(\n+                \"TestSource\",\n+                Optional.empty(),\n+                readQueries,\n+                writeQueries,\n+                buildTestSchema(SqlTypes.STRING),\n+                DataSourceType.KTABLE.getKsqlType(),\n+                \"2000-01-01\",\n+                \"stats\",\n+                \"errors\",\n+                true,\n+                \"kafka\",\n+                \"avro\",\n+                \"kadka-topic\",\n+                2, 1,\n+                \"sql statement text\",\n+                ImmutableList.of(\n+                    new QueryOffsetSummary(\n+                        \"consumer1\",\n+                        \"kadka-topic\",\n+                        ImmutableList.of(\n+                            new ConsumerPartitionOffsets(0, 100, 900, 800),\n+                            new ConsumerPartitionOffsets(1, 50, 900, 900)\n+                        ))\n+                )),\n+            Collections.emptyList()\n+        ))\n+    );\n+\n+    // When:\n+    console.printKsqlEntityList(entityList);\n+\n+    // Then:\n+    final String output = terminal.getOutputString();\n+    if (console.getOutputFormat() == OutputFormat.JSON) {\n+      assertThat(output, is(\"[ {\" + NEWLINE\n+          + \"  \\\"@type\\\" : \\\"sourceDescription\\\",\" + NEWLINE\n+          + \"  \\\"statementText\\\" : \\\"e\\\",\" + NEWLINE\n+          + \"  \\\"sourceDescription\\\" : {\" + NEWLINE\n+          + \"    \\\"name\\\" : \\\"TestSource\\\",\" + NEWLINE\n+          + \"    \\\"windowType\\\" : null,\" + NEWLINE\n+          + \"    \\\"readQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"read query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink1\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink1 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"readId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"writeQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"write query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink2\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink2 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"writeId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"fields\\\" : [ {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"ROWKEY\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"type\\\" : \\\"KEY\\\"\" + NEWLINE\n+          + \"    }, {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"f_0\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      }\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"type\\\" : \\\"TABLE\\\",\" + NEWLINE\n+          + \"    \\\"timestamp\\\" : \\\"2000-01-01\\\",\" + NEWLINE\n+          + \"    \\\"statistics\\\" : \\\"stats\\\",\" + NEWLINE\n+          + \"    \\\"errorStats\\\" : \\\"errors\\\",\" + NEWLINE\n+          + \"    \\\"extended\\\" : true,\" + NEWLINE\n+          + \"    \\\"keyFormat\\\" : \\\"kafka\\\",\" + NEWLINE\n+          + \"    \\\"valueFormat\\\" : \\\"avro\\\",\" + NEWLINE\n+          + \"    \\\"topic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"    \\\"partitions\\\" : 2,\" + NEWLINE\n+          + \"    \\\"replication\\\" : 1,\" + NEWLINE\n+          + \"    \\\"statement\\\" : \\\"sql statement text\\\",\" + NEWLINE\n+          + \"    \\\"queryOffsetSummaries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"groupId\\\" : \\\"consumer1\\\",\" + NEWLINE\n+          + \"      \\\"kafkaTopic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"      \\\"offsets\\\" : [ {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 0,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 100,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 800\" + NEWLINE\n+          + \"      }, {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 1,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 50,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 900\" + NEWLINE\n+          + \"      } ]\" + NEWLINE\n+          + \"    } ]\" + NEWLINE\n+          + \"  },\" + NEWLINE\n+          + \"  \\\"warnings\\\" : [ ]\" + NEWLINE\n+          + \"} ]\" + NEWLINE));\n+    } else {\n+      assertThat(output, is(\"\" + NEWLINE\n+          + \"Name                 : TestSource\" + NEWLINE\n+          + \"Type                 : TABLE\" + NEWLINE\n+          + \"Timestamp field      : 2000-01-01\" + NEWLINE\n+          + \"Key format           : kafka\" + NEWLINE\n+          + \"Value format         : avro\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic (partitions: 2, replication: 1)\" + NEWLINE\n+          + \"Statement            : sql statement text\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \" Field  | Type                           \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \" ROWKEY | VARCHAR(STRING)  (primary key) \" + NEWLINE\n+          + \" f_0    | VARCHAR(STRING)                \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that read from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"readId (\" + AGGREGATE_STATUS +\") : read query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that write from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"writeId (\" + AGGREGATE_STATUS + \") : write query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Local runtime statistics\" + NEWLINE\n+          + \"------------------------\" + NEWLINE\n+          + \"stats\" + NEWLINE\n+          + \"errors\" + NEWLINE\n+          + \"(Statistics of the local KSQL server interaction with the Kafka topic kadka-topic)\" + NEWLINE\n+          + NEWLINE\n+          + \"Consumer Group       : consumer1\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic\" + NEWLINE\n+          + \"Max lag              : 100\" + NEWLINE\n+          + NEWLINE\n+          + \" Partition | Start Offset | End Offset | Offset | Lag \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE\n+          + \" 0         | 100          | 900        | 800    | 100 \" + NEWLINE\n+          + \" 1         | 50           | 900        | 900    | 0   \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NDA0MQ=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTE1NTE0OnYy", "diffSide": "RIGHT", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNTowODoyNFrOG5nYEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMzowMDoyMlrOG8ImAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NzE1NQ==", "bodyText": "This inheritance smells!  A group is not a topic. So why inherit from TopicAuthorizationException?  Maybe consider inheriting from GroupAuthorizationException?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463067155", "createdAt": "2020-07-30T15:08:24Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.exception;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.kafka.common.acl.AclOperation;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n+\n+/**\n+ * Used to return custom error messages when TopicAuthorizationException returned from Kafka\n+ */\n+public class KsqlGroupAuthorizationException extends TopicAuthorizationException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODU0NA==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708544", "createdAt": "2020-08-05T13:00:22Z", "author": {"login": "jeqo"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.exception;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.kafka.common.acl.AclOperation;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n+\n+/**\n+ * Used to return custom error messages when TopicAuthorizationException returned from Kafka\n+ */\n+public class KsqlGroupAuthorizationException extends TopicAuthorizationException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NzE1NQ=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTE2ODM1OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToxMToyMlrOG5ngGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMzowMDoyMFrOG8Il5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2OTIwOA==", "bodyText": "sourceQueries and sinkQueries may be a little counterintuitive.  Can you rename sourceQueries -> readQueries, i.e. queries that read from this source and sinkQueries -> writeQueries i.e. queries that write to this source.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463069208", "createdAt": "2020-07-30T15:11:22Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +213,21 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODUxNw==", "bodyText": "ack.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708517", "createdAt": "2020-08-05T13:00:20Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +213,21 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2OTIwOA=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTIzMDA2OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNDo1NFrOG5oGUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMzowMDoxNFrOG8IlvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3ODk5Mw==", "bodyText": "Under what situations would this be null?", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463078993", "createdAt": "2020-07-30T15:24:54Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +238,86 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescription,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<QueryOffsetSummary> offsetSummaries(\n+      final KsqlConfig ksqlConfig,\n+      final ServiceContext serviceContext,\n+      final List<RunningQuery> sinkQueries\n+  ) {\n+    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n+    final Map<String, Map<TopicPartition, OffsetAndMetadata>> offsetsPerQuery =\n+        new HashMap<>(sinkQueries.size());\n+    final Map<String, Set<String>> topicsPerQuery = new HashMap<>();\n+    final Set<String> allTopics = new HashSet<>();\n+    // Get topics and offsets per running query\n+    for (RunningQuery query : sinkQueries) {\n+      final QueryId queryId = query.getId();\n+      final String applicationId =\n+          QueryApplicationId.build(ksqlConfig, true, queryId);\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+          serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+      offsetsPerQuery.put(applicationId, topicAndConsumerOffsets);\n+      final Set<String> topics = topicAndConsumerOffsets.keySet().stream()\n+          .map(TopicPartition::topic)\n+          .collect(Collectors.toSet());\n+      topicsPerQuery.put(applicationId, topics);\n+      allTopics.addAll(topics);\n+    }\n+    // Get topics descriptions and start/end offsets\n+    final Map<String, TopicDescription> sourceTopicDescriptions =\n+        serviceContext.getTopicClient().describeTopics(allTopics);\n+    final Map<TopicPartition, Long> topicAndStartOffsets =\n+        serviceContext.getTopicClient().listTopicsStartOffsets(allTopics);\n+    final Map<TopicPartition, Long> topicAndEndOffsets =\n+        serviceContext.getTopicClient().listTopicsEndOffsets(allTopics);\n+    // Build consumer offsets summary\n+    for (Entry<String, Set<String>> entry : topicsPerQuery.entrySet()) {\n+      for (String topic : entry.getValue()) {\n+        sourceConsumerOffsets.add(\n+            new QueryOffsetSummary(\n+                entry.getKey(),\n+                topic,\n+                consumerPartitionOffsets(\n+                    sourceTopicDescriptions.get(topic),\n+                    topicAndStartOffsets,\n+                    topicAndEndOffsets,\n+                    offsetsPerQuery.get(entry.getKey()))));\n+      }\n+    }\n+    return sourceConsumerOffsets;\n+  }\n+\n+  private static List<ConsumerPartitionOffsets> consumerPartitionOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, Long> topicAndStartOffsets,\n+      final Map<TopicPartition, Long> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<ConsumerPartitionOffsets> consumerPartitionOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final Long startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final Long endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      consumerPartitionOffsets.add(\n+          new ConsumerPartitionOffsets(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo,\n+              endOffsetResultInfo,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODQ3Nw==", "bodyText": "I've seen this when consumers haven't poll from a topic-partition yet, the list of offset and metadata only returns the ones that exist at the moment. Adding a short comment to clarify this.", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708477", "createdAt": "2020-08-05T13:00:14Z", "author": {"login": "jeqo"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +238,86 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescription,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<QueryOffsetSummary> offsetSummaries(\n+      final KsqlConfig ksqlConfig,\n+      final ServiceContext serviceContext,\n+      final List<RunningQuery> sinkQueries\n+  ) {\n+    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n+    final Map<String, Map<TopicPartition, OffsetAndMetadata>> offsetsPerQuery =\n+        new HashMap<>(sinkQueries.size());\n+    final Map<String, Set<String>> topicsPerQuery = new HashMap<>();\n+    final Set<String> allTopics = new HashSet<>();\n+    // Get topics and offsets per running query\n+    for (RunningQuery query : sinkQueries) {\n+      final QueryId queryId = query.getId();\n+      final String applicationId =\n+          QueryApplicationId.build(ksqlConfig, true, queryId);\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+          serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+      offsetsPerQuery.put(applicationId, topicAndConsumerOffsets);\n+      final Set<String> topics = topicAndConsumerOffsets.keySet().stream()\n+          .map(TopicPartition::topic)\n+          .collect(Collectors.toSet());\n+      topicsPerQuery.put(applicationId, topics);\n+      allTopics.addAll(topics);\n+    }\n+    // Get topics descriptions and start/end offsets\n+    final Map<String, TopicDescription> sourceTopicDescriptions =\n+        serviceContext.getTopicClient().describeTopics(allTopics);\n+    final Map<TopicPartition, Long> topicAndStartOffsets =\n+        serviceContext.getTopicClient().listTopicsStartOffsets(allTopics);\n+    final Map<TopicPartition, Long> topicAndEndOffsets =\n+        serviceContext.getTopicClient().listTopicsEndOffsets(allTopics);\n+    // Build consumer offsets summary\n+    for (Entry<String, Set<String>> entry : topicsPerQuery.entrySet()) {\n+      for (String topic : entry.getValue()) {\n+        sourceConsumerOffsets.add(\n+            new QueryOffsetSummary(\n+                entry.getKey(),\n+                topic,\n+                consumerPartitionOffsets(\n+                    sourceTopicDescriptions.get(topic),\n+                    topicAndStartOffsets,\n+                    topicAndEndOffsets,\n+                    offsetsPerQuery.get(entry.getKey()))));\n+      }\n+    }\n+    return sourceConsumerOffsets;\n+  }\n+\n+  private static List<ConsumerPartitionOffsets> consumerPartitionOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, Long> topicAndStartOffsets,\n+      final Map<TopicPartition, Long> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<ConsumerPartitionOffsets> consumerPartitionOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final Long startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final Long endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      consumerPartitionOffsets.add(\n+          new ConsumerPartitionOffsets(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo,\n+              endOffsetResultInfo,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3ODk5Mw=="}, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTIzMzY3OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNTo0M1rOG5oIkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNTo0M1rOG5oIkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3OTU3MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n          \n          \n            \n                List<QueryOffsetSummary> queryOffsetSummaries = new ArrayList<>();", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463079570", "createdAt": "2020-07-30T15:25:43Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +213,21 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription =\n+        Optional.empty();\n+    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTIzNDE4OnYy", "diffSide": "RIGHT", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNTo1MVrOG5oI6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNToyNTo1MVrOG5oI6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3OTY1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n          \n          \n            \n                final List<QueryOffsetSummary> queryOffsetSummaries = new ArrayList<>();", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463079658", "createdAt": "2020-07-30T15:25:51Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +238,86 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescription,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<QueryOffsetSummary> offsetSummaries(\n+      final KsqlConfig ksqlConfig,\n+      final ServiceContext serviceContext,\n+      final List<RunningQuery> sinkQueries\n+  ) {\n+    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMzA2OTM3OnYy", "diffSide": "RIGHT", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjo0NDo0MVrOG8x3NA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjo0NDo0MVrOG8x3NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NDY5Mg==", "bodyText": "Would be good to have some kind of heading here, e.g.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n          \n          \n            \n                writer().println(\"Consumers:\");\n          \n          \n            \n                writer().println(\"\");\n          \n          \n            \n                \n          \n          \n            \n                for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r466384692", "createdAt": "2020-08-06T12:44:41Z", "author": {"login": "big-andy-coates"}, "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -614,6 +616,33 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cafb125bb984eb42c39002b01d1251dbbe4e0191"}, "originalPosition": 49}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3287, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}