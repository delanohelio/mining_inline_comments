{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzODQ3MDM2", "number": 6813, "title": "feat: Make pull queries streamed asynchronously", "bodyText": "Description\nBefore this PR, pull queries were gathered in memory as a TableRows object containing a List<List<?>> rows across all of the endpoints where pull queries are exposed.\nThis change adds the class PullQueryQueue, which is a queue of result rows meant to decouple the producers and consumers of row data, allowing for producer calls that are partially complete to be enqueued across both local and remote sources of data.  This largely follows the methodology used by push queries.  This allows:\n\nFetches for large batches of data to begin immediately returning results rather than waiting until each batch from each partition is complete before returning the first row.\nFetches for large amounts of data being that may be prohibitively expensive or not possible to hold it all in memory using the old methodology, whereas this change keeps relatively few rows in memory at a time and can apply back-pressure to the producers if the queue is filled.\nQueries like table scans and range queries, which may return many more rows than past pull queries allowed for.\n\nSpecifically on each end of the queue:\n\nNew rows are produced and enqueued by PullPhysicalPlan if the request is being handled locally or HARouting if the request must be forwarded to another node.\nRows are consumed by the request thread of the endpoint.\n\nFor each of the endpoints:\n\nStreamedQueryResource: This uses chunked encoding responses using the class PullQueryStreamWriter which is a StreamingOutput response type.  This class periodically reads from the queue and writes a chunk to the response.\nQueryEndpoint: This uses a KsqlPullQueryHandle with the existing BlockingQueryPublisher to connect a publisher to the queued data.\nPullQueryPublisher: This uses PullQuerySubscription, a new PollingSubscription rather than the former block of logic that fed raw rows to the subscriber.\n\nTesting done\nRan unit and integration tests.  Also manually experimented with batches of rows being streamed back to the user by introducing artificial delays.\nReviewer checklist\n\n Ensure docs are updated if necessary. (eg. if a user visible feature is being added or changed).\n Ensure relevant issues are linked (description should include text like \"Fixes #\")", "createdAt": "2020-12-22T03:46:44Z", "url": "https://github.com/confluentinc/ksql/pull/6813", "merged": true, "mergeCommit": {"oid": "b69e3f8e459c3d0a055ec0738ae71e58bda7b28c"}, "closed": true, "closedAt": "2021-01-28T00:02:56Z", "author": {"login": "AlanConfluent"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdv10REAFqTU2NzUzNzAyMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABd0WlNVgH2gAyNTQzODQ3MDM2OmEyNWM3YTA4NzM4MTYyOWQ2ZjQzZjFkYmRkZjAwYjQ1M2U1ZjllOTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3NTM3MDIx", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-567537021", "createdAt": "2021-01-13T19:10:06Z", "commit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QxOToxMDowNlrOIS-PCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QyMDozMjowNFrOITBAYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc2NDkzOQ==", "bodyText": "what's the alternative to starting immediately? might be good to add some details here", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556764939", "createdAt": "2021-01-13T19:10:06Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/KsqlExecutionContext.java", "diffHunk": "@@ -145,18 +144,18 @@ TransientQueryMetadata executeQuery(\n    * plan. The physical plan is then traversed for every row in the state store.\n    * @param serviceContext The service context to execute the query in\n    * @param statement The pull query\n-   * @param routingFilterFactory The filters used to route requests for HA routing\n    * @param routingOptions Configuration parameters used for routing requests\n    * @param pullQueryMetrics JMX metrics\n+   * @param startImmediately Whether to start the pull query immediately", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3ODk4Ng==", "bodyText": "nit: alignment", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556778986", "createdAt": "2021-01-13T19:34:45Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/HARouting.java", "diffHunk": "@@ -228,12 +247,13 @@ PullQueryResult routeQuery(\n         Optional<PullQueryExecutorMetrics> pullQueryMetrics,\n          PullPhysicalPlan pullPhysicalPlan,\n          LogicalSchema outputSchema,\n-         QueryId queryId\n+         QueryId queryId,\n+        PullQueryQueue pullQueryQueue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NTM5Nw==", "bodyText": "when do we expect this to happen?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556785397", "createdAt": "2021-01-13T19:46:04Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullPhysicalPlan.java", "diffHunk": "@@ -57,22 +58,25 @@ public PullPhysicalPlan(\n         dataSourceOperator, \"dataSourceOperator\");\n   }\n \n-  public List<List<?>> execute(\n-      final List<KsqlPartitionLocation> locations) {\n+  public void execute(\n+      final List<KsqlPartitionLocation> locations,\n+      final PullQueryQueue pullQueryQueue,\n+      final BiFunction<List<?>, LogicalSchema, PullQueryRow> rowFactory) {\n \n     // We only know at runtime which partitions to get from which node.\n     // That's why we need to set this explicitly for the dataSource operators\n     dataSourceOperator.setPartitionLocations(locations);\n \n     open();\n-    final List<List<?>> localResult = new ArrayList<>();\n-    List<?> row = null;\n+    List<?> row;\n     while ((row = (List<?>)next()) != null) {\n-      localResult.add(row);\n+      if (pullQueryQueue.isClosed()) {\n+        // If the queue has been closed, we stop adding rows and cleanup.\n+        break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NjE2Nw==", "bodyText": "with this asynchronous model, how would we handle operators that require sorting? e.g. SELECT * FROM foo ORDER BY date\nOne option might be to block at the operator node itself, but I'm not sure what the canonical way of handling this is.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556786167", "createdAt": "2021-01-13T19:47:19Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullPhysicalPlan.java", "diffHunk": "@@ -57,22 +58,25 @@ public PullPhysicalPlan(\n         dataSourceOperator, \"dataSourceOperator\");\n   }\n \n-  public List<List<?>> execute(\n-      final List<KsqlPartitionLocation> locations) {\n+  public void execute(\n+      final List<KsqlPartitionLocation> locations,\n+      final PullQueryQueue pullQueryQueue,\n+      final BiFunction<List<?>, LogicalSchema, PullQueryRow> rowFactory) {\n \n     // We only know at runtime which partitions to get from which node.\n     // That's why we need to set this explicitly for the dataSource operators\n     dataSourceOperator.setPartitionLocations(locations);\n \n     open();\n-    final List<List<?>> localResult = new ArrayList<>();\n-    List<?> row = null;\n+    List<?> row;\n     while ((row = (List<?>)next()) != null) {\n-      localResult.add(row);\n+      if (pullQueryQueue.isClosed()) {\n+        // If the queue has been closed, we stop adding rows and cleanup.\n+        break;\n+      }\n+      pullQueryQueue.acceptRow(rowFactory.apply(row, schema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NzM3MQ==", "bodyText": "nit: might make sense to have this be a named interface just for readability (e.g. PullQueryQueuePopulator) - it's tough for me to understand what runner does just by looking at this class.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556787371", "createdAt": "2021-01-13T19:49:22Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullQueryResult.java", "diffHunk": "@@ -15,30 +15,39 @@\n \n package io.confluent.ksql.physical.pull;\n \n-import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.internal.PullQueryExecutorMetrics;\n+import io.confluent.ksql.query.PullQueryQueue;\n import io.confluent.ksql.query.QueryId;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n-import java.util.List;\n import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Consumer;\n+import java.util.function.Supplier;\n \n public class PullQueryResult {\n \n-  private final List<List<?>> tableRows;\n-  private final Optional<List<KsqlNode>> sourceNodes;\n   private final LogicalSchema schema;\n+  private final Supplier<CompletableFuture<Void>> runner;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MDEzOA==", "bodyText": "why do we need this field? can't we just directly set the onException/onCompletion on the underlying running future? (are you just using this future as a holder of the callbacks? if so, it might make sense to just hold them separately as fields)", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556790138", "createdAt": "2021-01-13T19:54:13Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullQueryResult.java", "diffHunk": "@@ -15,30 +15,39 @@\n \n package io.confluent.ksql.physical.pull;\n \n-import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.internal.PullQueryExecutorMetrics;\n+import io.confluent.ksql.query.PullQueryQueue;\n import io.confluent.ksql.query.QueryId;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n-import java.util.List;\n import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Consumer;\n+import java.util.function.Supplier;\n \n public class PullQueryResult {\n \n-  private final List<List<?>> tableRows;\n-  private final Optional<List<KsqlNode>> sourceNodes;\n   private final LogicalSchema schema;\n+  private final Supplier<CompletableFuture<Void>> runner;\n   private final QueryId queryId;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Optional<PullQueryExecutorMetrics> pullQueryMetrics;\n+\n+  private CompletableFuture<Void> future = new CompletableFuture<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MDQ5Mg==", "bodyText": "I think detailed javadocs would be useful here of the overall architecture of where this is used", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556790492", "createdAt": "2021-01-13T19:54:48Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MTM0Mg==", "bodyText": "nit: why not just have a method in this class?\nprivate static KeyValue<List<?>> pullQueryToKeyValue(final PullQueryRow) { ... }\nI've found that makes the stack traces a little nicer than lambdas (ever so slightly)", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556791342", "createdAt": "2021-01-13T19:56:16Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NTUyOA==", "bodyText": "LinkedBlockingQueue is pretty bad for GC characteristics since a new node is created on each enqueue and the amount of time it lives is likely to have it promoted into survivor space. Any reason not to use ArrayBlockingQueue since we know the size ahead of time?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556795528", "createdAt": "2021-01-13T20:03:21Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NTgxMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private LimitQueueCallback callback;\n          \n          \n            \n              private LimitQueueCallback onLimitExceededCallback;", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556795810", "createdAt": "2021-01-13T20:03:52Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NzMxNg==", "bodyText": "nit: if we implemented PQ_ROW_TO_KEY_VALUE to handle nulls (return null if the input is null) then we can avoid the Optional wrapping here and just return PQ_ROW_TO_KEY_VALUE.apply(rowQueue.poll(timeout, unit))", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556797316", "createdAt": "2021-01-13T20:06:48Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5ODExOA==", "bodyText": "can we avoid iterating all the elements in the queue twice? seems unnecessary", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556798118", "createdAt": "2021-01-13T20:08:14Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5ODcxMQ==", "bodyText": "what happens when we implement a LIMIT clause on pull queries (such as table scans)? Would we want to call limitReached twice or would that just be a physical plan node that just calls close on the underlying queue?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556798711", "createdAt": "2021-01-13T20:09:21Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMDQ4OQ==", "bodyText": "I think this should return a boolean accepted so that we can also short-circuit in accetpTableRows the first time !callback.shouldQueue()", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556800489", "createdAt": "2021-01-13T20:12:43Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.\n+    limitHandler.limitReached();\n+  }\n+\n+  public boolean isClosed() {\n+    return closed;\n+  }\n+\n+  public void acceptTableRows(final List<PullQueryRow> tableRows) {\n+    if (tableRows == null) {\n+      return;\n+    }\n+    for (PullQueryRow row : tableRows) {\n+      acceptRow(row);\n+    }\n+  }\n+\n+  public void acceptRow(final PullQueryRow row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMTE4Mg==", "bodyText": "out of interest, why did you make this change?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556801182", "createdAt": "2021-01-13T20:14:00Z", "author": {"login": "agavra"}, "path": "ksqldb-functional-tests/src/test/resources/rest-query-validation-tests/pull-queries-against-materialized-aggregates.json", "diffHunk": "@@ -1797,22 +1797,23 @@\n       \"statements\": [\n         \"CREATE STREAM INPUT (ID DOUBLE KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n         \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT WINDOW TUMBLING(SIZE 1 SECOND) GROUP BY ID;\",\n-        \"SELECT * FROM AGGREGATE WHERE ID IN (10.1, 8.1);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10.5, 8.5);\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMjg1Mw==", "bodyText": "I'm confused. Why is this necessary? Why would we otherwise have to enqueue another row?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556802853", "createdAt": "2021-01-13T20:17:17Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/api/impl/BlockingQueryPublisher.java", "diffHunk": "@@ -59,13 +60,23 @@ public BlockingQueryPublisher(final Context ctx,\n     this.workerExecutor = Objects.requireNonNull(workerExecutor);\n   }\n \n-  public void setQueryHandle(final PushQueryHandle queryHandle) {\n+  public void setQueryHandle(final QueryHandle queryHandle, final boolean isPullQuery) {\n     this.columnNames = queryHandle.getColumnNames();\n     this.columnTypes = queryHandle.getColumnTypes();\n     this.queue = queryHandle.getQueue();\n+    this.isPullQuery = isPullQuery;\n     this.queue.setQueuedCallback(this::maybeSend);\n-    this.queue.setLimitHandler(() -> complete = true);\n+    this.queue.setLimitHandler(() -> {\n+      complete = true;\n+      // This allows us to hit the limit without having to queue one last row\n+      if (queue.isEmpty()) {\n+        ctx.runOnContext(v -> sendComplete());\n+      } else {\n+        ctx.runOnContext(v -> doSend());\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwOTE1Nw==", "bodyText": "previously we were calling this inside the Subscription#request method, now we're doing it before calling the subscriber. I guess that this is intentional because of the new asynchronous nature of pull queries? Just wanted to confirm that I understand correctly (and to confirm that it is OK to do this here in the subscribe method instead of on the first poll call)", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556809157", "createdAt": "2021-01-13T20:29:48Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryPublisher.java", "diffHunk": "@@ -79,98 +78,68 @@\n \n   @Override\n   public synchronized void subscribe(final Subscriber<Collection<StreamedRow>> subscriber) {\n-    final PullQuerySubscription subscription = new PullQuerySubscription(\n-        subscriber,\n-        () -> {\n-          final RoutingOptions routingOptions = new PullQueryConfigRoutingOptions(\n-              query.getSessionConfig().getConfig(false),\n-              query.getSessionConfig().getOverrides(),\n-              ImmutableMap.of()\n-          );\n-\n-          PullQueryExecutionUtil.checkRateLimit(rateLimiter);\n-\n-          final PullQueryResult result = ksqlEngine.executePullQuery(\n-              serviceContext,\n-              query,\n-              routing,\n-              routingFilterFactory,\n-              routingOptions,\n-              pullQueryMetrics\n-          );\n-\n-          pullQueryMetrics.ifPresent(pullQueryExecutorMetrics -> pullQueryExecutorMetrics\n-              .recordLatency(startTimeNanos));\n-          return result;\n-        },\n-        query\n+    final RoutingOptions routingOptions = new PullQueryConfigRoutingOptions(\n+        query.getSessionConfig().getConfig(false),\n+        query.getSessionConfig().getOverrides(),\n+        ImmutableMap.of()\n+    );\n+\n+    PullQueryExecutionUtil.checkRateLimit(rateLimiter);\n+\n+    final PullQueryResult result = ksqlEngine.executePullQuery(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgxMDMzOQ==", "bodyText": "nit: most of the code uses:\n  // CHECKSTYLE_RULES.OFF: CyclomaticComplexity\n\njust around the specific method that breaks the rule", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556810339", "createdAt": "2021-01-13T20:32:04Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3NzcxNTkw", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-567771590", "createdAt": "2021-01-14T00:44:30Z", "commit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo0NDozMFrOITK0zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQwMDo1Mzo0OVrOITLAyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MTIxMw==", "bodyText": "I'm wondering how this works from a client perspective. It looks like we'd be flushing incomplete JSON, right? Is that what was happening previously for pull queries? Would clients know how to handle that properly?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556971213", "createdAt": "2021-01-14T00:44:30Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      StringBuilder sb = new StringBuilder();\n+      PullQueryRow head = null;\n+      long lastFlush = clock.millis();\n+\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      sb.append(\"[\").append(writeValueAsString(header));\n+\n+      while (!connectionClosed && !pullQueryQueue.isClosed() && !limitReached) {\n+\n+        final PullQueryRow row = pullQueryQueue.pollRow(\n+            Math.min(disconnectCheckInterval, MAX_FLUSH_MS),\n+            TimeUnit.MILLISECONDS\n+        );\n+        if (row != null) {\n+          // We keep track of a head of the queue so that we know when writing the current row if\n+          // there a next.  This is to write proper json\n+          final PullQueryRow toProcess = head;\n+          head = row;\n+          if (toProcess == null) {\n+            continue;\n+          }\n+          writeRow(toProcess, head, sb);\n+          if (sb.length() >= FLUSH_SIZE_BYTES || (clock.millis() - lastFlush) >= MAX_FLUSH_MS) {\n+            output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+            output.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MzA3Ng==", "bodyText": "there's a lot of different levels going on and wrapping my head around passing around different heads/string buffers and flushing in the middle (etc...) is a struggle. At a minimum, I think we can do with inline comments and javadocing (including the private methods), though I urge you to take another stab and see if you can refactor this to make it cleaner and reduce the cyclomatic complexity (this is one of those times where I think checkstyle got it right, it needs improvement).\nThere's a lot of complexity in creating proper JSON (commas in the right places, the header row being first, etc...). I wonder if there's a library that we can use to make that simpler.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556973076", "createdAt": "2021-01-14T00:50:12Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3NDI4MQ==", "bodyText": "did you mean to check this in?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r556974281", "createdAt": "2021-01-14T00:53:49Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/api/ApiTest.java", "diffHunk": "@@ -110,6 +115,53 @@ public void shouldExecutePullQuery() throws Exception {\n     assertThat(queryId, is(nullValue()));\n   }\n \n+  public static void main(String[] args) throws ExecutionException, InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY4NzM3OTU1", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-568737955", "createdAt": "2021-01-15T00:40:16Z", "commit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQwMDo0MDoxNlrOIT81xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQwMToyMjoyOVrOIT9p3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5MDY2Mg==", "bodyText": "is there any chance that the offer timeout of 1ms will cause any flakiness in the test? I guess we never expect to hit the limit except for in the test where we intentionally expect to hit the limit... but I'll float the question anyway", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557790662", "createdAt": "2021-01-15T00:40:16Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/query/PullQueryQueueTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+package io.confluent.ksql.query;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.IntStream;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryQueueTest {\n+  private static final int QUEUE_SIZE = 5;\n+\n+  private static final PullQueryRow VAL_ONE = mock(PullQueryRow.class);\n+  private static final PullQueryRow VAL_TWO = mock(PullQueryRow.class);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.seconds(10);\n+\n+  @Mock\n+  private LimitHandler limitHandler;\n+  private PullQueryQueue queue;\n+  private ScheduledExecutorService executorService;\n+\n+  @Before\n+  public void setUp() {\n+    givenQueue();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (executorService != null) {\n+      executorService.shutdownNow();\n+    }\n+  }\n+\n+  @Test\n+  public void shouldQueue() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then:\n+    assertThat(drainValues(), contains(VAL_ONE, VAL_TWO));\n+  }\n+\n+  @Test\n+  public void shouldQueueNullKey() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+\n+    // Then:\n+    assertThat(queue.size(), is(1));\n+    assertThat(drainValues(), contains(VAL_ONE));\n+  }\n+\n+  @Test\n+  public void shouldQueueUntilClosed() {\n+    // When:\n+    IntStream.range(0, QUEUE_SIZE)\n+        .forEach(idx -> {\n+          queue.acceptRow(VAL_ONE);\n+          if (idx == 2) {\n+            queue.close();\n+          }\n+        });\n+\n+    // Then:\n+    assertThat(queue.size(), is(3));\n+  }\n+\n+  @Test\n+  public void shouldPoll() throws Exception {\n+    // Given:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // When:\n+    final PullQueryRow result1 = queue.pollRow(1, TimeUnit.SECONDS);\n+    final PullQueryRow result2 = queue.pollRow(1, TimeUnit.SECONDS);\n+    final PullQueryRow result3 = queue.pollRow(1, TimeUnit.MICROSECONDS);\n+\n+    // Then:\n+    assertThat(result1, is(VAL_ONE));\n+    assertThat(result2, is(VAL_TWO));\n+    assertThat(result3, is(nullValue()));\n+  }\n+\n+  @Test\n+  public void shouldCallLimitHandlerOnClose() {\n+    // When:\n+    queue.close();\n+\n+    // Then:\n+    verify(limitHandler).limitReached();\n+  }\n+\n+\n+  @Test\n+  public void shouldBlockOnProduceOnceQueueLimitReachedAndUnblockOnClose() {\n+    // Given:\n+    givenQueue();\n+\n+    IntStream.range(0, QUEUE_SIZE)\n+        .forEach(idx -> queue.acceptRow(VAL_ONE));\n+\n+    givenWillCloseQueueAsync();\n+\n+    // When:\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then: did not block and:\n+    assertThat(queue.size(), is(QUEUE_SIZE));\n+  }\n+\n+  private void givenWillCloseQueueAsync() {\n+    executorService = Executors.newSingleThreadScheduledExecutor();\n+    executorService.schedule(queue::close, 200, TimeUnit.MILLISECONDS);\n+  }\n+\n+  private void givenQueue() {\n+    queue = new PullQueryQueue(QUEUE_SIZE, 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5MDkxMw==", "bodyText": "I'm not entirely sure what this test does? we're still passing in VAL_ONE?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557790913", "createdAt": "2021-01-15T00:41:04Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/query/PullQueryQueueTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+package io.confluent.ksql.query;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.IntStream;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryQueueTest {\n+  private static final int QUEUE_SIZE = 5;\n+\n+  private static final PullQueryRow VAL_ONE = mock(PullQueryRow.class);\n+  private static final PullQueryRow VAL_TWO = mock(PullQueryRow.class);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.seconds(10);\n+\n+  @Mock\n+  private LimitHandler limitHandler;\n+  private PullQueryQueue queue;\n+  private ScheduledExecutorService executorService;\n+\n+  @Before\n+  public void setUp() {\n+    givenQueue();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (executorService != null) {\n+      executorService.shutdownNow();\n+    }\n+  }\n+\n+  @Test\n+  public void shouldQueue() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then:\n+    assertThat(drainValues(), contains(VAL_ONE, VAL_TWO));\n+  }\n+\n+  @Test\n+  public void shouldQueueNullKey() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5NTI1MQ==", "bodyText": "unless I missed it, we don't have unit tests testing this or callback.onQueued", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557795251", "createdAt": "2021-01-15T00:54:59Z", "author": {"login": "agavra"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.\n+    limitHandler.limitReached();\n+  }\n+\n+  public boolean isClosed() {\n+    return closed;\n+  }\n+\n+  public void acceptTableRows(final List<PullQueryRow> tableRows) {\n+    if (tableRows == null) {\n+      return;\n+    }\n+    for (PullQueryRow row : tableRows) {\n+      acceptRow(row);\n+    }\n+  }\n+\n+  public void acceptRow(final PullQueryRow row) {\n+    try {\n+      if (row == null) {\n+        return;\n+      }\n+\n+      if (!callback.shouldQueue()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5NjMzMg==", "bodyText": "I wonder if it makes sense to have a TestPullQueryQueue because I see this pattern come up in the tests quite often (of mocking what gets returned on which calls). It'll make things a little bit easier to write tests going forward", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557796332", "createdAt": "2021-01-15T00:58:03Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryPublisherTest.java", "diffHunk": "@@ -103,26 +118,40 @@ public void setUp() {\n     publisher = new PullQueryPublisher(\n         engine,\n         serviceContext,\n+        exec,\n         statement,\n         Optional.empty(),\n         TIME_NANOS,\n         routingFilterFactory,\n         create(1),\n         haRouting);\n \n-\n-    when(statement.getStatementText()).thenReturn(\"\");\n     when(statement.getSessionConfig()).thenReturn(sessionConfig);\n     when(sessionConfig.getConfig(false)).thenReturn(ksqlConfig);\n     when(sessionConfig.getOverrides()).thenReturn(ImmutableMap.of());\n-    when(pullQueryResult.getQueryId()).thenReturn(queryId);\n     when(pullQueryResult.getSchema()).thenReturn(PULL_SCHEMA);\n-    when(pullQueryResult.getTableRows()).thenReturn(tableRows);\n-    when(pullQueryResult.getSourceNodes()).thenReturn(Optional.empty());\n-    when(engine.executePullQuery(any(), any(), any(), any(), any(), any()))\n+    when(pullQueryResult.getPullQueryQueue()).thenReturn(pullQueryQueue);\n+    doNothing().when(pullQueryResult).onException(onErrorCaptor.capture());\n+    doNothing().when(pullQueryResult).onCompletion(completeCaptor.capture());\n+    int[] times = new int[1];\n+    doAnswer(inv -> {\n+      Collection<? super KeyValue<List<?>, GenericRow>> c = inv.getArgument(0);\n+      if (times[0] == 0) {\n+        c.add(KV1);\n+      } else if (times[0] == 1) {\n+        c.add(KV2);\n+        completeCaptor.getValue().accept(null);\n+      }\n+      times[0]++;\n+      return null;\n+    }).when(pullQueryQueue).drainTo(any());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5ODE0MA==", "bodyText": "just wondering, how do we handle content that contains the line separator e.g. a column that has foo\\nbar as its contents? will that throw anything off? might be worth testing that", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557798140", "createdAt": "2021-01-15T01:04:14Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      StringBuilder sb = new StringBuilder();\n+      PullQueryRow head = null;\n+      long lastFlush = clock.millis();\n+\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      sb.append(\"[\").append(writeValueAsString(header));\n+\n+      while (!connectionClosed && !pullQueryQueue.isClosed() && !limitReached) {\n+\n+        final PullQueryRow row = pullQueryQueue.pollRow(\n+            Math.min(disconnectCheckInterval, MAX_FLUSH_MS),\n+            TimeUnit.MILLISECONDS\n+        );\n+        if (row != null) {\n+          // We keep track of a head of the queue so that we know when writing the current row if\n+          // there a next.  This is to write proper json\n+          final PullQueryRow toProcess = head;\n+          head = row;\n+          if (toProcess == null) {\n+            continue;\n+          }\n+          writeRow(toProcess, head, sb);\n+          if (sb.length() >= FLUSH_SIZE_BYTES || (clock.millis() - lastFlush) >= MAX_FLUSH_MS) {\n+            output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+            output.flush();\n+            sb = new StringBuilder();\n+            lastFlush = clock.millis();\n+          }\n+        }\n+        drainAndThrowOnError(head, output, sb);\n+      }\n+      if (!pullQueryQueue.isEmpty() || head != null) {\n+        drain(head, sb);\n+      }\n+      sb.append(\"]\");\n+      if (sb.length() > 0) {\n+        output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    } catch (InterruptedException e) {\n+      LOG.warn(\"Interrupted while writing to connection stream\");\n+    } catch (Throwable e) {\n+      LOG.error(\"Exception occurred while writing to connection stream: \", e);\n+      outputException(output, e);\n+    }\n+  }\n+\n+  private void writeRow(\n+      final PullQueryRow row,\n+      final PullQueryRow head,\n+      final StringBuilder sb\n+  ) {\n+    // Send for a comma after the header\n+    if (!sentAtLeastOneRow) {\n+      sb.append(\",\").append(System.lineSeparator());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzgwMzc4Mg==", "bodyText": "should we have negative tests for this and the above? i.e. does not drain if limit not reached or pull query not closed?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557803782", "createdAt": "2021-01-15T01:21:45Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriterTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.hasItems;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.query.QueryId;\n+import io.confluent.ksql.rest.ApiJsonMapper;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+import org.mockito.stubbing.Answer;\n+\n+@SuppressWarnings(\"unchecked\")\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryStreamWriterTest {\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"a\"), SqlTypes.STRING)\n+      .build();\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(30, TimeUnit.SECONDS)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+  @Mock\n+  private PullQueryResult pullQueryResult;\n+  @Mock\n+  private PullQueryQueue pullQueryQueue;\n+  @Mock\n+  private Clock clock;\n+\n+  @Captor\n+  private ArgumentCaptor<Consumer<Throwable>> throwableConsumerCapture;\n+  @Captor\n+  private ArgumentCaptor<Consumer<Void>> limitCapture;\n+\n+  private ByteArrayOutputStream out;\n+\n+\n+  private PullQueryStreamWriter writer;\n+\n+  @Before\n+  public void setUp() {\n+    when(pullQueryResult.getQueryId()).thenReturn(new QueryId(\"Query id\"));\n+    when(pullQueryResult.getSchema()).thenReturn(SCHEMA);\n+    doNothing().when(pullQueryResult).onCompletion(limitCapture.capture());\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    writer = new PullQueryStreamWriter(pullQueryResult, 1000, ApiJsonMapper.INSTANCE.get(),\n+        pullQueryQueue, clock, new CompletableFuture<>());\n+\n+    out = new ByteArrayOutputStream();\n+  }\n+\n+  @Test\n+  public void shouldWriteAnyPendingRowsBeforeReportingException() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    givenUncaughtException(new KsqlException(\"Server went Boom\"));\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\"),\n+        containsString(\"Server went Boom\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteNoRows() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows())\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfPullQueryQueueIsClosed() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfLimitReached() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzgwMzk5OQ==", "bodyText": "can we add a test for valid JSON? (,s in the right place, pairs of [] etc...)", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r557803999", "createdAt": "2021-01-15T01:22:29Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriterTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.hasItems;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.query.QueryId;\n+import io.confluent.ksql.rest.ApiJsonMapper;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+import org.mockito.stubbing.Answer;\n+\n+@SuppressWarnings(\"unchecked\")\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryStreamWriterTest {\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"a\"), SqlTypes.STRING)\n+      .build();\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(30, TimeUnit.SECONDS)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+  @Mock\n+  private PullQueryResult pullQueryResult;\n+  @Mock\n+  private PullQueryQueue pullQueryQueue;\n+  @Mock\n+  private Clock clock;\n+\n+  @Captor\n+  private ArgumentCaptor<Consumer<Throwable>> throwableConsumerCapture;\n+  @Captor\n+  private ArgumentCaptor<Consumer<Void>> limitCapture;\n+\n+  private ByteArrayOutputStream out;\n+\n+\n+  private PullQueryStreamWriter writer;\n+\n+  @Before\n+  public void setUp() {\n+    when(pullQueryResult.getQueryId()).thenReturn(new QueryId(\"Query id\"));\n+    when(pullQueryResult.getSchema()).thenReturn(SCHEMA);\n+    doNothing().when(pullQueryResult).onCompletion(limitCapture.capture());\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    writer = new PullQueryStreamWriter(pullQueryResult, 1000, ApiJsonMapper.INSTANCE.get(),\n+        pullQueryQueue, clock, new CompletableFuture<>());\n+\n+    out = new ByteArrayOutputStream();\n+  }\n+\n+  @Test\n+  public void shouldWriteAnyPendingRowsBeforeReportingException() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    givenUncaughtException(new KsqlException(\"Server went Boom\"));\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\"),\n+        containsString(\"Server went Boom\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteNoRows() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows())\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfPullQueryQueueIsClosed() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfLimitReached() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    limitCapture.getValue().accept(null);\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, hasItems(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+\n+  @Test\n+  public void shouldWriteOneAndClose() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(true);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(new PullQueryRow(ImmutableList.of(\"Row1\"), SCHEMA, Optional.empty()))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteTwoAndClose() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(true);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(streamRow(\"Row1\"))\n+        .thenReturn(streamRow(\"Row2\"))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteTwoAndCloseWithMoreOneQueue() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(streamRow(\"Row1\"))\n+        .thenReturn(streamRow(\"Row2\"))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+    doAnswer(streamRows(\"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")\n+    ));\n+  }\n+\n+  private void givenUncaughtException(final KsqlException e) {\n+    verify(pullQueryResult).onException(throwableConsumerCapture.capture());\n+    throwableConsumerCapture.getValue().accept(e);\n+  }\n+\n+  private static Answer<Void> streamRows(final Object... rows) {\n+    return inv -> {\n+      final Collection<PullQueryRow> output = inv.getArgument(0);\n+\n+      Arrays.stream(rows)\n+          .map(row -> new PullQueryRow(ImmutableList.of(row), SCHEMA, Optional.empty()))\n+          .forEach(output::add);\n+\n+      return null;\n+    };\n+  }\n+\n+  private static PullQueryRow streamRow(final Object row) {\n+    return new PullQueryRow(ImmutableList.of(row), SCHEMA, Optional.empty());\n+  }\n+\n+  private static List<String> getOutput(final ByteArrayOutputStream out) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 262}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5NzU5NDkw", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-569759490", "createdAt": "2021-01-15T23:15:11Z", "commit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoxNToxMVrOIUxIig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwNDo0OTozNVrOIU62RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY0NzQzNA==", "bodyText": "The alternative is just to start it after the fact with PullQueryResult.start.  Added more comments.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558647434", "createdAt": "2021-01-15T23:15:11Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/KsqlExecutionContext.java", "diffHunk": "@@ -145,18 +144,18 @@ TransientQueryMetadata executeQuery(\n    * plan. The physical plan is then traversed for every row in the state store.\n    * @param serviceContext The service context to execute the query in\n    * @param statement The pull query\n-   * @param routingFilterFactory The filters used to route requests for HA routing\n    * @param routingOptions Configuration parameters used for routing requests\n    * @param pullQueryMetrics JMX metrics\n+   * @param startImmediately Whether to start the pull query immediately", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc2NDkzOQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1MDYyNA==", "bodyText": "Fixed.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558650624", "createdAt": "2021-01-15T23:18:13Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/HARouting.java", "diffHunk": "@@ -228,12 +247,13 @@ PullQueryResult routeQuery(\n         Optional<PullQueryExecutorMetrics> pullQueryMetrics,\n          PullPhysicalPlan pullPhysicalPlan,\n          LogicalSchema outputSchema,\n-         QueryId queryId\n+         QueryId queryId,\n+        PullQueryQueue pullQueryQueue", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3ODk4Ng=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY2MDIyNw==", "bodyText": "When one of the streaming interfaces (web socket, new streaming api) is used, the client can kill their connection and then it results in this being closed.  In that case, there is no point in adding more rows.\nI took this cue (pun intended?) from push queries where it's much more likely to happen because they are unending and that's the only way to end the streaming of the data.  Here, it's finite, but could theoretically be quite large if you're doing a table scan.  If you have a slow connection and are scanning a lot of data, you might be able to trigger this.\nLet me add more comments and logging here since it's a bit of an exceptional situation anyway.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558660227", "createdAt": "2021-01-15T23:27:54Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullPhysicalPlan.java", "diffHunk": "@@ -57,22 +58,25 @@ public PullPhysicalPlan(\n         dataSourceOperator, \"dataSourceOperator\");\n   }\n \n-  public List<List<?>> execute(\n-      final List<KsqlPartitionLocation> locations) {\n+  public void execute(\n+      final List<KsqlPartitionLocation> locations,\n+      final PullQueryQueue pullQueryQueue,\n+      final BiFunction<List<?>, LogicalSchema, PullQueryRow> rowFactory) {\n \n     // We only know at runtime which partitions to get from which node.\n     // That's why we need to set this explicitly for the dataSource operators\n     dataSourceOperator.setPartitionLocations(locations);\n \n     open();\n-    final List<List<?>> localResult = new ArrayList<>();\n-    List<?> row = null;\n+    List<?> row;\n     while ((row = (List<?>)next()) != null) {\n-      localResult.add(row);\n+      if (pullQueryQueue.isClosed()) {\n+        // If the queue has been closed, we stop adding rows and cleanup.\n+        break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NTM5Nw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY4MTcyNA==", "bodyText": "I think what you're describing is likely how this would be done.  Obviously any operation that cannot be done on a per-row basis complicates streaming since it requires caching many rows from the lower layer before returning even a single one.\nProbably the easiest thing to do would be to allow for sorting up to N entries (or M bytes of memory) and if that's hit, throw an error.  If there was sufficient interest, we might consider doing something more complex.  You can do disk-based sorts if memory is limited.  Conventional DBs will try to consider exactly that and I believe try to sort in memory up to a point before spilling to disk and doing a kind of merge-sort from disk.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558681724", "createdAt": "2021-01-15T23:49:01Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullPhysicalPlan.java", "diffHunk": "@@ -57,22 +58,25 @@ public PullPhysicalPlan(\n         dataSourceOperator, \"dataSourceOperator\");\n   }\n \n-  public List<List<?>> execute(\n-      final List<KsqlPartitionLocation> locations) {\n+  public void execute(\n+      final List<KsqlPartitionLocation> locations,\n+      final PullQueryQueue pullQueryQueue,\n+      final BiFunction<List<?>, LogicalSchema, PullQueryRow> rowFactory) {\n \n     // We only know at runtime which partitions to get from which node.\n     // That's why we need to set this explicitly for the dataSource operators\n     dataSourceOperator.setPartitionLocations(locations);\n \n     open();\n-    final List<List<?>> localResult = new ArrayList<>();\n-    List<?> row = null;\n+    List<?> row;\n     while ((row = (List<?>)next()) != null) {\n-      localResult.add(row);\n+      if (pullQueryQueue.isClosed()) {\n+        // If the queue has been closed, we stop adding rows and cleanup.\n+        break;\n+      }\n+      pullQueryQueue.acceptRow(rowFactory.apply(row, schema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NjE2Nw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY5NjUwNQ==", "bodyText": "Done.  Used PullQueryQueuePopulator as you suggested.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558696505", "createdAt": "2021-01-16T00:06:57Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullQueryResult.java", "diffHunk": "@@ -15,30 +15,39 @@\n \n package io.confluent.ksql.physical.pull;\n \n-import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.internal.PullQueryExecutorMetrics;\n+import io.confluent.ksql.query.PullQueryQueue;\n import io.confluent.ksql.query.QueryId;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n-import java.util.List;\n import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Consumer;\n+import java.util.function.Supplier;\n \n public class PullQueryResult {\n \n-  private final List<List<?>> tableRows;\n-  private final Optional<List<KsqlNode>> sourceNodes;\n   private final LogicalSchema schema;\n+  private final Supplier<CompletableFuture<Void>> runner;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NzM3MQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcwMjk3MA==", "bodyText": "This field is used so that we have a future that can be listened on before the populator is started.  I find that this is a common pattern (since you don't want to miss any events, though it would still work even if you did it after completion anyway) and I believe this pattern is used in one of the interfaces.\nTo answer your question, it is used just to hold callbacks.  I find this a simpler way to do this because of how smart futures are (if I did it myself, I would have to check to see if the underlying populator future was created yet, which I would have to store in a threadsafe field, to see if I should just call the callback directly), whereas with this, I just add it to the given future.\nI'll add some comments to clarify why it's done this way.  Does that seem reasonable?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558702970", "createdAt": "2021-01-16T00:17:27Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/physical/pull/PullQueryResult.java", "diffHunk": "@@ -15,30 +15,39 @@\n \n package io.confluent.ksql.physical.pull;\n \n-import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.internal.PullQueryExecutorMetrics;\n+import io.confluent.ksql.query.PullQueryQueue;\n import io.confluent.ksql.query.QueryId;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n-import java.util.List;\n import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Consumer;\n+import java.util.function.Supplier;\n \n public class PullQueryResult {\n \n-  private final List<List<?>> tableRows;\n-  private final Optional<List<KsqlNode>> sourceNodes;\n   private final LogicalSchema schema;\n+  private final Supplier<CompletableFuture<Void>> runner;\n   private final QueryId queryId;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Optional<PullQueryExecutorMetrics> pullQueryMetrics;\n+\n+  private CompletableFuture<Void> future = new CompletableFuture<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MDEzOA=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyMjgxMQ==", "bodyText": "Added many more comments for the class, fields, non overriden methods.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558722811", "createdAt": "2021-01-16T00:52:34Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MDQ5Mg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyODYzOQ==", "bodyText": "Sure, changed it to a method.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558728639", "createdAt": "2021-01-16T01:03:11Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5MTM0Mg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczNjg1MQ==", "bodyText": "The real implementation that's used is QueueToCompletionCallback below.  The reason why I used a LimitQueueCallback at all is so that we could implement the limit clause somewhat easily in the future by just having QueueToCompletionCallback extend LimitedQueueCallback.  The reality is that at the moment all it does it check isClosed, which is tested in the below while anyway.\nGiven the above considerations, I removed shouldQueue in favor of a normal Runnable callback.\nI added a verify for callback.onQueued to the unit tests now.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558736851", "createdAt": "2021-01-16T01:18:46Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.\n+    limitHandler.limitReached();\n+  }\n+\n+  public boolean isClosed() {\n+    return closed;\n+  }\n+\n+  public void acceptTableRows(final List<PullQueryRow> tableRows) {\n+    if (tableRows == null) {\n+      return;\n+    }\n+    for (PullQueryRow row : tableRows) {\n+      acceptRow(row);\n+    }\n+  }\n+\n+  public void acceptRow(final PullQueryRow row) {\n+    try {\n+      if (row == null) {\n+        return;\n+      }\n+\n+      if (!callback.shouldQueue()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5NTI1MQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc1NjQ3MQ==", "bodyText": "I think a physical plan node is more consistent with how these things are normally done with DBs. The only benefit of trying to do it this way is to have some consistency with push queries.\nAlso, as I think about your question, while I don't think it would be an issue to call it twice, for one, it would be a little confusing to let that happen in practice (seems like a bug).  For another, without closing the queue, the underlying physical plan would continue to run, which could be wasteful, so it must be the case that this is done after a limit is hit anyway.\nFor push queries, it relies on this handler being set for each endpoint type, and when this limit is hit, it calls close() on the its queue.  This makes sense since it never ends on its own.\nFor pull queries, they can reach an end on their own whether a limit is set or not.  I would find it confusing for limit -> close sometimes (when limit is hit first), or close -> limit other times (when end is hit first).  Anyway, I don't feel like consistency is a big deal here since they work differently and it would be a bit funny to expose a separate method on the queue like hitTheEnd that would call the same limit callback and count on it resulting in closing the queue.  Since close is always the entrypoint for normal endings, it would probably be best to go that route for limits as well.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558756471", "createdAt": "2021-01-16T01:56:53Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5ODcxMQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwMjI4Mg==", "bodyText": "I changed this to Runnable queuedCallback since that's all it does now.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558802282", "createdAt": "2021-01-16T04:01:54Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NTgxMA=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwMzU1Nw==", "bodyText": "I agree it's simpler to understand.  I just love Optional so much.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558803557", "createdAt": "2021-01-16T04:16:12Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NzMxNg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNDA5OA==", "bodyText": "I could repeatedly call poll to drain it, though this is meant to be more efficient with all of the synchronization.  In the end, the array is only 50 elements long, max.\nI think it's probably better to take the small allocation rather than the synchronization hit, but maybe you disagree. I can change it, if you prefer.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558804098", "createdAt": "2021-01-16T04:21:48Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5ODExOA=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNDE5NA==", "bodyText": "Nope, no reason.  Good suggestion.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558804194", "createdAt": "2021-01-16T04:23:20Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc5NTUyOA=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNTAyOQ==", "bodyText": "Sure, that makes sense.  If it doesn't succeed, the thread has either been interrupted or the queue is closed, so no need to continue.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558805029", "createdAt": "2021-01-16T04:32:24Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/main/java/io/confluent/ksql/query/PullQueryQueue.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.util.KeyValue;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class PullQueryQueue implements BlockingRowQueue {\n+\n+  public static final int BLOCKING_QUEUE_CAPACITY = 50;\n+  private static final Function<PullQueryRow, KeyValue<List<?>, GenericRow>>\n+      PQ_ROW_TO_KEY_VALUE = row -> KeyValue.keyValue(null, row.getGenericRow());\n+\n+  private final BlockingQueue<PullQueryRow> rowQueue;\n+  private final int offerTimeoutMs;\n+  private volatile boolean closed = false;\n+\n+  private LimitHandler limitHandler;\n+  private LimitQueueCallback callback;\n+\n+  public PullQueryQueue() {\n+    this(BLOCKING_QUEUE_CAPACITY, 100);\n+  }\n+\n+  public PullQueryQueue(\n+      final int queueSizeLimit,\n+      final int offerTimeoutMs) {\n+    this.callback = new QueueToCompletionCallback(this::isClosed);\n+    this.rowQueue = new LinkedBlockingQueue<>(queueSizeLimit);\n+    this.offerTimeoutMs = offerTimeoutMs;\n+    this.limitHandler = () -> { };\n+  }\n+\n+  @Override\n+  public void setLimitHandler(final LimitHandler limitHandler) {\n+    this.limitHandler = limitHandler;\n+    callback.setLimitHandler(limitHandler);\n+  }\n+\n+  @Override\n+  public void setQueuedCallback(final Runnable queuedCallback) {\n+    final LimitQueueCallback parent = callback;\n+\n+    callback = new LimitQueueCallback() {\n+      @Override\n+      public boolean shouldQueue() {\n+        return parent.shouldQueue();\n+      }\n+\n+      @Override\n+      public void onQueued() {\n+        parent.onQueued();\n+        queuedCallback.run();\n+      }\n+\n+      @Override\n+      public void setLimitHandler(final LimitHandler limitHandler) {\n+        parent.setLimitHandler(limitHandler);\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll(final long timeout, final TimeUnit unit)\n+      throws InterruptedException {\n+    return Optional.ofNullable(rowQueue.poll(timeout, unit))\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public KeyValue<List<?>, GenericRow> poll() {\n+    return Optional.ofNullable(rowQueue.poll())\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .orElse(null);\n+  }\n+\n+  @Override\n+  public void drainTo(final Collection<? super KeyValue<List<?>, GenericRow>> collection) {\n+    final List<PullQueryRow> list = new ArrayList<>();\n+    drainRowsTo(list);\n+    list.stream()\n+        .map(PQ_ROW_TO_KEY_VALUE)\n+        .forEach(collection::add);\n+  }\n+\n+  public PullQueryRow pollRow(final long timeout, final TimeUnit unit) throws InterruptedException {\n+    return rowQueue.poll(timeout, unit);\n+  }\n+\n+  public void drainRowsTo(final Collection<PullQueryRow> collection) {\n+    rowQueue.drainTo(collection);\n+  }\n+\n+\n+  public int size() {\n+    return rowQueue.size();\n+  }\n+\n+\n+  public boolean isEmpty() {\n+    return rowQueue.isEmpty();\n+  }\n+\n+  // Unlike push queries that run forever until someone deliberately kills it, pull queries have an\n+  // ending.  When they've reached their end, this is expected to be called.  Also, if the system\n+  // wants to end pull queries prematurely, this should also be called.\n+  public void close() {\n+    closed = true;\n+    // Unlike limits based on a number of rows which can be checked and possibly triggered after\n+    // every queuing of a row, pull queries just declare they've reached their limit when close is\n+    // called.\n+    limitHandler.limitReached();\n+  }\n+\n+  public boolean isClosed() {\n+    return closed;\n+  }\n+\n+  public void acceptTableRows(final List<PullQueryRow> tableRows) {\n+    if (tableRows == null) {\n+      return;\n+    }\n+    for (PullQueryRow row : tableRows) {\n+      acceptRow(row);\n+    }\n+  }\n+\n+  public void acceptRow(final PullQueryRow row) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMDQ4OQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNTY0Nw==", "bodyText": "It should effectively busy loop checking close.  If for some reason, there was a race in the test between closing and enqueuing with a full queue (because that requires a dequeue to happen), then it could be flaky, but there isn't.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558805647", "createdAt": "2021-01-16T04:39:30Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/query/PullQueryQueueTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+package io.confluent.ksql.query;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.IntStream;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryQueueTest {\n+  private static final int QUEUE_SIZE = 5;\n+\n+  private static final PullQueryRow VAL_ONE = mock(PullQueryRow.class);\n+  private static final PullQueryRow VAL_TWO = mock(PullQueryRow.class);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.seconds(10);\n+\n+  @Mock\n+  private LimitHandler limitHandler;\n+  private PullQueryQueue queue;\n+  private ScheduledExecutorService executorService;\n+\n+  @Before\n+  public void setUp() {\n+    givenQueue();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (executorService != null) {\n+      executorService.shutdownNow();\n+    }\n+  }\n+\n+  @Test\n+  public void shouldQueue() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then:\n+    assertThat(drainValues(), contains(VAL_ONE, VAL_TWO));\n+  }\n+\n+  @Test\n+  public void shouldQueueNullKey() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+\n+    // Then:\n+    assertThat(queue.size(), is(1));\n+    assertThat(drainValues(), contains(VAL_ONE));\n+  }\n+\n+  @Test\n+  public void shouldQueueUntilClosed() {\n+    // When:\n+    IntStream.range(0, QUEUE_SIZE)\n+        .forEach(idx -> {\n+          queue.acceptRow(VAL_ONE);\n+          if (idx == 2) {\n+            queue.close();\n+          }\n+        });\n+\n+    // Then:\n+    assertThat(queue.size(), is(3));\n+  }\n+\n+  @Test\n+  public void shouldPoll() throws Exception {\n+    // Given:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // When:\n+    final PullQueryRow result1 = queue.pollRow(1, TimeUnit.SECONDS);\n+    final PullQueryRow result2 = queue.pollRow(1, TimeUnit.SECONDS);\n+    final PullQueryRow result3 = queue.pollRow(1, TimeUnit.MICROSECONDS);\n+\n+    // Then:\n+    assertThat(result1, is(VAL_ONE));\n+    assertThat(result2, is(VAL_TWO));\n+    assertThat(result3, is(nullValue()));\n+  }\n+\n+  @Test\n+  public void shouldCallLimitHandlerOnClose() {\n+    // When:\n+    queue.close();\n+\n+    // Then:\n+    verify(limitHandler).limitReached();\n+  }\n+\n+\n+  @Test\n+  public void shouldBlockOnProduceOnceQueueLimitReachedAndUnblockOnClose() {\n+    // Given:\n+    givenQueue();\n+\n+    IntStream.range(0, QUEUE_SIZE)\n+        .forEach(idx -> queue.acceptRow(VAL_ONE));\n+\n+    givenWillCloseQueueAsync();\n+\n+    // When:\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then: did not block and:\n+    assertThat(queue.size(), is(QUEUE_SIZE));\n+  }\n+\n+  private void givenWillCloseQueueAsync() {\n+    executorService = Executors.newSingleThreadScheduledExecutor();\n+    executorService.schedule(queue::close, 200, TimeUnit.MILLISECONDS);\n+  }\n+\n+  private void givenQueue() {\n+    queue = new PullQueryQueue(QUEUE_SIZE, 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5MDY2Mg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNTc0Nw==", "bodyText": "Me neither.  Removed.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558805747", "createdAt": "2021-01-16T04:40:30Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-engine/src/test/java/io/confluent/ksql/query/PullQueryQueueTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+package io.confluent.ksql.query;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.IntStream;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryQueueTest {\n+  private static final int QUEUE_SIZE = 5;\n+\n+  private static final PullQueryRow VAL_ONE = mock(PullQueryRow.class);\n+  private static final PullQueryRow VAL_TWO = mock(PullQueryRow.class);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.seconds(10);\n+\n+  @Mock\n+  private LimitHandler limitHandler;\n+  private PullQueryQueue queue;\n+  private ScheduledExecutorService executorService;\n+\n+  @Before\n+  public void setUp() {\n+    givenQueue();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (executorService != null) {\n+      executorService.shutdownNow();\n+    }\n+  }\n+\n+  @Test\n+  public void shouldQueue() {\n+    // When:\n+    queue.acceptRow(VAL_ONE);\n+    queue.acceptRow(VAL_TWO);\n+\n+    // Then:\n+    assertThat(drainValues(), contains(VAL_ONE, VAL_TWO));\n+  }\n+\n+  @Test\n+  public void shouldQueueNullKey() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5MDkxMw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODgwNjU5Nw==", "bodyText": "Because in floating point representation in the CPU (i.e. binary), .1 cannot be represented exactly: https://www.exploringbinary.com/why-0-point-1-does-not-exist-in-floating-point\nDoing a key lookup using equality with such a value creates weird flakey results sometimes: https://floating-point-gui.de/errors/comparison/\nI was banging my head against the wall with these tests not returning the row I thought it should until I tried this.  Look at the result and you'll see that the existing answer is wrong!  0.5 can be represented exactly in binary.  With all this, it probably doesn't make sense to do double key lookups, but I guess we allow it.\nUser beware!", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r558806597", "createdAt": "2021-01-16T04:49:35Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-functional-tests/src/test/resources/rest-query-validation-tests/pull-queries-against-materialized-aggregates.json", "diffHunk": "@@ -1797,22 +1797,23 @@\n       \"statements\": [\n         \"CREATE STREAM INPUT (ID DOUBLE KEY, IGNORED INT) WITH (kafka_topic='test_topic', value_format='JSON');\",\n         \"CREATE TABLE AGGREGATE AS SELECT ID, COUNT(1) AS COUNT FROM INPUT WINDOW TUMBLING(SIZE 1 SECOND) GROUP BY ID;\",\n-        \"SELECT * FROM AGGREGATE WHERE ID IN (10.1, 8.1);\",\n+        \"SELECT * FROM AGGREGATE WHERE ID IN (10.5, 8.5);\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMTE4Mg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eddb7861679ad448045400efe478f6b446357802", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/eddb7861679ad448045400efe478f6b446357802", "committedDate": "2021-01-16T04:51:09Z", "message": "More feedback with simplifying the callbacks in PullQueryQueue"}, "afterCommit": {"oid": "55d3da30f0bd2b9a658b8209bf72416aca1be252", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/55d3da30f0bd2b9a658b8209bf72416aca1be252", "committedDate": "2021-01-20T06:10:45Z", "message": "Feedback part 2"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcxNTQxMTg2", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-571541186", "createdAt": "2021-01-19T18:40:58Z", "commit": {"oid": "eddb7861679ad448045400efe478f6b446357802"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQxODo0MDo1OFrOIWcJDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNTozOToxMVrOIWt5aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDQwMDY1Mw==", "bodyText": "With push queries, the only thing that would stop a stream was a limit being hit.  The way that the code was set up was a row was enqueued and then if that made it hit the limit, it would fire off the limit handler.  Look below in doSend().  Effectively, the only way to trigger sendCompete was to push one more row into the queue.  In the pull query case, we don't know when we push a row on the queue if that's the last until we try to fetch another.\nTo fix this, I could have either changed pull query behavior to match push queries in the way, or made this publisher do a sendComplete without having to enqueue another row, and the latter is simpler and makes a fair amount of sense.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560400653", "createdAt": "2021-01-19T18:40:58Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/api/impl/BlockingQueryPublisher.java", "diffHunk": "@@ -59,13 +60,23 @@ public BlockingQueryPublisher(final Context ctx,\n     this.workerExecutor = Objects.requireNonNull(workerExecutor);\n   }\n \n-  public void setQueryHandle(final PushQueryHandle queryHandle) {\n+  public void setQueryHandle(final QueryHandle queryHandle, final boolean isPullQuery) {\n     this.columnNames = queryHandle.getColumnNames();\n     this.columnTypes = queryHandle.getColumnTypes();\n     this.queue = queryHandle.getQueue();\n+    this.isPullQuery = isPullQuery;\n     this.queue.setQueuedCallback(this::maybeSend);\n-    this.queue.setLimitHandler(() -> complete = true);\n+    this.queue.setLimitHandler(() -> {\n+      complete = true;\n+      // This allows us to hit the limit without having to queue one last row\n+      if (queue.isEmpty()) {\n+        ctx.runOnContext(v -> sendComplete());\n+      } else {\n+        ctx.runOnContext(v -> doSend());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwMjg1Mw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDQwNzAwMQ==", "bodyText": "That's a good question.  I think it was largely done that way so that we could have a big try/catch around it and ensure we called subscriber.onError(e); if we hit an exception.  In this case, even though it's kicked off immediately, it still sets result.onException(this::setError); which should allow us to handle errors correctly.\nI roughly patterned this off of push queries in PushQueryPublisher and they also start the query immediately, so I don't think there should be a big difference so far as I can tell.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560407001", "createdAt": "2021-01-19T18:51:28Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryPublisher.java", "diffHunk": "@@ -79,98 +78,68 @@\n \n   @Override\n   public synchronized void subscribe(final Subscriber<Collection<StreamedRow>> subscriber) {\n-    final PullQuerySubscription subscription = new PullQuerySubscription(\n-        subscriber,\n-        () -> {\n-          final RoutingOptions routingOptions = new PullQueryConfigRoutingOptions(\n-              query.getSessionConfig().getConfig(false),\n-              query.getSessionConfig().getOverrides(),\n-              ImmutableMap.of()\n-          );\n-\n-          PullQueryExecutionUtil.checkRateLimit(rateLimiter);\n-\n-          final PullQueryResult result = ksqlEngine.executePullQuery(\n-              serviceContext,\n-              query,\n-              routing,\n-              routingFilterFactory,\n-              routingOptions,\n-              pullQueryMetrics\n-          );\n-\n-          pullQueryMetrics.ifPresent(pullQueryExecutorMetrics -> pullQueryExecutorMetrics\n-              .recordLatency(startTimeNanos));\n-          return result;\n-        },\n-        query\n+    final RoutingOptions routingOptions = new PullQueryConfigRoutingOptions(\n+        query.getSessionConfig().getConfig(false),\n+        query.getSessionConfig().getOverrides(),\n+        ImmutableMap.of()\n+    );\n+\n+    PullQueryExecutionUtil.checkRateLimit(rateLimiter);\n+\n+    final PullQueryResult result = ksqlEngine.executePullQuery(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgwOTE1Nw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDQzNDM0OQ==", "bodyText": "I simplified the method and removed this", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560434349", "createdAt": "2021-01-19T19:37:22Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjgxMDMzOQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDU5MTI3OA==", "bodyText": "I added a ton of comments and simplified the methods.  Hopefully you agree it's easier to understand.\nI am using objectMapper.writeValueAsString to do the heavy lifting of json serialization.  The comma handling is a bit of a pain that I think I have to handle myself.  The reason is just due to the partial flushing.  I can't rely on a json library to write partial json (or at least I don't really know of any that do).  This is actually exactly what it does for push queries.  I'll take a look and see if any internet searching turns up any ideas.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560591278", "createdAt": "2021-01-20T00:27:52Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MzA3Ng=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY3OTU4MA==", "bodyText": "It converts it to JSON, and everything is JSON escaped using jackson.  I added a test for this case to show it works.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560679580", "createdAt": "2021-01-20T04:58:18Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      StringBuilder sb = new StringBuilder();\n+      PullQueryRow head = null;\n+      long lastFlush = clock.millis();\n+\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      sb.append(\"[\").append(writeValueAsString(header));\n+\n+      while (!connectionClosed && !pullQueryQueue.isClosed() && !limitReached) {\n+\n+        final PullQueryRow row = pullQueryQueue.pollRow(\n+            Math.min(disconnectCheckInterval, MAX_FLUSH_MS),\n+            TimeUnit.MILLISECONDS\n+        );\n+        if (row != null) {\n+          // We keep track of a head of the queue so that we know when writing the current row if\n+          // there a next.  This is to write proper json\n+          final PullQueryRow toProcess = head;\n+          head = row;\n+          if (toProcess == null) {\n+            continue;\n+          }\n+          writeRow(toProcess, head, sb);\n+          if (sb.length() >= FLUSH_SIZE_BYTES || (clock.millis() - lastFlush) >= MAX_FLUSH_MS) {\n+            output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+            output.flush();\n+            sb = new StringBuilder();\n+            lastFlush = clock.millis();\n+          }\n+        }\n+        drainAndThrowOnError(head, output, sb);\n+      }\n+      if (!pullQueryQueue.isEmpty() || head != null) {\n+        drain(head, sb);\n+      }\n+      sb.append(\"]\");\n+      if (sb.length() > 0) {\n+        output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    } catch (InterruptedException e) {\n+      LOG.warn(\"Interrupted while writing to connection stream\");\n+    } catch (Throwable e) {\n+      LOG.error(\"Exception occurred while writing to connection stream: \", e);\n+      outputException(output, e);\n+    }\n+  }\n+\n+  private void writeRow(\n+      final PullQueryRow row,\n+      final PullQueryRow head,\n+      final StringBuilder sb\n+  ) {\n+    // Send for a comma after the header\n+    if (!sentAtLeastOneRow) {\n+      sb.append(\",\").append(System.lineSeparator());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5ODE0MA=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY3OTk1OA==", "bodyText": "No, I didn't. :-).  Thanks for pointing that out.  Removed.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560679958", "createdAt": "2021-01-20T04:59:42Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/api/ApiTest.java", "diffHunk": "@@ -110,6 +115,53 @@ public void shouldExecutePullQuery() throws Exception {\n     assertThat(queryId, is(nullValue()));\n   }\n \n+  public static void main(String[] args) throws ExecutionException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3NDI4MQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4MzE1Nw==", "bodyText": "Given that the writing thread just blocks and repeatedly loops calling pollRow until completion, it's a bit of an awkward test to write.  Within an answer block for pollRow, I verified that  drainRowsTo hadn't been called yet, but you can tell that must be so by inspection.  So I don't think negative tests in this case are very valuable.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560683157", "createdAt": "2021-01-20T05:10:14Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriterTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.hasItems;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.query.QueryId;\n+import io.confluent.ksql.rest.ApiJsonMapper;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+import org.mockito.stubbing.Answer;\n+\n+@SuppressWarnings(\"unchecked\")\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryStreamWriterTest {\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"a\"), SqlTypes.STRING)\n+      .build();\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(30, TimeUnit.SECONDS)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+  @Mock\n+  private PullQueryResult pullQueryResult;\n+  @Mock\n+  private PullQueryQueue pullQueryQueue;\n+  @Mock\n+  private Clock clock;\n+\n+  @Captor\n+  private ArgumentCaptor<Consumer<Throwable>> throwableConsumerCapture;\n+  @Captor\n+  private ArgumentCaptor<Consumer<Void>> limitCapture;\n+\n+  private ByteArrayOutputStream out;\n+\n+\n+  private PullQueryStreamWriter writer;\n+\n+  @Before\n+  public void setUp() {\n+    when(pullQueryResult.getQueryId()).thenReturn(new QueryId(\"Query id\"));\n+    when(pullQueryResult.getSchema()).thenReturn(SCHEMA);\n+    doNothing().when(pullQueryResult).onCompletion(limitCapture.capture());\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    writer = new PullQueryStreamWriter(pullQueryResult, 1000, ApiJsonMapper.INSTANCE.get(),\n+        pullQueryQueue, clock, new CompletableFuture<>());\n+\n+    out = new ByteArrayOutputStream();\n+  }\n+\n+  @Test\n+  public void shouldWriteAnyPendingRowsBeforeReportingException() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    givenUncaughtException(new KsqlException(\"Server went Boom\"));\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\"),\n+        containsString(\"Server went Boom\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteNoRows() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows())\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfPullQueryQueueIsClosed() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfLimitReached() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzgwMzc4Mg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4MzM3NQ==", "bodyText": "Done.  Added a call to ApiJsonMapper.INSTANCE.get().readTree(out.toByteArray()); which will parse it and throw an error if invalid json.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560683375", "createdAt": "2021-01-20T05:10:56Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriterTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.hasItems;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.query.QueryId;\n+import io.confluent.ksql.rest.ApiJsonMapper;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.Mock;\n+import org.mockito.junit.MockitoJUnitRunner;\n+import org.mockito.stubbing.Answer;\n+\n+@SuppressWarnings(\"unchecked\")\n+@RunWith(MockitoJUnitRunner.class)\n+public class PullQueryStreamWriterTest {\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"a\"), SqlTypes.STRING)\n+      .build();\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(30, TimeUnit.SECONDS)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+  @Mock\n+  private PullQueryResult pullQueryResult;\n+  @Mock\n+  private PullQueryQueue pullQueryQueue;\n+  @Mock\n+  private Clock clock;\n+\n+  @Captor\n+  private ArgumentCaptor<Consumer<Throwable>> throwableConsumerCapture;\n+  @Captor\n+  private ArgumentCaptor<Consumer<Void>> limitCapture;\n+\n+  private ByteArrayOutputStream out;\n+\n+\n+  private PullQueryStreamWriter writer;\n+\n+  @Before\n+  public void setUp() {\n+    when(pullQueryResult.getQueryId()).thenReturn(new QueryId(\"Query id\"));\n+    when(pullQueryResult.getSchema()).thenReturn(SCHEMA);\n+    doNothing().when(pullQueryResult).onCompletion(limitCapture.capture());\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    writer = new PullQueryStreamWriter(pullQueryResult, 1000, ApiJsonMapper.INSTANCE.get(),\n+        pullQueryQueue, clock, new CompletableFuture<>());\n+\n+    out = new ByteArrayOutputStream();\n+  }\n+\n+  @Test\n+  public void shouldWriteAnyPendingRowsBeforeReportingException() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    givenUncaughtException(new KsqlException(\"Server went Boom\"));\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\"),\n+        containsString(\"Server went Boom\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteNoRows() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows())\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfPullQueryQueueIsClosed() {\n+    // Given:\n+    when(pullQueryQueue.isClosed()).thenReturn(true);\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+  @Test\n+  public void shouldExitAndDrainIfLimitReached() {\n+    // Given:\n+    doAnswer(streamRows(\"Row1\", \"Row2\", \"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    limitCapture.getValue().accept(null);\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, hasItems(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")));\n+  }\n+\n+\n+  @Test\n+  public void shouldWriteOneAndClose() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(true);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(new PullQueryRow(ImmutableList.of(\"Row1\"), SCHEMA, Optional.empty()))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteTwoAndClose() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(true);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(streamRow(\"Row1\"))\n+        .thenReturn(streamRow(\"Row2\"))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\")\n+    ));\n+  }\n+\n+  @Test\n+  public void shouldWriteTwoAndCloseWithMoreOneQueue() throws InterruptedException {\n+    // Given:\n+    when(pullQueryQueue.isEmpty()).thenReturn(false);\n+    when(pullQueryQueue.pollRow(anyLong(), any()))\n+        .thenReturn(streamRow(\"Row1\"))\n+        .thenReturn(streamRow(\"Row2\"))\n+        .thenAnswer(inv -> {\n+          limitCapture.getValue().accept(null);\n+          return null;\n+        });\n+    doAnswer(streamRows(\"Row3\"))\n+        .when(pullQueryQueue).drainRowsTo(any());\n+\n+    // When:\n+    writer.write(out);\n+\n+    // Then:\n+    final List<String> lines = getOutput(out);\n+    assertThat(lines, contains(\n+        containsString(\"header\"),\n+        containsString(\"Row1\"),\n+        containsString(\"Row2\"),\n+        containsString(\"Row3\")\n+    ));\n+  }\n+\n+  private void givenUncaughtException(final KsqlException e) {\n+    verify(pullQueryResult).onException(throwableConsumerCapture.capture());\n+    throwableConsumerCapture.getValue().accept(e);\n+  }\n+\n+  private static Answer<Void> streamRows(final Object... rows) {\n+    return inv -> {\n+      final Collection<PullQueryRow> output = inv.getArgument(0);\n+\n+      Arrays.stream(rows)\n+          .map(row -> new PullQueryRow(ImmutableList.of(row), SCHEMA, Optional.empty()))\n+          .forEach(output::add);\n+\n+      return null;\n+    };\n+  }\n+\n+  private static PullQueryRow streamRow(final Object row) {\n+    return new PullQueryRow(ImmutableList.of(row), SCHEMA, Optional.empty());\n+  }\n+\n+  private static List<String> getOutput(final ByteArrayOutputStream out) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzgwMzk5OQ=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4NDYzOA==", "bodyText": "Correct, we flush incomplete JSON.  This is how push queries work at the moment, so I just copied that behavior (Check out QueryStreamWriter).  If you look in this PR at KsqlTarget, you'll see how we do this for partial results for forwarded requests.  I assume that the CLI does something similar.", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560684638", "createdAt": "2021-01-20T05:15:39Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      StringBuilder sb = new StringBuilder();\n+      PullQueryRow head = null;\n+      long lastFlush = clock.millis();\n+\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      sb.append(\"[\").append(writeValueAsString(header));\n+\n+      while (!connectionClosed && !pullQueryQueue.isClosed() && !limitReached) {\n+\n+        final PullQueryRow row = pullQueryQueue.pollRow(\n+            Math.min(disconnectCheckInterval, MAX_FLUSH_MS),\n+            TimeUnit.MILLISECONDS\n+        );\n+        if (row != null) {\n+          // We keep track of a head of the queue so that we know when writing the current row if\n+          // there a next.  This is to write proper json\n+          final PullQueryRow toProcess = head;\n+          head = row;\n+          if (toProcess == null) {\n+            continue;\n+          }\n+          writeRow(toProcess, head, sb);\n+          if (sb.length() >= FLUSH_SIZE_BYTES || (clock.millis() - lastFlush) >= MAX_FLUSH_MS) {\n+            output.write(sb.toString().getBytes(StandardCharsets.UTF_8));\n+            output.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MTIxMw=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY5MTU2MA==", "bodyText": "I played a bit with trying to create one, and it's a little hard to do given how I'm using it in tests.  In some cases, a real queue is fine, which I create in some tests.  In others, I want to verify that the queue was used in a certain way, and a mock is required.  In some others, I want to test surrounding logic and call various callbacks, which I often do from queue \"answer\" methods to simulate things happening while waiting for new rows.  This could be done with a test queue, and methods like completeAfterEmpty which would implicitly know to call the completion method.  I'm not too sure how reusable given that the completion scenario is different in different tests.  What do you think?", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r560691560", "createdAt": "2021-01-20T05:39:11Z", "author": {"login": "AlanConfluent"}, "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryPublisherTest.java", "diffHunk": "@@ -103,26 +118,40 @@ public void setUp() {\n     publisher = new PullQueryPublisher(\n         engine,\n         serviceContext,\n+        exec,\n         statement,\n         Optional.empty(),\n         TIME_NANOS,\n         routingFilterFactory,\n         create(1),\n         haRouting);\n \n-\n-    when(statement.getStatementText()).thenReturn(\"\");\n     when(statement.getSessionConfig()).thenReturn(sessionConfig);\n     when(sessionConfig.getConfig(false)).thenReturn(ksqlConfig);\n     when(sessionConfig.getOverrides()).thenReturn(ImmutableMap.of());\n-    when(pullQueryResult.getQueryId()).thenReturn(queryId);\n     when(pullQueryResult.getSchema()).thenReturn(PULL_SCHEMA);\n-    when(pullQueryResult.getTableRows()).thenReturn(tableRows);\n-    when(pullQueryResult.getSourceNodes()).thenReturn(Optional.empty());\n-    when(engine.executePullQuery(any(), any(), any(), any(), any(), any()))\n+    when(pullQueryResult.getPullQueryQueue()).thenReturn(pullQueryQueue);\n+    doNothing().when(pullQueryResult).onException(onErrorCaptor.capture());\n+    doNothing().when(pullQueryResult).onCompletion(completeCaptor.capture());\n+    int[] times = new int[1];\n+    doAnswer(inv -> {\n+      Collection<? super KeyValue<List<?>, GenericRow>> c = inv.getArgument(0);\n+      if (times[0] == 0) {\n+        c.add(KV1);\n+      } else if (times[0] == 1) {\n+        c.add(KV2);\n+        completeCaptor.getValue().accept(null);\n+      }\n+      times[0]++;\n+      return null;\n+    }).when(pullQueryQueue).drainTo(any());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Nzc5NjMzMg=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcyOTgzNTQ2", "url": "https://github.com/confluentinc/ksql/pull/6813#pullrequestreview-572983546", "createdAt": "2021-01-21T04:56:57Z", "commit": {"oid": "55d3da30f0bd2b9a658b8209bf72416aca1be252"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQwNDo1Njo1N1rOIXlfiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQwNTowMzozNlrOIXlmwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTYwMjQ0Mw==", "bodyText": "I think this can be simplified by always prepending ,\\n when we write a row that's not the header (as opposed to only in the case of the header). then when we're done, just append \\n] (so we don't need to check hasAnotherRow)", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r561602443", "createdAt": "2021-01-21T04:56:57Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 50;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean completed = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private AtomicBoolean closed = new AtomicBoolean(false);\n+  private AtomicReference<Thread> writingThread = new AtomicReference<>(null);\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> {\n+      pullQueryException = t;\n+      interruptWriterThread();\n+    });\n+    result.onCompletion(v -> {\n+      completed = true;\n+      interruptWriterThread();\n+    });\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      final WriterState writerState = new WriterState(clock);\n+      final QueueWrapper queueWrapper = new QueueWrapper(pullQueryQueue, disconnectCheckInterval);\n+      writingThread.set(Thread.currentThread());\n+\n+\n+      // First write the header with the schema\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      writerState.append(\"[\").append(writeValueAsString(header));\n+\n+      // While the query is still running, and the client hasn't closed the connection, continue to\n+      // poll new rows.\n+      while (!connectionClosed && !isCompletedOrHasException()) {\n+        processRow(output, writerState, queueWrapper);\n+      }\n+      // If the query finished quickly, we might not have thrown the error\n+      drainAndThrowOnError(output, writerState, queueWrapper);\n+\n+      // If no error was thrown above, drain the queue\n+      drain(writerState, queueWrapper);\n+      writerState.append(\"]\");\n+      if (writerState.length() > 0) {\n+        output.write(writerState.getStringToFlush().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    } catch (InterruptedException e) {\n+      // The most likely cause of this is the server shutting down. Should just try to close\n+      // gracefully, without writing any more to the connection stream.\n+      LOG.warn(\"Interrupted while writing to connection stream\");\n+    } catch (Throwable e) {\n+      LOG.error(\"Exception occurred while writing to connection stream: \", e);\n+      outputException(output, e);\n+    } finally {\n+      // If the completion thread attempted to interrupt this writer but it never was unset, unset\n+      // it now.\n+      if (isCompletedOrHasException() && Thread.interrupted()) {\n+        LOG.info(\"Interrupted by completion thread; unsetting interrupted flag\");\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Processes a single row from the queue or times out waiting for one.  If an error has occurred\n+   * during pull query execution, the queue is drained and the error is thrown.\n+   *\n+   * <p>Also, the thread may be interrupted by the completion callback, in which case, this method\n+   * completes immediately.\n+   * @param output The output stream to write to\n+   * @param writerState writer state\n+   * @param queueWrapper the queue wrapper\n+   * @throws Throwable If an exception is found while running the pull query, it's rethrown here.\n+   */\n+  private void processRow(\n+      final OutputStream output,\n+      final WriterState writerState,\n+      final QueueWrapper queueWrapper\n+  ) throws Throwable {\n+    final PullQueryRow toProcess;\n+    try {\n+      toProcess = queueWrapper.pollNextRow();\n+    } catch (final InterruptedException e) {\n+      // If we deliberately interrupted this thread to say it's complete, finish it\n+      if (isCompletedOrHasException()) {\n+        return;\n+      } else {\n+        throw e;\n+      }\n+    }\n+    if (toProcess != null) {\n+      writeRow(toProcess, writerState, queueWrapper.hasAnotherRow());\n+      if (writerState.length() >= FLUSH_SIZE_BYTES\n+          || (clock.millis() - writerState.getLastFlushMs()) >= MAX_FLUSH_MS\n+      ) {\n+        output.write(writerState.getStringToFlush().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    }\n+    drainAndThrowOnError(output, writerState, queueWrapper);\n+  }\n+\n+  /**\n+   * Does the job of writing the row to the writer state.\n+   * @param row The row to write\n+   * @param writerState writer state\n+   * @param hasAnotherRow if there's another row after this one.  This is used for determining how\n+   *                      to write proper JSON, e.g. whether to add a comma.\n+   */\n+  private void writeRow(\n+      final PullQueryRow row,\n+      final WriterState writerState,\n+      final boolean hasAnotherRow\n+  ) {\n+    // Send for a comma after the header\n+    if (!sentAtLeastOneRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55d3da30f0bd2b9a658b8209bf72416aca1be252"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTYwNDE2Nw==", "bodyText": "i guess your algorithm above handles this gracefully, but the first time you call pollNextRow won't it always be null? might just want to add a little comment there as well so that the next person doesn't scratch their head \ud83d\ude04", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r561604167", "createdAt": "2021-01-21T05:03:03Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 50;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean completed = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private AtomicBoolean closed = new AtomicBoolean(false);\n+  private AtomicReference<Thread> writingThread = new AtomicReference<>(null);\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> {\n+      pullQueryException = t;\n+      interruptWriterThread();\n+    });\n+    result.onCompletion(v -> {\n+      completed = true;\n+      interruptWriterThread();\n+    });\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {\n+    try {\n+      final WriterState writerState = new WriterState(clock);\n+      final QueueWrapper queueWrapper = new QueueWrapper(pullQueryQueue, disconnectCheckInterval);\n+      writingThread.set(Thread.currentThread());\n+\n+\n+      // First write the header with the schema\n+      final StreamedRow header\n+          = StreamedRow.header(result.getQueryId(), result.getSchema());\n+      writerState.append(\"[\").append(writeValueAsString(header));\n+\n+      // While the query is still running, and the client hasn't closed the connection, continue to\n+      // poll new rows.\n+      while (!connectionClosed && !isCompletedOrHasException()) {\n+        processRow(output, writerState, queueWrapper);\n+      }\n+      // If the query finished quickly, we might not have thrown the error\n+      drainAndThrowOnError(output, writerState, queueWrapper);\n+\n+      // If no error was thrown above, drain the queue\n+      drain(writerState, queueWrapper);\n+      writerState.append(\"]\");\n+      if (writerState.length() > 0) {\n+        output.write(writerState.getStringToFlush().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    } catch (InterruptedException e) {\n+      // The most likely cause of this is the server shutting down. Should just try to close\n+      // gracefully, without writing any more to the connection stream.\n+      LOG.warn(\"Interrupted while writing to connection stream\");\n+    } catch (Throwable e) {\n+      LOG.error(\"Exception occurred while writing to connection stream: \", e);\n+      outputException(output, e);\n+    } finally {\n+      // If the completion thread attempted to interrupt this writer but it never was unset, unset\n+      // it now.\n+      if (isCompletedOrHasException() && Thread.interrupted()) {\n+        LOG.info(\"Interrupted by completion thread; unsetting interrupted flag\");\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Processes a single row from the queue or times out waiting for one.  If an error has occurred\n+   * during pull query execution, the queue is drained and the error is thrown.\n+   *\n+   * <p>Also, the thread may be interrupted by the completion callback, in which case, this method\n+   * completes immediately.\n+   * @param output The output stream to write to\n+   * @param writerState writer state\n+   * @param queueWrapper the queue wrapper\n+   * @throws Throwable If an exception is found while running the pull query, it's rethrown here.\n+   */\n+  private void processRow(\n+      final OutputStream output,\n+      final WriterState writerState,\n+      final QueueWrapper queueWrapper\n+  ) throws Throwable {\n+    final PullQueryRow toProcess;\n+    try {\n+      toProcess = queueWrapper.pollNextRow();\n+    } catch (final InterruptedException e) {\n+      // If we deliberately interrupted this thread to say it's complete, finish it\n+      if (isCompletedOrHasException()) {\n+        return;\n+      } else {\n+        throw e;\n+      }\n+    }\n+    if (toProcess != null) {\n+      writeRow(toProcess, writerState, queueWrapper.hasAnotherRow());\n+      if (writerState.length() >= FLUSH_SIZE_BYTES\n+          || (clock.millis() - writerState.getLastFlushMs()) >= MAX_FLUSH_MS\n+      ) {\n+        output.write(writerState.getStringToFlush().getBytes(StandardCharsets.UTF_8));\n+        output.flush();\n+      }\n+    }\n+    drainAndThrowOnError(output, writerState, queueWrapper);\n+  }\n+\n+  /**\n+   * Does the job of writing the row to the writer state.\n+   * @param row The row to write\n+   * @param writerState writer state\n+   * @param hasAnotherRow if there's another row after this one.  This is used for determining how\n+   *                      to write proper JSON, e.g. whether to add a comma.\n+   */\n+  private void writeRow(\n+      final PullQueryRow row,\n+      final WriterState writerState,\n+      final boolean hasAnotherRow\n+  ) {\n+    // Send for a comma after the header\n+    if (!sentAtLeastOneRow) {\n+      writerState.append(\",\").append(System.lineSeparator());\n+      sentAtLeastOneRow = true;\n+    }\n+    final StreamedRow streamedRow = StreamedRow\n+        .pullRow(row.getGenericRow(), toKsqlHostInfo(row.getSourceNode()));\n+    writerState.append(writeValueAsString(streamedRow));\n+    if (hasAnotherRow) {\n+      writerState.append(\",\").append(System.lineSeparator());\n+    }\n+  }\n+\n+  /**\n+   * If an error has been stored in pullQueryException, drains the queue and throws the exception.\n+   * @param output The output stream to write to\n+   * @param writerState writer state\n+   * @throws Throwable If an exception is stored, it's rethrown.\n+   */\n+  private void drainAndThrowOnError(\n+      final OutputStream output,\n+      final WriterState writerState,\n+      final QueueWrapper queueWrapper\n+  ) throws Throwable {\n+    if (pullQueryException != null) {\n+      drain(writerState, queueWrapper);\n+      output.write(writerState.getStringToFlush().getBytes(StandardCharsets.UTF_8));\n+      output.flush();\n+      throw pullQueryException;\n+    }\n+  }\n+\n+  /**\n+   * Drains the queue and writes the contained rows.\n+   * @param writerState writer state\n+   * @param queueWrapper the queue wrapper\n+   */\n+  private void drain(final WriterState writerState, final QueueWrapper queueWrapper) {\n+    final List<PullQueryRow> rows = queueWrapper.drain();\n+    int i = 0;\n+    for (final PullQueryRow row : rows) {\n+      writeRow(row, writerState, i + 1 < rows.size());\n+      i++;\n+    }\n+  }\n+\n+  /**\n+   * Outputs the given exception to the output stream.\n+   * @param out The output stream\n+   * @param exception The exception to write\n+   */\n+  private void outputException(final OutputStream out, final Throwable exception) {\n+    try {\n+      out.write(\",\\n\".getBytes(StandardCharsets.UTF_8));\n+      if (exception.getCause() instanceof KsqlException) {\n+        objectMapper.writeValue(out, StreamedRow\n+            .error(exception.getCause(), Errors.ERROR_CODE_SERVER_ERROR));\n+      } else {\n+        objectMapper.writeValue(out, StreamedRow\n+            .error(exception, Errors.ERROR_CODE_SERVER_ERROR));\n+      }\n+      out.write(\"]\\n\".getBytes(StandardCharsets.UTF_8));\n+      out.flush();\n+    } catch (final IOException e) {\n+      LOG.debug(\"Client disconnected while attempting to write an error message\");\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (!closed.getAndSet(true)) {\n+      result.stop();\n+    }\n+  }\n+\n+  private boolean isCompletedOrHasException() {\n+    return completed || pullQueryException != null;\n+  }\n+\n+  private void interruptWriterThread() {\n+    // Interrupt the writing thread in case it's blocking for another row.\n+    final Thread thread = writingThread.get();\n+    if (thread != null && Thread.currentThread() != thread) {\n+      thread.interrupt();\n+    }\n+  }\n+\n+  /**\n+   * Converts the object to json and returns the string.\n+   * @param object The object to convert\n+   * @return The serialized JSON\n+   */\n+  private String writeValueAsString(final Object object) {\n+    try {\n+      return objectMapper.writeValueAsString(object);\n+    } catch (final JsonProcessingException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Converts the KsqlNode to KsqlHostInfoEntity\n+   */\n+  private static Optional<KsqlHostInfoEntity> toKsqlHostInfo(final Optional<KsqlNode> ksqlNode) {\n+    return ksqlNode.map(\n+        node -> new KsqlHostInfoEntity(node.location().getHost(), node.location().getPort()));\n+  }\n+\n+  /**\n+   * State that's kept for the buffered response and the last flush time.\n+   */\n+  private static class WriterState {\n+    private final Clock clock;\n+    // The buffer of JSON that we're always flushing as we hit either time or size thresholds.\n+    private StringBuilder sb = new StringBuilder();\n+    // Last flush timestamp in millis\n+    private long lastFlushMs;\n+\n+    WriterState(final Clock clock) {\n+      this.clock = clock;\n+    }\n+\n+    public WriterState append(final String str) {\n+      sb.append(str);\n+      return this;\n+    }\n+\n+    public int length() {\n+      return sb.length();\n+    }\n+\n+    public long getLastFlushMs() {\n+      return lastFlushMs;\n+    }\n+\n+    public String getStringToFlush() {\n+      final String str = sb.toString();\n+      sb = new StringBuilder();\n+      lastFlushMs = clock.millis();\n+      return str;\n+    }\n+  }\n+\n+  /**\n+   * Wraps the PullQueryQueue to keep a hold of the head of the queue explicitly so it always knows\n+   * if there's something next.\n+   */\n+  private static final class QueueWrapper {\n+    private final PullQueryQueue pullQueryQueue;\n+    private final long disconnectCheckInterval;\n+    // We always keep a reference to the head of the queue so that we know if there's another\n+    // row in the result in order to produce proper JSON.\n+    private PullQueryRow head = null;\n+\n+    QueueWrapper(final PullQueryQueue pullQueryQueue, final long disconnectCheckInterval) {\n+      this.pullQueryQueue = pullQueryQueue;\n+      this.disconnectCheckInterval = disconnectCheckInterval;\n+    }\n+\n+    public boolean hasAnotherRow() {\n+      return head != null;\n+    }\n+\n+    public PullQueryRow pollNextRow() throws InterruptedException {\n+      final PullQueryRow row = pullQueryQueue.pollRow(\n+          disconnectCheckInterval,\n+          TimeUnit.MILLISECONDS\n+      );\n+\n+      if (row != null) {\n+        // The head becomes the next thing to process and the newly polled row becomes the head.\n+        final PullQueryRow toProcess = head;\n+        head = row;\n+        return toProcess;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55d3da30f0bd2b9a658b8209bf72416aca1be252"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTYwNDI5MA==", "bodyText": "\ud83d\udcaf this is way cleaner and easier for me to reason about. Thanks for the heavy refactoring!", "url": "https://github.com/confluentinc/ksql/pull/6813#discussion_r561604290", "createdAt": "2021-01-21T05:03:36Z", "author": {"login": "agavra"}, "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/resources/streaming/PullQueryStreamWriter.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"; you may not use\n+ * this file except in compliance with the License. You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.server.resources.streaming;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.Lists;\n+import io.confluent.ksql.api.server.StreamingOutput;\n+import io.confluent.ksql.execution.streams.materialization.Locator.KsqlNode;\n+import io.confluent.ksql.physical.pull.PullQueryResult;\n+import io.confluent.ksql.physical.pull.PullQueryRow;\n+import io.confluent.ksql.query.PullQueryQueue;\n+import io.confluent.ksql.rest.Errors;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.util.KsqlException;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Clock;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+public class PullQueryStreamWriter implements StreamingOutput {\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQueryStreamWriter.class);\n+\n+  private static final int FLUSH_SIZE_BYTES = 50 * 1024;\n+  private static final long MAX_FLUSH_MS = 30;\n+\n+  private final long disconnectCheckInterval;\n+  private final PullQueryQueue pullQueryQueue;\n+  private final Clock clock;\n+  private final PullQueryResult result;\n+  private final ObjectMapper objectMapper;\n+  private volatile boolean limitReached = false;\n+  private volatile boolean connectionClosed;\n+  private volatile Throwable pullQueryException;\n+  private boolean closed;\n+  private boolean sentAtLeastOneRow = false;\n+\n+  PullQueryStreamWriter(\n+      final PullQueryResult result,\n+      final long disconnectCheckInterval,\n+      final ObjectMapper objectMapper,\n+      final PullQueryQueue pullQueryQueue,\n+      final Clock clock,\n+      final CompletableFuture<Void> connectionClosedFuture\n+  ) {\n+    this.result = Objects.requireNonNull(result, \"result\");\n+    this.objectMapper = Objects.requireNonNull(objectMapper, \"objectMapper\");\n+    this.disconnectCheckInterval = disconnectCheckInterval;\n+    this.pullQueryQueue = Objects.requireNonNull(pullQueryQueue, \"pullQueryQueue\");\n+    this.clock = Objects.requireNonNull(clock, \"clock\");\n+    connectionClosedFuture.thenAccept(v -> connectionClosed = true);\n+    result.onException(t -> pullQueryException = t);\n+    result.onCompletion(v -> limitReached = true);\n+  }\n+\n+  @Override\n+  public void write(final OutputStream output) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MzA3Ng=="}, "originalCommit": {"oid": "d451280e2a727dc50c35657b326a97dd7378d348"}, "originalPosition": 79}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9168bd62c15bf61d591fca6757f7a65af5b5c50", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/f9168bd62c15bf61d591fca6757f7a65af5b5c50", "committedDate": "2021-01-27T18:33:23Z", "message": "Makes pull query interface async"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30e2976a47eeb394abf0e671da0541dbfecbbf30", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/30e2976a47eeb394abf0e671da0541dbfecbbf30", "committedDate": "2021-01-27T19:14:54Z", "message": "Fix lint"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe21cfcfda6a502f758d9f6444304961d75f4877", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/fe21cfcfda6a502f758d9f6444304961d75f4877", "committedDate": "2021-01-22T00:28:34Z", "message": "Updates comment per feedback"}, "afterCommit": {"oid": "30e2976a47eeb394abf0e671da0541dbfecbbf30", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/30e2976a47eeb394abf0e671da0541dbfecbbf30", "committedDate": "2021-01-27T19:14:54Z", "message": "Fix lint"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3d3d0f306d892013bdfacaaa0ac052263cd96e7", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/b3d3d0f306d892013bdfacaaa0ac052263cd96e7", "committedDate": "2021-01-27T20:14:25Z", "message": "fix test compilation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a25c7a087381629d6f43f1dbddf00b453e5f9e92", "author": {"user": {"login": "AlanConfluent", "name": "Alan Sheinberg"}}, "url": "https://github.com/confluentinc/ksql/commit/a25c7a087381629d6f43f1dbddf00b453e5f9e92", "committedDate": "2021-01-27T20:59:51Z", "message": "Fix another test compilation from rebase"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4537, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}