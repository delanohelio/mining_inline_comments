{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNjk1NjY3", "number": 4495, "title": "feat: New API - integrate push queries into backend", "bodyText": "Description\nImplements #4256\n\nIntegrates query streaming functionality with ksql engine\nIf the right ksql config setting is provided then starting the ksql rest app will also start the new API. The config setting is disabled by default. This is of course a temporary hack so we can have both the new API and and the old API running in parallel so we can run the \"ride share app\" demo. Once we have migrated the whole API, ksql-rest-app will be removed completely.\nThere's a bit of duplication in EngineExecutor/QueryExecutor to support the new way of creating transient queries. This is temporary and will disappear when we remove ksql-rest-app so won't need to support the old way of creating transient queries.\nA bunch of refactoring around test classes.\nProbably some other minor bits and pieces\n\nTesting done\nNew integration test for the new API\nMore api tests\nMore unit tests\nReviewer checklist\n\n Ensure docs are updated if necessary. (eg. if a user visible feature is being added or changed).\n Ensure relevant issues are linked (description should include text like \"Fixes #\")", "createdAt": "2020-02-08T08:54:17Z", "url": "https://github.com/confluentinc/ksql/pull/4495", "merged": true, "mergeCommit": {"oid": "055406edc50a63a4b1e207f40b2707e308ca9b22"}, "closed": true, "closedAt": "2020-02-11T21:42:10Z", "author": {"login": "purplefox"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcCaQotABqjMwMjAxNTEzNTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDYQhCgFqTM1Njk5MjQ4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "67a8589364ecb105ad8761fa6c571a5a49e07bf6", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/67a8589364ecb105ad8761fa6c571a5a49e07bf6", "committedDate": "2020-02-08T19:01:23Z", "message": "checkstyle and findbugs tweaks"}, "afterCommit": {"oid": "effd22c5d547415aa90d8f350b6542e2bc79611d", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/effd22c5d547415aa90d8f350b6542e2bc79611d", "committedDate": "2020-02-08T20:49:19Z", "message": "more foo"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52a9c75affc92081088f59506c4d728504402a9c", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/52a9c75affc92081088f59506c4d728504402a9c", "committedDate": "2020-02-09T12:32:38Z", "message": "foo"}, "afterCommit": {"oid": "2a3b8c3bf7bf3fe86f889369e1219082a5c2ff5d", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/2a3b8c3bf7bf3fe86f889369e1219082a5c2ff5d", "committedDate": "2020-02-09T12:48:44Z", "message": "feat: Integrate new API query publishing with engine plus some refactorings"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2a3b8c3bf7bf3fe86f889369e1219082a5c2ff5d", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/2a3b8c3bf7bf3fe86f889369e1219082a5c2ff5d", "committedDate": "2020-02-09T12:48:44Z", "message": "feat: Integrate new API query publishing with engine plus some refactorings"}, "afterCommit": {"oid": "52a9c75affc92081088f59506c4d728504402a9c", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/52a9c75affc92081088f59506c4d728504402a9c", "committedDate": "2020-02-09T12:32:38Z", "message": "foo"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52a9c75affc92081088f59506c4d728504402a9c", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/52a9c75affc92081088f59506c4d728504402a9c", "committedDate": "2020-02-09T12:32:38Z", "message": "foo"}, "afterCommit": {"oid": "64a3ee953675e1970fd448ed93223c36de1e24d0", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/64a3ee953675e1970fd448ed93223c36de1e24d0", "committedDate": "2020-02-09T12:59:28Z", "message": "Integrate new API with engine for query streaming plus some refactoringst"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "64a3ee953675e1970fd448ed93223c36de1e24d0", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/64a3ee953675e1970fd448ed93223c36de1e24d0", "committedDate": "2020-02-09T12:59:28Z", "message": "Integrate new API with engine for query streaming plus some refactoringst"}, "afterCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/8f948742a0ce64ab808dd9e5352bba82a613252d", "committedDate": "2020-02-09T13:01:06Z", "message": "Integrate new API with engine for query streaming plus some refactoringst"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MTUxMzgx", "url": "https://github.com/confluentinc/ksql/pull/4495#pullrequestreview-356151381", "createdAt": "2020-02-10T18:10:15Z", "commit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODoxMDoxNlrOFnwPqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjozNDo1N1rOFn4G_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTIyNg==", "bodyText": "can we avoid \"hacks\" like this? I've seen these things eventually end up leaking to the user and causing some level of confusion (e.g. an error message saying \"The supplied principal \"tim\" does not have permissions to access FOO\")", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377229226", "createdAt": "2020-02-10T18:10:16Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTQ1NQ==", "bodyText": "I'm guessing we're going to implement this soon? If so, should this throw UnsupportedOperationException for now?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377229455", "createdAt": "2020-02-10T18:10:46Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }\n+  }\n+\n+  @Override\n+  public InsertsSubscriber createInsertsSubscriber(final String target, final JsonObject properties,\n+      final Subscriber<JsonObject> acksSubscriber) {\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw==", "bodyText": "nit: all of our other configs have words separated by .s can we follow that here too? (and all the above configs)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377231777", "createdAt": "2020-02-10T18:15:31Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MTg3OQ==", "bodyText": "can we use List<String> in this API? Arrays are rather brittle and there's not much of a benefit of using them if it's non-primitive anyway", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377241879", "createdAt": "2020-02-10T18:35:39Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHandler.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import java.util.OptionalInt;\n+\n+/**\n+ * Handle to a push query running in the engine\n+ */\n+public interface PushQueryHandler {\n+\n+  String[] getColumnNames();\n+\n+  String[] getColumnTypes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MjY5MQ==", "bodyText": "is it safe to do this before we've stopped the query handler? (i.e. should we move this into executeBlocking?)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377242691", "createdAt": "2020-02-10T18:37:21Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MzI1NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private boolean checkLimit() {\n          \n          \n            \n              private boolean hasReachedLimit() {", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377243254", "createdAt": "2020-02-10T18:38:26Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ==", "bodyText": "what is this synchronizing against (i.e. other than accept it doesn't look like anything else is synchronized, did you mean to make close synchronized?) Would be nice to add @GuardedBy(\"this\") to any state that requires synchronization", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377244801", "createdAt": "2020-02-10T18:41:33Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ==", "bodyText": "see my other comment about this API, I don't think we should be suppressing this warning...", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377245811", "createdAt": "2020-02-10T18:43:28Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI1MTAzMA==", "bodyText": "can we avoid making demand protected state? Protected, non-final state opens the doors to loose abstractions; at a minimum it would be good to expose getDemand() as a protected method so that subclasses can't change it, but it would be even better to expose something like isAcceptingSends() which just checks demand > 0  in the base class", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377251030", "createdAt": "2020-02-10T18:53:08Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {\n+    if (limit.isPresent()) {\n+      final int lim = limit.getAsInt();\n+      if (numAccepted == lim) {\n+        // Reached limit\n+        return false;\n+      }\n+      if (numAccepted == lim - 1) {\n+        // Set to complete after delivering any buffered rows\n+        complete = true;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @SuppressFBWarnings(\n+      value = \"IS2_INCONSISTENT_SYNC\",\n+      justification = \"Vert.x ensures this is executed on event loop only\")\n+  private void doSend() {\n+    checkContext();\n+\n+    int num = 0;\n+    while (demand > 0 && !queue.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMwNzg1Nw==", "bodyText": "same comment as above, prefer List<String> to String[]", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377307857", "createdAt": "2020-02-10T20:49:02Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/spi/QueryPublisher.java", "diffHunk": "@@ -15,29 +15,29 @@\n \n package io.confluent.ksql.api.spi;\n \n-import io.vertx.core.json.JsonArray;\n+import io.confluent.ksql.GenericRow;\n import org.reactivestreams.Publisher;\n \n /**\n  * Represents a publisher of query results. An instance of this is provided by the back-end for each\n  * query that is executed. A subscriber from the API implementation then subscribes to it, then a\n  * stream of query results flows from back-end to front-end where they are written to the wire.\n  */\n-public interface QueryPublisher extends Publisher<JsonArray> {\n+public interface QueryPublisher extends Publisher<GenericRow> {\n \n   /**\n    * @return Array representing the names of the columns of the query results\n    */\n-  JsonArray getColumnNames();\n+  String[] getColumnNames();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzMjUyNA==", "bodyText": "I know this is just a test, but I feel like we should go through the real parsing mechanism here to figure out whether or not it's a push query. Eventually we may (see #3754) add something like EMIT FINAL to push queries, not just EMIT CHANGES and it would be nice not to need to hunt this down at that point.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377332524", "createdAt": "2020-02-10T21:40:14Z", "author": {"login": "agavra"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/TestEndpoints.java", "diffHunk": "@@ -31,27 +33,32 @@\n   private final Vertx vertx;\n   private Supplier<RowGenerator> rowGeneratorFactory;\n   private TestInsertsSubscriber insertsSubscriber;\n-  private TestAcksPublisher acksPublisher;\n   private String lastSql;\n-  private boolean push;\n   private JsonObject lastProperties;\n   private String lastTarget;\n   private Set<TestQueryPublisher> queryPublishers = new HashSet<>();\n   private int acksBeforePublisherError = -1;\n   private int rowsBeforePublisherError = -1;\n+  private RuntimeException createQueryPublisherException;\n \n   public TestEndpoints(final Vertx vertx) {\n     this.vertx = vertx;\n   }\n \n   @Override\n-  public synchronized QueryPublisher createQueryPublisher(final String sql, final boolean push,\n-      final JsonObject properties) {\n+  public synchronized QueryPublisher createQueryPublisher(final String sql,\n+      final JsonObject properties, final Context context, final WorkerExecutor workerExecutor) {\n+    if (createQueryPublisherException != null) {\n+      createQueryPublisherException.fillInStackTrace();\n+      throw createQueryPublisherException;\n+    }\n     this.lastSql = sql;\n-    this.push = push;\n     this.lastProperties = properties;\n-    TestQueryPublisher queryPublisher = new TestQueryPublisher(vertx, rowGeneratorFactory.get(),\n-        rowsBeforePublisherError, push);\n+    boolean push = sql.toLowerCase().contains(\"emit changes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTQ3Mw==", "bodyText": "nit: have the above call this", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377339473", "createdAt": "2020-02-10T21:54:46Z", "author": {"login": "agavra"}, "path": "ksql-common/src/main/java/io/confluent/ksql/GenericRow.java", "diffHunk": "@@ -42,6 +42,10 @@ public static GenericRow genericRow(final Object... columns) {\n     return new GenericRow().appendAll(Arrays.asList(columns));\n   }\n \n+  public static GenericRow fromList(final List<Object> columns) {\n+    return new GenericRow().appendAll(columns);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTYwNQ==", "bodyText": "same comment - please keep to the . convention for configs. Also since these things tend to stay in the code for a long time, we should name it more descriptively (maybe ksql.api.reactive.enabled)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377339605", "createdAt": "2020-02-10T21:55:04Z", "author": {"login": "agavra"}, "path": "ksql-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -254,12 +254,17 @@\n       + \"\\nKSQL also marks its own internal topics as read-only. This is not controlled by this \"\n       + \"config.\";\n \n+  public static final String KSQL_NEW_API_ENABLED = \"ksql.new-api-enabled\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0MDAxMg==", "bodyText": "I know this duplication is going away, but in the meantime please add documentation describing the difference between this and the above", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377340012", "createdAt": "2020-02-10T21:55:55Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/KsqlExecutionContext.java", "diffHunk": "@@ -106,6 +107,15 @@ TransientQueryMetadata executeQuery(\n       ConfiguredStatement<Query> statement\n   );\n \n+  /**\n+   * Executes a query using the supplied service context.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0OTczOQ==", "bodyText": "this seems a little too hacked together for my liking - it's like making a modification to the physical/logical plan \"outside\" of the physical and logical planners. Not sure I have suggestions at the moment, but give it a thought", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377349739", "createdAt": "2020-02-10T22:16:43Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "diffHunk": "@@ -182,6 +182,63 @@ public TransientQueryMetadata buildTransientQuery(\n     );\n   }\n \n+  public QueryMetadata buildTransientQuery(\n+      final String statementText,\n+      final QueryId queryId,\n+      final Set<SourceName> sources,\n+      final ExecutionStep<?> physicalPlan,\n+      final String planSummary,\n+      final LogicalSchema schema,\n+      final RowConsumer rowConsumer\n+  ) {\n+    final KsqlQueryBuilder ksqlQueryBuilder = queryBuilder(queryId);\n+    final PlanBuilder planBuilder = new KSPlanBuilder(ksqlQueryBuilder);\n+    final Object buildResult = physicalPlan.build(planBuilder);\n+    final KStream<?, GenericRow> kstream;\n+    if (buildResult instanceof KStreamHolder<?>) {\n+      kstream = ((KStreamHolder<?>) buildResult).getStream();\n+    } else if (buildResult instanceof KTableHolder<?>) {\n+      final KTable<?, GenericRow> ktable = ((KTableHolder<?>) buildResult).getTable();\n+      kstream = ktable.toStream();\n+    } else {\n+      throw new IllegalStateException(\"Unexpected type built from exection plan\");\n+    }\n+\n+    kstream.foreach((k, row) -> {\n+      if (row == null) {\n+        return;\n+      }\n+      rowConsumer.accept(row);\n+    });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1MDQ2Mw==", "bodyText": "can we just have Consumer<GenericRow>?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377350463", "createdAt": "2020-02-10T22:18:16Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/RowConsumer.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+\n+public interface RowConsumer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1NTg0MA==", "bodyText": "should we try/catch these as well?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377355840", "createdAt": "2020-02-10T22:29:59Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/main/java/io/confluent/ksql/rest/server/KsqlRestApplication.java", "diffHunk": "@@ -363,6 +389,15 @@ public void triggerShutdown() {\n       log.error(\"Exception while closing security extension\", e);\n     }\n \n+    if (apiServer != null) {\n+      apiServer.stop();\n+      apiServer = null;\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+      vertx = null;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1NjE1MQ==", "bodyText": "same comment as above, can we avoid the word New in the code? Especially after we're done with the migration this won't make much sense!", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377356151", "createdAt": "2020-02-10T22:30:37Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1ODA3OQ==", "bodyText": "is there any way to communicate this error so that if the test fails here we can figure out why?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377358079", "createdAt": "2020-02-10T22:34:57Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(CLUSTER, \"kafka-cluster\"),\n+                  ops(DESCRIBE_CONFIGS, CREATE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(TOPIC, \"_confluent-ksql-default_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, PAGE_VIEW_TOPIC),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_transient_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_query\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"X\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"AGG_TABLE\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(WRITE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(DESCRIBE)\n+              ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__consumer_offsets\"),\n+              ops(DESCRIBE)\n+          ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__transaction_state\"),\n+              ops(DESCRIBE)\n+          )\n+      )\n+      .build();\n+\n+  private static final TestKsqlRestApp REST_APP = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(\"security.protocol\", \"SASL_SSL\")\n+      .withProperty(\"sasl.mechanism\", \"PLAIN\")\n+      .withProperty(\"sasl.jaas.config\", SecureKafkaHelper.buildJaasConfig(NORMAL_USER))\n+      .withProperties(ClientTrustStore.trustStoreProps())\n+      .withProperty(\"ksql.new-api-enabled\", true)\n+      .withProperty(\"ksql.apiserver.host\", \"localhost\")\n+      .withProperty(\"ksql.apiserver.port\", 8089)\n+      .withProperty(\"ksql.apiserver.key-path\", findFilePath(\"test-server-key.pem\"))\n+      .withProperty(\"ksql.apiserver.cert-path\", findFilePath(\"test-server-cert.pem\"))\n+      .withProperty(\"ksql.apiserver.verticle-instances\", 4)\n+      .build();\n+\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain.outerRule(TEST_HARNESS).around(REST_APP);\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    TEST_HARNESS.ensureTopics(PAGE_VIEW_TOPIC);\n+\n+    TEST_HARNESS.produceRows(PAGE_VIEW_TOPIC, PAGE_VIEWS_PROVIDER, FormatFactory.JSON);\n+\n+    RestIntegrationTestUtil.createStream(REST_APP, PAGE_VIEWS_PROVIDER);\n+\n+    makeKsqlRequest(\"CREATE TABLE \" + AGG_TABLE + \" AS \"\n+        + \"SELECT COUNT(1) AS COUNT FROM \" + PAGE_VIEW_STREAM + \" GROUP BY USERID;\"\n+    );\n+  }\n+\n+  private Vertx vertx;\n+  private WebClient client;\n+\n+  @Before\n+  public void setUp() {\n+    vertx = Vertx.vertx();\n+    client = createClient();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (client != null) {\n+      client.close();\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+    }\n+    REST_APP.getServiceContext().close();\n+  }\n+\n+  private JsonArray expectedColumnNames = new JsonArray().add(\"ROWTIME\").add(\"ROWKEY\")\n+      .add(\"VIEWTIME\").add(\"USERID\").add(\"PAGEID\");\n+  private JsonArray expectedColumnTypes = new JsonArray().add(\"BIGINT\").add(\"BIGINT\")\n+      .add(\"BIGINT\").add(\"STRING\").add(\"STRING\");\n+\n+  @Test\n+  public void shouldExecutePushQueryWithLimit() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES LIMIT \" + 2 + \";\";\n+\n+    // When:\n+    QueryResponse response = executePushQuery(sql);\n+\n+    // Then:\n+    assertThat(response.rows, hasSize(2));\n+    assertThat(response.responseObject.getJsonArray(\"columnNames\"), is(expectedColumnNames));\n+    assertThat(response.responseObject.getJsonArray(\"columnTypes\"), is(expectedColumnTypes));\n+    assertThat(response.responseObject.getString(\"queryId\"), is(notNullValue()));\n+  }\n+\n+  @Test\n+  public void shouldFailWithInvalidSql() throws Exception {\n+\n+    // Given:\n+    String sql = \"SLECTT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"line 1:1: mismatched input 'SLECTT' expecting\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithMoreThanOneStatement() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\" +\n+        \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Expected exactly one KSQL statement; found 2 instead\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithNonQuery() throws Exception {\n+\n+    // Given:\n+    String sql =\n+        \"CREATE STREAM SOME_STREAM AS SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Not a query\");\n+  }\n+\n+  @Test\n+  public void shouldExecutePushQueryNoLimit() throws Exception {\n+\n+    KsqlEngine engine = (KsqlEngine) REST_APP.getEngine();\n+    // One persistent query for the agg table\n+    assertThatEventually(engine::numberOfLiveQueries, is(1));\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Create a write stream to capture the incomplete response\n+    ReceiveStream writeStream = new ReceiveStream(vertx);\n+\n+    // Make the request to stream a query\n+    JsonObject properties = new JsonObject();\n+    JsonObject requestBody = new JsonObject()\n+        .put(\"sql\", sql).put(\"properties\", properties);\n+    VertxCompletableFuture<HttpResponse<Void>> responseFuture = new VertxCompletableFuture<>();\n+    client.post(8089, \"localhost\", \"/query-stream\")\n+        .as(BodyCodec.pipe(writeStream))\n+        .sendJsonObject(requestBody, responseFuture);\n+\n+    assertThatEventually(engine::numberOfLiveQueries, is(2));\n+\n+    // Wait for all rows in the response to arrive\n+    assertThatEventually(() -> {\n+      try {\n+        Buffer buff = writeStream.getBody();\n+        QueryResponse queryResponse = new QueryResponse(buff.toString());\n+        return queryResponse.rows.size();\n+      } catch (Throwable t) {\n+        return Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 278}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2NDQyNjA3", "url": "https://github.com/confluentinc/ksql/pull/4495#pullrequestreview-356442607", "createdAt": "2020-02-11T06:56:07Z", "commit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNjo1NjowN1rOFn-xTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowNTo1OFrOFn-5mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzIxNQ==", "bodyText": "What's the purpose of this if-else block? Specifically:\n\nwhen/why would this method be called from a different context than the one passed when creating the publisher?\nwhy is having the if-else preferable to simply always making the call async (i.e., the contents of the \"else\" part)?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467215", "createdAt": "2020-02-11T06:56:07Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzM2Ng==", "bodyText": "How come we're not closing the subscriber anymore?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467366", "createdAt": "2020-02-11T06:56:45Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHolder.java", "diffHunk": "@@ -31,20 +32,23 @@\n   private final Server server;\n   private final PushQueryId id;\n   private final QuerySubscriber querySubscriber;\n+  private final QueryPublisher queryPublisher;\n   private final Consumer<PushQueryHolder> closeHandler;\n \n   PushQueryHolder(final Server server, final QuerySubscriber querySubscriber,\n+      final QueryPublisher queryPublisher,\n       final Consumer<PushQueryHolder> closeHandler) {\n     this.server = Objects.requireNonNull(server);\n     this.querySubscriber = Objects.requireNonNull(querySubscriber);\n+    this.queryPublisher = Objects.requireNonNull(queryPublisher);\n     this.closeHandler = Objects.requireNonNull(closeHandler);\n     this.id = new PushQueryId(UUID.randomUUID().toString());\n     server.registerQuery(this);\n   }\n \n   public void close() {\n     server.removeQuery(id);\n-    querySubscriber.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2Nzk1Nw==", "bodyText": "What's the purpose of asserting the context here, rather than in the other methods that call sendError()? Would the context here ever be different from the ones in those methods (doSubscribe() and doRequest())?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467957", "createdAt": "2020-02-11T06:59:22Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {\n+      doSubscribe(subscriber);\n+    } else {\n+      ctx.runOnContext(v -> doSubscribe(subscriber));\n+    }\n+  }\n+\n+  public void close() {\n+    ctx.runOnContext(v -> doClose());\n+  }\n+\n+  protected void checkContext() {\n+    if (Vertx.currentContext() != ctx) {\n+      throw new IllegalStateException(\"On wrong context\");\n+    }\n+  }\n+\n+  protected final void sendError(final Exception e) {\n+    checkContext();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODE1Mg==", "bodyText": "How come we don't check cancelled here?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468152", "createdAt": "2020-02-11T07:00:11Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODM3MA==", "bodyText": "Does this test offer anything beyond PublisherTestBase#shouldDeliverAllRequestingOneByOneLoadAfterSubscribe()? AFAICT they appear to be testing the same thing.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468370", "createdAt": "2020-02-11T07:01:08Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/BlockingQueryPublisherTest.java", "diffHunk": "@@ -0,0 +1,287 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api;\n+\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.utils.AsyncAssert;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * More BlockingQueryPublisher testing occurs in the TCK tests\n+ */\n+public class BlockingQueryPublisherTest extends PublisherTestBase<GenericRow> {\n+\n+  private WorkerExecutor workerExecutor;\n+  private TestQueryHandle queryHandle;\n+\n+  @Override\n+  protected Publisher<GenericRow> createPublisher() {\n+    this.workerExecutor = vertx.createSharedWorkerExecutor(\"test_workers\");\n+    BlockingQueryPublisher publisher = new BlockingQueryPublisher(context, workerExecutor);\n+    queryHandle = new TestQueryHandle(OptionalInt.empty());\n+    publisher.setQueryHandle(queryHandle);\n+    return publisher;\n+  }\n+\n+  @Override\n+  protected GenericRow expectedValue(final int i) {\n+    return generateRow(i);\n+  }\n+\n+  private BlockingQueryPublisher getBlockingQueryPublisher() {\n+    return (BlockingQueryPublisher) publisher;\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    super.tearDown();\n+    if (workerExecutor != null) {\n+      workerExecutor.close();\n+    }\n+  }\n+\n+  @Override\n+  protected void loadPublisher(final int num) throws Exception {\n+    for (int i = 0; i < num; i++) {\n+      getBlockingQueryPublisher().accept(generateRow(i));\n+    }\n+  }\n+\n+  private GenericRow generateRow(long num) {\n+    List<Object> l = new ArrayList<>();\n+    l.add(\"foo\" + num);\n+    l.add(num);\n+    l.add(num % 2 == 0);\n+    return GenericRow.fromList(l);\n+  }\n+\n+  @Test\n+  public void shouldStopQueryHandleOnClose() throws Exception {\n+    // When\n+    getBlockingQueryPublisher().close();\n+\n+    // Then\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+  }\n+\n+  @Test\n+  public void shouldNotStopQueryHandleOnCloseMoreThanOnce() throws Exception {\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+    Thread.sleep(100);\n+\n+    // Then:\n+    assertThat(queryHandle.getStopCalledTimes(), is(1));\n+  }\n+\n+  @Test\n+  public void shouldCompleteWhenLimitReached() throws Exception {\n+    queryHandle = new TestQueryHandle(OptionalInt.of(10));\n+    getBlockingQueryPublisher().setQueryHandle(queryHandle);\n+\n+    loadPublisher(20);\n+    AsyncAssert asyncAssert = new AsyncAssert();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        asyncAssert.assertAsync(isCompleted(), equalTo(false));\n+        getSub().request(1);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    assertThatEventually(subscriber::isCompleted, equalTo(true));\n+    assertThat(subscriber.getValues(), hasSize(10));\n+    for (int i = 0; i < 10; i++) {\n+      assertThat(subscriber.getValues().get(i), equalTo(expectedValue(i)));\n+    }\n+    asyncAssert.throwAssert();\n+  }\n+\n+  @Test\n+  public void shouldNotAcceptAfterClose() throws Exception {\n+\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+\n+    // When:\n+    AtomicBoolean onNextCalled = new AtomicBoolean();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        onNextCalled.set(true);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    loadPublisher(1);\n+\n+    // Then:\n+    Thread.sleep(100);\n+    assertThat(onNextCalled.get(), is(false));\n+  }\n+\n+  @Test\n+  public void shouldBlockIfQueueFull() throws Exception {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+\n+    // When:\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+\n+    // Then:\n+    assertThat(t.isAlive(), is(true));\n+    assertThat(exception.get(), is(nullValue()));\n+\n+    t.interrupt();\n+  }\n+\n+  @Test\n+  public void shouldReleaseBlockedThreadOnClose() {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+    assertThat(t.isAlive(), is(true));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+\n+    // Then:\n+    assertThatEventually(t::isAlive, is(false));\n+    assertThat(exception.get(), is(nullValue()));\n+  }\n+\n+  @Test\n+  public void shouldDeliverMoreThanMaxSendBatchSize() throws Exception {\n+    int num = 2 * BlockingQueryPublisher.SEND_MAX_BATCH_SIZE;\n+    loadPublisher(num);\n+    shouldDeliver(num, num);\n+  }\n+\n+  @Test\n+  public void shouldDeliverAfterSubscribe() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODUyOA==", "bodyText": "How do these TCK tests work? I'm having trouble finding docs.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468528", "createdAt": "2020-02-11T07:01:56Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/tck/BlockingQueryPublisherVerificationTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.tck;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.tck.PublisherVerification;\n+import org.reactivestreams.tck.TestEnvironment;\n+\n+public class BlockingQueryPublisherVerificationTest extends PublisherVerification<GenericRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODY0Mw==", "bodyText": "Why the decision to remove the 500 prefixes? These error codes are user-facing, right?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468643", "createdAt": "2020-02-11T07:02:25Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ErrorCodes.java", "diffHunk": "@@ -23,11 +23,12 @@\n   private ErrorCodes() {\n   }\n \n-  public static final int ERROR_CODE_MISSING_PARAM = 50001;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODcxOQ==", "bodyText": "Pardon the ignorance, but how do we know \"this stuff is slow\"?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468719", "createdAt": "2020-02-11T07:02:51Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODkyNg==", "bodyText": "Is this ACLs setup relevant to the tests in this file? If not, can we remove it?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468926", "createdAt": "2020-02-11T07:03:49Z", "author": {"login": "vcrfxia"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTIwNA==", "bodyText": "Where is this value coming from? Properties is empty and push queries default to using auto.offset.reset=latest so I'm surprised rows are being returned in this query.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377469204", "createdAt": "2020-02-11T07:05:13Z", "author": {"login": "vcrfxia"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(CLUSTER, \"kafka-cluster\"),\n+                  ops(DESCRIBE_CONFIGS, CREATE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(TOPIC, \"_confluent-ksql-default_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, PAGE_VIEW_TOPIC),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_transient_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_query\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"X\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"AGG_TABLE\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(WRITE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(DESCRIBE)\n+              ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__consumer_offsets\"),\n+              ops(DESCRIBE)\n+          ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__transaction_state\"),\n+              ops(DESCRIBE)\n+          )\n+      )\n+      .build();\n+\n+  private static final TestKsqlRestApp REST_APP = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(\"security.protocol\", \"SASL_SSL\")\n+      .withProperty(\"sasl.mechanism\", \"PLAIN\")\n+      .withProperty(\"sasl.jaas.config\", SecureKafkaHelper.buildJaasConfig(NORMAL_USER))\n+      .withProperties(ClientTrustStore.trustStoreProps())\n+      .withProperty(\"ksql.new-api-enabled\", true)\n+      .withProperty(\"ksql.apiserver.host\", \"localhost\")\n+      .withProperty(\"ksql.apiserver.port\", 8089)\n+      .withProperty(\"ksql.apiserver.key-path\", findFilePath(\"test-server-key.pem\"))\n+      .withProperty(\"ksql.apiserver.cert-path\", findFilePath(\"test-server-cert.pem\"))\n+      .withProperty(\"ksql.apiserver.verticle-instances\", 4)\n+      .build();\n+\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain.outerRule(TEST_HARNESS).around(REST_APP);\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    TEST_HARNESS.ensureTopics(PAGE_VIEW_TOPIC);\n+\n+    TEST_HARNESS.produceRows(PAGE_VIEW_TOPIC, PAGE_VIEWS_PROVIDER, FormatFactory.JSON);\n+\n+    RestIntegrationTestUtil.createStream(REST_APP, PAGE_VIEWS_PROVIDER);\n+\n+    makeKsqlRequest(\"CREATE TABLE \" + AGG_TABLE + \" AS \"\n+        + \"SELECT COUNT(1) AS COUNT FROM \" + PAGE_VIEW_STREAM + \" GROUP BY USERID;\"\n+    );\n+  }\n+\n+  private Vertx vertx;\n+  private WebClient client;\n+\n+  @Before\n+  public void setUp() {\n+    vertx = Vertx.vertx();\n+    client = createClient();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (client != null) {\n+      client.close();\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+    }\n+    REST_APP.getServiceContext().close();\n+  }\n+\n+  private JsonArray expectedColumnNames = new JsonArray().add(\"ROWTIME\").add(\"ROWKEY\")\n+      .add(\"VIEWTIME\").add(\"USERID\").add(\"PAGEID\");\n+  private JsonArray expectedColumnTypes = new JsonArray().add(\"BIGINT\").add(\"BIGINT\")\n+      .add(\"BIGINT\").add(\"STRING\").add(\"STRING\");\n+\n+  @Test\n+  public void shouldExecutePushQueryWithLimit() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES LIMIT \" + 2 + \";\";\n+\n+    // When:\n+    QueryResponse response = executePushQuery(sql);\n+\n+    // Then:\n+    assertThat(response.rows, hasSize(2));\n+    assertThat(response.responseObject.getJsonArray(\"columnNames\"), is(expectedColumnNames));\n+    assertThat(response.responseObject.getJsonArray(\"columnTypes\"), is(expectedColumnTypes));\n+    assertThat(response.responseObject.getString(\"queryId\"), is(notNullValue()));\n+  }\n+\n+  @Test\n+  public void shouldFailWithInvalidSql() throws Exception {\n+\n+    // Given:\n+    String sql = \"SLECTT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"line 1:1: mismatched input 'SLECTT' expecting\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithMoreThanOneStatement() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\" +\n+        \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Expected exactly one KSQL statement; found 2 instead\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithNonQuery() throws Exception {\n+\n+    // Given:\n+    String sql =\n+        \"CREATE STREAM SOME_STREAM AS SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Not a query\");\n+  }\n+\n+  @Test\n+  public void shouldExecutePushQueryNoLimit() throws Exception {\n+\n+    KsqlEngine engine = (KsqlEngine) REST_APP.getEngine();\n+    // One persistent query for the agg table\n+    assertThatEventually(engine::numberOfLiveQueries, is(1));\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Create a write stream to capture the incomplete response\n+    ReceiveStream writeStream = new ReceiveStream(vertx);\n+\n+    // Make the request to stream a query\n+    JsonObject properties = new JsonObject();\n+    JsonObject requestBody = new JsonObject()\n+        .put(\"sql\", sql).put(\"properties\", properties);\n+    VertxCompletableFuture<HttpResponse<Void>> responseFuture = new VertxCompletableFuture<>();\n+    client.post(8089, \"localhost\", \"/query-stream\")\n+        .as(BodyCodec.pipe(writeStream))\n+        .sendJsonObject(requestBody, responseFuture);\n+\n+    assertThatEventually(engine::numberOfLiveQueries, is(2));\n+\n+    // Wait for all rows in the response to arrive\n+    assertThatEventually(() -> {\n+      try {\n+        Buffer buff = writeStream.getBody();\n+        QueryResponse queryResponse = new QueryResponse(buff.toString());\n+        return queryResponse.rows.size();\n+      } catch (Throwable t) {\n+        return Integer.MAX_VALUE;\n+      }\n+    }, is(7));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTMzNw==", "bodyText": "nit: why not simply serviceContextFactory? (I assume there's a reason for diverging from convention -- I'm just not seeing what it is.)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377469337", "createdAt": "2020-02-11T07:05:58Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODUyODE3", "url": "https://github.com/confluentinc/ksql/pull/4495#pullrequestreview-356852817", "createdAt": "2020-02-11T17:30:55Z", "commit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzozMDo1NlrOFoSSAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0OTowMlrOFoS3tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4Njg4MQ==", "bodyText": "Can we use List rather than naked arrays please?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377786881", "createdAt": "2020-02-11T17:30:56Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }\n+  }\n+\n+  @Override\n+  public InsertsSubscriber createInsertsSubscriber(final String target, final JsonObject properties,\n+      final Subscriber<JsonObject> acksSubscriber) {\n+    return null;\n+  }\n+\n+  private static class KsqlQueryHandle implements PushQueryHandler {\n+\n+    private final QueryMetadata queryMetadata;\n+    private final OptionalInt limit;\n+\n+    KsqlQueryHandle(final QueryMetadata queryMetadata, final OptionalInt limit) {\n+      this.queryMetadata = queryMetadata;\n+      this.limit = limit;\n+    }\n+\n+    @Override\n+    public String[] getColumnNames() {\n+      return colNamesFromSchema(queryMetadata.getLogicalSchema());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4OTAzMQ==", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377789031", "createdAt": "2020-02-11T17:35:05Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MTY2Nw==", "bodyText": "Try to avoid such 'I wish Streams did it this way' style comments in the code.   Such points are for discussions, not comments in code.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377791667", "createdAt": "2020-02-11T17:40:13Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5Mjk3NQ==", "bodyText": "Can we use List not arrays please.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377792975", "createdAt": "2020-02-11T17:42:49Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MzIwNg==", "bodyText": "+1\nDon't expose mutable object state. It breaks encapsulation. Encapsulation is a pretty standard OO thing.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377793206", "createdAt": "2020-02-11T17:43:13Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NDI5NA==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377794294", "createdAt": "2020-02-11T17:45:15Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MzI1NA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NjUzMg==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377796532", "createdAt": "2020-02-11T17:49:02Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {\n+    if (limit.isPresent()) {\n+      final int lim = limit.getAsInt();\n+      if (numAccepted == lim) {\n+        // Reached limit\n+        return false;\n+      }\n+      if (numAccepted == lim - 1) {\n+        // Set to complete after delivering any buffered rows\n+        complete = true;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @SuppressFBWarnings(\n+      value = \"IS2_INCONSISTENT_SYNC\",\n+      justification = \"Vert.x ensures this is executed on event loop only\")\n+  private void doSend() {\n+    checkContext();\n+\n+    int num = 0;\n+    while (demand > 0 && !queue.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI1MTAzMA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 160}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2ODkzNzc3", "url": "https://github.com/confluentinc/ksql/pull/4495#pullrequestreview-356893777", "createdAt": "2020-02-11T18:32:16Z", "commit": {"oid": "2dc483704b3bd1bc55c67f28ed3c833e952b5a32"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5556da84c938f73c30ede292421f051a856bee55", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/5556da84c938f73c30ede292421f051a856bee55", "committedDate": "2020-02-11T20:42:09Z", "message": "Integrate new API with engine for query streaming plus some refactoringst"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "39a68078ac9a49e9561f8973880fa06479157fee", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/39a68078ac9a49e9561f8973880fa06479157fee", "committedDate": "2020-02-11T19:59:19Z", "message": "removed duplicate test"}, "afterCommit": {"oid": "5556da84c938f73c30ede292421f051a856bee55", "author": {"user": {"login": "purplefox", "name": "Tim Fox"}}, "url": "https://github.com/confluentinc/ksql/commit/5556da84c938f73c30ede292421f051a856bee55", "committedDate": "2020-02-11T20:42:09Z", "message": "Integrate new API with engine for query streaming plus some refactoringst"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2OTkyNDgx", "url": "https://github.com/confluentinc/ksql/pull/4495#pullrequestreview-356992481", "createdAt": "2020-02-11T21:03:37Z", "commit": {"oid": "5556da84c938f73c30ede292421f051a856bee55"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowMzozN1rOFoY_gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowMzozN1rOFoY_gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5NjgzNA==", "bodyText": "Why does it make sense to have so many workers relative to the number of verticles? Is the expectation that multiple blocking tasks created by the same will be running simultaneously?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377896834", "createdAt": "2020-02-11T21:03:37Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -30,31 +30,36 @@\n \n   private static final String PROPERTY_PREFIX = \"apiserver.\";\n \n-  public static final String VERTICLE_INSTANCES = propertyName(\"verticle-instances\");\n+  public static final String VERTICLE_INSTANCES = propertyName(\"verticle.instances\");\n   public static final int DEFAULT_VERTICLE_INSTANCES =\n       2 * Runtime.getRuntime().availableProcessors();\n   public static final String VERTICLE_INSTANCES_DOC =\n       \"The number of server verticle instances to start. Usually you want at least many instances\"\n           + \" as there are cores you want to use, as each instance is single threaded.\";\n \n-  public static final String LISTEN_HOST = propertyName(\"listen-host\");\n+  public static final String LISTEN_HOST = propertyName(\"listen.host\");\n   public static final String DEFAULT_LISTEN_HOST = \"0.0.0.0\";\n   public static final String LISTEN_HOST_DOC =\n       \"The hostname to listen on\";\n \n-  public static final String LISTEN_PORT = propertyName(\"listen-port\");\n+  public static final String LISTEN_PORT = propertyName(\"listen.port\");\n   public static final int DEFAULT_LISTEN_PORT = 8089;\n   public static final String LISTEN_PORT_DOC =\n       \"The port to listen on\";\n \n-  public static final String KEY_PATH = propertyName(\"key-path\");\n+  public static final String KEY_PATH = propertyName(\"key.path\");\n   public static final String KEY_PATH_DOC =\n       \"Path to key file\";\n \n-  public static final String CERT_PATH = propertyName(\"cert-path\");\n+  public static final String CERT_PATH = propertyName(\"cert.path\");\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker.pool.size\");\n+  public static final String WORKER_POOL_DOC =\n+      \"Max number of worker threads for executing blocking code\";\n+  public static final int DEFAULT_WORKER_POOL_SIZE = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5556da84c938f73c30ede292421f051a856bee55"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4991, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}