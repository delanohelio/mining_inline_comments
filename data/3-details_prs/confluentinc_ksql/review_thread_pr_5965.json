{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MzIyMDQx", "number": 5965, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzowMDowNlrOEWNKRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyMjo1MFrOEW6IaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzE5NzQ5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzowMDowNlrOG9Y7CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMToxOFrOG9fWjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyNDY0OA==", "bodyText": "We should think through the report format from a CI perspective. When YATT is driven by a CI process, it will be important to be easy to analyze the test reports programmatically. In addition to a human-readable report output, we should consider additional output formats like JSON.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467024648", "createdAt": "2020-08-07T13:00:06Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyOTk5Nw==", "bodyText": "I'll give this some thought and update the KLIP with some ideas - definitely a good point!", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467129997", "createdAt": "2020-08-07T16:01:18Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyNDY0OA=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzM0MjQyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo0MTowMFrOG9aTqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMjowMVrOG9fYKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NzMzOA==", "bodyText": "Is it implied that you could also use the following to ensure no data after a specified offset?\nASSERT NO DATA source_name OFFSET 2;", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467047338", "createdAt": "2020-08-07T13:41:00Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMDQwOQ==", "bodyText": "TBH, I hadn't thought about it but definitely a good spec to add; I'll make this explicit :)", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467130409", "createdAt": "2020-08-07T16:02:01Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NzMzOA=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzM2NTYzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo0Njo0NFrOG9ahRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMzowMVrOG9faGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MDgyMw==", "bodyText": "Can I request the ROWTIME column?\nIf/when we have support for PARTITION & OFFSET pseudocolumns in the future, could ASSERT DATA feasibly handle them, too?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467050823", "createdAt": "2020-08-07T13:46:44Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjY4NQ==", "bodyText": "The statement accepts a list of column names, so I don't see why any pseudo column couldn't be included.\nIn addition, we should support testing ALL columns.  If we only support testing the supplied list of columns, then there is no way to test no other columns are present.  I'd propose mirroring INSERT VALUES, so that the column list is optional. If not provided then all columns are expected, (excluding pseudo columns).  We should also syntax to allow all columns + pseudo columns, e.g. ASSERT DATA (ROWTIME, *) VALUES (123, 'a' , 1, 1.3).", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467122685", "createdAt": "2020-08-07T15:48:12Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MDgyMw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMDkwNg==", "bodyText": "yes, if you look at the example ROWTIME is already requested. OFFSET is a little special in that it lets you check the ordering of the messages, which is why I bumped it up to a first class citizen. PARTITION would totally make sense as a valid extension", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467130906", "createdAt": "2020-08-07T16:03:01Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MDgyMw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzM3MjExOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo0ODoyMVrOG9alOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo0ODoyMVrOG9alOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MTgzNA==", "bodyText": "I agree, it seems very useful to be able interactively explore and test expectations about data using the CLI. This implies support for the bring-your-own-Kafka extension described below.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467051834", "createdAt": "2020-08-07T13:48:21Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzM3OTU5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1MDoxMVrOG9aptw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzo1MDoyOVrOG9isoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1Mjk4Mw==", "bodyText": "Another extension consideration: support for watching the test directory for file changes, yielding new test runs.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467052983", "createdAt": "2020-08-07T13:50:11Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.\n+\n+### Extensions\n+\n+There are some extensions to this testing tool that we may want to consider. I'm listing them here\n+because I'm currently in the mindset of thinking about them, and I don't want to forget!\n+\n+- _BYO Kafka_: we may want to allow users to \"plug in their own kafka\" cluster and run these tests\n+    against a real Kafka cluster. This would allow users to produce whatever data they want\n+    outside of this tool, mitigating the limitations described above. It also would allow them\n+    to \"debug\" further when something doesn't go the way they want by examining the input/output\n+    topics through ksqlDB.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE4NDgwMA==", "bodyText": "YaaS (YATT as a Service)?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467184800", "createdAt": "2020-08-07T17:50:29Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.\n+\n+### Extensions\n+\n+There are some extensions to this testing tool that we may want to consider. I'm listing them here\n+because I'm currently in the mindset of thinking about them, and I don't want to forget!\n+\n+- _BYO Kafka_: we may want to allow users to \"plug in their own kafka\" cluster and run these tests\n+    against a real Kafka cluster. This would allow users to produce whatever data they want\n+    outside of this tool, mitigating the limitations described above. It also would allow them\n+    to \"debug\" further when something doesn't go the way they want by examining the input/output\n+    topics through ksqlDB.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1Mjk4Mw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzM5MzgxOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1Mzo1OVrOG9ayhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1Mzo1OVrOG9ayhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NTIzOQ==", "bodyText": "testing tool\n\nI assume this means the ksql-test-runner? Might be good to clarify the term.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467055239", "createdAt": "2020-08-07T13:53:59Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzQwNDE3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1NjozOFrOG9a5Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozNTo0MFrOG9eerQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NjkyMg==", "bodyText": "Makes sense not to cover a migration path for ksql-test-runner here, but it would be reasonable to consider deprecating and removing it over a few releases, right? We don't imagine a role for that tool in the future in other words, and it will be outmoded by the new tool before QTT.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467056922", "createdAt": "2020-08-07T13:56:38Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNTY5Mw==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467115693", "createdAt": "2020-08-07T15:35:40Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NjkyMg=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzc2NjQ3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozMjozN1rOG9eYUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozMjozN1rOG9eYUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNDA2NA==", "bodyText": "As already discussed offline, adding yet another testing tool has the potential to slow us down unless we commit to replacing existing testing tools with it, eventually.\nIt's not uncommon for changes or enhancements to the core engine to need changes to the existing testing tools. There are already:\n\nQTT: Json based testing tool used by ksqlDB internally to test the engine.\nRQTT: Json based testing tool used by ksqlDB internally to test the rest api.\nTesting tool: the user facing testing tool. Uses the same code classes as QTT, but in a weird way.\n\nHaving to change all three can be a drain on time and resources, ultimately slowing us down.  Adding a fourth testing tool with slow us down more. We should underestimate the cost of maintaining and enhancing these tools.\nI love this proposal. This public facing testing tool is... not great.  Having a mid term plan to replace it with this is awesome.  We just need to make sure we resource it, ideally also replacing QTT, and maybe even RQTT.  Having a single testing tool, dogfooded by us, would speed up core development considerably.\nWhy am I saying all this? I guess I'm looking to gauge how much buy in there is for such a piece of work. Historically, the user facing testing tool hasn't got much love. Replacing it with something we dog food is important IMHO. Likewise, committing to replacing QTT with this will increase productivity. Who's in?!?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467114064", "createdAt": "2020-08-07T15:32:37Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzc3MTI2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozNDowNVrOG9ebNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozNDowNVrOG9ebNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNDgwNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n          \n          \n            \n            Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n          \n      \n    \n    \n  \n\n(See above: QTT, RQTT + testing tool)", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467114805", "createdAt": "2020-08-07T15:34:05Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzc3NjAzOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozNToyMVrOG9eeBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzo1NjozNFrOG9i4JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNTUyNg==", "bodyText": "Can we include RQTT too do you think?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467115526", "createdAt": "2020-08-07T15:35:21Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE4Nzc0OQ==", "bodyText": "I'll take a stab at thinking about how we could do this. I imagine it should be somewhat straightforward when we implement BYOK. Then we can just spin up a real ksql server and just pipe things truly e2e", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467187749", "createdAt": "2020-08-07T17:56:34Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNTUyNg=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzc4NjY3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozODoyM1rOG9ekiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxMDo0NTozMlrOG-JmJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNzE5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The best way to describe the testing tool is by motivating example. Below is an example test\n          \n          \n            \n            The best way to describe the testing tool is by a motivating example. Below is an example test\n          \n      \n    \n    \n  \n\n??", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467117193", "createdAt": "2020-08-07T15:38:23Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE4Nzk1OQ==", "bodyText": "??\n\n??", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467187959", "createdAt": "2020-08-07T17:57:00Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNzE5Mw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgyMjExNg==", "bodyText": "Just mean I think you're missing the a.  super nit.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467822116", "createdAt": "2020-08-10T10:45:32Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNzE5Mw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzgzNzQ1OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTo1MjoyM1rOG9fDLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNjo0NjoyNFrOG-W5uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyNTAzOQ==", "bodyText": "With ASSERT TYPE coming later...", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467125039", "createdAt": "2020-08-07T15:52:23Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE4OTM2MA==", "bodyText": "Is this w.r.t to custom types (e.g. REGISTER TYPE)? Or did you mean to ASSERT STREAM vs ASSERT TABLE? I intended this assert statement to actually require either STREAM or TABLE (not both) (see example in the motivating example sql file, it asserts a specific source is a stream with schema).", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467189360", "createdAt": "2020-08-07T17:59:53Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyNTAzOQ=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgyMjMzMQ==", "bodyText": "The first one: custom types.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467822331", "createdAt": "2020-08-10T10:46:04Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyNTAzOQ=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0MDEyMQ==", "bodyText": "Added", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468040121", "createdAt": "2020-08-10T16:46:24Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyNTAzOQ=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg2NjE2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMDo0MVrOG9fU_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxMDo0Nzo0MVrOG-Jp3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyOTU5OA==", "bodyText": "One thing I'd love to get into QTT, but the large existing set of tests prohibits, is for tests to fail if anything unexpected ends up in the processing logger.  It would be great if we can add this, from the start, to YATT.\nOnly errors are written to the processing logger.  Test cases should not, generally, be generating such errors. Silently ignoring them means we/users can write tests that pass, yet would spam the processing log (and likely app log) if used in production.  We already know queries that generate a lot of processing errors have terrible performance.\nYATT could initially just fail the test case if there are entries in the processing logger. (We'll have to work out how to demux errors when running tests in parallel - probably need to add some kind of UUID or similar somehow).  Later we can add ASSERT LOG or similar to assert entries in the processing log, allowing for negative testing. Such negative testing is an important part of QTT.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467129598", "createdAt": "2020-08-07T16:00:41Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5MTIwMA==", "bodyText": "This seems like a good idea, might make migrating tests a little harder so we'd need to be able to disable this unless we wanted to block on adding the correct asserts or fixing up the tests.\nAlso we could use ASSERT DATA on the processing log stream, which should be registered by ksql anyway.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467191200", "createdAt": "2020-08-07T18:03:42Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyOTU5OA=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgyMzA2OA==", "bodyText": "Yeah, something on-by-default.  Can follow the same pattern of other tests in that they should fail if there is unexpected output byond what the test expected.  Unexpected output on the processing log = failed test.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467823068", "createdAt": "2020-08-10T10:47:41Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyOTU5OA=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 191}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg3MDU5OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMTo0M1rOG9fXiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowMTo0M1rOG9fXiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMDI0OQ==", "bodyText": "The parse outputs location information when parsing the SQL. So we can also link to the line within the test file that failed, much like QTT does today.  This can save a LOT of time when investigation test failures.\nCan we add this to the list please?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467130249", "createdAt": "2020-08-07T16:01:43Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg5MjU3OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowODoxNVrOG9fk9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxNjo0NjowOFrOG-W5LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw==", "bodyText": "I'm not seeing a sketch of how this will be implemented within the KLIP.\nWill this be implemented using a fully blown ksqlDB server, running against a (embedded) Kafka, Connect, Schema Registry? Or will it be using the Streams test driver?\nIf the former, it's likely to be much slower that QTT. Plus, how will the tool support running tests in parallel?\nAs discussed offline, ideally, it would support different modes of operation, e.g.\n\ntest driver vs running against Kafka\nUsing engine directly (like QTT) vs using the rest endpoint (like RQTT)\nembedded kafka and other services vs  external\n\nBut I'm really not sure this is easily achievable!  So it would be good to know how this initial version will be implemented...", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467133687", "createdAt": "2020-08-07T16:08:15Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5NDAwMQ==", "bodyText": "tl;dr I'm removing this bullet point from the KLIP \ud83d\ude02\n\nupdate: after prototyping, it's not so simple to roll a real kafka cluster. I'm going to go down the test driver route for now.\n\nInitially I was going to sketch out an implementation, but I think I might leave that for the PR phase because I'd like to do some more experimentation and I don't want to blow up the scope of (both) this KLIP and the deliverable that I need for query upgrade testing.\nMy intention is actually to see how bad it is with a full embedded kafka deployment and make a decision based on some \"benchmarking\". This also involves the least amount of test-specific code, and allows us to reuse as much of the production pipeline as possible (both increasing test coverage, and making things like RQTT migration easy).\nI'm somewhat skeptical that the kafka portion of the test was what was slowing it down (I can imagine spinning up a new one for each test would be bad, but if we can reuse an existing cluster I can't imagine that kafka would be a true bottleneck given how fast it can process data).\nIf I'm wrong, and it proves to be a bottleneck, I'll try to implement a chained topology-test-driver implementation.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467194001", "createdAt": "2020-08-07T18:09:37Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgyNzY4Ng==", "bodyText": "My reason for asking for implementation details in the KLIP is because it will strongly influence what's possible and the possible edge cases and issues.\nKafka is a high-throughput, fault-tolerant system. It's not particularly low-latency.  I'd be very surprised if switching to a real Kafka cluster doesn't increase test run time by an order of magnitude. Though feel free to test it. (RQTT has its own kafka and ksqlDB instances, so already does this).\nQTT already takes a fair while to run. It's also key to the way anyone working on the engine works. For example, if I'm working on engine features I'll generally be debugging a single QTT test repeatedly after each code change, running small sets of files less frequently to find the next issue, and the whole suite occasionally.\nIt's not just about how long it takes the full suite to run. Though I still think that will be a lot slower. If switching to Kafka means a single test takes longer to run, due to having to start Kafka, then it will drastically reduce developer productivity.  I believe this was the reason the test driver was written in the first place.\nIMHO, I would restrict this KLIP to implementation using the test driver only. This would allow us to replace QTT and the ksqlDB testing tool once YATT is feature complete enough.  BYOK and RQTT can be mentioned as future work only, requiring additional KLIPs.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467827686", "createdAt": "2020-08-10T10:58:29Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNjYwMQ==", "bodyText": "I tested this and you are right :D i'll restrict the implementation to test-driver", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468016601", "createdAt": "2020-08-10T16:07:16Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAzOTk4MA==", "bodyText": "Updated in the KLIP under implementation", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468039980", "createdAt": "2020-08-10T16:46:08Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDQ5NjI0OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowMjozM1rOG-ZhMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyNjo0OVrOG-aTzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4Mjk5Mg==", "bodyText": "What about asserting data in internal topics (e.g., repartition topics) as some QTT tests currently do?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468082992", "createdAt": "2020-08-10T18:02:33Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5NTk0OQ==", "bodyText": "I discussed this with @big-andy-coates and we came up with two conclusions:\n\nthis shouldn't be necessary anymore because the historical tests capture all internal topic data and assert that it's the same, so you shouldn't need to manually assert that it stays the same\nif you really wanted to, you could declare it as a stream/table and use the ASSERT DATA calls", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468095949", "createdAt": "2020-08-10T18:26:49Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4Mjk5Mg=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDUwODU2OnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowNjowMVrOG-ZopQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODo1MzoxOVrOG-bK7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDkwMQ==", "bodyText": "Is this not something we want to eventually support outside of testing purposes? Either (1) there are no valid use cases for inserting nulls into streams, in which case why are we testing it, or (2) there are valid use cases for inserting nulls into streams, and we should support it in ksqlDB.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468084901", "createdAt": "2020-08-10T18:06:01Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5NjUwNg==", "bodyText": "That's a good question, but even if we decide there are no valid use cases for insert nulls into streams it's still valid to test what happens if your source (bad) data has nulls in your streams.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468096506", "createdAt": "2020-08-10T18:27:53Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDkwMQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODEwNjUxNw==", "bodyText": "That's fair but it seems weird to add special support for this particular case of \"bad data\" in the testing tool, since there are other types of bad data we can't produce using SQL syntax either (garbage bytes, malformed data, etc). The KLIP already contains a proposal for BYO Kafka to allow for testing these scenarios. Why is the case of null values in streams special / different?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468106517", "createdAt": "2020-08-10T18:46:36Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDkwMQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODExMDA2MQ==", "bodyText": "the big one is parity with QTT/ksql-test-runner; they can't produce garbage bytes or malformed data either. BYOK is a pretty heavy extension if we wanted to bring this up to parity", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468110061", "createdAt": "2020-08-10T18:53:19Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDkwMQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDUyNDcyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxMDo1NVrOG-Zy1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxOToxMjoyN1rOG-byZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ==", "bodyText": "Does this mean repurposing the existing print topic syntax for use in the testing tool? This functionality makes a lot of sense but I wonder if repurposing existing syntax for something else will become confusing or problematic if we later add support for push queries/pull queries/print topic statements in the testing tool.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468087509", "createdAt": "2020-08-10T18:10:55Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5NzAxNw==", "bodyText": "I'm not entirely sure I understand the concern?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468097017", "createdAt": "2020-08-10T18:28:54Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODExMzQ3OA==", "bodyText": "My original thinking was: If YATT is eventually extended to include support for the current RQTT, then YATT will have to support statements for push and pull queries. If it's eventually extended to also support PRINT <topic>; statements (a natural extension to push query support), but the PRINT <topic>; syntax already has a specific meaning for the testing tool (i.e., print contents of the topic from the beginning) which conflicts with the meaning of the KSQL statement (print newly arriving data only), then it'll be confusing what the expected behavior is. To remedy the confusion we could:\n\nhave YATT assume auto.offset.reset=earliest by default, so the KSQL statement PRINT <topic>; also prints from the beginning\nrequire KSQL syntax for printing topics in YATT, e.g., PRINT 'foo'; complete with single quotes and everything. (I assume this was already the plan, in order to reuse the parser.)\n\nWhat are your thoughts on the former (defaulting to auto.offset.reset=earliest)? While reading the proposal I had in my mind we wouldn't do that, though the KLIP wasn't explicit one way or the other.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468113478", "createdAt": "2020-08-10T18:59:36Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODExNTQyNw==", "bodyText": "ah, my intention was actually to have YATT's PRINT statement match the semantics of KSQL's PRINT statement - the only difference would be that it knows when the topic ends and it doesn't hang forever (to be automation friendly) and instead ends on topic end.\nThat means that if you wanted to print from beginning you would need PRINT 'foo' FROM BEGINNING;.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468115427", "createdAt": "2020-08-10T19:03:21Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODEyMDE2Nw==", "bodyText": "Got'cha, that makes sense to me then. Thanks!", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468120167", "createdAt": "2020-08-10T19:12:27Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDU2NTUyOnYy", "diffSide": "RIGHT", "path": "design-proposals/klip-32-sql-testing-tool.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyMjo1MFrOG-aLuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyOToxMlrOG-aYlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5Mzg4MQ==", "bodyText": "Should we also support passing in a single test file directly, so users can debug individual tests/test files without having to run an entire directory of tests?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468093881", "createdAt": "2020-08-10T18:22:50Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5NzE3NA==", "bodyText": "yup! I will add this into the KLIP", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468097174", "createdAt": "2020-08-10T18:29:12Z", "author": {"login": "agavra"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5Mzg4MQ=="}, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 113}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3062, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}