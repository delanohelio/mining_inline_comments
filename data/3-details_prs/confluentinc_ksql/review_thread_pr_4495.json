{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNjk1NjY3", "number": 4495, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODoxMDoxNlrODeje4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowMzozN1rODe9dag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzY1MjE2OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODoxMDoxNlrOFnwPqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozMTowNVrOFoWO7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTIyNg==", "bodyText": "can we avoid \"hacks\" like this? I've seen these things eventually end up leaking to the user and causing some level of confusion (e.g. an error message saying \"The supplied principal \"tim\" does not have permissions to access FOO\")", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377229226", "createdAt": "2020-02-10T18:10:16Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NDAwNQ==", "bodyText": "Well, it has to return something, would you prefer \"almog\" ? ;)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377484005", "createdAt": "2020-02-11T08:00:16Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTIyNg=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4NDk4OA==", "bodyText": "while I'd love to have it be \"almog\" \ud83d\ude09  if it has to be set, can it be \"NO_PRINICPAL\"?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377784988", "createdAt": "2020-02-11T17:27:29Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTIyNg=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1MTYyOQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377851629", "createdAt": "2020-02-11T19:31:05Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTIyNg=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzY1MzYwOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODoxMDo0NlrOFnwQjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODowMDozOVrOFn_zXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTQ1NQ==", "bodyText": "I'm guessing we're going to implement this soon? If so, should this throw UnsupportedOperationException for now?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377229455", "createdAt": "2020-02-10T18:10:46Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }\n+  }\n+\n+  @Override\n+  public InsertsSubscriber createInsertsSubscriber(final String target, final JsonObject properties,\n+      final Subscriber<JsonObject> acksSubscriber) {\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NDEyNQ==", "bodyText": "It gets implemented in a follow up PR.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377484125", "createdAt": "2020-02-11T08:00:39Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }\n+  }\n+\n+  @Override\n+  public InsertsSubscriber createInsertsSubscriber(final String target, final JsonObject properties,\n+      final Subscriber<JsonObject> acksSubscriber) {\n+    return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyOTQ1NQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzY2ODIzOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODoxNTozMVrOFnwZoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozMToxN1rOFoWPRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw==", "bodyText": "nit: all of our other configs have words separated by .s can we follow that here too? (and all the above configs)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377231777", "createdAt": "2020-02-10T18:15:31Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4MTcwOA==", "bodyText": "I'm pretty confused by this convention.\nIt seems that dots are\na) used to create scoping for properties - this seems a very normal use of dots in properties to me. E.g. ksql.api.* contains all stuff related to the api. ksql.engine.* - contains all stuff related to engine. Seems very reasonable and expected.\nb) also used to separate words! Seems really odd, and breaks the scoping rules in a. Why do we do this? Seems broken to me.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377481708", "createdAt": "2020-02-11T07:52:54Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzczMTMyMA==", "bodyText": "@purplefox I think a main reason for .s is to follow the environment variable naming pattern, which translates e.g. an env var named KSQL_WORKER_POOL_SIZE into worker.pool.size in server properties.\nTo get a hyphenated property name, the env var would have to be e.g. KSQL_WORKER___POOL___SIZE \ud83d\ude43\nthe converter is called env_to_props:\nhttps://github.com/confluentinc/confluent-docker-utils/blob/3427c198e83b5d65d91b580a6df589f5d4799c14/confluent/docker_utils/dub.py#L50\nhere's how it applies for docker builds:\nhttps://github.com/confluentinc/ksql/blob/master/ksql-docker/src/include/docker/run#L32\n\n  \n    \n      ksql/config/ksqldb-server.properties.template\n    \n    \n         Line 16\n      in\n      c2e42dc\n    \n    \n    \n    \n\n        \n          \n           {% set kr_props = env_to_props('KSQL_', '') -%}", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377731320", "createdAt": "2020-02-11T16:02:36Z", "author": {"login": "colinhicks"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc2MDE2NA==", "bodyText": "^ The existing convention for env var names is well-established and shared across Confluent components and their documentation. From the perspective of code it is indeed awkward that it breaks expectations of namespacing. However, considering end-user ergonomics, I believe we should keep the pattern in place and use dots for the prop names.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377760164", "createdAt": "2020-02-11T16:47:02Z", "author": {"login": "colinhicks"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4NTUxNw==", "bodyText": "This also might be just a byproduct of a bygone age... I know that LinkedIn had the same convention", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377785517", "createdAt": "2020-02-11T17:28:27Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1MTcxOA==", "bodyText": "it's weird... but ack", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377851718", "createdAt": "2020-02-11T19:31:17Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -55,6 +55,11 @@\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker-pool-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIzMTc3Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzcyODY3OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODozNTozOVrOFnxBFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozMTo1N1rOFoWQow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MTg3OQ==", "bodyText": "can we use List<String> in this API? Arrays are rather brittle and there's not much of a benefit of using them if it's non-primitive anyway", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377241879", "createdAt": "2020-02-10T18:35:39Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHandler.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import java.util.OptionalInt;\n+\n+/**\n+ * Handle to a push query running in the engine\n+ */\n+public interface PushQueryHandler {\n+\n+  String[] getColumnNames();\n+\n+  String[] getColumnTypes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1MjA2Nw==", "bodyText": "I will change this.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377852067", "createdAt": "2020-02-11T19:31:57Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHandler.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import java.util.OptionalInt;\n+\n+/**\n+ * Handle to a push query running in the engine\n+ */\n+public interface PushQueryHandler {\n+\n+  String[] getColumnNames();\n+\n+  String[] getColumnTypes();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MTg3OQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzczMzUyOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODozNzoyMVrOFnxEQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODowMzoyMFrOFn_2Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MjY5MQ==", "bodyText": "is it safe to do this before we've stopped the query handler? (i.e. should we move this into executeBlocking?)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377242691", "createdAt": "2020-02-10T18:37:21Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NDg4Nw==", "bodyText": "I don't see it as an issue.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377484887", "createdAt": "2020-02-11T08:03:20Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MjY5MQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzczNzA0OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODozODoyNlrOFnxGdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozMjo1OFrOFoWSbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MzI1NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private boolean checkLimit() {\n          \n          \n            \n              private boolean hasReachedLimit() {", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377243254", "createdAt": "2020-02-10T18:38:26Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NDI5NA==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377794294", "createdAt": "2020-02-11T17:45:15Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MzI1NA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1MjUyNA==", "bodyText": "both work for me, but I'll change it if you think it's important.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377852524", "createdAt": "2020-02-11T19:32:58Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0MzI1NA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzc0NjY5OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODo0MTozM1rOFnxMgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODoyODo1N1rOFoUJiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ==", "bodyText": "what is this synchronizing against (i.e. other than accept it doesn't look like anything else is synchronized, did you mean to make close synchronized?) Would be nice to add @GuardedBy(\"this\") to any state that requires synchronization", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377244801", "createdAt": "2020-02-10T18:41:33Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NTUwMQ==", "bodyText": "accept could potentially be called by different threads, hence the synchronization", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377485501", "createdAt": "2020-02-11T08:05:37Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NjM4Mw==", "bodyText": "This is still access state that close() accesses. is it possible that the two of those can be concurrent? If so we should also make that synchronized", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377796383", "createdAt": "2020-02-11T17:48:46Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgwMDI3MQ==", "bodyText": "close() doesn't need synchronization as closed is volatile and workerExecutor is a final field", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377800271", "createdAt": "2020-02-11T17:56:13Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgxNzQ4MQ==", "bodyText": "I don't think volatile is sufficient in this context:\n\nif two close() calls happen simultaneously, there's no guarantees that they both get past L89\nif a close() and accept() happen at the same time, it is possible that the accept gets to L111 and then the close happens and the query handler stops.\n\nI'm not sure how \"bad\" these race conditions are, but they are still races and I don't think there's a harm in proactively synchronizing (it's not that performance critical since accept is already synch'd and close isn't called often)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377817481", "createdAt": "2020-02-11T18:28:57Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NDgwMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzc1Mjc1OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODo0MzoyOFrOFnxQcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozOTo1MlrOFoWgdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ==", "bodyText": "see my other comment about this API, I don't think we should be suppressing this warning...", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377245811", "createdAt": "2020-02-10T18:43:28Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MzIwNg==", "bodyText": "+1\nDon't expose mutable object state. It breaks encapsulation. Encapsulation is a pretty standard OO thing.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377793206", "createdAt": "2020-02-11T17:43:13Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1MjgyMw==", "bodyText": "I don't believe in dogmatic following of rules, sorry.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377852823", "createdAt": "2020-02-11T19:33:32Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1NjExNg==", "bodyText": "This discussion is probably irrelevant now (#4495 (comment)) since you'll expose a List but here it goes anyway:\nRules are rules for a reason - I think unless there's a good reason not to follow them we probably should be. Specifically for this one, it's so easy to accidentally change the contents of an array, so why not prefer it to be an immutable view of the underlying data? This class is what \"owns\" the data (the column names/types) and it really doesn't expect them to change. Codifying it seem strictly non-negative (we definitely don't lose anything making it immutable, and we possibly lose by making it mutable).", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377856116", "createdAt": "2020-02-11T19:39:52Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI0NTgxMQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzMzc4NTMyOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQxODo1MzowOFrOFnxk1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0OTowMlrOFoS3tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI1MTAzMA==", "bodyText": "can we avoid making demand protected state? Protected, non-final state opens the doors to loose abstractions; at a minimum it would be good to expose getDemand() as a protected method so that subclasses can't change it, but it would be even better to expose something like isAcceptingSends() which just checks demand > 0  in the base class", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377251030", "createdAt": "2020-02-10T18:53:08Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {\n+    if (limit.isPresent()) {\n+      final int lim = limit.getAsInt();\n+      if (numAccepted == lim) {\n+        // Reached limit\n+        return false;\n+      }\n+      if (numAccepted == lim - 1) {\n+        // Set to complete after delivering any buffered rows\n+        complete = true;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @SuppressFBWarnings(\n+      value = \"IS2_INCONSISTENT_SYNC\",\n+      justification = \"Vert.x ensures this is executed on event loop only\")\n+  private void doSend() {\n+    checkContext();\n+\n+    int num = 0;\n+    while (demand > 0 && !queue.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NjUzMg==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377796532", "createdAt": "2020-02-11T17:49:02Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {\n+      return;\n+    }\n+\n+    while (!closed) {\n+      try {\n+        // Don't block for more than a little while each time to allow close to work\n+        if (queue.offer(row, 250, TimeUnit.MILLISECONDS)) {\n+          numAccepted++;\n+          maybeSend();\n+          return;\n+        }\n+      } catch (InterruptedException ignore) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  public int queueSize() {\n+    return queue.size();\n+  }\n+\n+  @Override\n+  protected void maybeSend() {\n+    ctx.runOnContext(v -> doSend());\n+  }\n+\n+  @Override\n+  protected void afterSubscribe() {\n+    queryHandle.start();\n+  }\n+\n+  private boolean checkLimit() {\n+    if (limit.isPresent()) {\n+      final int lim = limit.getAsInt();\n+      if (numAccepted == lim) {\n+        // Reached limit\n+        return false;\n+      }\n+      if (numAccepted == lim - 1) {\n+        // Set to complete after delivering any buffered rows\n+        complete = true;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @SuppressFBWarnings(\n+      value = \"IS2_INCONSISTENT_SYNC\",\n+      justification = \"Vert.x ensures this is executed on event loop only\")\n+  private void doSend() {\n+    checkContext();\n+\n+    int num = 0;\n+    while (demand > 0 && !queue.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI1MTAzMA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDEzOTkxOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/spi/QueryPublisher.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMDo0OTowMlrOFn1C0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMDo0OTowMlrOFn1C0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMwNzg1Nw==", "bodyText": "same comment as above, prefer List<String> to String[]", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377307857", "createdAt": "2020-02-10T20:49:02Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/spi/QueryPublisher.java", "diffHunk": "@@ -15,29 +15,29 @@\n \n package io.confluent.ksql.api.spi;\n \n-import io.vertx.core.json.JsonArray;\n+import io.confluent.ksql.GenericRow;\n import org.reactivestreams.Publisher;\n \n /**\n  * Represents a publisher of query results. An instance of this is provided by the back-end for each\n  * query that is executed. A subscriber from the API implementation then subscribes to it, then a\n  * stream of query results flows from back-end to front-end where they are written to the wire.\n  */\n-public interface QueryPublisher extends Publisher<JsonArray> {\n+public interface QueryPublisher extends Publisher<GenericRow> {\n \n   /**\n    * @return Array representing the names of the columns of the query results\n    */\n-  JsonArray getColumnNames();\n+  String[] getColumnNames();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDI5NDA2OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/test/java/io/confluent/ksql/api/TestEndpoints.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo0MDoxNFrOFn2jLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODowNzoyMVrOFn_6zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzMjUyNA==", "bodyText": "I know this is just a test, but I feel like we should go through the real parsing mechanism here to figure out whether or not it's a push query. Eventually we may (see #3754) add something like EMIT FINAL to push queries, not just EMIT CHANGES and it would be nice not to need to hunt this down at that point.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377332524", "createdAt": "2020-02-10T21:40:14Z", "author": {"login": "agavra"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/TestEndpoints.java", "diffHunk": "@@ -31,27 +33,32 @@\n   private final Vertx vertx;\n   private Supplier<RowGenerator> rowGeneratorFactory;\n   private TestInsertsSubscriber insertsSubscriber;\n-  private TestAcksPublisher acksPublisher;\n   private String lastSql;\n-  private boolean push;\n   private JsonObject lastProperties;\n   private String lastTarget;\n   private Set<TestQueryPublisher> queryPublishers = new HashSet<>();\n   private int acksBeforePublisherError = -1;\n   private int rowsBeforePublisherError = -1;\n+  private RuntimeException createQueryPublisherException;\n \n   public TestEndpoints(final Vertx vertx) {\n     this.vertx = vertx;\n   }\n \n   @Override\n-  public synchronized QueryPublisher createQueryPublisher(final String sql, final boolean push,\n-      final JsonObject properties) {\n+  public synchronized QueryPublisher createQueryPublisher(final String sql,\n+      final JsonObject properties, final Context context, final WorkerExecutor workerExecutor) {\n+    if (createQueryPublisherException != null) {\n+      createQueryPublisherException.fillInStackTrace();\n+      throw createQueryPublisherException;\n+    }\n     this.lastSql = sql;\n-    this.push = push;\n     this.lastProperties = properties;\n-    TestQueryPublisher queryPublisher = new TestQueryPublisher(vertx, rowGeneratorFactory.get(),\n-        rowsBeforePublisherError, push);\n+    boolean push = sql.toLowerCase().contains(\"emit changes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NjAzMQ==", "bodyText": "The API test deliberately don't have any dependencies on the other backend stuff (engine etc) as they are designed to test to the protocol and the Vert.x api server implementation.\nThere are integration tests which test with all the parts plugged together.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377486031", "createdAt": "2020-02-11T08:07:21Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/TestEndpoints.java", "diffHunk": "@@ -31,27 +33,32 @@\n   private final Vertx vertx;\n   private Supplier<RowGenerator> rowGeneratorFactory;\n   private TestInsertsSubscriber insertsSubscriber;\n-  private TestAcksPublisher acksPublisher;\n   private String lastSql;\n-  private boolean push;\n   private JsonObject lastProperties;\n   private String lastTarget;\n   private Set<TestQueryPublisher> queryPublishers = new HashSet<>();\n   private int acksBeforePublisherError = -1;\n   private int rowsBeforePublisherError = -1;\n+  private RuntimeException createQueryPublisherException;\n \n   public TestEndpoints(final Vertx vertx) {\n     this.vertx = vertx;\n   }\n \n   @Override\n-  public synchronized QueryPublisher createQueryPublisher(final String sql, final boolean push,\n-      final JsonObject properties) {\n+  public synchronized QueryPublisher createQueryPublisher(final String sql,\n+      final JsonObject properties, final Context context, final WorkerExecutor workerExecutor) {\n+    if (createQueryPublisherException != null) {\n+      createQueryPublisherException.fillInStackTrace();\n+      throw createQueryPublisherException;\n+    }\n     this.lastSql = sql;\n-    this.push = push;\n     this.lastProperties = properties;\n-    TestQueryPublisher queryPublisher = new TestQueryPublisher(vertx, rowGeneratorFactory.get(),\n-        rowsBeforePublisherError, push);\n+    boolean push = sql.toLowerCase().contains(\"emit changes\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzMjUyNA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDMzNzg5OnYy", "diffSide": "RIGHT", "path": "ksql-common/src/main/java/io/confluent/ksql/GenericRow.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo1NDo0NlrOFn2-UQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo1NDo0NlrOFn2-UQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTQ3Mw==", "bodyText": "nit: have the above call this", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377339473", "createdAt": "2020-02-10T21:54:46Z", "author": {"login": "agavra"}, "path": "ksql-common/src/main/java/io/confluent/ksql/GenericRow.java", "diffHunk": "@@ -42,6 +42,10 @@ public static GenericRow genericRow(final Object... columns) {\n     return new GenericRow().appendAll(Arrays.asList(columns));\n   }\n \n+  public static GenericRow fromList(final List<Object> columns) {\n+    return new GenericRow().appendAll(columns);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDMzODc5OnYy", "diffSide": "RIGHT", "path": "ksql-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo1NTowNFrOFn2-1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0MToxM1rOFoSmjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTYwNQ==", "bodyText": "same comment - please keep to the . convention for configs. Also since these things tend to stay in the code for a long time, we should name it more descriptively (maybe ksql.api.reactive.enabled)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377339605", "createdAt": "2020-02-10T21:55:04Z", "author": {"login": "agavra"}, "path": "ksql-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -254,12 +254,17 @@\n       + \"\\nKSQL also marks its own internal topics as read-only. This is not controlled by this \"\n       + \"config.\";\n \n+  public static final String KSQL_NEW_API_ENABLED = \"ksql.new-api-enabled\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NjM1Mg==", "bodyText": "This is a temporary property and will go away before KLIP-15 is complete.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377486352", "createdAt": "2020-02-11T08:08:21Z", "author": {"login": "purplefox"}, "path": "ksql-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -254,12 +254,17 @@\n       + \"\\nKSQL also marks its own internal topics as read-only. This is not controlled by this \"\n       + \"config.\";\n \n+  public static final String KSQL_NEW_API_ENABLED = \"ksql.new-api-enabled\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTYwNQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MjE0MQ==", "bodyText": "famous last words \ud83d\ude02  all joking aside, I've seen \"temporary\" things stay in the code base a long time. let's spend an extra minute now so that our public API (configs) are sane going forward", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377792141", "createdAt": "2020-02-11T17:41:13Z", "author": {"login": "agavra"}, "path": "ksql-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -254,12 +254,17 @@\n       + \"\\nKSQL also marks its own internal topics as read-only. This is not controlled by this \"\n       + \"config.\";\n \n+  public static final String KSQL_NEW_API_ENABLED = \"ksql.new-api-enabled\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMzOTYwNQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDM0MTIyOnYy", "diffSide": "RIGHT", "path": "ksql-engine/src/main/java/io/confluent/ksql/KsqlExecutionContext.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo1NTo1NVrOFn3AbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMTo1NTo1NVrOFn3AbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0MDAxMg==", "bodyText": "I know this duplication is going away, but in the meantime please add documentation describing the difference between this and the above", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377340012", "createdAt": "2020-02-10T21:55:55Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/KsqlExecutionContext.java", "diffHunk": "@@ -106,6 +107,15 @@ TransientQueryMetadata executeQuery(\n       ConfiguredStatement<Query> statement\n   );\n \n+  /**\n+   * Executes a query using the supplied service context.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDQwMjcyOnYy", "diffSide": "RIGHT", "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjoxNjo0M1rOFn3maw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo1ODo1OFrOFoTMQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0OTczOQ==", "bodyText": "this seems a little too hacked together for my liking - it's like making a modification to the physical/logical plan \"outside\" of the physical and logical planners. Not sure I have suggestions at the moment, but give it a thought", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377349739", "createdAt": "2020-02-10T22:16:43Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "diffHunk": "@@ -182,6 +182,63 @@ public TransientQueryMetadata buildTransientQuery(\n     );\n   }\n \n+  public QueryMetadata buildTransientQuery(\n+      final String statementText,\n+      final QueryId queryId,\n+      final Set<SourceName> sources,\n+      final ExecutionStep<?> physicalPlan,\n+      final String planSummary,\n+      final LogicalSchema schema,\n+      final RowConsumer rowConsumer\n+  ) {\n+    final KsqlQueryBuilder ksqlQueryBuilder = queryBuilder(queryId);\n+    final PlanBuilder planBuilder = new KSPlanBuilder(ksqlQueryBuilder);\n+    final Object buildResult = physicalPlan.build(planBuilder);\n+    final KStream<?, GenericRow> kstream;\n+    if (buildResult instanceof KStreamHolder<?>) {\n+      kstream = ((KStreamHolder<?>) buildResult).getStream();\n+    } else if (buildResult instanceof KTableHolder<?>) {\n+      final KTable<?, GenericRow> ktable = ((KTableHolder<?>) buildResult).getTable();\n+      kstream = ktable.toStream();\n+    } else {\n+      throw new IllegalStateException(\"Unexpected type built from exection plan\");\n+    }\n+\n+    kstream.foreach((k, row) -> {\n+      if (row == null) {\n+        return;\n+      }\n+      rowConsumer.accept(row);\n+    });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NzE5Nw==", "bodyText": "It's pretty much an exact copy and paste of the existing code for transient queries", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377487197", "createdAt": "2020-02-11T08:11:03Z", "author": {"login": "purplefox"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "diffHunk": "@@ -182,6 +182,63 @@ public TransientQueryMetadata buildTransientQuery(\n     );\n   }\n \n+  public QueryMetadata buildTransientQuery(\n+      final String statementText,\n+      final QueryId queryId,\n+      final Set<SourceName> sources,\n+      final ExecutionStep<?> physicalPlan,\n+      final String planSummary,\n+      final LogicalSchema schema,\n+      final RowConsumer rowConsumer\n+  ) {\n+    final KsqlQueryBuilder ksqlQueryBuilder = queryBuilder(queryId);\n+    final PlanBuilder planBuilder = new KSPlanBuilder(ksqlQueryBuilder);\n+    final Object buildResult = physicalPlan.build(planBuilder);\n+    final KStream<?, GenericRow> kstream;\n+    if (buildResult instanceof KStreamHolder<?>) {\n+      kstream = ((KStreamHolder<?>) buildResult).getStream();\n+    } else if (buildResult instanceof KTableHolder<?>) {\n+      final KTable<?, GenericRow> ktable = ((KTableHolder<?>) buildResult).getTable();\n+      kstream = ktable.toStream();\n+    } else {\n+      throw new IllegalStateException(\"Unexpected type built from exection plan\");\n+    }\n+\n+    kstream.foreach((k, row) -> {\n+      if (row == null) {\n+        return;\n+      }\n+      rowConsumer.accept(row);\n+    });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0OTczOQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MTY0Ng==", "bodyText": "the main difference is that this adds a forEach, which actually affects the execution plan/topology", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377791646", "createdAt": "2020-02-11T17:40:11Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "diffHunk": "@@ -182,6 +182,63 @@ public TransientQueryMetadata buildTransientQuery(\n     );\n   }\n \n+  public QueryMetadata buildTransientQuery(\n+      final String statementText,\n+      final QueryId queryId,\n+      final Set<SourceName> sources,\n+      final ExecutionStep<?> physicalPlan,\n+      final String planSummary,\n+      final LogicalSchema schema,\n+      final RowConsumer rowConsumer\n+  ) {\n+    final KsqlQueryBuilder ksqlQueryBuilder = queryBuilder(queryId);\n+    final PlanBuilder planBuilder = new KSPlanBuilder(ksqlQueryBuilder);\n+    final Object buildResult = physicalPlan.build(planBuilder);\n+    final KStream<?, GenericRow> kstream;\n+    if (buildResult instanceof KStreamHolder<?>) {\n+      kstream = ((KStreamHolder<?>) buildResult).getStream();\n+    } else if (buildResult instanceof KTableHolder<?>) {\n+      final KTable<?, GenericRow> ktable = ((KTableHolder<?>) buildResult).getTable();\n+      kstream = ktable.toStream();\n+    } else {\n+      throw new IllegalStateException(\"Unexpected type built from exection plan\");\n+    }\n+\n+    kstream.foreach((k, row) -> {\n+      if (row == null) {\n+        return;\n+      }\n+      rowConsumer.accept(row);\n+    });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0OTczOQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgwMTc5NQ==", "bodyText": "The foreach is from here https://github.com/confluentinc/ksql/blob/master/ksql-engine/src/main/java/io/confluent/ksql/query/TransientQueryQueue.java#L59\nI've basically just simplified the pre-existing code into a single place. There's no new logic though.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377801795", "createdAt": "2020-02-11T17:58:58Z", "author": {"login": "purplefox"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/QueryExecutor.java", "diffHunk": "@@ -182,6 +182,63 @@ public TransientQueryMetadata buildTransientQuery(\n     );\n   }\n \n+  public QueryMetadata buildTransientQuery(\n+      final String statementText,\n+      final QueryId queryId,\n+      final Set<SourceName> sources,\n+      final ExecutionStep<?> physicalPlan,\n+      final String planSummary,\n+      final LogicalSchema schema,\n+      final RowConsumer rowConsumer\n+  ) {\n+    final KsqlQueryBuilder ksqlQueryBuilder = queryBuilder(queryId);\n+    final PlanBuilder planBuilder = new KSPlanBuilder(ksqlQueryBuilder);\n+    final Object buildResult = physicalPlan.build(planBuilder);\n+    final KStream<?, GenericRow> kstream;\n+    if (buildResult instanceof KStreamHolder<?>) {\n+      kstream = ((KStreamHolder<?>) buildResult).getStream();\n+    } else if (buildResult instanceof KTableHolder<?>) {\n+      final KTable<?, GenericRow> ktable = ((KTableHolder<?>) buildResult).getTable();\n+      kstream = ktable.toStream();\n+    } else {\n+      throw new IllegalStateException(\"Unexpected type built from exection plan\");\n+    }\n+\n+    kstream.foreach((k, row) -> {\n+      if (row == null) {\n+        return;\n+      }\n+      rowConsumer.accept(row);\n+    });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM0OTczOQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDQwNzQxOnYy", "diffSide": "RIGHT", "path": "ksql-engine/src/main/java/io/confluent/ksql/query/RowConsumer.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjoxODoxNlrOFn3pPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOToxOTozN1rOFoV2OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1MDQ2Mw==", "bodyText": "can we just have Consumer<GenericRow>?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377350463", "createdAt": "2020-02-10T22:18:16Z", "author": {"login": "agavra"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/RowConsumer.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+\n+public interface RowConsumer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NzM4Mg==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377487382", "createdAt": "2020-02-11T08:11:36Z", "author": {"login": "purplefox"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/RowConsumer.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+\n+public interface RowConsumer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1MDQ2Mw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg0NTMwNQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377845305", "createdAt": "2020-02-11T19:19:37Z", "author": {"login": "purplefox"}, "path": "ksql-engine/src/main/java/io/confluent/ksql/query/RowConsumer.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.query;\n+\n+import io.confluent.ksql.GenericRow;\n+\n+public interface RowConsumer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1MDQ2Mw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDQ0MTQ5OnYy", "diffSide": "RIGHT", "path": "ksql-rest-app/src/main/java/io/confluent/ksql/rest/server/KsqlRestApplication.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjoyOTo1OVrOFn3-QA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjoyOTo1OVrOFn3-QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1NTg0MA==", "bodyText": "should we try/catch these as well?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377355840", "createdAt": "2020-02-10T22:29:59Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/main/java/io/confluent/ksql/rest/server/KsqlRestApplication.java", "diffHunk": "@@ -363,6 +389,15 @@ public void triggerShutdown() {\n       log.error(\"Exception while closing security extension\", e);\n     }\n \n+    if (apiServer != null) {\n+      apiServer.stop();\n+      apiServer = null;\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+      vertx = null;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDQ0Mzc0OnYy", "diffSide": "RIGHT", "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjozMDozN1rOFn3_dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODoxMjozOVrOFoABdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1NjE1MQ==", "bodyText": "same comment as above, can we avoid the word New in the code? Especially after we're done with the migration this won't make much sense!", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377356151", "createdAt": "2020-02-10T22:30:37Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ4NzczMg==", "bodyText": "It's a temporary name and will be renamed before KLIP-15 is complete. The only reason new is there is to distinguish it from the current rest api test.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377487732", "createdAt": "2020-02-11T08:12:39Z", "author": {"login": "purplefox"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1NjE1MQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDQ1NjMxOnYy", "diffSide": "RIGHT", "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjozNDo1N1rOFn4G_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjozNDo1N1rOFn4G_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1ODA3OQ==", "bodyText": "is there any way to communicate this error so that if the test fails here we can figure out why?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377358079", "createdAt": "2020-02-10T22:34:57Z", "author": {"login": "agavra"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(CLUSTER, \"kafka-cluster\"),\n+                  ops(DESCRIBE_CONFIGS, CREATE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(TOPIC, \"_confluent-ksql-default_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, PAGE_VIEW_TOPIC),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_transient_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_query\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"X\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"AGG_TABLE\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(WRITE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(DESCRIBE)\n+              ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__consumer_offsets\"),\n+              ops(DESCRIBE)\n+          ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__transaction_state\"),\n+              ops(DESCRIBE)\n+          )\n+      )\n+      .build();\n+\n+  private static final TestKsqlRestApp REST_APP = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(\"security.protocol\", \"SASL_SSL\")\n+      .withProperty(\"sasl.mechanism\", \"PLAIN\")\n+      .withProperty(\"sasl.jaas.config\", SecureKafkaHelper.buildJaasConfig(NORMAL_USER))\n+      .withProperties(ClientTrustStore.trustStoreProps())\n+      .withProperty(\"ksql.new-api-enabled\", true)\n+      .withProperty(\"ksql.apiserver.host\", \"localhost\")\n+      .withProperty(\"ksql.apiserver.port\", 8089)\n+      .withProperty(\"ksql.apiserver.key-path\", findFilePath(\"test-server-key.pem\"))\n+      .withProperty(\"ksql.apiserver.cert-path\", findFilePath(\"test-server-cert.pem\"))\n+      .withProperty(\"ksql.apiserver.verticle-instances\", 4)\n+      .build();\n+\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain.outerRule(TEST_HARNESS).around(REST_APP);\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    TEST_HARNESS.ensureTopics(PAGE_VIEW_TOPIC);\n+\n+    TEST_HARNESS.produceRows(PAGE_VIEW_TOPIC, PAGE_VIEWS_PROVIDER, FormatFactory.JSON);\n+\n+    RestIntegrationTestUtil.createStream(REST_APP, PAGE_VIEWS_PROVIDER);\n+\n+    makeKsqlRequest(\"CREATE TABLE \" + AGG_TABLE + \" AS \"\n+        + \"SELECT COUNT(1) AS COUNT FROM \" + PAGE_VIEW_STREAM + \" GROUP BY USERID;\"\n+    );\n+  }\n+\n+  private Vertx vertx;\n+  private WebClient client;\n+\n+  @Before\n+  public void setUp() {\n+    vertx = Vertx.vertx();\n+    client = createClient();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (client != null) {\n+      client.close();\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+    }\n+    REST_APP.getServiceContext().close();\n+  }\n+\n+  private JsonArray expectedColumnNames = new JsonArray().add(\"ROWTIME\").add(\"ROWKEY\")\n+      .add(\"VIEWTIME\").add(\"USERID\").add(\"PAGEID\");\n+  private JsonArray expectedColumnTypes = new JsonArray().add(\"BIGINT\").add(\"BIGINT\")\n+      .add(\"BIGINT\").add(\"STRING\").add(\"STRING\");\n+\n+  @Test\n+  public void shouldExecutePushQueryWithLimit() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES LIMIT \" + 2 + \";\";\n+\n+    // When:\n+    QueryResponse response = executePushQuery(sql);\n+\n+    // Then:\n+    assertThat(response.rows, hasSize(2));\n+    assertThat(response.responseObject.getJsonArray(\"columnNames\"), is(expectedColumnNames));\n+    assertThat(response.responseObject.getJsonArray(\"columnTypes\"), is(expectedColumnTypes));\n+    assertThat(response.responseObject.getString(\"queryId\"), is(notNullValue()));\n+  }\n+\n+  @Test\n+  public void shouldFailWithInvalidSql() throws Exception {\n+\n+    // Given:\n+    String sql = \"SLECTT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"line 1:1: mismatched input 'SLECTT' expecting\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithMoreThanOneStatement() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\" +\n+        \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Expected exactly one KSQL statement; found 2 instead\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithNonQuery() throws Exception {\n+\n+    // Given:\n+    String sql =\n+        \"CREATE STREAM SOME_STREAM AS SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Not a query\");\n+  }\n+\n+  @Test\n+  public void shouldExecutePushQueryNoLimit() throws Exception {\n+\n+    KsqlEngine engine = (KsqlEngine) REST_APP.getEngine();\n+    // One persistent query for the agg table\n+    assertThatEventually(engine::numberOfLiveQueries, is(1));\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Create a write stream to capture the incomplete response\n+    ReceiveStream writeStream = new ReceiveStream(vertx);\n+\n+    // Make the request to stream a query\n+    JsonObject properties = new JsonObject();\n+    JsonObject requestBody = new JsonObject()\n+        .put(\"sql\", sql).put(\"properties\", properties);\n+    VertxCompletableFuture<HttpResponse<Void>> responseFuture = new VertxCompletableFuture<>();\n+    client.post(8089, \"localhost\", \"/query-stream\")\n+        .as(BodyCodec.pipe(writeStream))\n+        .sendJsonObject(requestBody, responseFuture);\n+\n+    assertThatEventually(engine::numberOfLiveQueries, is(2));\n+\n+    // Wait for all rows in the response to arrive\n+    assertThatEventually(() -> {\n+      try {\n+        Buffer buff = writeStream.getBody();\n+        QueryResponse queryResponse = new QueryResponse(buff.toString());\n+        return queryResponse.rows.size();\n+      } catch (Throwable t) {\n+        return Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE3NDM3OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNjo1NjowN1rOFn-xTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODoyNzo0OFrOFoAXNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzIxNQ==", "bodyText": "What's the purpose of this if-else block? Specifically:\n\nwhen/why would this method be called from a different context than the one passed when creating the publisher?\nwhy is having the if-else preferable to simply always making the call async (i.e., the contents of the \"else\" part)?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467215", "createdAt": "2020-02-11T06:56:07Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5MzMwMw==", "bodyText": "The class doesn't control who is calling subscribe so we can't assume it's always called from the same context.\nIf already on same context then it's safe to call directly which will be faster than doing it asynchronously.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377493303", "createdAt": "2020-02-11T08:27:48Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzIxNQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE3NTM4OnYy", "diffSide": "LEFT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHolder.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNjo1Njo0NVrOFn-x5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToyNzozMVrOFoZvdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzM2Ng==", "bodyText": "How come we're not closing the subscriber anymore?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467366", "createdAt": "2020-02-11T06:56:45Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHolder.java", "diffHunk": "@@ -31,20 +32,23 @@\n   private final Server server;\n   private final PushQueryId id;\n   private final QuerySubscriber querySubscriber;\n+  private final QueryPublisher queryPublisher;\n   private final Consumer<PushQueryHolder> closeHandler;\n \n   PushQueryHolder(final Server server, final QuerySubscriber querySubscriber,\n+      final QueryPublisher queryPublisher,\n       final Consumer<PushQueryHolder> closeHandler) {\n     this.server = Objects.requireNonNull(server);\n     this.querySubscriber = Objects.requireNonNull(querySubscriber);\n+    this.queryPublisher = Objects.requireNonNull(queryPublisher);\n     this.closeHandler = Objects.requireNonNull(closeHandler);\n     this.id = new PushQueryId(UUID.randomUUID().toString());\n     server.registerQuery(this);\n   }\n \n   public void close() {\n     server.removeQuery(id);\n-    querySubscriber.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5NTI5Nw==", "bodyText": "The subscriber doesn't have a close method any more, it's not needed. Closing the publisher will cause an onComplete to be sent to the subscriber which will result in the response being ended.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377495297", "createdAt": "2020-02-11T08:33:13Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHolder.java", "diffHunk": "@@ -31,20 +32,23 @@\n   private final Server server;\n   private final PushQueryId id;\n   private final QuerySubscriber querySubscriber;\n+  private final QueryPublisher queryPublisher;\n   private final Consumer<PushQueryHolder> closeHandler;\n \n   PushQueryHolder(final Server server, final QuerySubscriber querySubscriber,\n+      final QueryPublisher queryPublisher,\n       final Consumer<PushQueryHolder> closeHandler) {\n     this.server = Objects.requireNonNull(server);\n     this.querySubscriber = Objects.requireNonNull(querySubscriber);\n+    this.queryPublisher = Objects.requireNonNull(queryPublisher);\n     this.closeHandler = Objects.requireNonNull(closeHandler);\n     this.id = new PushQueryId(UUID.randomUUID().toString());\n     server.registerQuery(this);\n   }\n \n   public void close() {\n     server.removeQuery(id);\n-    querySubscriber.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzM2Ng=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkwOTEwOA==", "bodyText": "Do we still need to pass the subscriber into PushQueryHolder in that case? Looks like it's unused.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377909108", "createdAt": "2020-02-11T21:27:31Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/PushQueryHolder.java", "diffHunk": "@@ -31,20 +32,23 @@\n   private final Server server;\n   private final PushQueryId id;\n   private final QuerySubscriber querySubscriber;\n+  private final QueryPublisher queryPublisher;\n   private final Consumer<PushQueryHolder> closeHandler;\n \n   PushQueryHolder(final Server server, final QuerySubscriber querySubscriber,\n+      final QueryPublisher queryPublisher,\n       final Consumer<PushQueryHolder> closeHandler) {\n     this.server = Objects.requireNonNull(server);\n     this.querySubscriber = Objects.requireNonNull(querySubscriber);\n+    this.queryPublisher = Objects.requireNonNull(queryPublisher);\n     this.closeHandler = Objects.requireNonNull(closeHandler);\n     this.id = new PushQueryId(UUID.randomUUID().toString());\n     server.registerQuery(this);\n   }\n \n   public void close() {\n     server.removeQuery(id);\n-    querySubscriber.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2NzM2Ng=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE3OTQxOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNjo1OToyMlrOFn-0NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODozNDoyOVrOFoAg3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2Nzk1Nw==", "bodyText": "What's the purpose of asserting the context here, rather than in the other methods that call sendError()? Would the context here ever be different from the ones in those methods (doSubscribe() and doRequest())?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377467957", "createdAt": "2020-02-11T06:59:22Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {\n+      doSubscribe(subscriber);\n+    } else {\n+      ctx.runOnContext(v -> doSubscribe(subscriber));\n+    }\n+  }\n+\n+  public void close() {\n+    ctx.runOnContext(v -> doClose());\n+  }\n+\n+  protected void checkContext() {\n+    if (Vertx.currentContext() != ctx) {\n+      throw new IllegalStateException(\"On wrong context\");\n+    }\n+  }\n+\n+  protected final void sendError(final Exception e) {\n+    checkContext();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5NTc3NQ==", "bodyText": "This is a method that accesses internal state so seems like a sensible place to check the context.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377495775", "createdAt": "2020-02-11T08:34:29Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;\n+  }\n+\n+  /**\n+   * Subscribe a subscriber to this publisher. The publisher will allow at most one subscriber.\n+   *\n+   * @param subscriber The subscriber\n+   */\n+  @Override\n+  public void subscribe(final Subscriber<? super T> subscriber) {\n+    Objects.requireNonNull(subscriber);\n+    if (Vertx.currentContext() == ctx) {\n+      doSubscribe(subscriber);\n+    } else {\n+      ctx.runOnContext(v -> doSubscribe(subscriber));\n+    }\n+  }\n+\n+  public void close() {\n+    ctx.runOnContext(v -> doClose());\n+  }\n+\n+  protected void checkContext() {\n+    if (Vertx.currentContext() != ctx) {\n+      throw new IllegalStateException(\"On wrong context\");\n+    }\n+  }\n+\n+  protected final void sendError(final Exception e) {\n+    checkContext();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2Nzk1Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4MDY1OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMDoxMVrOFn-0-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMDoxMVrOFn-0-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODE1Mg==", "bodyText": "How come we don't check cancelled here?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468152", "createdAt": "2020-02-11T07:00:11Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;\n+  private OptionalInt limit;\n+  private int numAccepted;\n+  private boolean complete;\n+  private volatile boolean closed;\n+\n+  public BlockingQueryPublisher(final Context ctx,\n+      final WorkerExecutor workerExecutor) {\n+    super(ctx);\n+    this.workerExecutor = Objects.requireNonNull(workerExecutor);\n+  }\n+\n+  public void setQueryHandle(final PushQueryHandler queryHandle) {\n+    this.queryHandle = Objects.requireNonNull(queryHandle);\n+    this.limit = queryHandle.getLimit();\n+    this.columnNames = queryHandle.getColumnNames();\n+    this.columnTypes = queryHandle.getColumnTypes();\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnNames() {\n+    return columnNames;\n+  }\n+\n+  @SuppressFBWarnings(value = \"EI_EXPOSE_REP\")\n+  @Override\n+  public String[] getColumnTypes() {\n+    return columnTypes;\n+  }\n+\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    closed = true;\n+    // Run async as it can block\n+    workerExecutor.executeBlocking(p -> queryHandle.stop(), ar -> {\n+      if (ar.failed()) {\n+        log.error(\"Failed to close query\", ar.cause());\n+      }\n+    });\n+    super.close();\n+  }\n+\n+  @Override\n+  public synchronized void accept(final GenericRow row) {\n+    Objects.requireNonNull(row);\n+\n+    if (closed || complete || !checkLimit()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4MjIxOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/test/java/io/confluent/ksql/api/BlockingQueryPublisherTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMTowOFrOFn-10g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTo1NDowMlrOFoW9bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODM3MA==", "bodyText": "Does this test offer anything beyond PublisherTestBase#shouldDeliverAllRequestingOneByOneLoadAfterSubscribe()? AFAICT they appear to be testing the same thing.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468370", "createdAt": "2020-02-11T07:01:08Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/BlockingQueryPublisherTest.java", "diffHunk": "@@ -0,0 +1,287 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api;\n+\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.utils.AsyncAssert;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * More BlockingQueryPublisher testing occurs in the TCK tests\n+ */\n+public class BlockingQueryPublisherTest extends PublisherTestBase<GenericRow> {\n+\n+  private WorkerExecutor workerExecutor;\n+  private TestQueryHandle queryHandle;\n+\n+  @Override\n+  protected Publisher<GenericRow> createPublisher() {\n+    this.workerExecutor = vertx.createSharedWorkerExecutor(\"test_workers\");\n+    BlockingQueryPublisher publisher = new BlockingQueryPublisher(context, workerExecutor);\n+    queryHandle = new TestQueryHandle(OptionalInt.empty());\n+    publisher.setQueryHandle(queryHandle);\n+    return publisher;\n+  }\n+\n+  @Override\n+  protected GenericRow expectedValue(final int i) {\n+    return generateRow(i);\n+  }\n+\n+  private BlockingQueryPublisher getBlockingQueryPublisher() {\n+    return (BlockingQueryPublisher) publisher;\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    super.tearDown();\n+    if (workerExecutor != null) {\n+      workerExecutor.close();\n+    }\n+  }\n+\n+  @Override\n+  protected void loadPublisher(final int num) throws Exception {\n+    for (int i = 0; i < num; i++) {\n+      getBlockingQueryPublisher().accept(generateRow(i));\n+    }\n+  }\n+\n+  private GenericRow generateRow(long num) {\n+    List<Object> l = new ArrayList<>();\n+    l.add(\"foo\" + num);\n+    l.add(num);\n+    l.add(num % 2 == 0);\n+    return GenericRow.fromList(l);\n+  }\n+\n+  @Test\n+  public void shouldStopQueryHandleOnClose() throws Exception {\n+    // When\n+    getBlockingQueryPublisher().close();\n+\n+    // Then\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+  }\n+\n+  @Test\n+  public void shouldNotStopQueryHandleOnCloseMoreThanOnce() throws Exception {\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+    Thread.sleep(100);\n+\n+    // Then:\n+    assertThat(queryHandle.getStopCalledTimes(), is(1));\n+  }\n+\n+  @Test\n+  public void shouldCompleteWhenLimitReached() throws Exception {\n+    queryHandle = new TestQueryHandle(OptionalInt.of(10));\n+    getBlockingQueryPublisher().setQueryHandle(queryHandle);\n+\n+    loadPublisher(20);\n+    AsyncAssert asyncAssert = new AsyncAssert();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        asyncAssert.assertAsync(isCompleted(), equalTo(false));\n+        getSub().request(1);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    assertThatEventually(subscriber::isCompleted, equalTo(true));\n+    assertThat(subscriber.getValues(), hasSize(10));\n+    for (int i = 0; i < 10; i++) {\n+      assertThat(subscriber.getValues().get(i), equalTo(expectedValue(i)));\n+    }\n+    asyncAssert.throwAssert();\n+  }\n+\n+  @Test\n+  public void shouldNotAcceptAfterClose() throws Exception {\n+\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+\n+    // When:\n+    AtomicBoolean onNextCalled = new AtomicBoolean();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        onNextCalled.set(true);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    loadPublisher(1);\n+\n+    // Then:\n+    Thread.sleep(100);\n+    assertThat(onNextCalled.get(), is(false));\n+  }\n+\n+  @Test\n+  public void shouldBlockIfQueueFull() throws Exception {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+\n+    // When:\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+\n+    // Then:\n+    assertThat(t.isAlive(), is(true));\n+    assertThat(exception.get(), is(nullValue()));\n+\n+    t.interrupt();\n+  }\n+\n+  @Test\n+  public void shouldReleaseBlockedThreadOnClose() {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+    assertThat(t.isAlive(), is(true));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+\n+    // Then:\n+    assertThatEventually(t::isAlive, is(false));\n+    assertThat(exception.get(), is(nullValue()));\n+  }\n+\n+  @Test\n+  public void shouldDeliverMoreThanMaxSendBatchSize() throws Exception {\n+    int num = 2 * BlockingQueryPublisher.SEND_MAX_BATCH_SIZE;\n+    loadPublisher(num);\n+    shouldDeliver(num, num);\n+  }\n+\n+  @Test\n+  public void shouldDeliverAfterSubscribe() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg2MzUzNA==", "bodyText": "Ack", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377863534", "createdAt": "2020-02-11T19:54:02Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/BlockingQueryPublisherTest.java", "diffHunk": "@@ -0,0 +1,287 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api;\n+\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.utils.AsyncAssert;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * More BlockingQueryPublisher testing occurs in the TCK tests\n+ */\n+public class BlockingQueryPublisherTest extends PublisherTestBase<GenericRow> {\n+\n+  private WorkerExecutor workerExecutor;\n+  private TestQueryHandle queryHandle;\n+\n+  @Override\n+  protected Publisher<GenericRow> createPublisher() {\n+    this.workerExecutor = vertx.createSharedWorkerExecutor(\"test_workers\");\n+    BlockingQueryPublisher publisher = new BlockingQueryPublisher(context, workerExecutor);\n+    queryHandle = new TestQueryHandle(OptionalInt.empty());\n+    publisher.setQueryHandle(queryHandle);\n+    return publisher;\n+  }\n+\n+  @Override\n+  protected GenericRow expectedValue(final int i) {\n+    return generateRow(i);\n+  }\n+\n+  private BlockingQueryPublisher getBlockingQueryPublisher() {\n+    return (BlockingQueryPublisher) publisher;\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    super.tearDown();\n+    if (workerExecutor != null) {\n+      workerExecutor.close();\n+    }\n+  }\n+\n+  @Override\n+  protected void loadPublisher(final int num) throws Exception {\n+    for (int i = 0; i < num; i++) {\n+      getBlockingQueryPublisher().accept(generateRow(i));\n+    }\n+  }\n+\n+  private GenericRow generateRow(long num) {\n+    List<Object> l = new ArrayList<>();\n+    l.add(\"foo\" + num);\n+    l.add(num);\n+    l.add(num % 2 == 0);\n+    return GenericRow.fromList(l);\n+  }\n+\n+  @Test\n+  public void shouldStopQueryHandleOnClose() throws Exception {\n+    // When\n+    getBlockingQueryPublisher().close();\n+\n+    // Then\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+  }\n+\n+  @Test\n+  public void shouldNotStopQueryHandleOnCloseMoreThanOnce() throws Exception {\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+    assertThatEventually(queryHandle::getStopCalledTimes, is(1));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+    Thread.sleep(100);\n+\n+    // Then:\n+    assertThat(queryHandle.getStopCalledTimes(), is(1));\n+  }\n+\n+  @Test\n+  public void shouldCompleteWhenLimitReached() throws Exception {\n+    queryHandle = new TestQueryHandle(OptionalInt.of(10));\n+    getBlockingQueryPublisher().setQueryHandle(queryHandle);\n+\n+    loadPublisher(20);\n+    AsyncAssert asyncAssert = new AsyncAssert();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        asyncAssert.assertAsync(isCompleted(), equalTo(false));\n+        getSub().request(1);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    assertThatEventually(subscriber::isCompleted, equalTo(true));\n+    assertThat(subscriber.getValues(), hasSize(10));\n+    for (int i = 0; i < 10; i++) {\n+      assertThat(subscriber.getValues().get(i), equalTo(expectedValue(i)));\n+    }\n+    asyncAssert.throwAssert();\n+  }\n+\n+  @Test\n+  public void shouldNotAcceptAfterClose() throws Exception {\n+\n+    // Given:\n+    getBlockingQueryPublisher().close();\n+\n+    // When:\n+    AtomicBoolean onNextCalled = new AtomicBoolean();\n+    TestSubscriber<GenericRow> subscriber = new TestSubscriber<GenericRow>(context) {\n+      @Override\n+      public synchronized void onSubscribe(final Subscription sub) {\n+        super.onSubscribe(sub);\n+        sub.request(1);\n+      }\n+\n+      @Override\n+      public synchronized void onNext(final GenericRow value) {\n+        super.onNext(value);\n+        onNextCalled.set(true);\n+      }\n+    };\n+    subscribeOnContext(subscriber);\n+    loadPublisher(1);\n+\n+    // Then:\n+    Thread.sleep(100);\n+    assertThat(onNextCalled.get(), is(false));\n+  }\n+\n+  @Test\n+  public void shouldBlockIfQueueFull() throws Exception {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+\n+    // When:\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+\n+    // Then:\n+    assertThat(t.isAlive(), is(true));\n+    assertThat(exception.get(), is(nullValue()));\n+\n+    t.interrupt();\n+  }\n+\n+  @Test\n+  public void shouldReleaseBlockedThreadOnClose() {\n+    // Given:\n+    AtomicReference<Exception> exception = new AtomicReference<>();\n+    Thread t = new Thread(() -> {\n+      try {\n+        loadPublisher(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY + 1);\n+      } catch (Exception e) {\n+        exception.set(e);\n+      }\n+    });\n+    t.start();\n+    assertThatEventually(() -> getBlockingQueryPublisher().queueSize(),\n+        is(BlockingQueryPublisher.BLOCKING_QUEUE_CAPACITY));\n+    assertThat(t.isAlive(), is(true));\n+\n+    // When:\n+    getBlockingQueryPublisher().close();\n+\n+    // Then:\n+    assertThatEventually(t::isAlive, is(false));\n+    assertThat(exception.get(), is(nullValue()));\n+  }\n+\n+  @Test\n+  public void shouldDeliverMoreThanMaxSendBatchSize() throws Exception {\n+    int num = 2 * BlockingQueryPublisher.SEND_MAX_BATCH_SIZE;\n+    loadPublisher(num);\n+    shouldDeliver(num, num);\n+  }\n+\n+  @Test\n+  public void shouldDeliverAfterSubscribe() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODM3MA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 227}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4MzMxOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/test/java/io/confluent/ksql/api/tck/BlockingQueryPublisherVerificationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMTo1NlrOFn-2cA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODozNToyNFrOFoAiYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODUyOA==", "bodyText": "How do these TCK tests work? I'm having trouble finding docs.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468528", "createdAt": "2020-02-11T07:01:56Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/tck/BlockingQueryPublisherVerificationTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.tck;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.tck.PublisherVerification;\n+import org.reactivestreams.tck.TestEnvironment;\n+\n+public class BlockingQueryPublisherVerificationTest extends PublisherVerification<GenericRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5NjE2MA==", "bodyText": "https://github.com/reactive-streams/reactive-streams-jvm/tree/master/tck", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377496160", "createdAt": "2020-02-11T08:35:24Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/test/java/io/confluent/ksql/api/tck/BlockingQueryPublisherVerificationTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.tck;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.server.BlockingQueryPublisher;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.OptionalInt;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.tck.PublisherVerification;\n+import org.reactivestreams.tck.TestEnvironment;\n+\n+public class BlockingQueryPublisherVerificationTest extends PublisherVerification<GenericRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODUyOA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4NDA0OnYy", "diffSide": "LEFT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ErrorCodes.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMjoyNVrOFn-24w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowMDoxN1rOFoY5JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODY0Mw==", "bodyText": "Why the decision to remove the 500 prefixes? These error codes are user-facing, right?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468643", "createdAt": "2020-02-11T07:02:25Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ErrorCodes.java", "diffHunk": "@@ -23,11 +23,12 @@\n   private ErrorCodes() {\n   }\n \n-  public static final int ERROR_CODE_MISSING_PARAM = 50001;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5Nzk4NA==", "bodyText": "I don't think we should tie our error codes to HTTP status codes, they seem orthogonal.\nE.g. If I stream some inserts to the server, then I'll get a 200 OK response and the acks for the inserts will start coming up. Maybe the 100000th insert has some malformed JSON in which case the ack stream will contain an error \"malformed JSON\" and the response will be ended. The error here has got nothing to do with the HTTP status code (which was 200 in this case).", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377497984", "createdAt": "2020-02-11T08:40:18Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ErrorCodes.java", "diffHunk": "@@ -23,11 +23,12 @@\n   private ErrorCodes() {\n   }\n \n-  public static final int ERROR_CODE_MISSING_PARAM = 50001;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODY0Mw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5NTIwNA==", "bodyText": "These error codes are exposed to users, though, right? How will a user understand the meaning of the error codes? (Or is that not the purpose of the error codes?)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377895204", "createdAt": "2020-02-11T21:00:17Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ErrorCodes.java", "diffHunk": "@@ -23,11 +23,12 @@\n   private ErrorCodes() {\n   }\n \n-  public static final int ERROR_CODE_MISSING_PARAM = 50001;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODY0Mw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4NDU0OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMjo1MVrOFn-3Lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODo0Mjo1MFrOFoAteg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODcxOQ==", "bodyText": "Pardon the ignorance, but how do we know \"this stuff is slow\"?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468719", "createdAt": "2020-02-11T07:02:51Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5OTAwMg==", "bodyText": "Anything taking more than a very small number of milliseconds will be slow for an event loop. I think we should err on the side of caution.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377499002", "createdAt": "2020-02-11T08:42:50Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODcxOQ=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4NTk3OnYy", "diffSide": "RIGHT", "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowMzo0OVrOFn-3_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxMzowNDoxM1rOFotu5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODkyNg==", "bodyText": "Is this ACLs setup relevant to the tests in this file? If not, can we remove it?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377468926", "createdAt": "2020-02-11T07:03:49Z", "author": {"login": "vcrfxia"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODIzNjY0Ng==", "bodyText": "Yep, I think they can be removed :)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r378236646", "createdAt": "2020-02-12T13:04:13Z", "author": {"login": "purplefox"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2ODkyNg=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4Nzc1OnYy", "diffSide": "RIGHT", "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowNToxM1rOFn-5FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODo0NDo0OFrOFoAwrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTIwNA==", "bodyText": "Where is this value coming from? Properties is empty and push queries default to using auto.offset.reset=latest so I'm surprised rows are being returned in this query.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377469204", "createdAt": "2020-02-11T07:05:13Z", "author": {"login": "vcrfxia"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(CLUSTER, \"kafka-cluster\"),\n+                  ops(DESCRIBE_CONFIGS, CREATE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(TOPIC, \"_confluent-ksql-default_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, PAGE_VIEW_TOPIC),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_transient_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_query\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"X\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"AGG_TABLE\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(WRITE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(DESCRIBE)\n+              ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__consumer_offsets\"),\n+              ops(DESCRIBE)\n+          ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__transaction_state\"),\n+              ops(DESCRIBE)\n+          )\n+      )\n+      .build();\n+\n+  private static final TestKsqlRestApp REST_APP = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(\"security.protocol\", \"SASL_SSL\")\n+      .withProperty(\"sasl.mechanism\", \"PLAIN\")\n+      .withProperty(\"sasl.jaas.config\", SecureKafkaHelper.buildJaasConfig(NORMAL_USER))\n+      .withProperties(ClientTrustStore.trustStoreProps())\n+      .withProperty(\"ksql.new-api-enabled\", true)\n+      .withProperty(\"ksql.apiserver.host\", \"localhost\")\n+      .withProperty(\"ksql.apiserver.port\", 8089)\n+      .withProperty(\"ksql.apiserver.key-path\", findFilePath(\"test-server-key.pem\"))\n+      .withProperty(\"ksql.apiserver.cert-path\", findFilePath(\"test-server-cert.pem\"))\n+      .withProperty(\"ksql.apiserver.verticle-instances\", 4)\n+      .build();\n+\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain.outerRule(TEST_HARNESS).around(REST_APP);\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    TEST_HARNESS.ensureTopics(PAGE_VIEW_TOPIC);\n+\n+    TEST_HARNESS.produceRows(PAGE_VIEW_TOPIC, PAGE_VIEWS_PROVIDER, FormatFactory.JSON);\n+\n+    RestIntegrationTestUtil.createStream(REST_APP, PAGE_VIEWS_PROVIDER);\n+\n+    makeKsqlRequest(\"CREATE TABLE \" + AGG_TABLE + \" AS \"\n+        + \"SELECT COUNT(1) AS COUNT FROM \" + PAGE_VIEW_STREAM + \" GROUP BY USERID;\"\n+    );\n+  }\n+\n+  private Vertx vertx;\n+  private WebClient client;\n+\n+  @Before\n+  public void setUp() {\n+    vertx = Vertx.vertx();\n+    client = createClient();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (client != null) {\n+      client.close();\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+    }\n+    REST_APP.getServiceContext().close();\n+  }\n+\n+  private JsonArray expectedColumnNames = new JsonArray().add(\"ROWTIME\").add(\"ROWKEY\")\n+      .add(\"VIEWTIME\").add(\"USERID\").add(\"PAGEID\");\n+  private JsonArray expectedColumnTypes = new JsonArray().add(\"BIGINT\").add(\"BIGINT\")\n+      .add(\"BIGINT\").add(\"STRING\").add(\"STRING\");\n+\n+  @Test\n+  public void shouldExecutePushQueryWithLimit() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES LIMIT \" + 2 + \";\";\n+\n+    // When:\n+    QueryResponse response = executePushQuery(sql);\n+\n+    // Then:\n+    assertThat(response.rows, hasSize(2));\n+    assertThat(response.responseObject.getJsonArray(\"columnNames\"), is(expectedColumnNames));\n+    assertThat(response.responseObject.getJsonArray(\"columnTypes\"), is(expectedColumnTypes));\n+    assertThat(response.responseObject.getString(\"queryId\"), is(notNullValue()));\n+  }\n+\n+  @Test\n+  public void shouldFailWithInvalidSql() throws Exception {\n+\n+    // Given:\n+    String sql = \"SLECTT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"line 1:1: mismatched input 'SLECTT' expecting\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithMoreThanOneStatement() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\" +\n+        \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Expected exactly one KSQL statement; found 2 instead\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithNonQuery() throws Exception {\n+\n+    // Given:\n+    String sql =\n+        \"CREATE STREAM SOME_STREAM AS SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Not a query\");\n+  }\n+\n+  @Test\n+  public void shouldExecutePushQueryNoLimit() throws Exception {\n+\n+    KsqlEngine engine = (KsqlEngine) REST_APP.getEngine();\n+    // One persistent query for the agg table\n+    assertThatEventually(engine::numberOfLiveQueries, is(1));\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Create a write stream to capture the incomplete response\n+    ReceiveStream writeStream = new ReceiveStream(vertx);\n+\n+    // Make the request to stream a query\n+    JsonObject properties = new JsonObject();\n+    JsonObject requestBody = new JsonObject()\n+        .put(\"sql\", sql).put(\"properties\", properties);\n+    VertxCompletableFuture<HttpResponse<Void>> responseFuture = new VertxCompletableFuture<>();\n+    client.post(8089, \"localhost\", \"/query-stream\")\n+        .as(BodyCodec.pipe(writeStream))\n+        .sendJsonObject(requestBody, responseFuture);\n+\n+    assertThatEventually(engine::numberOfLiveQueries, is(2));\n+\n+    // Wait for all rows in the response to arrive\n+    assertThatEventually(() -> {\n+      try {\n+        Buffer buff = writeStream.getBody();\n+        QueryResponse queryResponse = new QueryResponse(buff.toString());\n+        return queryResponse.rows.size();\n+      } catch (Throwable t) {\n+        return Integer.MAX_VALUE;\n+      }\n+    }, is(7));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ5OTgyMw==", "bodyText": "I think it's set in TestKsqlRestApp. But we also need to set it in the query properties. Good catch!", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377499823", "createdAt": "2020-02-11T08:44:48Z", "author": {"login": "purplefox"}, "path": "ksql-rest-app/src/test/java/io/confluent/ksql/rest/integration/NewApiTest.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.api.utils.TestUtils.findFilePath;\n+import static io.confluent.ksql.test.util.AssertEventually.assertThatEventually;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER1;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.VALID_USER2;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.ops;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.prefixedResource;\n+import static io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster.resource;\n+import static org.apache.kafka.common.acl.AclOperation.ALL;\n+import static org.apache.kafka.common.acl.AclOperation.CREATE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\n+import static org.apache.kafka.common.acl.AclOperation.DESCRIBE_CONFIGS;\n+import static org.apache.kafka.common.acl.AclOperation.WRITE;\n+import static org.apache.kafka.common.resource.ResourceType.CLUSTER;\n+import static org.apache.kafka.common.resource.ResourceType.GROUP;\n+import static org.apache.kafka.common.resource.ResourceType.TOPIC;\n+import static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.startsWith;\n+\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.api.impl.VertxCompletableFuture;\n+import io.confluent.ksql.api.utils.QueryResponse;\n+import io.confluent.ksql.api.utils.ReceiveStream;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.test.util.EmbeddedSingleNodeKafkaCluster;\n+import io.confluent.ksql.test.util.secure.ClientTrustStore;\n+import io.confluent.ksql.test.util.secure.Credentials;\n+import io.confluent.ksql.test.util.secure.SecureKafkaHelper;\n+import io.confluent.ksql.util.PageViewDataProvider;\n+import io.vertx.core.Vertx;\n+import io.vertx.core.buffer.Buffer;\n+import io.vertx.core.http.HttpVersion;\n+import io.vertx.core.json.JsonArray;\n+import io.vertx.core.json.JsonObject;\n+import io.vertx.ext.web.client.HttpResponse;\n+import io.vertx.ext.web.client.WebClient;\n+import io.vertx.ext.web.client.WebClientOptions;\n+import io.vertx.ext.web.codec.BodyCodec;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+\n+@Category({IntegrationTest.class})\n+public class NewApiTest {\n+\n+  private static final PageViewDataProvider PAGE_VIEWS_PROVIDER = new PageViewDataProvider();\n+  private static final String PAGE_VIEW_TOPIC = PAGE_VIEWS_PROVIDER.topicName();\n+  private static final String PAGE_VIEW_STREAM = PAGE_VIEWS_PROVIDER.kstreamName();\n+\n+  private static final String AGG_TABLE = \"AGG_TABLE\";\n+  private static final Credentials SUPER_USER = VALID_USER1;\n+  private static final Credentials NORMAL_USER = VALID_USER2;\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.builder()\n+      .withKafkaCluster(\n+          EmbeddedSingleNodeKafkaCluster.newBuilder()\n+              .withoutPlainListeners()\n+              .withSaslSslListeners()\n+              .withAclsEnabled(SUPER_USER.username)\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(CLUSTER, \"kafka-cluster\"),\n+                  ops(DESCRIBE_CONFIGS, CREATE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(TOPIC, \"_confluent-ksql-default_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, PAGE_VIEW_TOPIC),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_transient_\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  prefixedResource(GROUP, \"_confluent-ksql-default_query\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"X\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TOPIC, \"AGG_TABLE\"),\n+                  ops(ALL)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(WRITE)\n+              )\n+              .withAcl(\n+                  NORMAL_USER,\n+                  resource(TRANSACTIONAL_ID, \"default_\"),\n+                  ops(DESCRIBE)\n+              ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__consumer_offsets\"),\n+              ops(DESCRIBE)\n+          ).withAcl(\n+              NORMAL_USER,\n+              resource(TOPIC, \"__transaction_state\"),\n+              ops(DESCRIBE)\n+          )\n+      )\n+      .build();\n+\n+  private static final TestKsqlRestApp REST_APP = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(\"security.protocol\", \"SASL_SSL\")\n+      .withProperty(\"sasl.mechanism\", \"PLAIN\")\n+      .withProperty(\"sasl.jaas.config\", SecureKafkaHelper.buildJaasConfig(NORMAL_USER))\n+      .withProperties(ClientTrustStore.trustStoreProps())\n+      .withProperty(\"ksql.new-api-enabled\", true)\n+      .withProperty(\"ksql.apiserver.host\", \"localhost\")\n+      .withProperty(\"ksql.apiserver.port\", 8089)\n+      .withProperty(\"ksql.apiserver.key-path\", findFilePath(\"test-server-key.pem\"))\n+      .withProperty(\"ksql.apiserver.cert-path\", findFilePath(\"test-server-cert.pem\"))\n+      .withProperty(\"ksql.apiserver.verticle-instances\", 4)\n+      .build();\n+\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain.outerRule(TEST_HARNESS).around(REST_APP);\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    TEST_HARNESS.ensureTopics(PAGE_VIEW_TOPIC);\n+\n+    TEST_HARNESS.produceRows(PAGE_VIEW_TOPIC, PAGE_VIEWS_PROVIDER, FormatFactory.JSON);\n+\n+    RestIntegrationTestUtil.createStream(REST_APP, PAGE_VIEWS_PROVIDER);\n+\n+    makeKsqlRequest(\"CREATE TABLE \" + AGG_TABLE + \" AS \"\n+        + \"SELECT COUNT(1) AS COUNT FROM \" + PAGE_VIEW_STREAM + \" GROUP BY USERID;\"\n+    );\n+  }\n+\n+  private Vertx vertx;\n+  private WebClient client;\n+\n+  @Before\n+  public void setUp() {\n+    vertx = Vertx.vertx();\n+    client = createClient();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (client != null) {\n+      client.close();\n+    }\n+    if (vertx != null) {\n+      vertx.close();\n+    }\n+    REST_APP.getServiceContext().close();\n+  }\n+\n+  private JsonArray expectedColumnNames = new JsonArray().add(\"ROWTIME\").add(\"ROWKEY\")\n+      .add(\"VIEWTIME\").add(\"USERID\").add(\"PAGEID\");\n+  private JsonArray expectedColumnTypes = new JsonArray().add(\"BIGINT\").add(\"BIGINT\")\n+      .add(\"BIGINT\").add(\"STRING\").add(\"STRING\");\n+\n+  @Test\n+  public void shouldExecutePushQueryWithLimit() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES LIMIT \" + 2 + \";\";\n+\n+    // When:\n+    QueryResponse response = executePushQuery(sql);\n+\n+    // Then:\n+    assertThat(response.rows, hasSize(2));\n+    assertThat(response.responseObject.getJsonArray(\"columnNames\"), is(expectedColumnNames));\n+    assertThat(response.responseObject.getJsonArray(\"columnTypes\"), is(expectedColumnTypes));\n+    assertThat(response.responseObject.getString(\"queryId\"), is(notNullValue()));\n+  }\n+\n+  @Test\n+  public void shouldFailWithInvalidSql() throws Exception {\n+\n+    // Given:\n+    String sql = \"SLECTT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"line 1:1: mismatched input 'SLECTT' expecting\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithMoreThanOneStatement() throws Exception {\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\" +\n+        \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Expected exactly one KSQL statement; found 2 instead\");\n+  }\n+\n+  @Test\n+  public void shouldFailWithNonQuery() throws Exception {\n+\n+    // Given:\n+    String sql =\n+        \"CREATE STREAM SOME_STREAM AS SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Then:\n+    shouldFail(sql, \"Not a query\");\n+  }\n+\n+  @Test\n+  public void shouldExecutePushQueryNoLimit() throws Exception {\n+\n+    KsqlEngine engine = (KsqlEngine) REST_APP.getEngine();\n+    // One persistent query for the agg table\n+    assertThatEventually(engine::numberOfLiveQueries, is(1));\n+\n+    // Given:\n+    String sql = \"SELECT * from \" + PAGE_VIEW_STREAM + \" EMIT CHANGES;\";\n+\n+    // Create a write stream to capture the incomplete response\n+    ReceiveStream writeStream = new ReceiveStream(vertx);\n+\n+    // Make the request to stream a query\n+    JsonObject properties = new JsonObject();\n+    JsonObject requestBody = new JsonObject()\n+        .put(\"sql\", sql).put(\"properties\", properties);\n+    VertxCompletableFuture<HttpResponse<Void>> responseFuture = new VertxCompletableFuture<>();\n+    client.post(8089, \"localhost\", \"/query-stream\")\n+        .as(BodyCodec.pipe(writeStream))\n+        .sendJsonObject(requestBody, responseFuture);\n+\n+    assertThatEventually(engine::numberOfLiveQueries, is(2));\n+\n+    // Wait for all rows in the response to arrive\n+    assertThatEventually(() -> {\n+      try {\n+        Buffer buff = writeStream.getBody();\n+        QueryResponse queryResponse = new QueryResponse(buff.toString());\n+        return queryResponse.rows.size();\n+      } catch (Throwable t) {\n+        return Integer.MAX_VALUE;\n+      }\n+    }, is(7));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTIwNA=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNTE4ODU4OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwNzowNTo1OFrOFn-5mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwODo0NTozMFrOFoAxzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTMzNw==", "bodyText": "nit: why not simply serviceContextFactory? (I assume there's a reason for diverging from convention -- I'm just not seeing what it is.)", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377469337", "createdAt": "2020-02-11T07:05:58Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzUwMDEwOA==", "bodyText": "It's vestigial from when there was something else called serviceContextFactory", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377500108", "createdAt": "2020-02-11T08:45:30Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ2OTMzNw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzIyNTYyOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzozMDo1NlrOFoSSAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzozMDo1NlrOFoSSAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4Njg4MQ==", "bodyText": "Can we use List rather than naked arrays please?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377786881", "createdAt": "2020-02-11T17:30:56Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/plugin/KsqlServerEndpoints.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.plugin;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.ksql.api.impl.Utils;\n+import io.confluent.ksql.api.server.BaseServerEndpoints;\n+import io.confluent.ksql.api.server.PushQueryHandler;\n+import io.confluent.ksql.api.spi.InsertsSubscriber;\n+import io.confluent.ksql.engine.KsqlEngine;\n+import io.confluent.ksql.parser.KsqlParser.ParsedStatement;\n+import io.confluent.ksql.parser.KsqlParser.PreparedStatement;\n+import io.confluent.ksql.parser.tree.Query;\n+import io.confluent.ksql.parser.tree.Statement;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.FormatOptions;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.registry.KsqlSchemaRegistryClientFactory;\n+import io.confluent.ksql.security.KsqlSecurityExtension;\n+import io.confluent.ksql.services.ServiceContext;\n+import io.confluent.ksql.statement.ConfiguredStatement;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlStatementException;\n+import io.confluent.ksql.util.QueryMetadata;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import io.vertx.core.json.JsonObject;\n+import java.security.Principal;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.function.Supplier;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.reactivestreams.Subscriber;\n+\n+public class KsqlServerEndpoints extends BaseServerEndpoints {\n+\n+  private final KsqlEngine ksqlEngine;\n+  private final KsqlConfig ksqlConfig;\n+  private final KsqlSecurityExtension securityExtension;\n+  private final ServiceContextFactory theServiceContextFactory;\n+\n+  public interface ServiceContextFactory {\n+\n+    ServiceContext create(\n+        KsqlConfig ksqlConfig,\n+        Optional<String> authHeader,\n+        KafkaClientSupplier kafkaClientSupplier,\n+        Supplier<SchemaRegistryClient> srClientFactory\n+    );\n+  }\n+\n+  public KsqlServerEndpoints(\n+      final KsqlEngine ksqlEngine,\n+      final KsqlConfig ksqlConfig,\n+      final KsqlSecurityExtension securityExtension,\n+      final ServiceContextFactory theServiceContextFactory) {\n+    this.ksqlEngine = Objects.requireNonNull(ksqlEngine);\n+    this.ksqlConfig = Objects.requireNonNull(ksqlConfig);\n+    this.securityExtension = Objects.requireNonNull(securityExtension);\n+    this.theServiceContextFactory = Objects.requireNonNull(theServiceContextFactory);\n+  }\n+\n+  @Override\n+  protected PushQueryHandler createQuery(final String sql, final JsonObject properties,\n+      final Context context, final WorkerExecutor workerExecutor, final RowConsumer rowConsumer) {\n+    // Must be run on worker as all this stuff is slow\n+    Utils.checkIsWorker();\n+\n+    final ServiceContext serviceContext = createServiceContext(new DummyPrincipal());\n+    final ConfiguredStatement<Query> statement = createStatement(sql, properties.getMap());\n+\n+    final QueryMetadata queryMetadata = ksqlEngine\n+        .executeQuery(serviceContext, statement, rowConsumer);\n+    return new KsqlQueryHandle(queryMetadata, statement.getStatement().getLimit());\n+  }\n+\n+  private ConfiguredStatement<Query> createStatement(final String queryString,\n+      final Map<String, Object> properties) {\n+    final List<ParsedStatement> statements = ksqlEngine.parse(queryString);\n+    if ((statements.size() != 1)) {\n+      throw new KsqlStatementException(\n+          String.format(\"Expected exactly one KSQL statement; found %d instead\", statements.size()),\n+          queryString);\n+    }\n+    final PreparedStatement<?> ps = ksqlEngine.prepare(statements.get(0));\n+    final Statement statement = ps.getStatement();\n+    if (!(statement instanceof Query)) {\n+      throw new KsqlStatementException(\"Not a query\", queryString);\n+    }\n+    @SuppressWarnings(\"unchecked\") final PreparedStatement<Query> psq =\n+        (PreparedStatement<Query>) ps;\n+    return ConfiguredStatement.of(psq, properties, ksqlConfig);\n+  }\n+\n+  private ServiceContext createServiceContext(final Principal principal) {\n+    // Creates a ServiceContext using the user's credentials, so the WS query topics are\n+    // accessed with the user permission context (defaults to KSQL service context)\n+\n+    if (!securityExtension.getUserContextProvider().isPresent()) {\n+      return createServiceContext(new DefaultKafkaClientSupplier(),\n+          new KsqlSchemaRegistryClientFactory(ksqlConfig, Collections.emptyMap())::get);\n+    }\n+\n+    return securityExtension.getUserContextProvider()\n+        .map(provider ->\n+            createServiceContext(\n+                provider.getKafkaClientSupplier(principal),\n+                provider.getSchemaRegistryClientFactory(principal)\n+            ))\n+        .get();\n+  }\n+\n+  private ServiceContext createServiceContext(\n+      final KafkaClientSupplier kafkaClientSupplier,\n+      final Supplier<SchemaRegistryClient> srClientFactory\n+  ) {\n+    return theServiceContextFactory.create(ksqlConfig,\n+        Optional.empty(),\n+        kafkaClientSupplier, srClientFactory);\n+  }\n+\n+  private static class DummyPrincipal implements Principal {\n+\n+    @Override\n+    public String getName() {\n+      return \"tim\";\n+    }\n+  }\n+\n+  @Override\n+  public InsertsSubscriber createInsertsSubscriber(final String target, final JsonObject properties,\n+      final Subscriber<JsonObject> acksSubscriber) {\n+    return null;\n+  }\n+\n+  private static class KsqlQueryHandle implements PushQueryHandler {\n+\n+    private final QueryMetadata queryMetadata;\n+    private final OptionalInt limit;\n+\n+    KsqlQueryHandle(final QueryMetadata queryMetadata, final OptionalInt limit) {\n+      this.queryMetadata = queryMetadata;\n+      this.limit = limit;\n+    }\n+\n+    @Override\n+    public String[] getColumnNames() {\n+      return colNamesFromSchema(queryMetadata.getLogicalSchema());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzIzOTI0OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzozNTowNVrOFoSaZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzozNTowNVrOFoSaZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4OTAzMQ==", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377789031", "createdAt": "2020-02-11T17:35:05Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BasePublisher.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import io.vertx.core.Context;\n+import io.vertx.core.Vertx;\n+import java.util.Objects;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Base class for our reactive streams publishers\n+ *\n+ * @param <T> the type of the element\n+ */\n+public abstract class BasePublisher<T> implements Publisher<T> {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BasePublisher.class);\n+\n+  protected final Context ctx;\n+  protected Subscriber<? super T> subscriber;\n+  protected long demand;\n+  protected boolean cancelled;\n+\n+  public BasePublisher(final Context ctx) {\n+    this.ctx = ctx;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzI1NjE1OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0MDoxM1rOFoSksw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxODo1NjoxMFrOFoVEAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MTY2Nw==", "bodyText": "Try to avoid such 'I wish Streams did it this way' style comments in the code.   Such points are for discussions, not comments in code.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377791667", "createdAt": "2020-02-11T17:40:13Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgzMjQ0OQ==", "bodyText": "+1 one this one", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377832449", "createdAt": "2020-02-11T18:56:10Z", "author": {"login": "agavra"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5MTY2Nw=="}, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzI2NDMzOnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0Mjo0OVrOFoSpzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxNzo0Mjo0OVrOFoSpzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5Mjk3NQ==", "bodyText": "Can we use List not arrays please.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377792975", "createdAt": "2020-02-11T17:42:49Z", "author": {"login": "big-andy-coates"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/BlockingQueryPublisher.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.api.server;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.api.spi.QueryPublisher;\n+import io.confluent.ksql.query.RowConsumer;\n+import io.vertx.core.Context;\n+import io.vertx.core.WorkerExecutor;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A query publisher that uses an internal blocking queue to store rows for delivery. It's currently\n+ * necessary to use a blocking queue as Kafka Streams delivers message in a synchronous fashion with\n+ * no back pressure. If the queue was not blocking then if the subscriber was slow the messages\n+ * could build up on the queue eventually resulting in out of memory. The only mechanism we have to\n+ * slow streams down is to block the thread. Kafka Streams uses dedicated streams per topology so\n+ * this won't prevent the thread from doing useful work elsewhere but it does mean we can't have too\n+ * many push queries in the server at any one time as we can end up with a lot of threads. Ideally\n+ * Kafka Streams would use a non-blocking reactive model with back-pressure, e.g. using reactive\n+ * streams. That way a small number of threads would be required to service all topologies and we\n+ * wouldn't need to block.\n+ */\n+public class BlockingQueryPublisher extends BasePublisher<GenericRow>\n+    implements QueryPublisher, RowConsumer {\n+\n+  private static final Logger log = LoggerFactory.getLogger(BlockingQueryPublisher.class);\n+\n+  public static final int SEND_MAX_BATCH_SIZE = 10;\n+  public static final int BLOCKING_QUEUE_CAPACITY = 1000;\n+\n+  private final BlockingQueue<GenericRow> queue = new LinkedBlockingQueue<>(\n+      BLOCKING_QUEUE_CAPACITY);\n+  private final WorkerExecutor workerExecutor;\n+  private PushQueryHandler queryHandle;\n+  private String[] columnNames;\n+  private String[] columnTypes;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8f948742a0ce64ab808dd9e5352bba82a613252d"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzkwODI2OnYy", "diffSide": "RIGHT", "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowMzozN1rOFoY_gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo0MDowNlrOFoaIuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5NjgzNA==", "bodyText": "Why does it make sense to have so many workers relative to the number of verticles? Is the expectation that multiple blocking tasks created by the same will be running simultaneously?", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377896834", "createdAt": "2020-02-11T21:03:37Z", "author": {"login": "vcrfxia"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -30,31 +30,36 @@\n \n   private static final String PROPERTY_PREFIX = \"apiserver.\";\n \n-  public static final String VERTICLE_INSTANCES = propertyName(\"verticle-instances\");\n+  public static final String VERTICLE_INSTANCES = propertyName(\"verticle.instances\");\n   public static final int DEFAULT_VERTICLE_INSTANCES =\n       2 * Runtime.getRuntime().availableProcessors();\n   public static final String VERTICLE_INSTANCES_DOC =\n       \"The number of server verticle instances to start. Usually you want at least many instances\"\n           + \" as there are cores you want to use, as each instance is single threaded.\";\n \n-  public static final String LISTEN_HOST = propertyName(\"listen-host\");\n+  public static final String LISTEN_HOST = propertyName(\"listen.host\");\n   public static final String DEFAULT_LISTEN_HOST = \"0.0.0.0\";\n   public static final String LISTEN_HOST_DOC =\n       \"The hostname to listen on\";\n \n-  public static final String LISTEN_PORT = propertyName(\"listen-port\");\n+  public static final String LISTEN_PORT = propertyName(\"listen.port\");\n   public static final int DEFAULT_LISTEN_PORT = 8089;\n   public static final String LISTEN_PORT_DOC =\n       \"The port to listen on\";\n \n-  public static final String KEY_PATH = propertyName(\"key-path\");\n+  public static final String KEY_PATH = propertyName(\"key.path\");\n   public static final String KEY_PATH_DOC =\n       \"Path to key file\";\n \n-  public static final String CERT_PATH = propertyName(\"cert-path\");\n+  public static final String CERT_PATH = propertyName(\"cert.path\");\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker.pool.size\");\n+  public static final String WORKER_POOL_DOC =\n+      \"Max number of worker threads for executing blocking code\";\n+  public static final int DEFAULT_WORKER_POOL_SIZE = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5556da84c938f73c30ede292421f051a856bee55"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkxNTU3OQ==", "bodyText": "We choose the number of event loops to be about the same as the number of cores because event loops should always be live, therefore you shouldn't need any more to utilise all the cores.\nWorkers are often used to execute blocking tasks - often blocking on IO, e.g. waiting on the network or file system etc. In this case the threads can spend a lot of their time inactive so in order to utilise all cores we choose a much larger number of threads than number of cores.\nIn our case if we find our workers aren't blocking on IO much, just doing long lived CPU tasks then we can probably get away with fewer, although we probably still want a reasonable amount so we don't end up with very long lived tasks causing those behind them to queue too long.", "url": "https://github.com/confluentinc/ksql/pull/4495#discussion_r377915579", "createdAt": "2020-02-11T21:40:06Z", "author": {"login": "purplefox"}, "path": "ksql-api/src/main/java/io/confluent/ksql/api/server/ApiServerConfig.java", "diffHunk": "@@ -30,31 +30,36 @@\n \n   private static final String PROPERTY_PREFIX = \"apiserver.\";\n \n-  public static final String VERTICLE_INSTANCES = propertyName(\"verticle-instances\");\n+  public static final String VERTICLE_INSTANCES = propertyName(\"verticle.instances\");\n   public static final int DEFAULT_VERTICLE_INSTANCES =\n       2 * Runtime.getRuntime().availableProcessors();\n   public static final String VERTICLE_INSTANCES_DOC =\n       \"The number of server verticle instances to start. Usually you want at least many instances\"\n           + \" as there are cores you want to use, as each instance is single threaded.\";\n \n-  public static final String LISTEN_HOST = propertyName(\"listen-host\");\n+  public static final String LISTEN_HOST = propertyName(\"listen.host\");\n   public static final String DEFAULT_LISTEN_HOST = \"0.0.0.0\";\n   public static final String LISTEN_HOST_DOC =\n       \"The hostname to listen on\";\n \n-  public static final String LISTEN_PORT = propertyName(\"listen-port\");\n+  public static final String LISTEN_PORT = propertyName(\"listen.port\");\n   public static final int DEFAULT_LISTEN_PORT = 8089;\n   public static final String LISTEN_PORT_DOC =\n       \"The port to listen on\";\n \n-  public static final String KEY_PATH = propertyName(\"key-path\");\n+  public static final String KEY_PATH = propertyName(\"key.path\");\n   public static final String KEY_PATH_DOC =\n       \"Path to key file\";\n \n-  public static final String CERT_PATH = propertyName(\"cert-path\");\n+  public static final String CERT_PATH = propertyName(\"cert.path\");\n   public static final String CERT_PATH_DOC =\n       \"Path to cert file\";\n \n+  public static final String WORKER_POOL_SIZE = propertyName(\"worker.pool.size\");\n+  public static final String WORKER_POOL_DOC =\n+      \"Max number of worker threads for executing blocking code\";\n+  public static final int DEFAULT_WORKER_POOL_SIZE = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5NjgzNA=="}, "originalCommit": {"oid": "5556da84c938f73c30ede292421f051a856bee55"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2132, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}