{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MzIyMDQx", "number": 5965, "title": "docs: klip-32: A SQL Testing Tool", "bodyText": "Rendered: https://github.com/confluentinc/ksql/blob/master/design-proposals/klip-32-sql-testing-tool.md", "createdAt": "2020-08-07T00:08:49Z", "url": "https://github.com/confluentinc/ksql/pull/5965", "merged": true, "mergeCommit": {"oid": "b65d82587b9a55dfeb7b4001fb047f1333ac897a"}, "closed": true, "closedAt": "2020-08-11T22:17:06Z", "author": {"login": "agavra"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc8Y-z8AH2gAyNDY0MzIyMDQxOmVjNzRkN2Y1ZDgxYmEyOGVkOTY1YzM3NDAzOGM2YmRhMDA5MTNlYzk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc99kocgH2gAyNDY0MzIyMDQxOjQyNjMxNzNjOTg5MTAzMjdkZDQ0OTk1ZDQyZDg3OWRlYjA4YzE0ZmI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9", "author": {"user": {"login": "agavra", "name": "Almog Gavra"}}, "url": "https://github.com/confluentinc/ksql/commit/ec74d7f5d81ba28ed965c374038c6bda00913ec9", "committedDate": "2020-08-07T00:07:52Z", "message": "docs: klip-32 - a SQL testing tool"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzMjkxNzM1", "url": "https://github.com/confluentinc/ksql/pull/5965#pullrequestreview-463291735", "createdAt": "2020-08-07T13:00:06Z", "commit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzowMDowNlrOG9Y7CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1NjozOFrOG9a5Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyNDY0OA==", "bodyText": "We should think through the report format from a CI perspective. When YATT is driven by a CI process, it will be important to be easy to analyze the test reports programmatically. In addition to a human-readable report output, we should consider additional output formats like JSON.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467024648", "createdAt": "2020-08-07T13:00:06Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NzMzOA==", "bodyText": "Is it implied that you could also use the following to ensure no data after a specified offset?\nASSERT NO DATA source_name OFFSET 2;", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467047338", "createdAt": "2020-08-07T13:41:00Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MDgyMw==", "bodyText": "Can I request the ROWTIME column?\nIf/when we have support for PARTITION & OFFSET pseudocolumns in the future, could ASSERT DATA feasibly handle them, too?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467050823", "createdAt": "2020-08-07T13:46:44Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MTgzNA==", "bodyText": "I agree, it seems very useful to be able interactively explore and test expectations about data using the CLI. This implies support for the bring-your-own-Kafka extension described below.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467051834", "createdAt": "2020-08-07T13:48:21Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1Mjk4Mw==", "bodyText": "Another extension consideration: support for watching the test directory for file changes, yielding new test runs.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467052983", "createdAt": "2020-08-07T13:50:11Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents\n+\n+When data comparisons fail, we will leverage our SQL formatters to display the errors the\n+same way we display `SELECT` statements - in a tabular, easy-to-digest way.\n+\n+### Extensions\n+\n+There are some extensions to this testing tool that we may want to consider. I'm listing them here\n+because I'm currently in the mindset of thinking about them, and I don't want to forget!\n+\n+- _BYO Kafka_: we may want to allow users to \"plug in their own kafka\" cluster and run these tests\n+    against a real Kafka cluster. This would allow users to produce whatever data they want\n+    outside of this tool, mitigating the limitations described above. It also would allow them\n+    to \"debug\" further when something doesn't go the way they want by examining the input/output\n+    topics through ksqlDB.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NTIzOQ==", "bodyText": "testing tool\n\nI assume this means the ksql-test-runner? Might be good to clarify the term.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467055239", "createdAt": "2020-08-07T13:53:59Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NjkyMg==", "bodyText": "Makes sense not to cover a migration path for ksql-test-runner here, but it would be reasonable to consider deprecating and removing it over a few releases, right? We don't imagine a role for that tool in the future in other words, and it will be outmoded by the new tool before QTT.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467056922", "createdAt": "2020-08-07T13:56:38Z", "author": {"login": "colinhicks"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNDA3Njk1", "url": "https://github.com/confluentinc/ksql/pull/5965#pullrequestreview-463407695", "createdAt": "2020-08-07T15:32:37Z", "commit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTozMjozN1rOG9eYUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowODoxNVrOG9fk9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNDA2NA==", "bodyText": "As already discussed offline, adding yet another testing tool has the potential to slow us down unless we commit to replacing existing testing tools with it, eventually.\nIt's not uncommon for changes or enhancements to the core engine to need changes to the existing testing tools. There are already:\n\nQTT: Json based testing tool used by ksqlDB internally to test the engine.\nRQTT: Json based testing tool used by ksqlDB internally to test the rest api.\nTesting tool: the user facing testing tool. Uses the same code classes as QTT, but in a weird way.\n\nHaving to change all three can be a drain on time and resources, ultimately slowing us down.  Adding a fourth testing tool with slow us down more. We should underestimate the cost of maintaining and enhancing these tools.\nI love this proposal. This public facing testing tool is... not great.  Having a mid term plan to replace it with this is awesome.  We just need to make sure we resource it, ideally also replacing QTT, and maybe even RQTT.  Having a single testing tool, dogfooded by us, would speed up core development considerably.\nWhy am I saying all this? I guess I'm looking to gauge how much buy in there is for such a piece of work. Historically, the user facing testing tool hasn't got much love. Replacing it with something we dog food is important IMHO. Likewise, committing to replacing QTT with this will increase productivity. Who's in?!?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467114064", "createdAt": "2020-08-07T15:32:37Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNDgwNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n          \n          \n            \n            Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n          \n      \n    \n    \n  \n\n(See above: QTT, RQTT + testing tool)", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467114805", "createdAt": "2020-08-07T15:34:05Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNTUyNg==", "bodyText": "Can we include RQTT too do you think?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467115526", "createdAt": "2020-08-07T15:35:21Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNTY5Mw==", "bodyText": "+1", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467115693", "createdAt": "2020-08-07T15:35:40Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NjkyMg=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExNzE5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The best way to describe the testing tool is by motivating example. Below is an example test\n          \n          \n            \n            The best way to describe the testing tool is by a motivating example. Below is an example test\n          \n      \n    \n    \n  \n\n??", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467117193", "createdAt": "2020-08-07T15:38:23Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjY4NQ==", "bodyText": "The statement accepts a list of column names, so I don't see why any pseudo column couldn't be included.\nIn addition, we should support testing ALL columns.  If we only support testing the supplied list of columns, then there is no way to test no other columns are present.  I'd propose mirroring INSERT VALUES, so that the column list is optional. If not provided then all columns are expected, (excluding pseudo columns).  We should also syntax to allow all columns + pseudo columns, e.g. ASSERT DATA (ROWTIME, *) VALUES (123, 'a' , 1, 1.3).", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467122685", "createdAt": "2020-08-07T15:48:12Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1MDgyMw=="}, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyNTAzOQ==", "bodyText": "With ASSERT TYPE coming later...", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467125039", "createdAt": "2020-08-07T15:52:23Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyOTU5OA==", "bodyText": "One thing I'd love to get into QTT, but the large existing set of tests prohibits, is for tests to fail if anything unexpected ends up in the processing logger.  It would be great if we can add this, from the start, to YATT.\nOnly errors are written to the processing logger.  Test cases should not, generally, be generating such errors. Silently ignoring them means we/users can write tests that pass, yet would spam the processing log (and likely app log) if used in production.  We already know queries that generate a lot of processing errors have terrible performance.\nYATT could initially just fail the test case if there are entries in the processing logger. (We'll have to work out how to demux errors when running tests in parallel - probably need to add some kind of UUID or similar somehow).  Later we can add ASSERT LOG or similar to assert entries in the processing log, allowing for negative testing. Such negative testing is an important part of QTT.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467129598", "createdAt": "2020-08-07T16:00:41Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMDI0OQ==", "bodyText": "The parse outputs location information when parsing the SQL. So we can also link to the line within the test file that failed, much like QTT does today.  This can save a LOT of time when investigation test failures.\nCan we add this to the list please?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467130249", "createdAt": "2020-08-07T16:01:43Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from QTT/testing-tool\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by motivating example. Below is an example test\n+file:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing testing tool, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name;\n+```\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and the testing tool.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing testing tool is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzY4Nw==", "bodyText": "I'm not seeing a sketch of how this will be implemented within the KLIP.\nWill this be implemented using a fully blown ksqlDB server, running against a (embedded) Kafka, Connect, Schema Registry? Or will it be using the Streams test driver?\nIf the former, it's likely to be much slower that QTT. Plus, how will the tool support running tests in parallel?\nAs discussed offline, ideally, it would support different modes of operation, e.g.\n\ntest driver vs running against Kafka\nUsing engine directly (like QTT) vs using the rest endpoint (like RQTT)\nembedded kafka and other services vs  external\n\nBut I'm really not sure this is easily achievable!  So it would be good to know how this initial version will be implemented...", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r467133687", "createdAt": "2020-08-07T16:08:15Z", "author": {"login": "big-andy-coates"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,238 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing testing tool and maybe QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have two existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the testing tool and QTT have, over time, diverged significantly from the ksqlDB engine and require\n+    lots of custom code (the ksqldb-functional-test module has 11k lines of Java). Specifically, the\n+    custom JSON format requires maintaining a parallel, testing-only framework and serde to convert \n+    inputs/outputs to their desired serialization format. Instead, we should leverage the production \n+    SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace QTT/testing-tool\n+- sketch out an implementation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec74d7f5d81ba28ed965c374038c6bda00913ec9"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ab47b5b5b490e75943ed2826509edf6062df2fa", "author": {"user": {"login": "agavra", "name": "Almog Gavra"}}, "url": "https://github.com/confluentinc/ksql/commit/9ab47b5b5b490e75943ed2826509edf6062df2fa", "committedDate": "2020-08-07T18:10:15Z", "message": "docs: address andy and colins feedback on klip-32"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629", "author": {"user": {"login": "agavra", "name": "Almog Gavra"}}, "url": "https://github.com/confluentinc/ksql/commit/10ff6e5e588d1116543ce9ebc0a5f897dd3f1629", "committedDate": "2020-08-10T16:45:47Z", "message": "chore: add impl and assert type"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0NDU1Njc4", "url": "https://github.com/confluentinc/ksql/pull/5965#pullrequestreview-464455678", "createdAt": "2020-08-10T18:02:33Z", "commit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowMjozM1rOG-ZhMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoyMjo1MFrOG-aLuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4Mjk5Mg==", "bodyText": "What about asserting data in internal topics (e.g., repartition topics) as some QTT tests currently do?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468082992", "createdAt": "2020-08-10T18:02:33Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDkwMQ==", "bodyText": "Is this not something we want to eventually support outside of testing purposes? Either (1) there are no valid use cases for inserting nulls into streams, in which case why are we testing it, or (2) there are valid use cases for inserting nulls into streams, and we should support it in ksqlDB.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468084901", "createdAt": "2020-08-10T18:06:01Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NzUwOQ==", "bodyText": "Does this mean repurposing the existing print topic syntax for use in the testing tool? This functionality makes a lot of sense but I wonder if repurposing existing syntax for something else will become confusing or problematic if we later add support for push queries/pull queries/print topic statements in the testing tool.", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468087509", "createdAt": "2020-08-10T18:10:55Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the\n+tests in the directory in a single JVM.\n+\n+Each test file can contain one or more tests, separated by the `--@test` directive. This will\n+improve on a pain point of the existing ksql-test-runner, which requires three files for a single\n+test. The test name is a concatenation of the test file time and the contents of the `--@test`\n+directive.\n+\n+Then, the test can contain any number of interleaved statements that are supported by the testing\n+tool. Data will be inserted into topics using `INSERT INTO` and asserts will be executed using the\n+new `ASSERT` syntax (described below).\n+\n+There will be additional \"meta\" directives that will allow for testing functionality like expected\n+exceptions.\n+\n+### The `ASSERT` Statement\n+\n+`ASSERT` is the primary way to ensure conditions in YATT, and will be parsed and implemented \n+using ksqlDB's `AstBuilder` to make sure YATT can leverage the existing expression parsing support. \n+\n+At first, `ASSERT` statements will fail if they are sent to a production server, but we may consider\n+allowing `ASSERT` in the future in order to allow REPL-based testing/experimentation.\n+\n+#### `ASSERT DATA`\n+\n+The `ASSERT DATA` statement asserts that data exists at an optionally provided offset (or otherwise \n+sequential based on the last `ASSERT DATA`) with the given values. If the `ASSERT DATA` does not\n+specify certain columns they will be ignored (allowing users to assert only subset of the columns\n+match what they expect).\n+\n+The values will be created in the same way that `INSERT INTO` creates values:\n+```sql\n+ASSERT DATA source_name [OFFSET at_offset] ( { column_name } [, ...] ) VALUES ( value [, ...] );\n+```\n+\n+The `ASSERT DATA` statement will allow users to specify psuedocolumns as well, such as `ROWTIME`,\n+and when ksqlDB supports constructs such as `PARTITION` and/or headers, YATT will inherit these\n+psuedocolumns as well.\n+\n+#### `ASSERT NO DATA`\n+\n+The `ASSERT NO DATA` statement will allow users to ensure that no more data exists in a specified\n+source:\n+```sql\n+ASSERT NO DATA source_name [OFFSET from_offset];\n+```\n+\n+#### `ASSERT SOURCE`\n+\n+The `ASSERT (TABLE | STREAM)` statement asserts that the given stream or table has the specified\n+columns and physical properties.\n+```sql\n+ASSERT (STREAM | TABLE) source_name ( { column_name data_type [[PRIMARY] KEY] } [, ...] )\n+    WITH (property_name = expression [, ...] );\n+```\n+\n+#### `ASSERT TYPE`\n+\n+The `ASSERT TYPE` statement ensures that a custom type has the expected type. This is especially\n+useful when chaining multiple `CREATE TYPE` statements together and asserting the types later\n+in the chain are correct.\n+```sql\n+ASSERT TYPE type_name AS type;\n+```\n+\n+#### Considered Alternative\n+\n+It is possible to use a meta directive (see below) in order to drive asserts as well. This was\n+considered and rejected because it would require additional parsing and more testing-specific code\n+to convert to expressions and pass into the codegen, which diverges from the motivating principle \n+of minimizing testing-only code.\n+\n+### Meta Directives\n+\n+Some functionality cannot be addressed in standard SQL syntax and will instead be supported using\n+meta directives. Some examples of these directives:\n+\n+- The `--@expected.error` and `--@expected.message` directives will indicate that the below test \n+    expects an exception to be thrown with the specified message and type.\n+- The `--@topic.denylist` directive will ensure that any topics matching the denylist will not\n+    be present at the end of the test execution.\n+\n+### Global Checks\n+\n+These are a set of checks that will happen without any directive or `ASSERT` statement. Some\n+checks that we may consider including:\n+\n+- _Processing Log Check_: this check will ensure that there are no failures in the processing log.\n+    If the test case intends to check that the processing log contains certain entries, this check\n+    can be disabled on a test-by-test basis and assert via `ASSERT DATA` on the processing log\n+    stream.\n+    \n+### Language Gaps\n+\n+There are some features that we need to implement in ksqlDB's SQL language in order to put YATT\n+on par with the existing testing tools. Some of those are outlined here:\n+\n+- _Windowed Keys_: when we have structured key support, we can `ASSERT DATA` and just provide the\n+    struct as the value. Until then, windowed keys will either be unsupported or stringified.\n+- _Tombstones_: we will implement `DELETE FROM table WHERE key = value` syntax to allow tombstones\n+    to be inserted into tables, both for ksqlDB and YATT.\n+- _Null Values for Streams_: insert `null` values into streams will not be supported for the first\n+    iteration of YATT. Eventually, if we want to support this, we can add a directive like\n+    `--@null.value: topic key` to produce a `null` valued record into a specific topic/source.\n+- _Other Unsupported Inserts_: there are other types of inserts that ksqlDB doesn't allow for at the\n+    moment (e.g. `enum` support or binary formats). This is only somewhat a regression from the \n+    existing (R)QTT tests, so we may or may not decide to support it in YATT.\n+    \n+### Error Reporting & UX\n+\n+One of the bigger concerns with the existing ksql-test-runner is that it's helpful in letting\n+us know _that_ an error happened, but not so much _why_ it happened or even at times _what_\n+exactly happened. To avoid this pitfall, the errors will include:\n+\n+- the assert statement that failed, with a link to the line that failed in the test file\n+- the actual data, and the expected data\n+- the ability to add `PRINT` statements to debug the issue further by printing topic contents", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5Mzg4MQ==", "bodyText": "Should we also support passing in a single test file directly, so users can debug individual tests/test files without having to run an entire directory of tests?", "url": "https://github.com/confluentinc/ksql/pull/5965#discussion_r468093881", "createdAt": "2020-08-10T18:22:50Z", "author": {"login": "vcrfxia"}, "path": "design-proposals/klip-32-sql-testing-tool.md", "diffHunk": "@@ -0,0 +1,284 @@\n+# KLIP 32 - A SQL Testing  Tool\n+\n+**Author**: agavra | \n+**Release Target**: TBD | \n+**Status**: _Discussion_ | \n+**Discussion**: TBD\n+\n+**tl;dr:** _Introduce **Y**et **A**nother **T**esting **T**ool (YATT) that is written and driven \n+primarily by SQL-based syntax to add coverage for test cases that require interleaving statements\n+and inserts. Eventually, the goal is to replace the external facing ksql-test-runner and maybe \n+(R)QTT tests._\n+           \n+## Motivation and background\n+\n+Our existing testing infrastructure is powerful and can handle many of our existing use cases. It,\n+however, lacks the flexibility to be useful when testing query upgrades: neither framework supports\n+interleaving statements/inserts/asserts. This KLIP proposes a new, imperative, testing tool to\n+cover that use case and describes a way to replace the existing tools with this functionality.\n+\n+The list below enumerates the motivating design principles:\n+\n+1. the tool should support imperative tests, allowing interleaved statements/inserts/asserts\n+1. the tool should allow tests to be written primarily in SQL, avoiding special-syntax directives\n+    when possible\n+1. the tool should leverage as much of the ksqlDB engine as possible, keeping testing-specific\n+    code to a minimum. Whenever we require new functionality for the testing tool, we will first\n+    consider supporting it directly in ksqlDB.\n+1. the tool should run many tests quickly, with the ability to parallelize test runs\n+\n+Another minor benefit of a SQL-based testing tool is to allow inline comments describing statements\n+and asserts.\n+\n+### Why Not Reuse?\n+\n+Why introduce YATT (Yet Another Testing Tool) when we already have three existing options? There\n+are a few factors here:\n+\n+- the primary motivation is practical in nature: We need interleaved testing for the query upgrades \n+    work, and building this into QTT would require essentially an entire rewrite of both the existing\n+    tool and the historical test execution while maintaining or migrating all the historical plans. \n+    In order to deliver query upgrades (see KLIP-28) quickly, we will begin work on YATT while \n+    maintaining support for QTT tests.\n+- the ksql-test-runner and (R)QTT have, over time, diverged significantly from the ksqlDB engine and \n+    require lots of custom code (the ksqldb-functional-test module has 11k lines of Java). \n+    Specifically, the custom JSON format requires maintaining a parallel, testing-only framework and \n+    serde to convert inputs/outputs to their desired serialization format. Instead, we should \n+    leverage the production SQL expression functionality to produce data.\n+- providing a SQL-driven testing tool will mesh better with our product offering, allowing users to\n+    write tests without every leaving the \"SQL mindset\"\n+    \n+## Scope\n+\n+This KLIP covers:\n+\n+- design of the test file format\n+- design of the testing-specific directives\n+- outline features that need to be implemented in order to replace (R)QTT/testing-tool\n+\n+This KLIP does not cover:\n+\n+- implementation of a migration path from (R)QTT/testing-tool, but we plan to deprecate and remove\n+    the ksql-test-runner over the next few releases. At the time when we remove it, we will build\n+    some convenience scripts to help generate YATT sql files.\n+- generating historical plans from YATT sql tests\n+\n+The intention of this KLIP is to provide a solid foundation for YATT to eventually replace the other\n+testing tools and to make sure we are working towards a cleaner code base instead of littering\n+it with testing code that needs to be maintained and will eventually slow down our development\n+velocity. That being said, it does not cover all the details required, or provide a timeline, to\n+migrate the existing tools.\n+\n+## Design\n+\n+The best way to describe the testing tool is by a motivating example, such as the test file below:\n+\n+```sql\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - add filter\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE STREAM bar AS SELECT * FROM foo;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+ASSERT DATA bar OFFSET 1 (rowtime, id, col1) VALUES (1, '1', 1);\n+\n+CREATE OR REPLACE STREAM bar AS SELECT * FROM foo WHERE col1 = 123;\n+\n+INSERT INTO foo (rowtime, id, col1) VALUES (2, '2', 2);\n+INSERT INTO foo (rowtime, id, col1) VALUES (3, '3', 123);\n+\n+ASSERT DATA bar OFFSET 2 (rowtime, id, col1) VALUES (3, '3', 123);\n+ASSERT STREAM bar (id VARCHAR KEY, col1 INT) WITH (kafka_topic='BAR', value_format='JSON');\n+\n+---------------------------------------------------------------------------------------------------\n+--@test: dml - stream - change column\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: \"Cannot REPLACE data source: DataSource '`BAR`' has schema ...\"\n+---------------------------------------------------------------------------------------------------\n+\n+CREATE STREAM foo (id VARCHAR KEY, col1 VARCHAR, col2 VARCHAR) \n+    WITH (kafka_topic='foo', value_format='JSON');\n+\n+-- the below operation is invalid because \"col2\" will be missing from the\n+-- schema after renaming it to col3\n+CREATE STREAM bar AS SELECT id, col1, col2 FROM foo;\n+CREATE OR REPLACE STREAM bar AS SELECT id, col1, col2 AS col3 FROM foo;\n+```\n+\n+### Test Structure\n+\n+YATT will accept as a parameter a directory containing testing files, and will run all the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10ff6e5e588d1116543ce9ebc0a5f897dd3f1629"}, "originalPosition": 113}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b2f26e4dd362ac604619bccf5841655b0bfd0ca", "author": {"user": {"login": "agavra", "name": "Almog Gavra"}}, "url": "https://github.com/confluentinc/ksql/commit/2b2f26e4dd362ac604619bccf5841655b0bfd0ca", "committedDate": "2020-08-11T00:39:20Z", "message": "doc: minor changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4263173c98910327dd44995d42d879deb08c14fb", "author": {"user": {"login": "agavra", "name": "Almog Gavra"}}, "url": "https://github.com/confluentinc/ksql/commit/4263173c98910327dd44995d42d879deb08c14fb", "committedDate": "2020-08-11T21:19:41Z", "message": "docs: add CI/CD implications section"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4796, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}