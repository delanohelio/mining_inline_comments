{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0OTY1MzMz", "number": 4921, "reviewThreads": {"totalCount": 99, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxMjoxOFrODvm8Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxOTozOFrODwg9vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjQ3NjUwOnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxMjoxOFrOGCIa_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxMjoxOFrOGCIa_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg4ODMxNw==", "bodyText": "Some paragraph breaks might help readability. Also note typo in final sentence.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n          \n          \n            \n            A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n          \n          \n            \n            \n          \n          \n            \n            Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n          \n          \n            \n            \n          \n          \n            \n            A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404888317", "createdAt": "2020-04-07T15:12:18Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjQ4NDI0OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxMzo1MVrOGCIfsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxMzo1MVrOGCIfsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg4OTUyMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n          \n          \n            \n            Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n          \n          \n            \n            \n          \n          \n            \n            ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404889522", "createdAt": "2020-04-07T15:13:51Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjQ5ODY0OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxNjo1MlrOGCIowQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxNjo1MlrOGCIowQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5MTg0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n          \n          \n            \n            There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404891841", "createdAt": "2020-04-07T15:16:52Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjQ5OTk4OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxNzoxM1rOGCIpsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxNzoxM1rOGCIpsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5MjA4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n          \n          \n            \n            We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404892081", "createdAt": "2020-04-07T15:17:13Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUwOTQzOnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxOTowNlrOGCIvbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxOTowNlrOGCIvbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5MzU1MA==", "bodyText": "consider using env vars passed to the container here, e.g. MONGO_INITDB_ROOT_USERNAME", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404893550", "createdAt": "2020-04-07T15:19:06Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUxMjI4OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxOTo0MFrOGCIxPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToxOTo0MFrOGCIxPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5NDAxMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n          \n          \n            \n            Create the user for Debezium. This user has `root` on the `admin` database, and it can also access other databases needed for replication:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404894012", "createdAt": "2020-04-07T15:19:40Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUyMTY1OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMTozOFrOGCI3SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMTozOFrOGCI3SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5NTU2MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n          \n          \n            \n            Now we can ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404895560", "createdAt": "2020-04-07T15:21:38Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 355}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUyNDQ3OnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMjoxMlrOGCI5Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMjoxMlrOGCI5Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5NjAzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n          \n          \n            \n            Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelope that includes many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404896038", "createdAt": "2020-04-07T15:22:12Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 376}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUyNTYyOnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMjoyNVrOGCI53w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMjoyNVrOGCI53w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5NjIyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n          \n          \n            \n            Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelope:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404896223", "createdAt": "2020-04-07T15:22:25Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 378}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjUzMDAwOnYy", "diffSide": "RIGHT", "path": "docs-md/app/etl.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMzoxNFrOGCI8gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyMzoxNFrOGCI8gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5Njg5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n          \n          \n            \n            For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It simply infers the schema that Debezium writes with.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404896898", "createdAt": "2020-04-07T15:23:14Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/etl.md", "diffHunk": "@@ -0,0 +1,608 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it. Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems. A streaming ETL pipeline lets you stream events between arbitrary sources and sinks helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, your stream processors. It's hard to operate the entire stack as one. ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set` Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username and password specified in the Docker Compose file:\n+\n+```\n+mongo -u mongo-user -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 401}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjU0MTg0OnYy", "diffSide": "RIGHT", "path": "docs-md/app/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNTozNFrOGCJDxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNTozNFrOGCJDxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5ODc1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n          \n          \n            \n            A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n          \n          \n            \n            \n          \n          \n            \n            A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404898758", "createdAt": "2020-04-07T15:25:34Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/materialized.md", "diffHunk": "@@ -0,0 +1,365 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjU0Mzc2OnYy", "diffSide": "RIGHT", "path": "docs-md/app/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNTo1OFrOGCJFCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNTo1OFrOGCJFCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5OTA4MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can look query the materializations. This can work, but is there a better way?\n          \n          \n            \n            One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404899080", "createdAt": "2020-04-07T15:25:58Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/materialized.md", "diffHunk": "@@ -0,0 +1,365 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can look query the materializations. This can work, but is there a better way?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjU1ODc1OnYy", "diffSide": "RIGHT", "path": "docs-md/app/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyOTowMFrOGCJOVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyOTowMFrOGCJOVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDkwMTQ2MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n          \n          \n            \n            Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404901461", "createdAt": "2020-04-07T15:29:00Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/materialized.md", "diffHunk": "@@ -0,0 +1,365 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can look query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjU2Mjg5OnYy", "diffSide": "RIGHT", "path": "docs-md/app/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyOTo1MFrOGCJQ3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyOTo1MFrOGCJQ3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDkwMjEwOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            After running this, you should have a folder named `confluent-hub-components` with some jar files in it.\n          \n          \n            \n            After running this, you should have a directory named `confluent-hub-components` with some jar files in it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404902108", "createdAt": "2020-04-07T15:29:50Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/materialized.md", "diffHunk": "@@ -0,0 +1,365 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can look query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a folder named `confluent-hub-components` with some jar files in it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjU4MTExOnYy", "diffSide": "RIGHT", "path": "docs-md/app/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNTozMzoyMVrOGCJb6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNTozMzoyMVrOGCJb6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDkwNDkzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n          \n          \n            \n            For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r404904939", "createdAt": "2020-04-07T15:33:21Z", "author": {"login": "colinhicks"}, "path": "docs-md/app/materialized.md", "diffHunk": "@@ -0,0 +1,365 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads. A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: style=\"width:90%;\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can look query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: style=\"width:60%;\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a folder named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with. Run the following at the ksqlDB CLI:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab4fac68b1136a2c520ed0d790afd710db4c8637"}, "originalPosition": 279}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAwMjA0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0NzoxNFrOGCXXUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0NzoxNFrOGCXXUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzMzEzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ----------\n          \n          \n            \n            -----------", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405133136", "createdAt": "2020-04-07T21:47:14Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAwNDgzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0ODoxN1rOGCXZGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0ODoxN1rOGCXZGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzMzU5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n          \n          \n            \n            A streaming ETL pipeline, sometimes called a \u201cstreaming data pipeline\u201d, is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405133595", "createdAt": "2020-04-07T21:48:17Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAwODI4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0OToxOVrOGCXbJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo0OToxOVrOGCXbJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNDExOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n          \n          \n            \n            Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes, you may need to do something more complex, like enrich the events by joining them with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405134118", "createdAt": "2020-04-07T21:49:19Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAxMTIxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MDoxOFrOGCXc-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MDoxOFrOGCXc-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNDU4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n          \n          \n            \n            A streaming ETL pipeline enables streaming events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405134587", "createdAt": "2020-04-07T21:50:18Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAxNTcxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MjowNVrOGCXf6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MjowNVrOGCXf6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNTMzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n          \n          \n            \n            One way you might do this is to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in {{ site.ak }}, where a series of deployed programs transforms, aggregates, and joins the data together. The processed data can be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405135338", "createdAt": "2020-04-07T21:52:05Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAxNzE5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MjozOFrOGCXg3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MjozOFrOGCXg3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNTU4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Why ksqlDB\n          \n          \n            \n            Why ksqlDB?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405135583", "createdAt": "2020-04-07T21:52:38Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAxNzkyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1Mjo1MFrOGCXhPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1Mjo1MFrOGCXhPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNTY3Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ----------\n          \n          \n            \n            -----------", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405135677", "createdAt": "2020-04-07T21:52:50Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAyMDEzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MzozNVrOGCXijg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1MzozNVrOGCXijg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNjAxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n          \n          \n            \n            Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for {{ site.ak }}, connectors, and your stream processors. It's challenging to operate the entire stack as one.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405136014", "createdAt": "2020-04-07T21:53:35Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAyMzg5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NDo0OVrOGCXk6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NDo0OVrOGCXk6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNjYxNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n          \n          \n            \n            ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage ({{ site.ak }}) and compute (ksqlDB).", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405136617", "createdAt": "2020-04-07T21:54:49Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAyNjIyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NTo0NlrOGCXmWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NTo0NlrOGCXmWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNjk4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n          \n          \n            \n            Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can transform, join, and aggregate all of your streams together by using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405136987", "createdAt": "2020-04-07T21:55:46Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAyOTk1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NzowM1rOGCXopg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1NzowM1rOGCXopg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzNzU3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n          \n          \n            \n            This tutorial shows how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. It demonstrates capturing changes from Postgres and MongoDB databases, forwarding them into {{ site.ak }}, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405137574", "createdAt": "2020-04-07T21:57:03Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAzMzk5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1ODoyN1rOGCXrGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1ODoyN1rOGCXrGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzODIwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n          \n          \n            \n            To get started, download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get them by using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405138201", "createdAt": "2020-04-07T21:58:27Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAzNzY2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1OTo0M1rOGCXtaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTo1OTo0M1rOGCXtaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzODc5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n          \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405138793", "createdAt": "2020-04-07T21:59:43Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAzODQyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowMDowMFrOGCXt5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowMDowMFrOGCXt5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzODkxNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n          \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405138917", "createdAt": "2020-04-07T22:00:00Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDAzOTM1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowMDoyMVrOGCXugg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowMDoyMVrOGCXugg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEzOTA3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n          \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:{{ site.cprelease }}", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405139074", "createdAt": "2020-04-07T22:00:21Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA1MDU3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNDozMFrOGCX1gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNDozMFrOGCX1gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MDg2NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n          \n          \n            \n            Next, set up and launch the services in the stack. But before you bring it up, you need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, you launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Also, you must create an additional configuration file at `postgres/custom-config.conf` with the following content:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405140864", "createdAt": "2020-04-07T22:04:30Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA1NDgwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNjoyM1rOGCX4Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNjoyM1rOGCX4Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MTU5NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n          \n          \n            \n            With the Postgres configuration file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405141594", "createdAt": "2020-04-07T22:06:23Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA1NzEwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNzoxNFrOGCX5rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowNzoxNFrOGCX5rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MTkzMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n          \n          \n            \n            There are a couple things to notice here. The Postgres image mounts the custom configuration file that you wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables you gave it also set up a blank database called `customers`, along with a user named `postgres-user` that can access it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405141933", "createdAt": "2020-04-07T22:07:14Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 193}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA1OTc1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowODoyMVrOGCX7Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowODoyMVrOGCX7Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MjM3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n          \n          \n            \n            The compose file also sets up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB runs in this configuration to pick up changes from its oplog. In this case, you're just running a single-node replica set.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405142374", "createdAt": "2020-04-07T22:08:21Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 195}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA2MzM2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowOTo0NVrOGCX9xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjowOTo0NVrOGCX9xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0Mjk4MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n          \n          \n            \n            Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that you downloaded need to be on the classpath of ksqlDB when the server starts up.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405142980", "createdAt": "2020-04-07T22:09:45Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA2NDgxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxMDoxOVrOGCX-qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxMDoxOVrOGCX-qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MzIxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n          \n          \n            \n            It's pretty common for companies to keep their customer data in a relational database. You can model this information in a Postgres table. Start by logging into the container:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405143210", "createdAt": "2020-04-07T22:10:19Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 207}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA2NjgzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxMTowNFrOGCX_3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxMTowNFrOGCX_3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0MzUxOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n          \n          \n            \n            Create a table that represents the customers. For simplicity, model a customer with three columns: an id, a name, and the age of the person:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405143518", "createdAt": "2020-04-07T22:11:04Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA3NjY3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNDo0NlrOGCYF1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNDo0NlrOGCYF1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NTA0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n          \n          \n            \n            Now that Postgres is setup, you can configure MongoDB. Start by logging into the container:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405145044", "createdAt": "2020-04-07T22:14:46Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA3ODI1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNToyMFrOGCYGuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNToyMFrOGCYGuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NTI3Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n          \n          \n            \n            Now that this node has become the primary in the replica set, you need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405145273", "createdAt": "2020-04-07T22:15:20Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 253}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4MDAwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNjowM1rOGCYH0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNjowM1rOGCYH0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NTU1NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n          \n          \n            \n            Create a new role for Debezium. This role enables the user that you will create to access system-level collections, which are normally restricted:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405145554", "createdAt": "2020-04-07T22:16:03Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4MDc1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNjoyNVrOGCYIXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNjoyNVrOGCYIXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NTY5Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n          \n          \n            \n            Switch into the `admin` database and create the user here so that it can be authenticated:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405145692", "createdAt": "2020-04-07T22:16:25Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4MzExOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNzozMFrOGCYJ2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNzozMFrOGCYJ2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NjA3Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n          \n          \n            \n            With the user created, you can create the database for orders and shipments, which are stored as collections in a database called `logistics`:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405146073", "createdAt": "2020-04-07T22:17:30Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 309}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4NDI2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNzo1M1rOGCYKfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxNzo1M1rOGCYKfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NjIzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n          \n          \n            \n            Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that you created in your Postgres customers table:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405146239", "createdAt": "2020-04-07T22:17:53Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 327}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4NTA0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxODoxNVrOGCYLBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxODoxNVrOGCYLBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NjM3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n          \n          \n            \n            Do the same for shipments. Notice that the `order_id` references order ids you created in the previous collection.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405146374", "createdAt": "2020-04-07T22:18:15Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 337}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4NjUyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxODo1MFrOGCYL4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxODo1MFrOGCYL4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NjU5Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n          \n          \n            \n            With all of the seed data in place, you can process it with ksqlDB. Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405146592", "createdAt": "2020-04-07T22:18:50Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 349}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA4NzkwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxOToxM1rOGCYMpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxOToxM1rOGCYMpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0Njc4OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n          \n          \n            \n            Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405146789", "createdAt": "2020-04-07T22:19:13Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 355}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA5MDQ5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyMDoxM1rOGCYOKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyMDoxM1rOGCYOKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzE3Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n          \n          \n            \n            Now you can ask Debezium to stream the Postgres changelog into {{ site.ak }}. Invoke the following command in ksqlDB, which creates a Debezium source connector and writes all of its changes to {{ site.ak }} topics:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405147176", "createdAt": "2020-04-07T22:20:13Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 361}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDA5MjUyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyMTowOVrOGCYPbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMjo0MjozMlrOGDDgKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzUwMQ==", "bodyText": "You might want to add a \"Your output should resemble:\" block.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405147501", "createdAt": "2020-04-07T22:21:09Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg1NjI5Nw==", "bodyText": "Done, thanks!", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405856297", "createdAt": "2020-04-08T22:42:32Z", "author": {"login": "MichaelDrogalis"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzUwMQ=="}, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 381}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEwMjE4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNDo0MFrOGCYVRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNDo0MFrOGCYVRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0ODk5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n          \n          \n            \n            Notice that this statement specifies an `unwrap` transform. By default, Debezium sends all events in an envelope that includes many pieces of information about the change captured. For this tutorial, the app only uses the value after it changed, so the command tells {{ site.kconnectlong }} to keep this information and discard the rest.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405148997", "createdAt": "2020-04-07T22:24:40Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 382}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEwMzcxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNToxOFrOGCYWPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNToxOFrOGCYWPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0OTI0Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n          \n          \n            \n            Run another source connector to ingest the changes from MongoDB. Specify the same behavior for discarding the Debezium envelope:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405149246", "createdAt": "2020-04-07T22:25:18Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 384}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEwNzc5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNjo1NlrOGCYYug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNjo1NlrOGCYYug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0OTg4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n          \n          \n            \n            For ksqlDB to be able to use the topics that Debezium created, you must declare streams over it. Because you configured {{ site.kconnectlong }} with {{ site.sr }}, you don't need to declare the schema of the data for the streams, because it's inferred from the schema that Debezium writes with.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405149882", "createdAt": "2020-04-07T22:26:56Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 407}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEwODQ4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNzoxNFrOGCYZIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyNzoxNFrOGCYZIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0OTk4NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run the following to create a stream over the `customers` table:\n          \n          \n            \n            Run the following statement to create a stream over the `customers` table:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405149985", "createdAt": "2020-04-07T22:27:14Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 409}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDExMjIyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyODo0NlrOGCYbeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyODo0NlrOGCYbeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MDU4Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n          \n          \n            \n            Do the same for `orders`. For this stream, specify that the timestamp of the event is derived from the data itself. Specifically, it's extracted and parsed from the `ts` field.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405150586", "createdAt": "2020-04-07T22:28:46Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 418}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDExNDQ1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyOToyNlrOGCYcqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyOToyNlrOGCYcqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MDg5MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Lastly, repeat the same for `shipments`:\n          \n          \n            \n            Finally, repeat the same for `shipments`:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405150891", "createdAt": "2020-04-07T22:29:26Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 429}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDExODQ0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMTowMFrOGCYfKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMTowMFrOGCYfKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MTUyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n          \n          \n            \n            The goal is to create a unified view of the activity of shipped orders. To do this, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. Use this identifier to look up the rest of the information by using a stream/table join. To do this, you must re-key the stream into a table by the `id` field:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405151528", "createdAt": "2020-04-07T22:31:00Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 442}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDExOTYwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMTozNlrOGCYf5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMTozNlrOGCYf5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MTcxOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:\n          \n          \n            \n            Now you can enrich the orders with more customer information. The following stream/table join creates a new stream that lifts the customer information into the order event:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405151719", "createdAt": "2020-04-07T22:31:36Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n+\n+```sql\n+CREATE TABLE customers_by_key AS\n+    SELECT id,\n+           latest_by_offset(name) AS name,\n+           latest_by_offset(age) AS age\n+    FROM customers\n+    GROUP BY id\n+    EMIT CHANGES;\n+```\n+\n+Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 454}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEyMDkxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMjowNlrOGCYgsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozMjowNlrOGCYgsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MTkyMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We can take this further by enriching all shipments with more information about the order and customer. We use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:\n          \n          \n            \n            You can take this further by enriching all shipments with more information about the order and customer. Use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405151922", "createdAt": "2020-04-07T22:32:06Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n+\n+```sql\n+CREATE TABLE customers_by_key AS\n+    SELECT id,\n+           latest_by_offset(name) AS name,\n+           latest_by_offset(age) AS age\n+    FROM customers\n+    GROUP BY id\n+    EMIT CHANGES;\n+```\n+\n+Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:\n+\n+```sql\n+CREATE STREAM enriched_orders AS\n+    SELECT o.order_id,\n+           o.price,\n+           o.currency,\n+           c.id AS customer_id,\n+           c.name AS customer_name,\n+           c.age AS customer_age\n+    FROM orders AS o\n+    LEFT JOIN customers_by_key c\n+    ON o.customer_id = c.ROWKEY\n+    EMIT CHANGES;\n+```\n+\n+We can take this further by enriching all shipments with more information about the order and customer. We use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 470}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEyNjM4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNDowOVrOGCYjxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNDowOVrOGCYjxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MjcwOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should see something like the following:\n          \n          \n            \n            Your output should resemble:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405152709", "createdAt": "2020-04-07T22:34:09Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n+\n+```sql\n+CREATE TABLE customers_by_key AS\n+    SELECT id,\n+           latest_by_offset(name) AS name,\n+           latest_by_offset(age) AS age\n+    FROM customers\n+    GROUP BY id\n+    EMIT CHANGES;\n+```\n+\n+Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:\n+\n+```sql\n+CREATE STREAM enriched_orders AS\n+    SELECT o.order_id,\n+           o.price,\n+           o.currency,\n+           c.id AS customer_id,\n+           c.name AS customer_name,\n+           c.age AS customer_age\n+    FROM orders AS o\n+    LEFT JOIN customers_by_key c\n+    ON o.customer_id = c.ROWKEY\n+    EMIT CHANGES;\n+```\n+\n+We can take this further by enriching all shipments with more information about the order and customer. We use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:\n+\n+```sql\n+CREATE STREAM shipped_orders WITH (\n+    kafka_topic = 'shipped_orders'\n+) AS\n+    SELECT o.order_id,\n+           s.shipment_id,\n+           o.customer_id,\n+           o.customer_name,\n+           o.customer_age,\n+           s.origin,\n+           o.price,\n+           o.currency\n+    FROM enriched_orders AS o\n+    INNER JOIN shipments s\n+    WITHIN 7 DAYS\n+    ON s.order_id = o.order_id\n+    EMIT CHANGES;\n+```\n+\n+### Start the Elasticsearch sink connector\n+\n+We want to perform searches and analytics over this unified stream of information. Let's spill the information out to Elasticsearch to make that easy. Simply run the following connector to sink the topic:\n+\n+```sql\n+CREATE SINK CONNECTOR enriched_writer WITH (\n+    'connector.class' = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',\n+    'connection.url' = 'http://elastic:9200',\n+    'type.name' = 'kafka-connect',\n+    'topics' = 'shipped_orders'\n+);\n+```\n+\n+Check that the data arrived in the index by running the following from your host:\n+\n+```\n+curl http://localhost:9200/shipped_orders/_search?pretty\n+```\n+\n+You should see something like the following:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 510}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEyODQ4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNTowNFrOGCYlCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNTowNFrOGCYlCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MzAzNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We want to perform searches and analytics over this unified stream of information. Let's spill the information out to Elasticsearch to make that easy. Simply run the following connector to sink the topic:\n          \n          \n            \n            The application must perform searches and analytics over this unified stream of information. To make this easy, spill the information out to Elasticsearch and run the following connector to sink the topic:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405153035", "createdAt": "2020-04-07T22:35:04Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n+\n+```sql\n+CREATE TABLE customers_by_key AS\n+    SELECT id,\n+           latest_by_offset(name) AS name,\n+           latest_by_offset(age) AS age\n+    FROM customers\n+    GROUP BY id\n+    EMIT CHANGES;\n+```\n+\n+Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:\n+\n+```sql\n+CREATE STREAM enriched_orders AS\n+    SELECT o.order_id,\n+           o.price,\n+           o.currency,\n+           c.id AS customer_id,\n+           c.name AS customer_name,\n+           c.age AS customer_age\n+    FROM orders AS o\n+    LEFT JOIN customers_by_key c\n+    ON o.customer_id = c.ROWKEY\n+    EMIT CHANGES;\n+```\n+\n+We can take this further by enriching all shipments with more information about the order and customer. We use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:\n+\n+```sql\n+CREATE STREAM shipped_orders WITH (\n+    kafka_topic = 'shipped_orders'\n+) AS\n+    SELECT o.order_id,\n+           s.shipment_id,\n+           o.customer_id,\n+           o.customer_name,\n+           o.customer_age,\n+           s.origin,\n+           o.price,\n+           o.currency\n+    FROM enriched_orders AS o\n+    INNER JOIN shipments s\n+    WITHIN 7 DAYS\n+    ON s.order_id = o.order_id\n+    EMIT CHANGES;\n+```\n+\n+### Start the Elasticsearch sink connector\n+\n+We want to perform searches and analytics over this unified stream of information. Let's spill the information out to Elasticsearch to make that easy. Simply run the following connector to sink the topic:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 493}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDEzMDYwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNTo1NlrOGCYmWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozNTo1NlrOGCYmWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1MzM2OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Check that the data arrived in the index by running the following from your host:\n          \n          \n            \n            Check that the data arrived in the index by running the following command from your host:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405153368", "createdAt": "2020-04-07T22:35:56Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+----------\n+\n+A streaming ETL pipeline (sometimes called a \u201cstreaming data pipeline\u201d) is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes you may need to do something more complex, like enrich the events by joining it with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline lets you stream events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in Kafka where a series of deployed programs transform, aggregate, and join the data together. The data can finally be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB\n+----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for Kafka, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding them in ksqlDB's servers. You can transform, join, and aggregate all of your streams together using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+In this tutorial, we\u2019ll show you how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. We\u2019ll demonstrate capturing changes from Postgres and MongoDB databases, forwarding them into Kafka, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, we'll need to download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.1 confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:5.4.1\n+```\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. But before we bring it up, we\u2019ll need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, we\u2019ll launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Beyond that, we\u2019ll need to make an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a couple things to notice here. The Postgres image mounts the custom configuration file that we wrote. Postgres adds these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `customers` along with a user named `postgres-user` that can access it.\n+\n+We\u2019ve also set up MongoDB as a replica set named `my-replica-set`. Debezium requires that MongoDB run in this configuration to pick up changes from its oplog. In this case, we\u2019re just running a single node replica set.\n+\n+Finally, note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the customers table in Postgres\n+\n+It's pretty common for companies to keep their customer data in a relational database. Let's model that information in a Postgres table. Start by logging into the container:\n+\n+```\n+docker exec -it postgres /bin/bash\n+```\n+\n+Log into Postgres as the user created by default:\n+\n+```\n+psql -U postgres-user customers\n+```\n+\n+Create a table that represents the customers. For simplicity's sake, we'll just model this with three columns: an id, a name, and the age of the person:\n+\n+```sql\n+CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);\n+```\n+\n+Seed the table with some initial data:\n+\n+```sql\n+INSERT INTO customers (id, name, age) VALUES ('5', 'fred', 34);\n+INSERT INTO customers (id, name, age) VALUES ('7', 'sue', 25);\n+INSERT INTO customers (id, name, age) VALUES ('2', 'bill', 51);\n+```\n+\n+### Configure MongoDB for Debezium\n+\n+Now that Postgres is setup, let's configure MongoDB. Start by logging into the container:\n+\n+```\n+docker exec -it mongo /bin/bash\n+```\n+\n+Log into the Mongo console using the username specified in the Docker Compose file:\n+\n+```\n+mongo -u $MONGO_INITDB_ROOT_USERNAME -p mongo-pw admin\n+```\n+\n+Because MongoDB has been started as a replica set, it needs to be initiated. Run the following command to kick it off:\n+\n+```javascript\n+rs.initiate()\n+```\n+\n+Now that this node has become the primary in the replica set, we need to configure access so that Debezium can replicate changes remotely. Switch into the `config` database:\n+\n+```\n+use config\n+```\n+\n+Create a new role for Debezium. This role will enable the user that we create to access system-level collections, which are normally restricted:\n+\n+```javascript\n+db.createRole({\n+    role: \"dbz-role\",\n+    privileges: [\n+        {\n+            resource: { db: \"config\", collection: \"system.sessions\" },\n+            actions: [ \"find\", \"update\", \"insert\", \"remove\" ]\n+        }\n+    ],\n+    roles: [\n+       { role: \"dbOwner\", db: \"config\" },\n+       { role: \"dbAdmin\", db: \"config\" },\n+       { role: \"readWrite\", db: \"config\" }\n+    ]\n+})\n+```\n+\n+Switch into the `admin` database. We need to create our user here so that it can be authenticated:\n+\n+```\n+use admin\n+```\n+\n+Create the user for Debezium. This user has `root` on the `admin` database, and can also access other databases needed for replication:\n+\n+```javascript\n+db.createUser({\n+  \"user\" : \"dbz-user\",\n+  \"pwd\": \"dbz-pw\",\n+  \"roles\" : [\n+    {\n+      \"role\" : \"root\",\n+      \"db\" : \"admin\"\n+    },\n+    {\n+      \"role\" : \"readWrite\",\n+      \"db\" : \"logistics\"\n+    },\n+    {\n+      \"role\" : \"dbz-role\",\n+      \"db\" : \"config\"\n+    }\n+  ]\n+})\n+```\n+\n+### Create the logistics collections in MongoDB\n+\n+With our user created, we can create our database for orders and shipments. We'll store both as collections in a database called `logistics`:\n+\n+```\n+use logistics\n+```\n+\n+First create the `orders`:\n+\n+```javascript\n+db.createCollection(\"orders\")\n+```\n+\n+And likewise the `shipments`:\n+\n+```javascript\n+db.createCollection(\"shipments\")\n+```\n+\n+Populate the `orders` collection with some initial data. Notice that the `customer_id` references identifiers that we created in our Postgres customers table:\n+\n+```javascript\n+db.orders.insert({\"customer_id\": \"2\", \"order_id\": \"13\", \"price\": 50.50, \"currency\": \"usd\", \"ts\": \"2020-04-03T11:20:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"29\", \"price\": 15.00, \"currency\": \"aud\", \"ts\": \"2020-04-02T12:36:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"17\", \"price\": 25.25, \"currency\": \"eur\", \"ts\": \"2020-04-02T17:22:00\"})\n+db.orders.insert({\"customer_id\": \"5\", \"order_id\": \"15\", \"price\": 13.75, \"currency\": \"usd\", \"ts\": \"2020-04-03T02:55:00\"})\n+db.orders.insert({\"customer_id\": \"7\", \"order_id\": \"22\", \"price\": 29.71, \"currency\": \"aud\", \"ts\": \"2020-04-04T00:12:00\"})\n+```\n+\n+Do the same for shipments. Notice that the `order_id` references order ids we created in the previous collection.\n+\n+```javascript\n+db.shipments.insert({\"order_id\": \"17\", \"shipment_id\": \"75\", \"origin\": \"texas\", \"ts\": \"2020-04-04T19:20:00\"})\n+db.shipments.insert({\"order_id\": \"22\", \"shipment_id\": \"71\", \"origin\": \"iowa\", \"ts\": \"2020-04-04T12:25:00\"})\n+db.shipments.insert({\"order_id\": \"29\", \"shipment_id\": \"89\", \"origin\": \"california\", \"ts\": \"2020-04-05T13:21:00\"})\n+db.shipments.insert({\"order_id\": \"13\", \"shipment_id\": \"92\", \"origin\": \"maine\", \"ts\": \"2020-04-04T06:13:00\"})\n+db.shipments.insert({\"order_id\": \"15\", \"shipment_id\": \"95\", \"origin\": \"florida\", \"ts\": \"2020-04-04T01:13:00\"})\n+```\n+\n+### Start the Postgres and MongoDB Debezium source connectors\n+\n+With all of our seed data in place, we can process it with ksqlDB. Connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream Postgres' changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR customers_reader WITH (\n+    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',\n+    'database.hostname' = 'postgres',\n+    'database.port' = '5432',\n+    'database.user' = 'postgres-user',\n+    'database.password' = 'postgres-pw',\n+    'database.dbname' = 'customers',\n+    'database.server.name' = 'customers',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'customers',\n+    'table.whitelist' = 'public.customers',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'rewrite'\n+);\n+```\n+\n+Notice that we specified an `unwrap` transform. By default, Debezium sends all events in an envelop that include many pieces of information about the change captured. Here, we only care about the value after it changed, so we instruct Kafka Connect to simply keep that information and discard the rest.\n+\n+Run another source connector to ingest the changes from MongoDB. We specify the same behavior for discarding the Debezium envelop:\n+\n+```sql\n+CREATE SOURCE CONNECTOR logistics_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mongodb.MongoDbConnector',\n+    'mongodb.hosts' = 'mongo:27017',\n+    'mongodb.name' = 'my-replica-set',\n+    'mongodb.authsource' = 'admin',\n+    'mongodb.user' = 'dbz-user',\n+    'mongodb.password' = 'dbz-pw',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'logistics',\n+    'collection.whitelist' = 'logistics.*',\n+    'transforms' = 'unwrap',\n+    'transforms.unwrap.type' = 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState',\n+    'transforms.unwrap.drop.tombstones' = 'false',\n+    'transforms.unwrap.delete.handling.mode' = 'drop',\n+    'transforms.unwrap.operation.header' = 'true'\n+);\n+```\n+\n+### Create the ksqlDB source streams\n+\n+For ksqlDB to be able to use the topics that Debezium created, we need to declare streams over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred the schema that Debezium writes with.\n+\n+Run the following to create a stream over the `customers` table:\n+\n+```sql\n+CREATE STREAM customers WITH (\n+    kafka_topic = 'customers.public.customers',\n+    value_format = 'avro'\n+);\n+```\n+\n+Do the same for `orders`. For this stream, we specify that the timestamp of the event should be derived from the data itself. Namely, it will be extracted and parsed from the `ts` field.\n+\n+```sql\n+CREATE STREAM orders WITH (\n+    kafka_topic = 'my-replica-set.logistics.orders',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Lastly, repeat the same for `shipments`:\n+\n+```sql\n+CREATE STREAM shipments WITH (\n+    kafka_topic = 'my-replica-set.logistics.shipments',\n+    value_format = 'avro',\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+### Join the streams together\n+\n+We want to create a unified view of the activity of shipped orders. To do that, we want to include as much customer information on each shipment as possible. Recall that the `orders` collection that we created in MongoDB only had an identifier for each customer, but not their name. We'll use that identifier to look up the rest of the information using a stream/table join. To do that, we need to rekey the stream into a table by `id`:\n+\n+```sql\n+CREATE TABLE customers_by_key AS\n+    SELECT id,\n+           latest_by_offset(name) AS name,\n+           latest_by_offset(age) AS age\n+    FROM customers\n+    GROUP BY id\n+    EMIT CHANGES;\n+```\n+\n+Now we can enrich the orders with more customer information. This stream/table join creates a new stream that lifts the customer information into the order event:\n+\n+```sql\n+CREATE STREAM enriched_orders AS\n+    SELECT o.order_id,\n+           o.price,\n+           o.currency,\n+           c.id AS customer_id,\n+           c.name AS customer_name,\n+           c.age AS customer_age\n+    FROM orders AS o\n+    LEFT JOIN customers_by_key c\n+    ON o.customer_id = c.ROWKEY\n+    EMIT CHANGES;\n+```\n+\n+We can take this further by enriching all shipments with more information about the order and customer. We use a stream/stream join to find orders in the relevant window of time. This creates a new stream called `shipped_orders` that unifies the shipment, order, and customer information:\n+\n+```sql\n+CREATE STREAM shipped_orders WITH (\n+    kafka_topic = 'shipped_orders'\n+) AS\n+    SELECT o.order_id,\n+           s.shipment_id,\n+           o.customer_id,\n+           o.customer_name,\n+           o.customer_age,\n+           s.origin,\n+           o.price,\n+           o.currency\n+    FROM enriched_orders AS o\n+    INNER JOIN shipments s\n+    WITHIN 7 DAYS\n+    ON s.order_id = o.order_id\n+    EMIT CHANGES;\n+```\n+\n+### Start the Elasticsearch sink connector\n+\n+We want to perform searches and analytics over this unified stream of information. Let's spill the information out to Elasticsearch to make that easy. Simply run the following connector to sink the topic:\n+\n+```sql\n+CREATE SINK CONNECTOR enriched_writer WITH (\n+    'connector.class' = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',\n+    'connection.url' = 'http://elastic:9200',\n+    'type.name' = 'kafka-connect',\n+    'topics' = 'shipped_orders'\n+);\n+```\n+\n+Check that the data arrived in the index by running the following from your host:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 504}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI0NTkzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyMzo1M1rOGCZqLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyMzo1M1rOGCZqLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MDczNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n          \n          \n            \n            A materialized view, sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\", is an approach to precomputing the results of a query and storing them for fast read access. In contrast with a regular database query, which does all of its work at read-time, a materialized view does nearly all of its work at write-time. This is why materialized views can offer highly performant reads.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405170735", "createdAt": "2020-04-07T23:23:53Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI0NzIyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNDozM1rOGCZq_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNDozM1rOGCZq_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MDk0Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n          \n          \n            \n            A standard way of building a materialized cache is to capture the changelog of a database and process it as a stream of events. This enables creating multiple distributed materializations that best suit each application's query patterns.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405170943", "createdAt": "2020-04-07T23:24:33Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI0OTIxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNToyM1rOGCZsFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNToyM1rOGCZsFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MTIyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n          \n          \n            \n            One way you might do this is to capture the changelog of MySQL using the Debezium {{ site.kconnectlong }}. The changelog is stored in {{ site.ak }} and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405171223", "createdAt": "2020-04-07T23:25:23Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI0OTcyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNTozNVrOGCZsYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNTozNVrOGCZsYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MTI5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Why ksqlDB\n          \n          \n            \n            Why ksqlDB?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405171298", "createdAt": "2020-04-07T23:25:35Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI1MDI0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNTo1NFrOGCZsuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzoyNTo1NFrOGCZsuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MTM4NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ----------\n          \n          \n            \n            -----------", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405171385", "createdAt": "2020-04-07T23:25:54Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI2Mzc0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozMjoxNlrOGCZ0pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozMjoxNlrOGCZ0pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MzQxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n          \n          \n            \n            Running all of the above systems is a lot to manage. In addition to your database, you end up managing clusters for {{ site.ak }}, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage ({{ site.ak }}) and compute (ksqlDB).", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405173414", "createdAt": "2020-04-07T23:32:16Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI2NzY1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNDowNlrOGCZ3AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNDowNlrOGCZ3AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDAxNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n          \n          \n            \n            Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, eliminating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405174017", "createdAt": "2020-04-07T23:34:06Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI2OTc2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNDo1NlrOGCZ4MA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNDo1NlrOGCZ4MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDMyMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n          \n          \n            \n            This tutorial shows how to create and query a set of materialized views about phone calls made to the call center. It demonstrates capturing changes from a MySQL database, forwarding them into {{ site.ak }}, creating materialized views with ksqlDB, and querying them from your applications.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405174320", "createdAt": "2020-04-07T23:34:56Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3MDM5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNToxNlrOGCZ4nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNToxNlrOGCZ4nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDQyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n          \n          \n            \n            To get started, download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405174428", "createdAt": "2020-04-07T23:35:16Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3MTc3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNTo1NlrOGCZ5ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNTo1NlrOGCZ5ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDYyOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n          \n          \n            \n            docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405174629", "createdAt": "2020-04-07T23:35:56Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3NDEzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNzowNlrOGCZ6yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNzowNlrOGCZ6yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDk4Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n          \n          \n            \n            To set up and launch the services in the stack, make a couple of files.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405174986", "createdAt": "2020-04-07T23:37:06Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3NTc2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNzo1NlrOGCZ70g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNzo1NlrOGCZ70g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTI1MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n          \n          \n            \n            MySQL requires some custom configuration to play well with Debezium, so take care of this first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) if you're interested, but this guide covers just the essentials. Create a new file at `mysql/custom-config.cnf` with the following content:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405175250", "createdAt": "2020-04-07T23:37:56Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3NzA1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozODozOVrOGCZ8kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozODozOVrOGCZ8kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTQ0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n          \n          \n            \n            With this file in place, create a `docker-compose.yml` file that defines the services to launch:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405175441", "createdAt": "2020-04-07T23:38:39Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI3OTUzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozOTo0M1rOGCZ-Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozOTo0M1rOGCZ-Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTgxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n          \n          \n            \n            There are a few things to notice here. The MySQL image mounts the custom configuration file that you wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables you gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405175814", "createdAt": "2020-04-07T23:39:43Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4MDIxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MDowNFrOGCZ-eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MDowNFrOGCZ-eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTkyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n          \n          \n            \n            Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that you downloaded need to be on the classpath of ksqlDB when the server starts up.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405175928", "createdAt": "2020-04-07T23:40:04Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4MTgwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MDo1M1rOGCZ_ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MDo1M1rOGCZ_ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NjE3MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n          \n          \n            \n            MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. You already set up the `example-user` by default in the Docker Compose file. Now you just need to give it the right privileges. You can do this by logging in to the MySQL container:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176170", "createdAt": "2020-04-07T23:40:53Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4MjYzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MToyNVrOGCZ_-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MToyNVrOGCZ_-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NjMxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n          \n          \n            \n            For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world, you'd want to manage your permissions much more tightly.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176313", "createdAt": "2020-04-07T23:41:25Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4MzQwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MTo0OFrOGCaAcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MTo0OFrOGCaAcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NjQzNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Grant the privileges for replication by executing the following at the MySQL prompt:\n          \n          \n            \n            Grant the privileges for replication by executing the following statement at the MySQL prompt:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176435", "createdAt": "2020-04-07T23:41:48Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4NDQwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MjoxM1rOGCaBDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MjoxM1rOGCaBDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NjU5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n          \n          \n            \n            Seed your blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176590", "createdAt": "2020-04-07T23:42:13Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4NTgzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0Mjo1MlrOGCaBzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0Mjo1MlrOGCaBzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3Njc4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n          \n          \n            \n            Create a table that represents phone calls that were made. Keep this table simple: the columns represent the name of the person calling, the reason that they called, and the duration in seconds of the call.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176782", "createdAt": "2020-04-07T23:42:52Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4NjUzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MzoxMVrOGCaCOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MzoxMVrOGCaCOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3Njg4OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And now add some initial data. We'll add more later, but this will suffice for now:\n          \n          \n            \n            And now add some initial data. You'll add more later, but this will suffice for now:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405176889", "createdAt": "2020-04-07T23:43:11Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4NzU3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MzozMlrOGCaCzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0MzozMlrOGCaCzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NzAzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n          \n          \n            \n            With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following command from your host:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405177038", "createdAt": "2020-04-07T23:43:32Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4ODM4OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0Mzo1MlrOGCaDQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0Mzo1MlrOGCaDQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NzE1NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n          \n          \n            \n            Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405177154", "createdAt": "2020-04-07T23:43:52Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI4OTUzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NDozMVrOGCaD-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NDozMVrOGCaD-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NzMzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n          \n          \n            \n            Now you can connect ask Debezium to stream MySQL's changelog into {{ site.ak }}. Invoke the following command in ksqlDB, which creates a Debezium source connector and writes all of its changes to {{ site.ak }} topics:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405177338", "createdAt": "2020-04-07T23:44:31Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 241}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI5MDMyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NDo1NVrOGCaEcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NDo1NVrOGCaEcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NzQ1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n          \n          \n            \n            After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that you seeded the calls table with:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405177458", "createdAt": "2020-04-07T23:44:55Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 261}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI5MjI3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NTo1OFrOGCaFkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NTo1OFrOGCaFkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3Nzc0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n          \n          \n            \n            For ksqlDB to be able to use the topic that Debezium created, you must declare a stream over it. Because you configured {{ site.kconnectlong }} with {{ site.sr }}, you don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405177744", "createdAt": "2020-04-07T23:45:58Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 281}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI5NDUxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NzowMlrOGCaG9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NzowMlrOGCaG9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3ODEwMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n          \n          \n            \n            A common situation in call centers is the need to know what the current caller has called about in the past. Create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives you an idea of how many kinds of inquiries the caller has raised and also gives you context based on the last time they called.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405178102", "createdAt": "2020-04-07T23:47:02Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 292}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI5NTQ2OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NzozM1rOGCaHkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0NzozM1rOGCaHkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3ODI1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n          \n          \n            \n            You do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, you can retain the last reason the person called for with the `latest_by_offset` aggregation.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405178258", "createdAt": "2020-04-07T23:47:33Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 294}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDI5OTc1OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0OToxOVrOGCaJ6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo0OToxOVrOGCaJ6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3ODg1Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run this statement at the prompt:\n          \n          \n            \n            In the ksqlDB CLI, run the following statement:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405178857", "createdAt": "2020-04-07T23:49:19Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 299}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMwMzAxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MDo1OFrOGCaL4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MDo1OFrOGCaL4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3OTM2Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We have our first materialized view in place. Let's create one more.\n          \n          \n            \n            You have your first materialized view in place. Now create one more.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405179363", "createdAt": "2020-04-07T23:50:58Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:\n+\n+```sql\n+CREATE TABLE support_view AS\n+    SELECT after->name AS name,\n+           count_distinct(after->reason) AS distinct_reasons,\n+           latest_by_offset(after->reason) AS last_reason\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+We have our first materialized view in place. Let's create one more.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 311}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMwNTQ3OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MjozMFrOGCaNhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MjozMFrOGCaNhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3OTc4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time we have a question about a caller, a materialized view makes it easy to incrementally update the answer as new information arrives over time. In this materialized view, we count the total number of times each person has called. We also compute the total number of minutes we've spent on the phone with this person. Run the following:\n          \n          \n            \n            It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time there is a question about a caller, a materialized view makes it easy to update the answer incrementally as new information arrives over time. The following materialized view counts the total number of times each person has called and computes the total number of minutes spent on the phone with this person.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405179781", "createdAt": "2020-04-07T23:52:30Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:\n+\n+```sql\n+CREATE TABLE support_view AS\n+    SELECT after->name AS name,\n+           count_distinct(after->reason) AS distinct_reasons,\n+           latest_by_offset(after->reason) AS last_reason\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+We have our first materialized view in place. Let's create one more.\n+\n+It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time we have a question about a caller, a materialized view makes it easy to incrementally update the answer as new information arrives over time. In this materialized view, we count the total number of times each person has called. We also compute the total number of minutes we've spent on the phone with this person. Run the following:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 313}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMwNzQwOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MzozNFrOGCaOwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1MzozNFrOGCaOwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MDA5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can query our materialized views to look up the values for keys with low latency. How many reasons has Derek called for, and what was the last thing he called about? Run this at the prompt:\n          \n          \n            \n            Now you can query our materialized views to look up the values for keys with low latency. How many reasons has Derek called for, and what was the last thing he called about? In the ksqlDB CLI, run the following statement:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405180097", "createdAt": "2020-04-07T23:53:34Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:\n+\n+```sql\n+CREATE TABLE support_view AS\n+    SELECT after->name AS name,\n+           count_distinct(after->reason) AS distinct_reasons,\n+           latest_by_offset(after->reason) AS last_reason\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+We have our first materialized view in place. Let's create one more.\n+\n+It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time we have a question about a caller, a materialized view makes it easy to incrementally update the answer as new information arrives over time. In this materialized view, we count the total number of times each person has called. We also compute the total number of minutes we've spent on the phone with this person. Run the following:\n+\n+```sql\n+CREATE TABLE lifetime_view AS\n+    SELECT after->name AS name,\n+           count(after->reason) AS total_calls,\n+           (sum(after->duration_seconds) / 60) as minutes_engaged\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+### Query the materialized views\n+\n+Now we can query our materialized views to look up the values for keys with low latency. How many reasons has Derek called for, and what was the last thing he called about? Run this at the prompt:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 327}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMxMDgxOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1NTowN1rOGCaQ1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1NTowN1rOGCaQ1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MDYyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Which should result in:\n          \n          \n            \n            Your output should resemble:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405180628", "createdAt": "2020-04-07T23:55:07Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:\n+\n+```sql\n+CREATE TABLE support_view AS\n+    SELECT after->name AS name,\n+           count_distinct(after->reason) AS distinct_reasons,\n+           latest_by_offset(after->reason) AS last_reason\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+We have our first materialized view in place. Let's create one more.\n+\n+It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time we have a question about a caller, a materialized view makes it easy to incrementally update the answer as new information arrives over time. In this materialized view, we count the total number of times each person has called. We also compute the total number of minutes we've spent on the phone with this person. Run the following:\n+\n+```sql\n+CREATE TABLE lifetime_view AS\n+    SELECT after->name AS name,\n+           count(after->reason) AS total_calls,\n+           (sum(after->duration_seconds) / 60) as minutes_engaged\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+### Query the materialized views\n+\n+Now we can query our materialized views to look up the values for keys with low latency. How many reasons has Derek called for, and what was the last thing he called about? Run this at the prompt:\n+\n+```sql\n+SELECT name, distinct_reasons, last_reason\n+FROM support_view\n+WHERE ROWKEY = 'derek';\n+```\n+\n+Which should result in:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDMxMTg0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1NTozM1rOGCaRaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzo1NTozM1rOGCaRaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MDc3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should see:\n          \n          \n            \n            Your output should resemble:", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r405180779", "createdAt": "2020-04-07T23:55:33Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,367 @@\n+What is it?\n+----------\n+\n+A materialized view (sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\") is an approach to precomputing the results of a query and storing them for fast read access. By contrast to a regular database query, which does all of its work at read-time, materialized views do nearly all of their work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A pretty standard way to build a materialized cache is to capture the changelog of a database and process it as a stream of events. This lets you create multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this would be to capture the changelog of MySQL using the Debezium Kafka Connector. The changelog is stored in Kafka and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB\n+----------\n+\n+Running all of the above systems is admittedly a lot to manage. In addition to your database, you end up managing clusters for Kafka, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage (Kafka) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any Kafka Connect connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, obviating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+In this tutorial, we'll show you how to create and query a set of materialized views about phone calls made to the call center. We'll demonstrate capturing changes from a MySQL database, forwarding them into Kafka, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, we'll need to download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:0.8.0 confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+We'll need to set up and launch the services in the stack. To do this, we'll need to make a couple of files.\n+\n+MySQL requires some custom configuration to play well with Debezium, so let's take care of that first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) on this if you're interested, but this guide covers just the essentials. Make a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With that file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There's a few things to notice here. The MySQL image mounts the custom configuration file that we wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables we gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that we downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. We already set up the `example-user` by default in the Docker Compose file. Now we just need to give it the right privileges. We can do this by logging into the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Let's seed our blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that will represent phone calls that were made. We'll keep this table simple. The columns will represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. We'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before we issue more commands, instruct ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now we can connect ask Debezium to stream MySQL's changelog into Kafka. Invoke the following command in ksqlDB. This creates a Debezium source connector and writes all of its changes to Kafka topics:\n+\n+```sql\n+CREATE SOURCE CONNECTOR calls_reader WITH (\n+    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',\n+    'database.hostname' = 'mysql',\n+    'database.port' = '3306',\n+    'database.user' = 'example-user',\n+    'database.password' = 'example-pw',\n+    'database.allowPublicKeyRetrieval' = 'true',\n+    'database.server.id' = '184054',\n+    'database.server.name' = 'call-center-db',\n+    'database.whitelist' = 'call-center',\n+    'database.history.kafka.bootstrap.servers' = 'broker:9092',\n+    'database.history.kafka.topic' = 'call-center',\n+    'table.whitelist' = 'call-center.calls',\n+    'include.schema.changes' = 'false'\n+);\n+```\n+\n+After a few seconds, it should create a topic named `call-center-db.call-center.calls`. Print the raw topic contents to make sure it captured the initial rows that we seeded the calls table with:\n+\n+```sql\n+PRINT 'call-center-db.call-center.calls' FROM BEGINNING;\n+```\n+\n+If nothing prints out, the connector probably failed to launch. You can check ksqlDB's logs with:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+You can also show the status of the connector in the ksqlDB CLI with:\n+\n+```\n+DESCRIBE CONNECTOR calls_reader;\n+```\n+\n+### Create the ksqlDB calls stream\n+\n+For ksqlDB to be able to use the topic that Debezium created, we need to declare a stream over it. Because we configured Kafka Connect with Schema Registry, we don't need to declare the schema of the data for the streams. It is simply inferred from the schema that Debezium writes with. Run the following at the ksqlDB CLI:\n+\n+```sql\n+CREATE STREAM calls WITH (\n+    kafka_topic = 'call-center-db.call-center.calls',\n+    value_format = 'avro'\n+);\n+```\n+\n+### Create the materialized views\n+\n+A pretty common situation in call centers is the need to know what the current caller has called about in the past. Let's create a simple materialized view that keeps track of the distinct number of reasons that a user called for, and what the last reason was that they called for, too. This gives us an idea of how many kinds of inquiries the caller has raised, and also gives us context based on the last time they called.\n+\n+We do this by declaring a table called `support_view`. Keeping track of the distinct number of reasons a caller raised is as simple as grouping by the user name, then aggregating with `count_distinct` over the `reason` value. Similarly, we can retain the last reason the person called for with the `latest_by_offset` aggregation.\n+\n+Notice that Debezium writes events to the topic in the form of a map with \"before\" and \"after\" keys to make it clear what changed in each operation. That is why each column uses arrow syntax to drill into the nested `after` key.\n+\n+\n+Run this statement at the prompt:\n+\n+```sql\n+CREATE TABLE support_view AS\n+    SELECT after->name AS name,\n+           count_distinct(after->reason) AS distinct_reasons,\n+           latest_by_offset(after->reason) AS last_reason\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+We have our first materialized view in place. Let's create one more.\n+\n+It's useful to have an idea of the lifetime behavior of each caller. Rather than issuing a query over all the data every time we have a question about a caller, a materialized view makes it easy to incrementally update the answer as new information arrives over time. In this materialized view, we count the total number of times each person has called. We also compute the total number of minutes we've spent on the phone with this person. Run the following:\n+\n+```sql\n+CREATE TABLE lifetime_view AS\n+    SELECT after->name AS name,\n+           count(after->reason) AS total_calls,\n+           (sum(after->duration_seconds) / 60) as minutes_engaged\n+    FROM calls\n+    GROUP BY after->name\n+    EMIT CHANGES;\n+```\n+\n+### Query the materialized views\n+\n+Now we can query our materialized views to look up the values for keys with low latency. How many reasons has Derek called for, and what was the last thing he called about? Run this at the prompt:\n+\n+```sql\n+SELECT name, distinct_reasons, last_reason\n+FROM support_view\n+WHERE ROWKEY = 'derek';\n+```\n+\n+Which should result in:\n+\n+```\n++---------+-------------------+------------+\n+|NAME     |DISTINCT_REASONS   |LAST_REASON |\n++---------+-------------------+------------+\n+|derek    |3                  |refund      |\n+```\n+\n+How many times has Michael called us, and how many minutes has he spent on the line?\n+\n+```sql\n+SELECT name, total_calls, minutes_engaged\n+FROM lifetime_view\n+WHERE ROWKEY = 'michael';\n+```\n+\n+You should see:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da8b8533021245710d33fe5a004175e417c130c1"}, "originalPosition": 352}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTg4MzMyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzo1MTowMFrOGDjJpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDoyMDo1MlrOGDoBZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM3NDgyMw==", "bodyText": "0.8.1 is the current version. Should this use {{ site.release }} variable or something?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406374823", "createdAt": "2020-04-09T17:51:00Z", "author": {"login": "spena"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+-----------\n+\n+A streaming ETL pipeline, sometimes called a \u201cstreaming data pipeline\u201d, is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes, you may need to do something more complex, like enrich the events by joining them with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline enables streaming events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in {{ site.ak }}, where a series of deployed programs transforms, aggregates, and joins the data together. The processed data can be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB?\n+-----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for {{ site.ak }}, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can transform, join, and aggregate all of your streams together by using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+This tutorial shows how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. It demonstrates capturing changes from Postgres and MongoDB databases, forwarding them into {{ site.ak }}, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get them by using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:{{ site.cprelease }}\n+```\n+\n+### Start the stack\n+\n+Next, set up and launch the services in the stack. But before you bring it up, you need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, you launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Also, you must create an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With the Postgres configuration file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQyODg3Nw==", "bodyText": "Yes! These tokens work inside code blocks, too. Also, you can use {{ site.cprelease }} for the CP component tags.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406428877", "createdAt": "2020-04-09T19:30:16Z", "author": {"login": "JimGalasyn"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+-----------\n+\n+A streaming ETL pipeline, sometimes called a \u201cstreaming data pipeline\u201d, is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes, you may need to do something more complex, like enrich the events by joining them with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline enables streaming events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in {{ site.ak }}, where a series of deployed programs transforms, aggregates, and joins the data together. The processed data can be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB?\n+-----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for {{ site.ak }}, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can transform, join, and aggregate all of your streams together by using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+This tutorial shows how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. It demonstrates capturing changes from Postgres and MongoDB databases, forwarding them into {{ site.ak }}, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get them by using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:{{ site.cprelease }}\n+```\n+\n+### Start the stack\n+\n+Next, set up and launch the services in the stack. But before you bring it up, you need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, you launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Also, you must create an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With the Postgres configuration file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM3NDgyMw=="}, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NDYyOA==", "bodyText": "Great catch.", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406454628", "createdAt": "2020-04-09T20:20:52Z", "author": {"login": "MichaelDrogalis"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+-----------\n+\n+A streaming ETL pipeline, sometimes called a \u201cstreaming data pipeline\u201d, is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes, you may need to do something more complex, like enrich the events by joining them with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline enables streaming events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in {{ site.ak }}, where a series of deployed programs transforms, aggregates, and joins the data together. The processed data can be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB?\n+-----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for {{ site.ak }}, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can transform, join, and aggregate all of your streams together by using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+This tutorial shows how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. It demonstrates capturing changes from Postgres and MongoDB databases, forwarding them into {{ site.ak }}, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get them by using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:{{ site.cprelease }}\n+```\n+\n+### Start the stack\n+\n+Next, set up and launch the services in the stack. But before you bring it up, you need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, you launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Also, you must create an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With the Postgres configuration file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM3NDgyMw=="}, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTg4MzU5OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/etl.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzo1MTowNlrOGDjJ0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzo1MTowNlrOGDjJ0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM3NDg2Nw==", "bodyText": "0.8.1 is the current version. Should this use {{ site.release }} variable or something?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406374867", "createdAt": "2020-04-09T17:51:06Z", "author": {"login": "spena"}, "path": "docs-md/tutorials/etl.md", "diffHunk": "@@ -0,0 +1,614 @@\n+What is it?\n+-----------\n+\n+A streaming ETL pipeline, sometimes called a \u201cstreaming data pipeline\u201d, is a set of software services that ingests events, transforms them, and loads them into destination storage systems. It\u2019s often the case that you have data in one place and want to move it to another as soon as you receive it, but you need to make some changes to the data as you transfer it.\n+\n+Maybe you need to do something simple, like transform the events to strip out any personally identifiable information. Sometimes, you may need to do something more complex, like enrich the events by joining them with data from another system. Or perhaps you want to pre-aggregate the events to reduce how much data you send to the downstream systems.\n+\n+A streaming ETL pipeline enables streaming events between arbitrary sources and sinks, and it helps you make changes to the data while it\u2019s in-flight.\n+\n+![hard](../img/etl-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelogs of upstream Postgres and MongoDB databases. The changelog can be stored in {{ site.ak }}, where a series of deployed programs transforms, aggregates, and joins the data together. The processed data can be streamed out to ElasticSearch for indexing. Many people build this sort of architecture, but could it be made simpler?\n+\n+Why ksqlDB?\n+-----------\n+\n+Gluing all of the above services together is certainly a challenge. Along with your original databases and target analytical data store, you end up managing clusters for {{ site.ak }}, connectors, and your stream processors. It's challenging to operate the entire stack as one.\n+\n+ksqlDB helps streamline how you write and deploy streaming data pipelines by boiling it down to just two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/etl-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can transform, join, and aggregate all of your streams together by using a coherent, powerful SQL language. This gives you a slender architecture for managing the end-to-end flow of your data pipeline.\n+\n+Implement it\n+------------\n+\n+Suppose you work at a retail company that sells and ships orders to online customers. You want to analyze the shipment activity of orders as they happen in real-time. Because the company is somewhat large, the data for customers, orders, and shipments are spread across different databases and tables.\n+\n+This tutorial shows how to create a streaming ETL pipeline that ingests and joins events together to create a cohesive view of orders that shipped. It demonstrates capturing changes from Postgres and MongoDB databases, forwarding them into {{ site.ak }}, joining them together with ksqlDB, and sinking them out to ElasticSearch for analytics.\n+\n+### Get the connectors\n+\n+To get started, download the connectors for Postgres, MongoDB, and Elasticsearch to a fresh directory. You can either get them by using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker commands that wrap it.\n+\n+First, acquire the Postgres Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-postgresql:1.1.0\n+```\n+\n+Likewise for the MongoDB Debezium connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mongodb:1.1.0\n+```\n+\n+And finally, the Elasticsearch connector:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:{{ site.cprelease }}\n+```\n+\n+### Start the stack\n+\n+Next, set up and launch the services in the stack. But before you bring it up, you need to make a few changes to the way that Postgres launches so that it works well with Debezium.  Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/connectors/postgresql.html) on this if you're interested, but this guide covers just the essentials. To simplify some of this, you launch a Postgres Docker container [extended by Debezium](https://hub.docker.com/r/debezium/postgres) to handle some of the customization. Also, you must create an additional configuration file at `postgres/custom-config.conf` with the following content:\n+\n+```\n+listen_addresses = '*'\n+wal_level = 'logical'\n+max_wal_senders = 1\n+max_replication_slots = 1\n+```\n+\n+This sets up Postgres so that Debezium can watch for changes as they occur.\n+\n+With the Postgres configuration file in place, create a `docker-compose.yml` file that defines the services to launch. You may need to increase the amount of memory that you give to Docker when you launch it:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mongo:\n+    image: mongo:4.2.5\n+    hostname: mongo\n+    container_name: mongo\n+    ports:\n+      - \"27017:27017\"\n+    environment:\n+      MONGO_INITDB_ROOT_USERNAME: mongo-user\n+      MONGO_INITDB_ROOT_PASSWORD: mongo-pw\n+      MONGO_REPLICA_SET_NAME: my-replica-set\n+    command: --replSet my-replica-set --bind_ip_all\n+\n+  postgres:\n+    image: debezium/postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+    environment:\n+      POSTGRES_USER: postgres-user\n+      POSTGRES_PASSWORD: postgres-pw\n+      POSTGRES_DB: customers\n+    volumes:\n+      - ./postgres/custom-config.conf:/etc/postgresql/postgresql.conf\n+    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n+\n+  elastic:\n+    image: elasticsearch:7.6.2\n+    hostname: elastic\n+    container_name: elastic\n+    ports:\n+      - \"9200:9200\"\n+      - \"9300:9300\"\n+    environment:\n+      discovery.type: single-node\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/:/usr/share/kafka/plugins/\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTk2MTU0OnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxMzozMVrOGDj63w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxMzozMVrOGDj63w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM4NzQyMw==", "bodyText": "Replace with a variable to use 0.8.1?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406387423", "createdAt": "2020-04-09T18:13:31Z", "author": {"login": "spena"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,373 @@\n+What is it?\n+----------\n+\n+A materialized view, sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\", is an approach to precomputing the results of a query and storing them for fast read access. In contrast with a regular database query, which does all of its work at read-time, a materialized view does nearly all of its work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A standard way of building a materialized cache is to capture the changelog of a database and process it as a stream of events. This enables creating multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelog of MySQL using the Debezium {{ site.kconnectlong }}. The changelog is stored in {{ site.ak }} and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB?\n+-----------\n+\n+Running all of the above systems is a lot to manage. In addition to your database, you end up managing clusters for {{ site.ak }}, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, eliminating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+This tutorial shows how to create and query a set of materialized views about phone calls made to the call center. It demonstrates capturing changes from a MySQL database, forwarding them into {{ site.ak }}, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+To set up and launch the services in the stack, a few files need to be created first.\n+\n+MySQL requires some custom configuration to play well with Debezium, so take care of this first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) if you're interested, but this guide covers just the essentials. Create a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With this file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTk2MjAzOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxMzozNlrOGDj7Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxMzozNlrOGDj7Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM4NzQ5NA==", "bodyText": "Replace with a variable to use 0.8.1?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406387494", "createdAt": "2020-04-09T18:13:36Z", "author": {"login": "spena"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,373 @@\n+What is it?\n+----------\n+\n+A materialized view, sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\", is an approach to precomputing the results of a query and storing them for fast read access. In contrast with a regular database query, which does all of its work at read-time, a materialized view does nearly all of its work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A standard way of building a materialized cache is to capture the changelog of a database and process it as a stream of events. This enables creating multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelog of MySQL using the Debezium {{ site.kconnectlong }}. The changelog is stored in {{ site.ak }} and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB?\n+-----------\n+\n+Running all of the above systems is a lot to manage. In addition to your database, you end up managing clusters for {{ site.ak }}, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, eliminating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+This tutorial shows how to create and query a set of materialized views about phone calls made to the call center. It demonstrates capturing changes from a MySQL database, forwarding them into {{ site.ak }}, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+To set up and launch the services in the stack, a few files need to be created first.\n+\n+MySQL requires some custom configuration to play well with Debezium, so take care of this first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) if you're interested, but this guide covers just the essentials. Create a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With this file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTk4MzMyOnYy", "diffSide": "RIGHT", "path": "docs-md/tutorials/materialized.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxODoxOTozOFrOGDkIbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDoyMToyOVrOGDoCaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM5MDg5Mw==", "bodyText": "you can connect ask Debezium?\nShould it be you can ask Debezium?", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406390893", "createdAt": "2020-04-09T18:19:38Z", "author": {"login": "spena"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,373 @@\n+What is it?\n+----------\n+\n+A materialized view, sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\", is an approach to precomputing the results of a query and storing them for fast read access. In contrast with a regular database query, which does all of its work at read-time, a materialized view does nearly all of its work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A standard way of building a materialized cache is to capture the changelog of a database and process it as a stream of events. This enables creating multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelog of MySQL using the Debezium {{ site.kconnectlong }}. The changelog is stored in {{ site.ak }} and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB?\n+-----------\n+\n+Running all of the above systems is a lot to manage. In addition to your database, you end up managing clusters for {{ site.ak }}, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, eliminating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+This tutorial shows how to create and query a set of materialized views about phone calls made to the call center. It demonstrates capturing changes from a MySQL database, forwarding them into {{ site.ak }}, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+To set up and launch the services in the stack, a few files need to be created first.\n+\n+MySQL requires some custom configuration to play well with Debezium, so take care of this first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) if you're interested, but this guide covers just the essentials. Create a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With this file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a few things to notice here. The MySQL image mounts the custom configuration file that you wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables you gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that you downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. You already set up the `example-user` by default in the Docker Compose file. Now you just need to give it the right privileges. You can do this by logging in to the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world, you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following statement at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Seed your blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that represents phone calls that were made. Keep this table simple: the columns represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. You'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now you can connect ask Debezium to stream MySQL's changelog into {{ site.ak }}. Invoke the following command in ksqlDB, which creates a Debezium source connector and writes all of its changes to {{ site.ak }} topics:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NDg4OA==", "bodyText": "Thanks!", "url": "https://github.com/confluentinc/ksql/pull/4921#discussion_r406454888", "createdAt": "2020-04-09T20:21:29Z", "author": {"login": "MichaelDrogalis"}, "path": "docs-md/tutorials/materialized.md", "diffHunk": "@@ -0,0 +1,373 @@\n+What is it?\n+----------\n+\n+A materialized view, sometimes called a \"[materialized cache](https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/)\", is an approach to precomputing the results of a query and storing them for fast read access. In contrast with a regular database query, which does all of its work at read-time, a materialized view does nearly all of its work at write-time. This is why materialized views can offer highly performant reads.\n+\n+A standard way of building a materialized cache is to capture the changelog of a database and process it as a stream of events. This enables creating multiple distributed materializations that best suit each application's query patterns.\n+\n+![hard](../img/mv-hard.png){: class=\"centered-img\"}\n+\n+One way you might do this is to capture the changelog of MySQL using the Debezium {{ site.kconnectlong }}. The changelog is stored in {{ site.ak }} and processed by a stream processor. As the materialization updates, it's updated in Redis so that applications can query the materializations. This can work, but is there a better way?\n+\n+Why ksqlDB?\n+-----------\n+\n+Running all of the above systems is a lot to manage. In addition to your database, you end up managing clusters for {{ site.ak }}, connectors, the stream processor, and another data store. It's challenging to monitor, secure, and scale all of these systems as one. ksqlDB helps to consolidate this complexity by slimming the architecture down to two things: storage ({{ site.ak }}) and compute (ksqlDB).\n+\n+![easy](../img/mv-easy.png){: class=\"centered-img\" style=\"width: 80%\"}\n+\n+Using ksqlDB, you can run any {{ site.kconnectlong }} connector by embedding it in ksqlDB's servers. You can also directly query ksqlDB's tables of state, eliminating the need to sink your data to another data store. This gives you one mental model, in SQL, for managing your materialized views end-to-end.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a company with a call center. People frequently call in about purchasing a product, to ask for a refund, and other things. Because the volume of calls is rather high, it isn't practical to run queries over the database storing all the calls every time someone calls in.\n+\n+This tutorial shows how to create and query a set of materialized views about phone calls made to the call center. It demonstrates capturing changes from a MySQL database, forwarding them into {{ site.ak }}, creating materialized views with ksqlDB, and querying them from your applications.\n+\n+### Get the Debezium connector\n+\n+To get started, download the Debezium connector to a fresh directory. You can either get that using [confluent-hub](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), or by running the following one-off Docker command that wraps it:\n+\n+```\n+docker run --rm -v $PWD/confluent-hub-components:/share/confluent-hub-components confluentinc/ksqldb-server:{{ site.release }} confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.1.0\n+```\n+\n+After running this, you should have a directory named `confluent-hub-components` with some jar files in it.\n+\n+### Start the stack\n+\n+To set up and launch the services in the stack, a few files need to be created first.\n+\n+MySQL requires some custom configuration to play well with Debezium, so take care of this first. Debezium has a dedicated [tutorial](https://debezium.io/documentation/reference/1.1/assemblies/cdc-mysql-connector/as_setup-the-mysql-server.html) if you're interested, but this guide covers just the essentials. Create a new file at `mysql/custom-config.cnf` with the following content:\n+\n+```\n+[mysqld]\n+server-id                = 223344 \n+log_bin                  = mysql-bin \n+binlog_format            = ROW \n+binlog_row_image         = FULL \n+expire_logs_days         = 10\n+gtid_mode                = ON\n+enforce_gtid_consistency = ON\n+```\n+\n+This sets up MySQL's transaction log so that Debezium can watch for changes as they occur.\n+\n+With this file in place, create a `docker-compose.yml` file that defines the services to launch:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  mysql:\n+    image: mysql:8.0.19\n+    hostname: mysql\n+    container_name: mysql\n+    ports:\n+      - \"3306:3306\"\n+    environment:\n+      MYSQL_ROOT_PASSWORD: mysql-pw\n+      MYSQL_DATABASE: call-center\n+      MYSQL_USER: example-user\n+      MYSQL_PASSWORD: example-pw\n+    volumes:\n+      - \"./mysql/custom-config.cnf:/etc/mysql/conf.d/custom-config.cnf\"\n+\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.8.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.8.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+There are a few things to notice here. The MySQL image mounts the custom configuration file that you wrote. MySQL merges these configuration settings into its system-wide configuration. The environment variables you gave it also set up a blank database called `call-center` along with a user named `example-user` that can access it.\n+\n+Also note that the ksqlDB server image mounts the `confluent-hub-components` directory, too. The jar files that you downloaded need to be on the classpath of ksqlDB when the server starts up.\n+\n+Bring up the entire stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Configure MySQL for Debezium\n+\n+MySQL requires just a bit more modification before it can work with Debezium. Debezium needs to connect to MySQL as a user that has a specific set of privileges to replicate its changelog. You already set up the `example-user` by default in the Docker Compose file. Now you just need to give it the right privileges. You can do this by logging in to the MySQL container:\n+\n+```\n+docker exec -it mysql /bin/bash\n+```\n+\n+And then logging into MySQL as root:\n+\n+```\n+mysql -u root -p\n+```\n+\n+The root password, as specified in the Docker Compose file, is `mysql-pw`.\n+\n+For simplicity, this tutorial grants all privileges to `example-user` connecting from any host. In the real world, you'd want to manage your permissions much more tightly.\n+\n+Grant the privileges for replication by executing the following statement at the MySQL prompt:\n+\n+```sql\n+GRANT ALL PRIVILEGES ON *.* TO 'example-user' WITH GRANT OPTION;\n+ALTER USER 'example-user'@'%' IDENTIFIED WITH mysql_native_password BY 'example-pw';\n+FLUSH PRIVILEGES;\n+```\n+\n+### Create the calls table in MySQL\n+\n+Seed your blank database with some initial state. In the same MySQL CLI, switch into the `call-center` database:\n+\n+```sql\n+USE call-center;\n+```\n+\n+Create a table that represents phone calls that were made. Keep this table simple: the columns represent the name of the person calling, the reason that they called, and the duration in seconds of the call.\n+\n+```sql\n+CREATE TABLE calls (name TEXT, reason TEXT, duration_seconds INT);\n+```\n+\n+And now add some initial data. You'll add more later, but this will suffice for now:\n+\n+```sql\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"purchase\", 540);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 224);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"help\", 802);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"purchase\", 10204);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 600);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"refund\", 105);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"michael\", \"help\", 2030);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"colin\", \"purchase\", 800);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"help\", 2514);\n+INSERT INTO calls (name, reason, duration_seconds) VALUES (\"derek\", \"refund\", 325);\n+```\n+\n+### Start the Debezium connector\n+\n+With MySQL ready to go, connect to ksqlDB's server using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Now you can connect ask Debezium to stream MySQL's changelog into {{ site.ak }}. Invoke the following command in ksqlDB, which creates a Debezium source connector and writes all of its changes to {{ site.ak }} topics:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM5MDg5Mw=="}, "originalCommit": {"oid": "6b7a7175eb17370c124a9e97a6639c836a722b7a"}, "originalPosition": 241}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3695, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}