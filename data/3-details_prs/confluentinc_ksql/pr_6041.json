{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY5MTEwNzEx", "number": 6041, "title": "New sections for lexical structure, ddl, and primer docs", "bodyText": "Ports selective pages from #5935.", "createdAt": "2020-08-17T23:08:16Z", "url": "https://github.com/confluentinc/ksql/pull/6041", "merged": true, "mergeCommit": {"oid": "d38746b6ccbb6a6874667f5c4f39baaef145de10"}, "closed": true, "closedAt": "2020-09-01T22:55:01Z", "author": {"login": "MichaelDrogalis"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc_6seZgH2gAyNDY5MTEwNzExOjA5ZTkwYWUwMWI3MGViMmE0MzQwMWQyODIwY2MyZDdlMjI1MTU4Nzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEve7RgH2gAyNDY5MTEwNzExOjllM2RkNTY2M2FlOTVkNGRiZmZmMzg1OGUxNzViODM3NTIyZWZkMjk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "09e90ae01b70eb2a43401d2820cc2d7e22515877", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/09e90ae01b70eb2a43401d2820cc2d7e22515877", "committedDate": "2020-08-17T23:06:23Z", "message": "docs: ports lexical structure, ddl, and primer docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c990aa7a9d7107394b257d41aab8cc514cf9232f", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/c990aa7a9d7107394b257d41aab8cc514cf9232f", "committedDate": "2020-08-17T23:25:34Z", "message": "docs: remove old docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38a199eebb7f81dd8313ecdd55e6adbc2d836719", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/38a199eebb7f81dd8313ecdd55e6adbc2d836719", "committedDate": "2020-08-17T23:26:35Z", "message": "docs: fix redirects"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c10b014d6b1c490fca6a319050a353c5336aebfe", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/c10b014d6b1c490fca6a319050a353c5336aebfe", "committedDate": "2020-08-17T23:28:42Z", "message": "docs: remove dead links"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f0fe45a42492d5cda4e9b679d2a246e784230a8e", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/f0fe45a42492d5cda4e9b679d2a246e784230a8e", "committedDate": "2020-08-17T23:33:27Z", "message": "docs: fixes more dead links"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7857ad6505a29bbf2ebedfa1bab547b573fa536", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/e7857ad6505a29bbf2ebedfa1bab547b573fa536", "committedDate": "2020-08-17T23:54:27Z", "message": "docs: adds appendix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26", "author": {"user": {"login": "JimGalasyn", "name": "Jim Galasyn"}}, "url": "https://github.com/confluentinc/ksql/commit/52cee8f2674b5558b17c9da6897f3f6f90a62b26", "committedDate": "2020-08-18T18:00:24Z", "message": "docs: copy edit new ddl and lexical structure topics (DOCS-5143) (#6046)\n\n* docs: copy edit new sql reference topics\r\n\r\n* docs: copy edit lexical structure topic"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NzI2MTcy", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-469726172", "createdAt": "2020-08-18T18:40:37Z", "commit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyNzExNjk4", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-472711698", "createdAt": "2020-08-21T18:45:22Z", "commit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "state": "APPROVED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxODo0NToyMlrOHE3kVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxOToyMTozNVrOHE4zYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg2Njc3NQ==", "bodyText": "Is it canonical to say that a record carries a topic, partition, and offset? In my mind a record carries/consists of data (key, value, headers), has an associated timestamp, and occupies a particular offset of a particular partition of a particular topic. It feels weird to think about the topic, partition, and offset as being part of the record, though perhaps the record contains this meta-information about where in Apache Kafka the record resides.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474866775", "createdAt": "2020-08-21T18:45:22Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg2OTU3Nw==", "bodyText": "It seems reasonable to say the topic describes which larger collection of events this event belongs to, but it seems odd to say the partition does as well, since partitions feel more arbitrary/abstracted rather than being intentional in terms of how they group events (with different keys).", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474869577", "createdAt": "2020-08-21T18:51:38Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3MDg2Mg==", "bodyText": "As a new reader I'd be wondering about the difference between streams and tables. Consider adding a link?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474870862", "createdAt": "2020-08-21T18:54:29Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3MTc5NA==", "bodyText": "nit: don't love the word \"shape\". How about \"structure\" or \"data layout\" (or similar)?\nMy gut also says \"specifies\" rather than \"controls\" but I'm just nit-picking :)", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474871794", "createdAt": "2020-08-21T18:56:28Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3MzQ3Mw==", "bodyText": "Clarify that this is a new partition of a new topic?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474873473", "createdAt": "2020-08-21T19:00:26Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3NDE1Nw==", "bodyText": "This is the first appearance of the term \"broker\" in this topic. Either add a \"see below\" note or move the section on brokers to precede this one?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474874157", "createdAt": "2020-08-21T19:02:01Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3NjM1Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The process that converts to and from byte representations is called\n          \n          \n            \n            _serialization_.\n          \n          \n            \n            The processes of converting to and from byte representations are called\n          \n          \n            \n            _serialization_ and _deserialization_, respectively.\n          \n      \n    \n    \n  \n\nNot a huge fan of the suggested wording but I think it's useful to throw in the term \"deserialization\" since it's used throughout ksqlDB. I also see the use of \"deserializer\" below.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474876356", "createdAt": "2020-08-21T19:07:11Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3NzQ5NQ==", "bodyText": "I'm again unclear what \"shape\" means. To me the structure consists of fields and associated data types. Does \"shape\" refer to something different?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474877495", "createdAt": "2020-08-21T19:09:47Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3ODYwMg==", "bodyText": "The phrase \"map with keys foo, bar, and baz\" worries me a little since maps in ksqlDB don't have named keys; this \"map\" is really a ksqlDB struct, not a ksqlDB map.\nI think the risk of misinterpretation here is low, though, so perhaps I'm overly paranoid.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474878602", "createdAt": "2020-08-21T19:11:19Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3OTU5Ng==", "bodyText": "Do docs substitutions work inside links? I would guess so but I noticed \"Apache Kafka documentation\" (rather than \"{{ site.ak }} documentation\") was written out explicitly above so I'm not sure anymore.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474879596", "createdAt": "2020-08-21T19:12:30Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?\n+\n+Without any mechanism for enforcement, schemas are implicit. A consumer,\n+somehow, needs to know the form of the produced data. Frequently this happens\n+by getting a group of people to agree  verbally on the schema. This approach,\n+however, is error prone. It's often better if the schema can be managed\n+centrally, audited, and enforced programmatically.\n+\n+[Confluent {{ site.sr }}](https://docs.confluent.io/current/schema-registry/index.html), a project outside of {{ site.ak }}, helps with schema", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg4MjA5MA==", "bodyText": "\"one\" what? I'm sure I'm missing something but have no idea what \ud83d\ude06\nBest guess is that this is supposed to say \"records in a single partition are meant to be consumed in order\"?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474882090", "createdAt": "2020-08-21T19:15:29Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?\n+\n+Without any mechanism for enforcement, schemas are implicit. A consumer,\n+somehow, needs to know the form of the produced data. Frequently this happens\n+by getting a group of people to agree  verbally on the schema. This approach,\n+however, is error prone. It's often better if the schema can be managed\n+centrally, audited, and enforced programmatically.\n+\n+[Confluent {{ site.sr }}](https://docs.confluent.io/current/schema-registry/index.html), a project outside of {{ site.ak }}, helps with schema\n+management. {{ site.sr }} enables producers to register a topic with a schema\n+so that when any further data is produced, it is rejected if it doesn't\n+conform to the schema. Consumers can consult {{ site.sr }} to find the schema\n+for topics they don't know about.\n+\n+Rather than having you glue together producers, consumers, and schema\n+configuration, ksqlDB integrates transparently with {{ site.sr }}. By enabling\n+a configuration option so that the two systems can talk to each other, ksqlDB\n+stores all stream and table schemas in {{ site.sr }}. These schemas can then be\n+downloaded and used by any application working with ksqlDB data. Moreover,\n+ksqlDB can infer the schemas of existing topics automatically, so that you \n+don't need to declare their structure when you define the stream or table over\n+it.\n+\n+## Consumer groups\n+\n+When a consumer program boots up, it registers itself into a _consumer group_,\n+which multiple consumers can enter. Each time a record is eligible to be\n+consumed, exactly one consumer in the group reads it. This effectively provides\n+a way for a set of processes to coordinate and load balance the consumption of\n+records.\n+\n+Because the records in a single topic are meant to be consumed in one, each", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg4NDE0NQ==", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            all ten. If you add four more servers, each rebalances to process two partitions.\n          \n          \n            \n            all ten. If instead you add four more servers, each rebalances to process two partitions.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474884145", "createdAt": "2020-08-21T19:18:03Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?\n+\n+Without any mechanism for enforcement, schemas are implicit. A consumer,\n+somehow, needs to know the form of the produced data. Frequently this happens\n+by getting a group of people to agree  verbally on the schema. This approach,\n+however, is error prone. It's often better if the schema can be managed\n+centrally, audited, and enforced programmatically.\n+\n+[Confluent {{ site.sr }}](https://docs.confluent.io/current/schema-registry/index.html), a project outside of {{ site.ak }}, helps with schema\n+management. {{ site.sr }} enables producers to register a topic with a schema\n+so that when any further data is produced, it is rejected if it doesn't\n+conform to the schema. Consumers can consult {{ site.sr }} to find the schema\n+for topics they don't know about.\n+\n+Rather than having you glue together producers, consumers, and schema\n+configuration, ksqlDB integrates transparently with {{ site.sr }}. By enabling\n+a configuration option so that the two systems can talk to each other, ksqlDB\n+stores all stream and table schemas in {{ site.sr }}. These schemas can then be\n+downloaded and used by any application working with ksqlDB data. Moreover,\n+ksqlDB can infer the schemas of existing topics automatically, so that you \n+don't need to declare their structure when you define the stream or table over\n+it.\n+\n+## Consumer groups\n+\n+When a consumer program boots up, it registers itself into a _consumer group_,\n+which multiple consumers can enter. Each time a record is eligible to be\n+consumed, exactly one consumer in the group reads it. This effectively provides\n+a way for a set of processes to coordinate and load balance the consumption of\n+records.\n+\n+Because the records in a single topic are meant to be consumed in one, each\n+partition in the subscription is read by only one consumer at a time. The number\n+of partitions that each consumer is responsible for is defined by the total\n+number of source partitions divided by the number of consumers. If a consumer\n+dynamically joins the group, the ownership is recomputed and the partitions\n+reassigned. If a consumer leaves the group, the same computation takes place.\n+\n+ksqlDB builds on this powerful load balancing primitive. When you deploy a\n+persistent query to a cluster of ksqlDB servers, the workload is distributed\n+across the cluster according to the number of source partitions. You don't need\n+to manage group membership explicitly, because all of this happens automatically.\n+\n+For example, if you deploy a persistent query with ten source partitions to a\n+ksqlDB cluster with two nodes, each node processes five partitions. If you lose\n+a server, the sole remaining server will rebalance automatically and process\n+all ten. If you add four more servers, each rebalances to process two partitions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg4Njc4Nw==", "bodyText": "This feels misleading since topics can have infinite retention, as is commonly the case for compacted topics.\nPerhaps combine this section and the next into a single \"Retention and Compaction\" section and have an intro sentence saying there are different options for when old records are deleted in order to free up space?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474886787", "createdAt": "2020-08-21T19:21:18Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?\n+\n+Without any mechanism for enforcement, schemas are implicit. A consumer,\n+somehow, needs to know the form of the produced data. Frequently this happens\n+by getting a group of people to agree  verbally on the schema. This approach,\n+however, is error prone. It's often better if the schema can be managed\n+centrally, audited, and enforced programmatically.\n+\n+[Confluent {{ site.sr }}](https://docs.confluent.io/current/schema-registry/index.html), a project outside of {{ site.ak }}, helps with schema\n+management. {{ site.sr }} enables producers to register a topic with a schema\n+so that when any further data is produced, it is rejected if it doesn't\n+conform to the schema. Consumers can consult {{ site.sr }} to find the schema\n+for topics they don't know about.\n+\n+Rather than having you glue together producers, consumers, and schema\n+configuration, ksqlDB integrates transparently with {{ site.sr }}. By enabling\n+a configuration option so that the two systems can talk to each other, ksqlDB\n+stores all stream and table schemas in {{ site.sr }}. These schemas can then be\n+downloaded and used by any application working with ksqlDB data. Moreover,\n+ksqlDB can infer the schemas of existing topics automatically, so that you \n+don't need to declare their structure when you define the stream or table over\n+it.\n+\n+## Consumer groups\n+\n+When a consumer program boots up, it registers itself into a _consumer group_,\n+which multiple consumers can enter. Each time a record is eligible to be\n+consumed, exactly one consumer in the group reads it. This effectively provides\n+a way for a set of processes to coordinate and load balance the consumption of\n+records.\n+\n+Because the records in a single topic are meant to be consumed in one, each\n+partition in the subscription is read by only one consumer at a time. The number\n+of partitions that each consumer is responsible for is defined by the total\n+number of source partitions divided by the number of consumers. If a consumer\n+dynamically joins the group, the ownership is recomputed and the partitions\n+reassigned. If a consumer leaves the group, the same computation takes place.\n+\n+ksqlDB builds on this powerful load balancing primitive. When you deploy a\n+persistent query to a cluster of ksqlDB servers, the workload is distributed\n+across the cluster according to the number of source partitions. You don't need\n+to manage group membership explicitly, because all of this happens automatically.\n+\n+For example, if you deploy a persistent query with ten source partitions to a\n+ksqlDB cluster with two nodes, each node processes five partitions. If you lose\n+a server, the sole remaining server will rebalance automatically and process\n+all ten. If you add four more servers, each rebalances to process two partitions.\n+\n+## Retention\n+\n+When you create a topic, you must set a retention duration. Retention", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg4NzAwOQ==", "bodyText": "Clarify that opt-in is per topic.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r474887009", "createdAt": "2020-08-21T19:21:35Z", "author": {"login": "vcrfxia"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition set\n+to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.\n+The process that converts to and from byte representations is called\n+_serialization_.\n+\n+When a producer sends a record to a topic, it must decide which serializers to\n+use to convert the key and value to byte arrays. The key and value\n+serializers are chosen independently. When a consumer receives a record, it\n+must decide which deserializer to use to convert the byte arrays back to\n+their original values. Serializers and deserializers come in pairs. If you use\n+a different deserializer, you won't be able to make sense of the byte contents.\n+\n+ksqlDB raises the abstraction of serialization substantially. Instead of\n+configuring serializers manually, you declare formats using configuration\n+options at stream/table creation time. Instead of having to keep track of which\n+topics are serialized which way, ksqlDB maintains metadata about the byte\n+representations of each stream and table. Consumers are configured automatically\n+to use the correct deserializers.\n+\n+## Schemas\n+\n+Although the records serialized to {{ site.ak }} are opaque bytes, they must have\n+some rules about their structure to make it possible to process them. One aspect of this\n+structure is the schema of the data, which defines its shape and fields. Is it\n+an integer? Is it a map with keys `foo`, `bar`, and `baz`? Something else?\n+\n+Without any mechanism for enforcement, schemas are implicit. A consumer,\n+somehow, needs to know the form of the produced data. Frequently this happens\n+by getting a group of people to agree  verbally on the schema. This approach,\n+however, is error prone. It's often better if the schema can be managed\n+centrally, audited, and enforced programmatically.\n+\n+[Confluent {{ site.sr }}](https://docs.confluent.io/current/schema-registry/index.html), a project outside of {{ site.ak }}, helps with schema\n+management. {{ site.sr }} enables producers to register a topic with a schema\n+so that when any further data is produced, it is rejected if it doesn't\n+conform to the schema. Consumers can consult {{ site.sr }} to find the schema\n+for topics they don't know about.\n+\n+Rather than having you glue together producers, consumers, and schema\n+configuration, ksqlDB integrates transparently with {{ site.sr }}. By enabling\n+a configuration option so that the two systems can talk to each other, ksqlDB\n+stores all stream and table schemas in {{ site.sr }}. These schemas can then be\n+downloaded and used by any application working with ksqlDB data. Moreover,\n+ksqlDB can infer the schemas of existing topics automatically, so that you \n+don't need to declare their structure when you define the stream or table over\n+it.\n+\n+## Consumer groups\n+\n+When a consumer program boots up, it registers itself into a _consumer group_,\n+which multiple consumers can enter. Each time a record is eligible to be\n+consumed, exactly one consumer in the group reads it. This effectively provides\n+a way for a set of processes to coordinate and load balance the consumption of\n+records.\n+\n+Because the records in a single topic are meant to be consumed in one, each\n+partition in the subscription is read by only one consumer at a time. The number\n+of partitions that each consumer is responsible for is defined by the total\n+number of source partitions divided by the number of consumers. If a consumer\n+dynamically joins the group, the ownership is recomputed and the partitions\n+reassigned. If a consumer leaves the group, the same computation takes place.\n+\n+ksqlDB builds on this powerful load balancing primitive. When you deploy a\n+persistent query to a cluster of ksqlDB servers, the workload is distributed\n+across the cluster according to the number of source partitions. You don't need\n+to manage group membership explicitly, because all of this happens automatically.\n+\n+For example, if you deploy a persistent query with ten source partitions to a\n+ksqlDB cluster with two nodes, each node processes five partitions. If you lose\n+a server, the sole remaining server will rebalance automatically and process\n+all ten. If you add four more servers, each rebalances to process two partitions.\n+\n+## Retention\n+\n+When you create a topic, you must set a retention duration. Retention\n+defines how long a record is stored before it's deleted. Retention is one of the\n+only ways to delete a record in a topic. This parameter is\n+particularly important in stream processing because it defines the time\n+horizon that you can replay a stream of events. Replay is useful if you're\n+fixing a bug, building a new application, or backtesting some existing piece of\n+logic.\n+\n+ksqlDB enables you to control the retention of the underlying topics of base\n+streams and tables directly, so it's important to understand the concept. For\n+more information see [Topics and Logs in the Kafka docs](https://kafka.apache.org/documentation/#intro_topics).\n+\n+## Compaction\n+\n+Compaction is a process that runs in the background on each {{ site.ak }}\n+broker that periodically deletes all but the latest record per key. It is an\n+optional, opt-in process. Compaction is particularly useful when your records", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 237}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f489b4630c72a2cba0ab7b6e6c93c9a79b23adff", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/f489b4630c72a2cba0ab7b6e6c93c9a79b23adff", "committedDate": "2020-08-26T21:57:53Z", "message": "docs: more carefully describe partition"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4078867c6bb7b1fbd74d283d0de35ca96c79dc46", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/4078867c6bb7b1fbd74d283d0de35ca96c79dc46", "committedDate": "2020-08-26T22:00:45Z", "message": "docs: clarify"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "024834fbf549563caa2a622e70ea2d4298e4eb38", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/024834fbf549563caa2a622e70ea2d4298e4eb38", "committedDate": "2020-08-26T22:04:00Z", "message": "docs: link to s/t"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2582edf0eb6257015dd40e0ed073d566a0fc57b1", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/2582edf0eb6257015dd40e0ed073d566a0fc57b1", "committedDate": "2020-08-26T22:07:06Z", "message": "docs: suggestions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "124d8c113204e850b9f0d04a8bcf66afcd641e29", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/124d8c113204e850b9f0d04a8bcf66afcd641e29", "committedDate": "2020-08-26T22:12:30Z", "message": "docs: clarify language"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ead9b76bd3fe1eccea64ba6585a67db42bf95f15", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/ead9b76bd3fe1eccea64ba6585a67db42bf95f15", "committedDate": "2020-08-26T22:15:35Z", "message": "docs: ombine retention & compaction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/1a682fa7badd159dee941e4169a036c34795e881", "committedDate": "2020-08-26T22:21:34Z", "message": "Merge branch 'master' into mdrogalis/sql-docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2MjI2MjQw", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-476226240", "createdAt": "2020-08-27T00:36:31Z", "commit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozNjozMlrOHHpysA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozNjozMlrOHHpysA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NjgwMA==", "bodyText": "Should be s1 instead of s3 right?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r477786800", "createdAt": "2020-08-27T00:36:32Z", "author": {"login": "vpapavas"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s3` EMIT CHANGES;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2MjMwMzI3", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-476230327", "createdAt": "2020-08-27T00:38:58Z", "commit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozODo1OFrOHHqEsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozODo1OFrOHHqEsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc5MTQwOQ==", "bodyText": "Rephrase: No spaces, underscores, or any other characters are allowed in the constant", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r477791409", "createdAt": "2020-08-27T00:38:58Z", "author": {"login": "vpapavas"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s3` EMIT CHANGES;\n+```\n+\n+## Constants\n+\n+There are three implicitly typed constants, or literals, in ksqlDB: strings,\n+numbers, and booleans.\n+\n+### String constants\n+\n+A string constant is an arbitrary series of characters surrounded by single\n+quotes (`'`), like `'Hello world'`. To include a quote inside of a string\n+literal, escape the quote by prefixing it with another quote, for example\n+`'You can call me ''Stuart'', or Stu.'`\n+\n+### Numeric constants\n+\n+Numeric constants are accepted in the following forms:\n+\n+1. **_`digits`_**\n+2. **_`digits`_**`.[`**_`digits`_**`][e[+-]`**_`digits`_**`]`\n+3. `[`**_`digits`_**`].`**_`digits`_**`[e[+-]`**_`digits`_**`]`\n+4. **_`digits`_**`e[+-]`**_`digits`_**\n+\n+where **_`digits`_** is one or more single-digit integers (`0` through `9`).\n+\n+- At least one digit must be present before or after the decimal point, if\n+  there is one.\n+- At least one digit must follow the exponent symbol `e`, if there is one.\n+- Spaces and underscores (nor any other characters) are allowed in the constant.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2MjMxMTg0", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-476231184", "createdAt": "2020-08-27T00:39:29Z", "commit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozOToyOVrOHHqIaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDozOToyOVrOHHqIaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc5MjM2Mw==", "bodyText": "but this IS considered", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r477792363", "createdAt": "2020-08-27T00:39:29Z", "author": {"login": "vpapavas"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s3` EMIT CHANGES;\n+```\n+\n+## Constants\n+\n+There are three implicitly typed constants, or literals, in ksqlDB: strings,\n+numbers, and booleans.\n+\n+### String constants\n+\n+A string constant is an arbitrary series of characters surrounded by single\n+quotes (`'`), like `'Hello world'`. To include a quote inside of a string\n+literal, escape the quote by prefixing it with another quote, for example\n+`'You can call me ''Stuart'', or Stu.'`\n+\n+### Numeric constants\n+\n+Numeric constants are accepted in the following forms:\n+\n+1. **_`digits`_**\n+2. **_`digits`_**`.[`**_`digits`_**`][e[+-]`**_`digits`_**`]`\n+3. `[`**_`digits`_**`].`**_`digits`_**`[e[+-]`**_`digits`_**`]`\n+4. **_`digits`_**`e[+-]`**_`digits`_**\n+\n+where **_`digits`_** is one or more single-digit integers (`0` through `9`).\n+\n+- At least one digit must be present before or after the decimal point, if\n+  there is one.\n+- At least one digit must follow the exponent symbol `e`, if there is one.\n+- Spaces and underscores (nor any other characters) are allowed in the constant.\n+- Numeric constants may also have a `+` or `-` prefix, but this considered to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "originalPosition": 124}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2MjM1NjI3", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-476235627", "createdAt": "2020-08-27T00:42:05Z", "commit": {"oid": "1a682fa7badd159dee941e4169a036c34795e881"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/0c1fbf05b1a7441db4a10972f6a989c7a7bbb913", "committedDate": "2020-08-27T15:14:07Z", "message": "docs: address Vicky's feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4OTczMDQw", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-478973040", "createdAt": "2020-08-31T21:37:16Z", "commit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMTozNzoxNlrOHKKHbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQyMzowNjozMlrOHKM8jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQxMzU1MQ==", "bodyText": "in the spirit of nitpicking: the word \"is\" is a little strong here. A table is often a topic and a materialized RocksDB store (so really, the kafka topic is the changelog topic for the table). For stream, this is more or less accurate - though it logically represents more than just the topic (while topic is the physical representation of the stream, the stream logically contains all the data that was ever put into the topic even if it's expired)", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480413551", "createdAt": "2020-08-31T21:37:16Z", "author": {"login": "agavra"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQxMzc5MQ==", "bodyText": "ordered with respect to offset, not timestamp - might be worth clarifying", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480413791", "createdAt": "2020-08-31T21:37:50Z", "author": {"login": "agavra"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0ODkxMQ==", "bodyText": "as of #3754 , CHANGES no longer specifies a push query, but rather the output refinement (in contrast with FINAL). It is the presence of EMIT that specifies a push query (cf. no EMIT means pull query).", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480448911", "createdAt": "2020-08-31T22:49:30Z", "author": {"login": "agavra"}, "path": "docs/reference/sql/appendix.md", "diffHunk": "@@ -0,0 +1,150 @@\n+---\n+layout: page\n+title: ksqlDB SQL keywords and operators\n+tagline: SQL language keywords\n+description: Tables listing all valid keywords and operators in ksqlDB SQL\n+keywords: ksqldb, sql, keyword, operators\n+---\n+\n+## Keywords\n+\n+The following table shows all keywords in the language.\n+\n+| keyword      | description                             | example                                                              |\n+|--------------|-----------------------------------------|----------------------------------------------------------------------|\n+| `ADVANCE`      | hop size in hopping window            | `WINDOW HOPPING (SIZE 30 SECONDS, ADVANCE BY 10 SECONDS)`            |\n+| `ALL`          | list hidden topics                    | `SHOW ALL TOPICS`                                                    |\n+| `AND`          | logical \"and\" operator                | `WHERE userid<>'User_1' AND userid<>'User_2'`                        |\n+| `ARRAY`        | one-indexed array of elements         | `SELECT ARRAY[1, 2] FROM s1 EMIT CHANGES;`                           |\n+| `AS`           | alias a column, expression, or type   |                                                                      |\n+| `BEGINNING`    | print from start of topic             | `PRINT <topic-name> FROM BEGINNING;`                                 |\n+| `BETWEEN`      | constrain a value to a range          | `SELECT event FROM events WHERE event_id BETWEEN 10 AND 20 \u2026`        |\n+| `BY`           | specify expression                    | `GROUP BY regionid`, `ADVANCE BY 10 SECONDS`, `PARTITION BY userid`  |\n+| `CASE`         | select a condition from expressions   | `SELECT CASE WHEN condition THEN result [ WHEN \u2026 THEN \u2026 ] \u2026 END`     |\n+| `CAST`         | change expression type                | `SELECT id, CONCAT(CAST(COUNT(*) AS VARCHAR), '_HELLO') FROM views \u2026`|\n+| `CHANGES`      | specify push query                    | `SELECT * FROM users EMIT CHANGES;`                                  |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0OTE3Mw==", "bodyText": "this is awesome, but I'm worried about it staying in sync with the code \ud83e\udd14", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480449173", "createdAt": "2020-08-31T22:49:56Z", "author": {"login": "agavra"}, "path": "docs/reference/sql/appendix.md", "diffHunk": "@@ -0,0 +1,150 @@\n+---\n+layout: page\n+title: ksqlDB SQL keywords and operators\n+tagline: SQL language keywords\n+description: Tables listing all valid keywords and operators in ksqlDB SQL\n+keywords: ksqldb, sql, keyword, operators\n+---\n+\n+## Keywords\n+\n+The following table shows all keywords in the language.\n+\n+| keyword      | description                             | example                                                              |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ1MzUyOA==", "bodyText": "what is the verdict on WINDOWSTART and WINDOWEND?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480453528", "createdAt": "2020-08-31T22:56:28Z", "author": {"login": "agavra"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.\n+\n+Syntactically, declaring a table is similar to declaring a stream. The following\n+example statement declares a `current_location` table that has a key field \n+named `person`.\n+\n+```sql\n+CREATE TABLE current_location (\n+    person VARCHAR PRIMARY KEY,\n+    location VARCHAR\n+) WITH (\n+    kafka_topic = 'current_location',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+As with a stream, you can declare a table directly on top of an existing\n+{{ site.ak }} topic by omitting the number of partitions in the `WITH` clause.\n+\n+## Keys\n+\n+You can mark a column with the `KEY` keyword to indicate that it's a key\n+column. Key columns constitute the key portion of the row's underlying\n+{{ site.ak }} record. Only streams can mark columns as keys, and it's optional\n+for them to do do. Tables must use the `PRIMARY KEY` constraint instead.\n+\n+In the following example statement, `k1`'s data is stored in the key portion of\n+the row, and `v1`'s data is stored in the value.\n+\n+```sql\n+CREATE STREAM s3 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's3',\n+    value_format = 'json'\n+);\n+```\n+\n+The ability to declare key columns explicitly is especially useful when you're\n+creating a stream over an existing topic. If ksqlDB can't infer what data is in\n+the key of the underlying {{ site.ak }} record, it must perform a repartition\n+of the rows internally. If you're not sure what data is in the key or you simply\n+don't need it, you can omit the `KEY` keyword.\n+\n+## Default values\n+\n+If a column is declared in a schema, but no attribute is present in the\n+underlying {{ site.ak }} record, the value for the row's column is populated as\n+`null`.\n+\n+## Pseudocolumns\n+\n+A pseudocolumn is a column that's automatically populated by ksqlDB and contains\n+meta-information that can be inferred about the row at creation time. By default,\n+pseudocolumns aren't returned when selecting all columns with the star (`*`)\n+special character. You must select them explicitly, as shown in the following\n+example statement.\n+\n+```sql\n+SELECT ROWTIME, * FROM s1 EMIT CHANGES;\n+```\n+\n+The following table lists all pseudocolumns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ1OTkxNw==", "bodyText": "not all keywords are reserved - I believe all of the following can be used without escaping them:\nnonReserved\n    : SHOW | TABLES | COLUMNS | COLUMN | PARTITIONS | FUNCTIONS | FUNCTION | SESSION\n    | STRUCT | MAP | ARRAY | PARTITION\n    | INTEGER | DATE | TIME | TIMESTAMP | INTERVAL | ZONE\n    | YEAR | MONTH | DAY | HOUR | MINUTE | SECOND\n    | EXPLAIN | ANALYZE | TYPE | TYPES\n    | SET | RESET\n    | IF\n    | SOURCE | SINK\n    | PRIMARY | KEY\n    | EMIT\n    | CHANGES\n    | FINAL\n    | ESCAPE\n    | REPLACE\n    | ASSERT\n    ;", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r480459917", "createdAt": "2020-08-31T23:06:32Z", "author": {"login": "agavra"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5NzQwNTIw", "url": "https://github.com/confluentinc/ksql/pull/6041#pullrequestreview-479740520", "createdAt": "2020-09-01T13:28:38Z", "commit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "state": "COMMENTED", "comments": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMzoyODozOFrOHK2R6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNDo0NTo1NFrOHK5s5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEzNzEzMA==", "bodyText": "I find this overly confusing.\nAll this talk of larger collection and subset of events is unnecessarily vague and using alternative nomenclature to what Kafka uses.  Given this page is all about the Kafka terms users will need to understand, does it not make sense to use the terms Kafka uses?\nWould it not be better to just give a quick outline of what a topic is in Kafka, including that its broken into partitions, then explain that the offset if the offset into a particular topic-partition?\nOr, given you explain these concepts in more depth below, simple say:\n\nThe offset denotes the position of the record within a specific partition of a topic, (more on these below).\n\nThis avoids terms such as  collections (topic) and subsets (partitions).", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481137130", "createdAt": "2020-09-01T13:28:38Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE0Mzg2Mg==", "bodyText": "I'm with Vic here on the controls. Then suggests some kind of check to ensure messages with the wrong schema are not produced a topic by some other producer, and such a check does not exist.  This is particularly important distinction for source topics, which ksqlDB only reads from.\nThough it's very common for all messages to have the same schema, even this isn't a requirement for some formats, e.g. JSON can handle topics where values have different schemas: the declared stream or table can either define the superset of fields, some common subset, or any other combination.\nHow about just saying something about ksqlDB adding a SQL abstraction over the data held in Kafka, and that SQL statically typed?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481143862", "createdAt": "2020-09-01T13:38:07Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,244 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection of events\n+this event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through _streams_ and\n+_tables_. A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg3MTc5NA=="}, "originalCommit": {"oid": "52cee8f2674b5558b17c9da6897f3f6f90a62b26"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE0NzA5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A stream or table is a {{ site.ak }} topic with a registered schema.\n          \n          \n            \n            A stream or table associates a SQL schema with a {{ site.ak }} topic.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481147093", "createdAt": "2020-09-01T13:42:41Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQxMzU1MQ=="}, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1MTAyMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A partition is a totally ordered sequence of records. Topics have multiple\n          \n          \n            \n            A partition is a totally ordered sequence of records. Topics may have multiple", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481151020", "createdAt": "2020-09-01T13:48:25Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1NDA4MQ==", "bodyText": "When records are appended, they follow the correct offset order, even in the presence of failures or faults\n\nIt's unclear to me what this means. Either a message is or is not appended to a partition. This is controlled by the Kafka broker.  It doesn't make sense to me to say that we ensure records are appended with the correct offset order.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481154081", "createdAt": "2020-09-01T13:52:52Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1Njk4MQ==", "bodyText": "When a record is processed, its key content is hashed so that its new downstream partition will be consistent with all other records with the same key\n\nI think this is misleading. The key of the result may be different to the key of the record being produced. There may not even be a record produced, or there may be multiple.\nMaybe say:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When a record is processed, its key content is hashed so that its new downstream\n          \n          \n            \n            partition will be consistent with all other records with the same key. When records are\n          \n          \n            \n            ksqlDB ensures all records with the same key are produced to the same downstream partition. When records are", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481156981", "createdAt": "2020-09-01T13:56:51Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1NzYyMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            records keys are recalculated, and the records are sent to a new partition in\n          \n          \n            \n            the new topic set to perform the computation.\n          \n          \n            \n            records keys are recalculated, and the records are sent to a new partition in\n          \n          \n            \n            the new topic.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481157622", "createdAt": "2020-09-01T13:57:49Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition in\n+the new topic set to perform the computation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1OTE2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            point of view, each ksqlDB server is like a client. No processing takes place\n          \n          \n            \n            point of view, each ksqlDB server is just a client. No processing takes place", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481159165", "createdAt": "2020-09-01T13:59:56Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition in\n+the new topic set to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers (see below) over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2MDgxNQ==", "bodyText": "It's not just key and value, there's headers too:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            designed to be agnostic to the data contents in the key and value portions of\n          \n          \n            \n            its records. When records move from client to broker, the user payload (key and\n          \n          \n            \n            value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n          \n          \n            \n            designed to be agnostic to the data contents of its records. \n          \n          \n            \n            When records move from client to broker, the user payload \n          \n          \n            \n            must be transformed to byte arrays. This enables {{ site.ak }} to work", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481160815", "createdAt": "2020-09-01T14:02:15Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition in\n+the new topic set to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers (see below) over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2MTEwNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            transformed back into their original topics to be meaningful to the application.\n          \n          \n            \n            transformed back into their original form to be meaningful to the application.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481161104", "createdAt": "2020-09-01T14:02:42Z", "author": {"login": "big-andy-coates"}, "path": "docs/overview/apache-kafka-primer.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+layout: page\n+title: Apache Kafka\u00ae primer\n+tagline: Kafka concepts you need to use ksqlDB\n+description: Learn the minimum number of Kafka concepts to use ksqlDB effectively\n+keywords: ksqldb, kafka\n+---\n+\n+ksqlDB is an event streaming database built specifically for {{ site.aktm }}.\n+Although it's designed to give you a higher-level set of primitives than\n+{{ site.ak }} has, it's inevitable that all of {{ site.ak }}'s concepts can't be, and\n+shouldn't be, abstracted away entirely. This section describes the minimum\n+number of {{ site.ak }} concepts that you need to use ksqlDB effectively.\n+For more information, consult the official [Apache Kafka documentation](https://kafka.apache.org/documentation/).\n+\n+## Records\n+\n+The primary unit of data in {{ site.ak }} is the event. An event models\n+something that happened in the world at a point in time. In {{ site.ak }},\n+you represent each event using a data construct known as a record. A record\n+carries a few different kinds of data in it: key, value, timestamp, topic, partition, offset, and headers.\n+\n+The _key_ of a record is an arbitrary piece of data that denotes the identity\n+of the event. If the events are clicks on a web page, a suitable key might be\n+the ID of the user who did the clicking.\n+\n+The _value_ is also an arbitrary piece of data that represents the primary data of\n+interest. The value of a click event probably contains the page that it\n+happened on, the DOM element that was clicked, and other interesting tidbits\n+of information.\n+\n+The _timestamp_ denotes when the event happened. There are a few different \"kinds\"\n+of time that can be tracked. These aren\u2019t discussed here, but they\u2019re useful to\n+[learn about](../../../concepts/time-and-windows-in-ksqldb-queries/#time-semantics) nonetheless.\n+\n+The _topic_ and _partition_ describe which larger collection and subset of events\n+this particular event belongs to, and the _offset_ describes its exact position within\n+that larger collection (more on that below).\n+\n+Finally, the _headers_ carry arbitrary, user-supplied metadata about the record.\n+\n+ksqlDB abstracts over some of these pieces of information so you don\u2019t need to\n+think about them. Others are exposed directly and are an integral part of the\n+programming model. For example, the fundamental unit of data in ksqlDB is the\n+_row_. A row is a helpful abstraction over a {{ site.ak }} record. Rows have\n+columns of two kinds: key columns and value columns. They also carry\n+pseudocolumns for metadata, like a `timestamp`.\n+\n+In general, ksqlDB avoids raising up {{ site.ak }}-level implementation details\n+that don\u2019t contribute to a high-level programming model.\n+\n+## Topics\n+\n+Topics are named collections of records. Their purpose is to let you hold\n+events of mutual interest together. A series of click records might get stored\n+in a \"clicks\" topic so that you can access them all in one place. Topics are\n+append-only. Once you add a record to a topic, you can\u2019t change or delete it\n+individually.\n+\n+There are no rules for what kinds of records can be placed into topics. They\n+don't need to conform to the same structure, relate to the same situation, or\n+anything like that. The way you manage publication to topics is entirely a\n+matter of user convention and enforcement.\n+\n+ksqlDB provides higher-level abstractions over a topic through\n+_[streams](../reference/sql/data-definition.md#streams)_ and\n+_[tables](../reference/sql/data-definition.md#tables)_.\n+A stream or table is a {{ site.ak }} topic with a registered schema.\n+The schema controls the shape of records that are allowed to be stored in the\n+topic. This kind of static typing makes it easier to understand what sort of\n+rows are in your topic and generally helps you make fewer mistakes in your\n+programs that process them.\n+\n+## Partitions\n+\n+When a record is placed into a topic, it is placed into a particular partition.\n+A partition is a totally ordered sequence of records. Topics have multiple\n+partitions to make storage and processing more scalable. When you create a\n+topic, you choose how many partitions it has.\n+\n+When you append a record to a topic, a partitioning strategy chooses which\n+partition it is stored in. There are many partitioning strategies. The most common\n+one is to hash the contents of the record's key against the total number of\n+partitions. This has the effect of placing all records with the same identity\n+into the same partition, which is useful because of the strong ordering\n+guarantees.\n+\n+The order of the records is tracked by a piece of data known as an offset,\n+which is set when the record is appended. A record with offset of _10_ happened\n+earlier than a record in the same partition with offset of _20_.\n+\n+Much of the mechanics here are handled automatically by ksqlDB on your behalf.\n+When you create a stream or table, you choose the number of partitions for the\n+underlying topic so that you can have control over its scalability. When you\n+declare a schema, you choose which columns are part of the key and which are\n+part of the value. Beyond this, you don't need to think about individual partitions\n+or offsets. Here are some examples of that.\n+\n+When a record is processed, its key content is hashed so that its new downstream\n+partition will be consistent with all other records with the same key. When records are\n+appended, they follow the correct offset order, even in the presence of\n+failures or faults. When a stream's key content changes because of how a query\n+wants to process the rows (via `GROUP BY` or `PARTITION BY`), the underlying\n+records keys are recalculated, and the records are sent to a new partition in\n+the new topic set to perform the computation.\n+\n+## Producers and consumers\n+\n+Producers and consumers facilitate the movement of records to and from topics.\n+When an application wants to either publish records or subscribe to them, it\n+invokes the APIs (generally called the _client_) to do so. Clients communicate\n+with the brokers (see below) over a structured network protocol.\n+\n+When consumers read records from a topic, they never delete them or mutate\n+them in any way. This pattern of being able to repeatedly read the same\n+information is helpful for building multiple applications over the same data\n+set in a non-conflicting way. It's also the primary building block for\n+supporting \"replay\", where an application can rewind its event stream and read\n+old information again.\n+\n+Producers and consumers expose a fairly low-level API. You need to construct\n+your own records, manage their schemas, configure their serialization, and\n+handle what you send where.\n+\n+ksqlDB behaves as a high-level, continuous producer and consumer. You simply\n+declare the shape of your records, then issue high-level SQL commands that\n+describe how to populate, alter, and query the data. These SQL programs are\n+translated into low-level client API invocations that take care of the details\n+for you.\n+\n+## Brokers\n+\n+The brokers are servers that store and manage access to topics. Multiple brokers\n+can cluster together to replicate topics in a highly-available, fault-tolerant\n+manner. Clients communicate with the brokers to read and write records.\n+\n+When you run a ksqlDB server or cluster, each of its nodes communicates with\n+the {{ site.ak }} brokers to do its processing. From the {{ site.ak }} brokers'\n+point of view, each ksqlDB server is like a client. No processing takes place\n+on the broker. ksqlDB's servers do all of their computation on their own nodes.\n+\n+## Serializers\n+\n+Because no data format is a perfect fit for all problems, {{ site.ak }} was\n+designed to be agnostic to the data contents in the key and value portions of\n+its records. When records move from client to broker, the user payload (key and\n+value) must be transformed to byte arrays. This enables {{ site.ak }} to work\n+with an opaque series of bytes without needing to know anything about what they\n+are. When records are delivered to a consumer, those byte arrays need to be\n+transformed back into their original topics to be meaningful to the application.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2NzAzNw==", "bodyText": "May be worth also mentioning that they key column also controls ordering guarantees, i.e. only rows in the same partition have any relative ordering guarantees.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481167037", "createdAt": "2020-09-01T14:11:00Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2Nzc3OA==", "bodyText": "Worth mentioning that large schemas generally mean large serde costs?   Good to encourage people to keep their schemas simple!", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481167778", "createdAt": "2020-09-01T14:11:57Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2ODkzMw==", "bodyText": "worth mentioning that while rows can be deleted, they can be 'aged out' and link to the retention bit above?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481168933", "createdAt": "2020-09-01T14:13:37Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3MDA4OQ==", "bodyText": "Every row, implicitly or explicitly, has a key that represents its identity.\n\nThis isn't true - ksqlDB supports keyless streams.  Keyless streams ignore the key of the source and produce to the output with no key set (meaning rows are randomly assigned a partition).", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481170089", "createdAt": "2020-09-01T14:15:11Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3MDY0OQ==", "bodyText": "Me too.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481170649", "createdAt": "2020-09-01T14:16:00Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/appendix.md", "diffHunk": "@@ -0,0 +1,150 @@\n+---\n+layout: page\n+title: ksqlDB SQL keywords and operators\n+tagline: SQL language keywords\n+description: Tables listing all valid keywords and operators in ksqlDB SQL\n+keywords: ksqldb, sql, keyword, operators\n+---\n+\n+## Keywords\n+\n+The following table shows all keywords in the language.\n+\n+| keyword      | description                             | example                                                              |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0OTE3Mw=="}, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3MTA5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            with a registered schema. If the backing topic for a stream doesn't exist when\n          \n          \n            \n            with an associated schema. If the backing topic for a stream doesn't exist when", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481171093", "createdAt": "2020-09-01T14:16:41Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3MTk2Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can also declare a stream on top of an existing topic. When you do that,\n          \n          \n            \n            ksqlDB simply registers its associated schema. If topic `s2` already exists,\n          \n          \n            \n            the following statement register a new stream over it:\n          \n          \n            \n            You can also declare a stream on top of an existing topic. When you do that,\n          \n          \n            \n            ksqlDB simply associates the schema with the topic. If topic `s2` already exists,\n          \n          \n            \n            the following statement register a new stream over it:", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481171966", "createdAt": "2020-09-01T14:17:49Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3MzYxNQ==", "bodyText": "a table represents what is true as of \"now\"\n\nHummm... you can also use a table to ask 'what was true as of '.   So I'm not sure this is a good description of a table.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481173615", "createdAt": "2020-09-01T14:20:10Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3ODY0MQ==", "bodyText": "I think we should be careful in our use of the terms row and record or event.\nIn my mind, row is referring to the row in the table, where are record or event is the record coming from the Kafka topic we're processing, that may mutate the row in the table.\nA non-temporal table only has one row with a specific key. Hence, IMHO, the phrase \"If a sequence of rows share a key\" is unintuitive ... as only one row can have a specific key.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Tables work by leveraging the keys of each row. If a sequence of rows shares a\n          \n          \n            \n            key, the last row for a given key represents the most up-to-date information\n          \n          \n            \n            for that key's identity. A background process periodically runs and deletes all\n          \n          \n            \n            but the newest rows for each key.\n          \n          \n            \n            Tables work by leveraging the keys of each row. If a sequence of records shares a\n          \n          \n            \n            key, the last record for a given key represents the most up-to-date row\n          \n          \n            \n            for that key's identity. A background process periodically runs and deletes all\n          \n          \n            \n            but the newest record for each key.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481178641", "createdAt": "2020-09-01T14:26:35Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3ODk4MQ==", "bodyText": "Maybe add link to compaction in the last sentence?", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481178981", "createdAt": "2020-09-01T14:27:01Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3ODY0MQ=="}, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE4MDc2NQ==", "bodyText": "If ksqlDB can't infer what data is in the key of the underlying {{ site.ak }} record...\n\nI find this misleading. ksqlDB never tries to infer what's in the key of the record.  The user must tell ksqlDB what the key is...", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481180765", "createdAt": "2020-09-01T14:29:20Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.\n+\n+Syntactically, declaring a table is similar to declaring a stream. The following\n+example statement declares a `current_location` table that has a key field \n+named `person`.\n+\n+```sql\n+CREATE TABLE current_location (\n+    person VARCHAR PRIMARY KEY,\n+    location VARCHAR\n+) WITH (\n+    kafka_topic = 'current_location',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+As with a stream, you can declare a table directly on top of an existing\n+{{ site.ak }} topic by omitting the number of partitions in the `WITH` clause.\n+\n+## Keys\n+\n+You can mark a column with the `KEY` keyword to indicate that it's a key\n+column. Key columns constitute the key portion of the row's underlying\n+{{ site.ak }} record. Only streams can mark columns as keys, and it's optional\n+for them to do do. Tables must use the `PRIMARY KEY` constraint instead.\n+\n+In the following example statement, `k1`'s data is stored in the key portion of\n+the row, and `v1`'s data is stored in the value.\n+\n+```sql\n+CREATE STREAM s3 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's3',\n+    value_format = 'json'\n+);\n+```\n+\n+The ability to declare key columns explicitly is especially useful when you're\n+creating a stream over an existing topic. If ksqlDB can't infer what data is in\n+the key of the underlying {{ site.ak }} record, it must perform a repartition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE4MjYyMA==", "bodyText": "If you're not sure what data is in the key or you simply don't need it, you can omit the KEY keyword.\n\nIf find this misleading.  This suggests omitting just the KEY keyword is sufficient for ksqlDB to load the k1 column in the above example correctly.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            of the rows internally. If you're not sure what data is in the key or you simply\n          \n          \n            \n            don't need it, you can omit the `KEY` keyword.\n          \n          \n            \n            of the rows internally. If you're not sure what data is in the key or you simply\n          \n          \n            \n            don't need it, you can omit the `KEY` column. For example:\n          \n          \n            \n            \n          \n          \n            \n            ```sql\n          \n          \n            \n            CREATE STREAM s3 (\n          \n          \n            \n                v1 VARCHAR\n          \n          \n            \n            ) WITH (\n          \n          \n            \n                kafka_topic = 's3',\n          \n          \n            \n                value_format = 'json'\n          \n          \n            \n            );", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481182620", "createdAt": "2020-09-01T14:31:39Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.\n+\n+Syntactically, declaring a table is similar to declaring a stream. The following\n+example statement declares a `current_location` table that has a key field \n+named `person`.\n+\n+```sql\n+CREATE TABLE current_location (\n+    person VARCHAR PRIMARY KEY,\n+    location VARCHAR\n+) WITH (\n+    kafka_topic = 'current_location',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+As with a stream, you can declare a table directly on top of an existing\n+{{ site.ak }} topic by omitting the number of partitions in the `WITH` clause.\n+\n+## Keys\n+\n+You can mark a column with the `KEY` keyword to indicate that it's a key\n+column. Key columns constitute the key portion of the row's underlying\n+{{ site.ak }} record. Only streams can mark columns as keys, and it's optional\n+for them to do do. Tables must use the `PRIMARY KEY` constraint instead.\n+\n+In the following example statement, `k1`'s data is stored in the key portion of\n+the row, and `v1`'s data is stored in the value.\n+\n+```sql\n+CREATE STREAM s3 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's3',\n+    value_format = 'json'\n+);\n+```\n+\n+The ability to declare key columns explicitly is especially useful when you're\n+creating a stream over an existing topic. If ksqlDB can't infer what data is in\n+the key of the underlying {{ site.ak }} record, it must perform a repartition\n+of the rows internally. If you're not sure what data is in the key or you simply\n+don't need it, you can omit the `KEY` keyword.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE4NTY3MQ==", "bodyText": "They're currently not defined as pseudo columns. They are included by default with a select *.\nI guess at the moment they're defined a 'system columns'. There names are reserved for system use.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481185671", "createdAt": "2020-09-01T14:35:29Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.\n+\n+Syntactically, declaring a table is similar to declaring a stream. The following\n+example statement declares a `current_location` table that has a key field \n+named `person`.\n+\n+```sql\n+CREATE TABLE current_location (\n+    person VARCHAR PRIMARY KEY,\n+    location VARCHAR\n+) WITH (\n+    kafka_topic = 'current_location',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+As with a stream, you can declare a table directly on top of an existing\n+{{ site.ak }} topic by omitting the number of partitions in the `WITH` clause.\n+\n+## Keys\n+\n+You can mark a column with the `KEY` keyword to indicate that it's a key\n+column. Key columns constitute the key portion of the row's underlying\n+{{ site.ak }} record. Only streams can mark columns as keys, and it's optional\n+for them to do do. Tables must use the `PRIMARY KEY` constraint instead.\n+\n+In the following example statement, `k1`'s data is stored in the key portion of\n+the row, and `v1`'s data is stored in the value.\n+\n+```sql\n+CREATE STREAM s3 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's3',\n+    value_format = 'json'\n+);\n+```\n+\n+The ability to declare key columns explicitly is especially useful when you're\n+creating a stream over an existing topic. If ksqlDB can't infer what data is in\n+the key of the underlying {{ site.ak }} record, it must perform a repartition\n+of the rows internally. If you're not sure what data is in the key or you simply\n+don't need it, you can omit the `KEY` keyword.\n+\n+## Default values\n+\n+If a column is declared in a schema, but no attribute is present in the\n+underlying {{ site.ak }} record, the value for the row's column is populated as\n+`null`.\n+\n+## Pseudocolumns\n+\n+A pseudocolumn is a column that's automatically populated by ksqlDB and contains\n+meta-information that can be inferred about the row at creation time. By default,\n+pseudocolumns aren't returned when selecting all columns with the star (`*`)\n+special character. You must select them explicitly, as shown in the following\n+example statement.\n+\n+```sql\n+SELECT ROWTIME, * FROM s1 EMIT CHANGES;\n+```\n+\n+The following table lists all pseudocolumns.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ1MzUyOA=="}, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE4NjkwMA==", "bodyText": "This is mixing up the concept of a table with a changelog.\nThe table can only have one row with a specific primary key.  A changelog can contain multiple rows with the same key.  The changelog can be materialized into a table.\nThis is the same as a traditional database.\nWe should explain this table/changelog duality somewhere ;)\nYes, unfortunately at the moment we can INSERT VALUES into a table with an key for a row that already exists and it will work.  However, this is a bug IMHO, (it should be UPSERT not INSERT),.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481186900", "createdAt": "2020-09-01T14:37:18Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/data-definition.md", "diffHunk": "@@ -0,0 +1,228 @@\n+---\n+layout: page\n+title: Data definition\n+tagline: Use DDL to structure data \n+description: How to use DDL to structure data in ksqlDB\n+keywords: ksqldb, sql, ddl\n+---\n+\n+This section covers how you create the structures that store your events.\n+ksqlDB abstracts events as rows with columns and stores them in streams\n+and tables.\n+\n+## Rows and columns\n+\n+Streams and tables help you model collections of events that accrete over time.\n+Both are represented as a series of rows and columns with a schema, much like a\n+relational database table. Rows represent individual events. Columns represent\n+the attributes of those events.\n+\n+Each column has a data type. The data type limits the span of permissible values\n+that you can assign. For example, if a column is declared as type `INT`, it can't\n+be assigned the value of string `'foo'`.\n+\n+In contrast to relational database tables, the columns of a row in ksqlDB are\n+divided into _key_ and _value_ columns. The key columns control which partition\n+a row resides in. The value columns, by convention, store the main data of\n+interest. Controlling the key columns is useful for manipulating the underlying\n+data locality, and enables you to integrate with the wider {{ site.ak }}\n+ecosystem, which uses the same key/value data model. By default, a column is a\n+value column. Marking a column as a `(PRIMARY) KEY` makes it a key column.\n+\n+Internally, each row is backed by a [Kafka record](../../../overview/apache-kafka-primer/#records).\n+In {{ site.ak }}, the key and value parts of a record are\n+[serialized](../../../overview/apache-kafka-primer/#serializers) independently.\n+ksqlDB enables you to exercise this same flexibility and builds on the semantics\n+of {{ site.ak }} records, rather than hiding them.\n+\n+There is no theoretical limit on the number of columns in a stream or table.\n+In practice, the limit is determined by the maximum message size that {{ site.ak }}\n+can store and the resources dedicated to ksqlDB.\n+\n+## Streams\n+\n+A stream is a partitioned, immutable, append-only collection that represents a\n+series of historical facts. For example, the rows of a stream could model a\n+sequence of financial transactions, like \"Alice sent $100 to Bob\", followed by\n+\"Charlie sent $50 to Bob\".\n+\n+Once a row is inserted into a stream, it can never change. New rows can be\n+appended at the end of the stream, but existing rows can never be updated or\n+deleted.\n+\n+Each row is stored in a particular partition. Every row, implicitly or explicitly,\n+has a key that represents its identity. All rows with the same key reside in the\n+same partition.\n+\n+To create a stream, use the `CREATE STREAM` command. The following example\n+statement specifies a name for the new stream, the names of the columns, and\n+the data type of each column.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR KEY,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+This creates a new stream named `s1` with three columns: `k`, `v1`, and `v2`.\n+The column `k` is designated as the key of this stream, which controls the\n+partition that each row is stored in. When the data is stored, the value\n+portion of each row's underlying {{ site.ak }} record is serialized in the\n+JSON format.\n+\n+Under the hood, each stream corresponds to a [Kafka topic](../../../overview/apache-kafka-primer/#topics)\n+with a registered schema. If the backing topic for a stream doesn't exist when\n+you declare it, ksqlDB creates it on your behalf, as shown in the previous\n+example statement.\n+\n+You can also declare a stream on top of an existing topic. When you do that,\n+ksqlDB simply registers its associated schema. If topic `s2` already exists,\n+the following statement register a new stream over it:\n+\n+```sql\n+CREATE STREAM s2 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's2',\n+    value_format = 'json'\n+);\n+```\n+\n+!!! tip\n+    When you create a stream on an existing topic, you don't need to declare\n+    the number of partitions for the topic. ksqlDB infers the partition count\n+    from the existing topic.\n+\n+## Tables\n+\n+A table is a mutable, partitioned collection that models change over time. In\n+contrast with a stream, which represents a historical sequence of events, a\n+table represents what is true as of \"now\". For example, you might use a table\n+to model the locations where someone has lived as a stream: first Miami, then\n+New York, then London, and so forth.\n+\n+Tables work by leveraging the keys of each row. If a sequence of rows shares a\n+key, the last row for a given key represents the most up-to-date information\n+for that key's identity. A background process periodically runs and deletes all\n+but the newest rows for each key.\n+\n+Syntactically, declaring a table is similar to declaring a stream. The following\n+example statement declares a `current_location` table that has a key field \n+named `person`.\n+\n+```sql\n+CREATE TABLE current_location (\n+    person VARCHAR PRIMARY KEY,\n+    location VARCHAR\n+) WITH (\n+    kafka_topic = 'current_location',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+As with a stream, you can declare a table directly on top of an existing\n+{{ site.ak }} topic by omitting the number of partitions in the `WITH` clause.\n+\n+## Keys\n+\n+You can mark a column with the `KEY` keyword to indicate that it's a key\n+column. Key columns constitute the key portion of the row's underlying\n+{{ site.ak }} record. Only streams can mark columns as keys, and it's optional\n+for them to do do. Tables must use the `PRIMARY KEY` constraint instead.\n+\n+In the following example statement, `k1`'s data is stored in the key portion of\n+the row, and `v1`'s data is stored in the value.\n+\n+```sql\n+CREATE STREAM s3 (\n+    k1 VARCHAR KEY,\n+    v1 VARCHAR\n+) WITH (\n+    kafka_topic = 's3',\n+    value_format = 'json'\n+);\n+```\n+\n+The ability to declare key columns explicitly is especially useful when you're\n+creating a stream over an existing topic. If ksqlDB can't infer what data is in\n+the key of the underlying {{ site.ak }} record, it must perform a repartition\n+of the rows internally. If you're not sure what data is in the key or you simply\n+don't need it, you can omit the `KEY` keyword.\n+\n+## Default values\n+\n+If a column is declared in a schema, but no attribute is present in the\n+underlying {{ site.ak }} record, the value for the row's column is populated as\n+`null`.\n+\n+## Pseudocolumns\n+\n+A pseudocolumn is a column that's automatically populated by ksqlDB and contains\n+meta-information that can be inferred about the row at creation time. By default,\n+pseudocolumns aren't returned when selecting all columns with the star (`*`)\n+special character. You must select them explicitly, as shown in the following\n+example statement.\n+\n+```sql\n+SELECT ROWTIME, * FROM s1 EMIT CHANGES;\n+```\n+\n+The following table lists all pseudocolumns.\n+\n+| pseudocolumn | meaning                        |\n+|--------------|--------------------------------|\n+| `ROWTIME`    | Row timestamp, inferred from the underlying Kafka record if not overridden. |\n+\n+You can't create additional pseudocolumns beyond these.\n+\n+## Constraints\n+\n+Although data types help limit the range of values that can be accepted by\n+ksqlDB, sometimes it's useful to have more sophisticated restrictions.\n+_Constraints_ enable you to exercise this type of logic directly in your schema.\n+\n+### Primary key constraints\n+\n+In a relational database, a primary key indicates that a column will be used as\n+a unique identifier for all rows in a table. If you have a table that has a row\n+with primary key `5`, you can't insert another row whose primary key is also `5`.\n+\n+ksqlDB uses primary keys in a similar way, but there are a few differences,\n+because ksqlDB is an event streaming database, not a relational database.\n+\n+- Only tables can have primary keys. Streams do not support them.\n+- Adding multiple rows to a table with the same primary key doesn't cause the\n+  subsequent rows to be rejected.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE5MjI5Mw==", "bodyText": "FYI, e is case-insensitive. So can be E too.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481192293", "createdAt": "2020-09-01T14:44:37Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s1` EMIT CHANGES;\n+```\n+\n+## Constants\n+\n+There are three implicitly typed constants, or literals, in ksqlDB: strings,\n+numbers, and booleans.\n+\n+### String constants\n+\n+A string constant is an arbitrary series of characters surrounded by single\n+quotes (`'`), like `'Hello world'`. To include a quote inside of a string\n+literal, escape the quote by prefixing it with another quote, for example\n+`'You can call me ''Stuart'', or Stu.'`\n+\n+### Numeric constants\n+\n+Numeric constants are accepted in the following forms:\n+\n+1. **_`digits`_**\n+2. **_`digits`_**`.[`**_`digits`_**`][e[+-]`**_`digits`_**`]`\n+3. `[`**_`digits`_**`].`**_`digits`_**`[e[+-]`**_`digits`_**`]`\n+4. **_`digits`_**`e[+-]`**_`digits`_**\n+\n+where **_`digits`_** is one or more single-digit integers (`0` through `9`).\n+\n+- At least one digit must be present before or after the decimal point, if\n+  there is one.\n+- At least one digit must follow the exponent symbol `e`, if there is one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE5Mjk3NA==", "bodyText": "We've used the term user-defined in other places:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            doesn't allow you to add user-space operators.\n          \n          \n            \n            doesn't allow you to add user-defined operators.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481192974", "createdAt": "2020-09-01T14:45:36Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s1` EMIT CHANGES;\n+```\n+\n+## Constants\n+\n+There are three implicitly typed constants, or literals, in ksqlDB: strings,\n+numbers, and booleans.\n+\n+### String constants\n+\n+A string constant is an arbitrary series of characters surrounded by single\n+quotes (`'`), like `'Hello world'`. To include a quote inside of a string\n+literal, escape the quote by prefixing it with another quote, for example\n+`'You can call me ''Stuart'', or Stu.'`\n+\n+### Numeric constants\n+\n+Numeric constants are accepted in the following forms:\n+\n+1. **_`digits`_**\n+2. **_`digits`_**`.[`**_`digits`_**`][e[+-]`**_`digits`_**`]`\n+3. `[`**_`digits`_**`].`**_`digits`_**`[e[+-]`**_`digits`_**`]`\n+4. **_`digits`_**`e[+-]`**_`digits`_**\n+\n+where **_`digits`_** is one or more single-digit integers (`0` through `9`).\n+\n+- At least one digit must be present before or after the decimal point, if\n+  there is one.\n+- At least one digit must follow the exponent symbol `e`, if there is one.\n+- No spaces, underscores, or any other characters are allowed in the constant.\n+- Numeric constants may also have a `+` or `-` prefix, but this is considered to\n+  be a function applied to the constant, not the constant itself.\n+\n+Here are some examples of valid numeric constants:\n+\n+- `5`\n+- `7.2`\n+- `0.0087`\n+- `1.`\n+- `.5`\n+- `1e-3`\n+- `1.332434e+2`\n+- `+100`\n+- `-250`\n+\n+### Boolean constants\n+\n+A boolean constant is represented as either the identifer `true` or `false`.\n+Boolean constants are not case-sensitive, meaning `true` evaluates to the same\n+value as `TRUE`.\n+\n+## Operators\n+\n+Operators are infix functions composed of special characters. A complete list\n+of operators can be found in the [appendix](../appendix.md#operators). ksqlDB\n+doesn't allow you to add user-space operators.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE5MzE4OA==", "bodyText": "And maps.", "url": "https://github.com/confluentinc/ksql/pull/6041#discussion_r481193188", "createdAt": "2020-09-01T14:45:54Z", "author": {"login": "big-andy-coates"}, "path": "docs/reference/sql/syntax/lexical-structure.md", "diffHunk": "@@ -0,0 +1,201 @@\n+---\n+layout: page\n+title: Lexical structure data\n+tagline: Structure of SQL commands and statements in ksqlDB \n+description: Details about SQL commands and statements in ksqlDB \n+keywords: ksqldb, sql, keyword, identifier, constant, operator\n+---\n+\n+SQL is a domain-specific language for managing and manipulating data. It\u2019s\n+used primarily to work with structured data, where the types and relationships\n+across entities are well-defined. Originally adopted for relational databases,\n+SQL is rapidly becoming the language of choice for stream processing. It\u2019s\n+declarative, expressive, and ubiquitous.\n+\n+The American National Standards Institute (ANSI) maintains a standard for the\n+specification of SQL. SQL-92, the third revision to the standard, is generally\n+the most recognized form of the specification. Beyond the standard, there are\n+many flavors and extensions to SQL so that it can express programs beyond\n+what's possible with the SQL-92 grammar.\n+\n+ksqlDB\u2019s SQL grammar was built  initially around Presto's grammar and has been\n+extended judiciously. ksqlDB goes beyond SQL-92, because the standard currently\n+has no constructs for streaming queries, which are a core aspect of this project.\n+\n+## Syntax\n+\n+SQL inputs are made up of a series of statements. Each statements is made up of\n+a series of tokens and ends in a semicolon (`;`). The tokens that apply depend\n+on the statement being invoked.\n+\n+A token is any keyword, identifier, backticked identifier, literal, or special\n+character. By convention, tokens are separated by whitespace, unless there is\n+no ambiguity in the grammar. This happens when tokens flank a special character.\n+\n+The following example statements are syntactically valid ksqlDB SQL input:\n+\n+```sql\n+INSERT INTO s1 (a, b) VALUES ('k1', 'v1');\n+\n+CREATE STREAM s2 AS\n+    SELECT a, b\n+    FROM s1\n+    EMIT CHANGES;\n+\n+SELECT * FROM t1 WHERE k1='foo' EMIT CHANGES;\n+```\n+\n+## Keywords\n+\n+Some tokens, such as `SELECT`, `INSERT`, and `CREATE`, are _keywords_.\n+Keywords are reserved tokens that have a specific meaning in ksqlDB's syntax.\n+They control their surrounding allowable tokens and execution semantics.\n+Keywords are case insensitive, meaning `SELECT` and `select` are equivalent.\n+You can't create an identifier that is already a keyword, unless you use\n+backticked identifiers.\n+\n+A complete list of keywords can be found in the [appendix](../appendix.md#keywords).\n+\n+## Identifiers\n+\n+Identifiers are symbols that represent user-space entities, like streams,\n+tables, columns, and other objects. For example, if you have a stream named\n+`s1`, `s1` is an _identifier_ for that stream. By default, identifiers are\n+case-insensitive, meaning `s1` and `S1` refer to the same stream. Under the\n+hood, ksqlDB capitalizes all of the characters in the identifier for all\n+future display purposes.\n+\n+Unless an identifier is backticked, it may be composed only of characters that\n+are a letter, number, or underscore. There is no imposed limit on the number of\n+characters.\n+\n+To make it possible to use any character in an identifier, you can enclose it\n+in backticks (``` ` ```) when you declare and use it. A _backticked identifier_\n+is useful when you don't control the data, so it might have special characters,\n+or even keywords. When you use backticked identifers, ksqlDB  captures the case \n+exactly, and any future references to the identifer become case-sensitive. For\n+example, if you declare the following stream:\n+\n+```sql\n+CREATE STREAM `s1` (\n+    k VARCHAR KEY,\n+    `@MY-identifier-stream-column!` INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 3,\n+    value_format = 'json'\n+);\n+```\n+\n+You must select from it by backticking the stream name and column name and\n+using the original casing:\n+\n+```sql\n+SELECT `@MY-identifier-stream-column!` FROM `s1` EMIT CHANGES;\n+```\n+\n+## Constants\n+\n+There are three implicitly typed constants, or literals, in ksqlDB: strings,\n+numbers, and booleans.\n+\n+### String constants\n+\n+A string constant is an arbitrary series of characters surrounded by single\n+quotes (`'`), like `'Hello world'`. To include a quote inside of a string\n+literal, escape the quote by prefixing it with another quote, for example\n+`'You can call me ''Stuart'', or Stu.'`\n+\n+### Numeric constants\n+\n+Numeric constants are accepted in the following forms:\n+\n+1. **_`digits`_**\n+2. **_`digits`_**`.[`**_`digits`_**`][e[+-]`**_`digits`_**`]`\n+3. `[`**_`digits`_**`].`**_`digits`_**`[e[+-]`**_`digits`_**`]`\n+4. **_`digits`_**`e[+-]`**_`digits`_**\n+\n+where **_`digits`_** is one or more single-digit integers (`0` through `9`).\n+\n+- At least one digit must be present before or after the decimal point, if\n+  there is one.\n+- At least one digit must follow the exponent symbol `e`, if there is one.\n+- No spaces, underscores, or any other characters are allowed in the constant.\n+- Numeric constants may also have a `+` or `-` prefix, but this is considered to\n+  be a function applied to the constant, not the constant itself.\n+\n+Here are some examples of valid numeric constants:\n+\n+- `5`\n+- `7.2`\n+- `0.0087`\n+- `1.`\n+- `.5`\n+- `1e-3`\n+- `1.332434e+2`\n+- `+100`\n+- `-250`\n+\n+### Boolean constants\n+\n+A boolean constant is represented as either the identifer `true` or `false`.\n+Boolean constants are not case-sensitive, meaning `true` evaluates to the same\n+value as `TRUE`.\n+\n+## Operators\n+\n+Operators are infix functions composed of special characters. A complete list\n+of operators can be found in the [appendix](../appendix.md#operators). ksqlDB\n+doesn't allow you to add user-space operators.\n+\n+## Special characters\n+\n+Some characters have a particular meaning that doesn't correspond to an\n+operator. The following list describes the special characters and their\n+purpose.\n+\n+- Parentheses (`()`) retain their usual meaning in programming languages for\n+  grouping expressions and controlling the order of evaluation.\n+- Brackets (`[]`) are used to work with arrays, both in their construction and\n+  subscript access.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1fbf05b1a7441db4a10972f6a989c7a7bbb913"}, "originalPosition": 160}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e3dd5663ae95d4dbfff3858e175b837522efd29", "author": {"user": {"login": "MichaelDrogalis", "name": "Michael Drogalis"}}, "url": "https://github.com/confluentinc/ksql/commit/9e3dd5663ae95d4dbfff3858e175b837522efd29", "committedDate": "2020-09-01T22:52:15Z", "message": "docs: almog feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4702, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}