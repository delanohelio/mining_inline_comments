{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzMzY4NDQy", "number": 4262, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOToxMzo1NlrOE6uZfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOToxMzo1NlrOE6uZfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDEzMDU1OnYy", "diffSide": "RIGHT", "path": "bigtable/spark/src/test/scala/example/IntegrationTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOToxMzo1NlrOH1-EJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNTo0ODozOFrOH2lFjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM1MzQ0NQ==", "bodyText": "@kolea2 I presume there isn't an alternative yet?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/4262#discussion_r526353445", "createdAt": "2020-11-18T19:13:56Z", "author": {"login": "lesv"}, "path": "bigtable/spark/src/test/scala/example/IntegrationTest.scala", "diffHunk": "@@ -15,44 +15,63 @@\n  */\n package example\n \n+import java.util.UUID\n+\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.admin.v2.BigtableTableAdminClient\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.admin.v2.models.CreateTableRequest\n import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.models.Query\n import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.{BigtableDataClient, BigtableDataSettings}\n import org.scalatest.flatspec._\n import org.scalatest.matchers._\n \n class IntegrationTest extends AnyFlatSpec\n-    with should.Matchers {\n+  with should.Matchers {\n \n   def getOrThrowException(envName: String): String = {\n     sys.env.getOrElse(\n       envName,\n       throw new IllegalStateException(s\"Environment variable '$envName' is required to perform this integration test.\"))\n   }\n-  val projectId = getOrThrowException(\"BIGTABLE_SPARK_PROJECT_ID\")\n-  val instanceId = getOrThrowException(\"BIGTABLE_SPARK_INSTANCE_ID\")\n-  val table_wordcount = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_TABLE\")\n+  val projectId: String = getOrThrowException(\"GOOGLE_CLOUD_PROJECT\")\n+  val instanceId: String = getOrThrowException(\"BIGTABLE_TESTING_INSTANCE\")\n   val file = \"src/test/resources/Romeo-and-Juliet-prologue.txt\"\n-  val table_copytable = getOrThrowException(\"BIGTABLE_SPARK_COPYTABLE_TABLE\")\n-  val rowCount = 88\n+\n+  val wordcount_table_name: String = \"spark-wordcount-\" + UUID.randomUUID.toString.substring(0, 20);\n+  val copytable_table_name: String = \"spark-copytable-\" + UUID.randomUUID.toString.substring(0, 20);\n+\n+  val tableClient: BigtableTableAdminClient = BigtableTableAdminClient.create(projectId, instanceId)\n+  val settings: BigtableDataSettings =\n+    BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n+  val dataClient: BigtableDataClient = BigtableDataClient.create(settings)\n \n   \"IntegrationTest\" should \"write records to Bigtable, copy them between tables\" in {\n-    import org.apache.spark.{SparkConf, SparkContext}\n-    val appName = getClass.getSimpleName.replace(\"$\", \"\")\n-    val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)\n-    SparkContext.getOrCreate(config)\n-\n-    val wordcountArgs = Array(projectId, instanceId, table_wordcount, file)\n-    Wordcount.main(wordcountArgs)\n-    val copytableArgs = Array(projectId, instanceId, table_wordcount, table_copytable)\n-    CopyTable.main(copytableArgs)\n-\n-    val settings =\n-      BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n-    val dataClient = BigtableDataClient.create(settings)\n-    import collection.JavaConverters._\n-    val wordcountRowCount = dataClient.readRows(Query.create(table_wordcount)).iterator().asScala.length\n-    val copytableRowCount = dataClient.readRows(Query.create(table_copytable)).iterator().asScala.length\n-    wordcountRowCount should be(rowCount)\n-    wordcountRowCount should be(copytableRowCount)\n+    try {\n+      import org.apache.spark.{SparkConf, SparkContext}\n+      val appName = getClass.getSimpleName.replace(\"$\", \"\")\n+      val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc5d6b54db1b7723abd42e3429ff88b269da805a"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk5Mjc4Mg==", "bodyText": "@lesv Yes, I tried looking for a different API but couldn't find anything. If anyone is aware of an alternative, please LMK!", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/4262#discussion_r526992782", "createdAt": "2020-11-19T15:48:38Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/test/scala/example/IntegrationTest.scala", "diffHunk": "@@ -15,44 +15,63 @@\n  */\n package example\n \n+import java.util.UUID\n+\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.admin.v2.BigtableTableAdminClient\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.admin.v2.models.CreateTableRequest\n import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.models.Query\n import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.{BigtableDataClient, BigtableDataSettings}\n import org.scalatest.flatspec._\n import org.scalatest.matchers._\n \n class IntegrationTest extends AnyFlatSpec\n-    with should.Matchers {\n+  with should.Matchers {\n \n   def getOrThrowException(envName: String): String = {\n     sys.env.getOrElse(\n       envName,\n       throw new IllegalStateException(s\"Environment variable '$envName' is required to perform this integration test.\"))\n   }\n-  val projectId = getOrThrowException(\"BIGTABLE_SPARK_PROJECT_ID\")\n-  val instanceId = getOrThrowException(\"BIGTABLE_SPARK_INSTANCE_ID\")\n-  val table_wordcount = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_TABLE\")\n+  val projectId: String = getOrThrowException(\"GOOGLE_CLOUD_PROJECT\")\n+  val instanceId: String = getOrThrowException(\"BIGTABLE_TESTING_INSTANCE\")\n   val file = \"src/test/resources/Romeo-and-Juliet-prologue.txt\"\n-  val table_copytable = getOrThrowException(\"BIGTABLE_SPARK_COPYTABLE_TABLE\")\n-  val rowCount = 88\n+\n+  val wordcount_table_name: String = \"spark-wordcount-\" + UUID.randomUUID.toString.substring(0, 20);\n+  val copytable_table_name: String = \"spark-copytable-\" + UUID.randomUUID.toString.substring(0, 20);\n+\n+  val tableClient: BigtableTableAdminClient = BigtableTableAdminClient.create(projectId, instanceId)\n+  val settings: BigtableDataSettings =\n+    BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n+  val dataClient: BigtableDataClient = BigtableDataClient.create(settings)\n \n   \"IntegrationTest\" should \"write records to Bigtable, copy them between tables\" in {\n-    import org.apache.spark.{SparkConf, SparkContext}\n-    val appName = getClass.getSimpleName.replace(\"$\", \"\")\n-    val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)\n-    SparkContext.getOrCreate(config)\n-\n-    val wordcountArgs = Array(projectId, instanceId, table_wordcount, file)\n-    Wordcount.main(wordcountArgs)\n-    val copytableArgs = Array(projectId, instanceId, table_wordcount, table_copytable)\n-    CopyTable.main(copytableArgs)\n-\n-    val settings =\n-      BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n-    val dataClient = BigtableDataClient.create(settings)\n-    import collection.JavaConverters._\n-    val wordcountRowCount = dataClient.readRows(Query.create(table_wordcount)).iterator().asScala.length\n-    val copytableRowCount = dataClient.readRows(Query.create(table_copytable)).iterator().asScala.length\n-    wordcountRowCount should be(rowCount)\n-    wordcountRowCount should be(copytableRowCount)\n+    try {\n+      import org.apache.spark.{SparkConf, SparkContext}\n+      val appName = getClass.getSimpleName.replace(\"$\", \"\")\n+      val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM1MzQ0NQ=="}, "originalCommit": {"oid": "dc5d6b54db1b7723abd42e3429ff88b269da805a"}, "originalPosition": 61}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 732, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}