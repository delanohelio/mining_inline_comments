{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY1MTUxOTU2", "number": 3487, "title": "[WIP] Spark Applications for Cloud Bigtable", "bodyText": "As per conversation with @igorbernstein2 and @kolea2 I'm pushing this Spark applications project for code review (and further refinement).", "createdAt": "2020-08-09T13:24:36Z", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487", "merged": true, "mergeCommit": {"oid": "6f1fff9738c519c42fc6a8de191b3928de49b95f"}, "closed": true, "closedAt": "2020-09-24T19:09:42Z", "author": {"login": "jaceklaskowski"}, "timelineItems": {"totalCount": 45, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc9_YhcAFqTQ2NTQ3ODA1Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdL87E5AH2gAyNDY1MTUxOTU2OmUzZmQwYWUyOTEzZWU4Y2JhODVhMmVkMTk4NDNlOWY3Nzg1YjFkNGU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NDc4MDU3", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-465478057", "createdAt": "2020-08-11T22:44:29Z", "commit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjo0NDozMFrOG_LvHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzoyNjoxMVrOG_Mkmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNTc1OQ==", "bodyText": "Can you group like dependencies together?\n\norg.apache.spark deps\nhbase deps\nbigtable deps\nworkaround deps\n\nAlso please extract the group versions into a const (sparkVersion, hbaseVersion, bigtableVersion)", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468905759", "createdAt": "2020-08-11T22:44:30Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjY3NA==", "bodyText": "Should this be moved to dependencyOverrides?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468906674", "createdAt": "2020-08-11T22:47:00Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjk3Mg==", "bodyText": "I'm a bit confused by this: this dependency is provided so it should be on the classpath already, so how could this cause a runtime error?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468906972", "createdAt": "2020-08-11T22:47:57Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw==", "bodyText": "This needs a bit of explanation", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468907133", "createdAt": "2020-08-11T22:48:27Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwODIzMg==", "bodyText": "Can you look into passing project, instance id in during runtime programmatically?  There should be a way to pass the configuration options. Maybe via the options()?\nIdeally the project id & instance id can be passed in as parameters alongside the table id", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468908232", "createdAt": "2020-08-11T22:51:40Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/resources/hbase-site.xml", "diffHunk": "@@ -0,0 +1,49 @@\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<!--\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+-->\n+<configuration>\n+    <property>\n+        <name>google.bigtable.project.id</name>\n+        <value>your-google-cloud-project</value>\n+    </property>\n+    <property>\n+        <name>google.bigtable.instance.id</name>\n+        <value>your-bigtable-instance</value>\n+    </property>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMDc4NA==", "bodyText": "Can this code be reorganized into a setup phase and query phase? Currently the data operations interwoven with the configuration.\nMaybe something like this:\n// Spark representation of the row\ncase class BigtableRecord (...)\n\n// Map  the scala representation to Bigtable\nval catalog = s\"\"\"....\"\"\"\n\n\n// configure the output sink\nval tableOpts = Map( ... )\n\n// Generate some data in memory\nval numRecords = Try(args(1).toInt).getOrElse(10)\nval records = (0 until numRecords).map(BigtableRecord.apply).toDF\n\n// write it to bigtable\nrecords.write....", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468910784", "createdAt": "2020-08-11T22:59:18Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTcxNA==", "bodyText": "Not sure if this is a scala thing, but why are the imports sprinkled throughout the file?\n\nHBaseTableCatalog is import for the entire file\nSparkSession is in the function scope\nspark implicits are in the middle\n\nCan you group them? Maybe move the non-implicits to the file level and the implicits to the top of the function", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468911714", "createdAt": "2020-08-11T23:01:57Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMjIwMA==", "bodyText": "Why is the launcher necessary? it doesnt seem to be used", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468912200", "createdAt": "2020-08-11T23:03:32Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/test/scala/example/WordcountLuncher.scala", "diffHunk": "@@ -0,0 +1,7 @@\n+package example", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzcyMg==", "bodyText": "This seems like a workaround similar to some of the options set in the xml. Can we keep all of them together?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913722", "createdAt": "2020-08-11T23:08:24Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    \"hbase.spark.use.hbasecontext\" -> \"false\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzgxMw==", "bodyText": "What is \"5\" here?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913813", "createdAt": "2020-08-11T23:08:43Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzkxNA==", "bodyText": "Please add a comment pointing users where they can find more options they can use", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913914", "createdAt": "2020-08-11T23:09:02Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDM3MQ==", "bodyText": "Also, add a comment as to where this version came from", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468914371", "createdAt": "2020-08-11T23:10:11Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjY3NA=="}, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDg0Mg==", "bodyText": "Also, please leave comments as to why the versions were chosen.\nie dataproc version x uses spark y and hbase z\nThere is a version conflict between hbase & spark for jackson, so we force the version to match spark", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468914842", "createdAt": "2020-08-11T23:11:41Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNTc1OQ=="}, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTAwNA==", "bodyText": "here and everywhere else, please add trailing newlines", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468915004", "createdAt": "2020-08-11T23:12:16Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(\n+  ExclusionRule(organization = \"asm\", \"asm\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils-core\"),\n+  ExclusionRule(organization = \"org.mortbay.jetty\", \"servlet-api\")\n+)\n+\n+assemblyMergeStrategy in assembly := {\n+  case PathList(\"META-INF\", \"io.netty.versions.properties\") => MergeStrategy.first\n+  case PathList(\"META-INF\", \"MANIFEST.MF\") => MergeStrategy.discard\n+  case PathList(\"mozilla\", \"public-suffix-list.txt\") => MergeStrategy.first\n+  case PathList(\"google\", xs @ _*) => xs match {\n+    case ps @ (x :: xs) if ps.last.endsWith(\".proto\") => MergeStrategy.first\n+    case _ => MergeStrategy.deduplicate\n+  }\n+  case x =>\n+    val oldStrategy = (assemblyMergeStrategy in assembly).value\n+    oldStrategy(x)\n+    // FIXME\n+    MergeStrategy.first\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNjYxOA==", "bodyText": "Can you add a paragraph explaining how all of the project interact?\nsomething along the lines of: Spark is the execution environment that can parallelize data processing. The spark provides an api for storage system to plug into this environment. The HBase Spark Connector implements an adapter for Hbase. bigtable-hbase-2.x-hadoop provides a bridge from the HBase api to cloud bigtable. Thus allowing spark pipelines to interact with bigtable using the native spark api.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468916618", "createdAt": "2020-08-11T23:17:13Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTE1OQ==", "bodyText": "All of these steps should be under a single section \"Test using the Bigtable emulator\"\nSomething like (note I'm using 4 backticks so that this comment would render)\n## Test using the Bigtable emulator\n``\n# Build an uber/fat jar\nsbt clean assembly\n\n# Start the emulator\ngcloud beta emulators bigtable start\n$(gcloud beta emulators bigtable env-init)\n\n# Start the job\n$SPARK_HOME/bin/spark-submit \\\n  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n  $BIGTABLE_TABLE $NUMBER_OF_ROWS\n\n# Verify \n$ cbt \\\n  -project=your-google-cloud-project \\\n  -instance=your-bigtable-instance \\\n  ls\nwordcount\n``\n\n\nThen we can add a section in the future for dataproc and real bigtable", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468919159", "createdAt": "2020-08-11T23:25:13Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Example\n+\n+The Spark application can be assembled into an uber/fat jar with all of its dependencies and configuration. To build use `sbt` as follows:\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+**NOTE**: Since the Cloud Bigtable configuration is included in the (fat) jar, any changes will require re-assembling it.\n+\n+## Start Bigtable Emulator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTQ1MQ==", "bodyText": "why wordcount?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468919451", "createdAt": "2020-08-11T23:26:11Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Example\n+\n+The Spark application can be assembled into an uber/fat jar with all of its dependencies and configuration. To build use `sbt` as follows:\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+**NOTE**: Since the Cloud Bigtable configuration is included in the (fat) jar, any changes will require re-assembling it.\n+\n+## Start Bigtable Emulator\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+## Configure the Example\n+\n+Create environment variables for the following commands:\n+\n+```\n+BIGTABLE_TABLE=wordcount\n+NUMBER_OF_ROWS=5\n+```\n+\n+## Run\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $BIGTABLE_TABLE $NUMBER_OF_ROWS\n+```\n+\n+## Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=your-google-cloud-project \\\n+  -instance=your-bigtable-instance \\\n+  ls\n+wordcount", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTA4ODU3", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-469508857", "createdAt": "2020-08-18T14:25:23Z", "commit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDoyNToyM1rOHCXGTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDoyNzozMFrOHCXMcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzY0Ng==", "bodyText": "typo in 'Launcher'", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472237646", "createdAt": "2020-08-18T14:25:23Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/test/scala/example/DataFrameDemoLuncher.scala", "diffHunk": "@@ -0,0 +1,7 @@\n+package example\n+\n+object DataFrameDemoLuncher extends App {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzOTIxNg==", "bodyText": "if these are meant to be uncommented by the developer, please add that as a comment", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472239216", "createdAt": "2020-08-18T14:27:30Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTk5MDAx", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-469599001", "createdAt": "2020-08-18T15:57:43Z", "commit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "state": "DISMISSED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo1Nzo0M1rOHCbPTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjozMDo0MVrOHCci5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwNTQ4NA==", "bodyText": "The pricing sentence seems extraneous...none of the other samples mention that", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472305484", "createdAt": "2020-08-18T15:57:43Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwNzU0OA==", "bodyText": "I would like to avoid specifying dependencies at runtime. This is create a lot of headaches for support engineers. Please move it to sbt file (same for below)", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472307548", "createdAt": "2020-08-18T16:00:43Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwOTg3Mg==", "bodyText": "Now that the sample is counting words it makes more sense.\nHowever the previous 2 steps will create 2 tables named \"wordcount-dataframe\" and \"wordcount-rdd\". So please update this section appropriately", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472309872", "createdAt": "2020-08-18T16:04:18Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Example\n+\n+The Spark application can be assembled into an uber/fat jar with all of its dependencies and configuration. To build use `sbt` as follows:\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+**NOTE**: Since the Cloud Bigtable configuration is included in the (fat) jar, any changes will require re-assembling it.\n+\n+## Start Bigtable Emulator\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+## Configure the Example\n+\n+Create environment variables for the following commands:\n+\n+```\n+BIGTABLE_TABLE=wordcount\n+NUMBER_OF_ROWS=5\n+```\n+\n+## Run\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $BIGTABLE_TABLE $NUMBER_OF_ROWS\n+```\n+\n+## Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=your-google-cloud-project \\\n+  -instance=your-bigtable-instance \\\n+  ls\n+wordcount", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTQ1MQ=="}, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg==", "bodyText": "the number 5 needs an explanation", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472310712", "createdAt": "2020-08-18T16:05:36Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMTgwMw==", "bodyText": "Also, why is the table called wordcount? the job is not counting words", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472311803", "createdAt": "2020-08-18T16:07:17Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg=="}, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg==", "bodyText": "I'm a bit confused why this wasnt done for the emulator? Which will confuse users if the connector autocreates tables or not. Please be consistent between the 2 examples", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472312982", "createdAt": "2020-08-18T16:09:18Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+```\n+APP_NAME=example.Wordcount\n+WORDCOUNT_BIGTABLE_TABLE=wordcount-rdd\n+WORDCOUNT_FILE=README.md\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $WORDCOUNT_BIGTABLE_TABLE", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMzU3Nw==", "bodyText": "Also, to makes this a bit shorter, please use the families= parameter of createtable", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472313577", "createdAt": "2020-08-18T16:10:04Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+```\n+APP_NAME=example.Wordcount\n+WORDCOUNT_BIGTABLE_TABLE=wordcount-rdd\n+WORDCOUNT_FILE=README.md\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $WORDCOUNT_BIGTABLE_TABLE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg=="}, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMzgzMQ==", "bodyText": "which version of data proc?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472313831", "createdAt": "2020-08-18T16:10:27Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,55 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+// Versions to match Dataproc", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxNDk5NA==", "bodyText": "Needs more explanation: which deps already provide this?\nWithout this information how will we know that this can be removed when updating versions", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472314994", "createdAt": "2020-08-18T16:12:22Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxNTM5Mw==", "bodyText": "did you forget to push a commit? that newline is still missing", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472315393", "createdAt": "2020-08-18T16:13:01Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(\n+  ExclusionRule(organization = \"asm\", \"asm\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils-core\"),\n+  ExclusionRule(organization = \"org.mortbay.jetty\", \"servlet-api\")\n+)\n+\n+assemblyMergeStrategy in assembly := {\n+  case PathList(\"META-INF\", \"io.netty.versions.properties\") => MergeStrategy.first\n+  case PathList(\"META-INF\", \"MANIFEST.MF\") => MergeStrategy.discard\n+  case PathList(\"mozilla\", \"public-suffix-list.txt\") => MergeStrategy.first\n+  case PathList(\"google\", xs @ _*) => xs match {\n+    case ps @ (x :: xs) if ps.last.endsWith(\".proto\") => MergeStrategy.first\n+    case _ => MergeStrategy.deduplicate\n+  }\n+  case x =>\n+    val oldStrategy = (assemblyMergeStrategy in assembly).value\n+    oldStrategy(x)\n+    // FIXME\n+    MergeStrategy.first\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTAwNA=="}, "originalCommit": {"oid": "85dee393bb9c2c52b027c4d75f633e05db70ba2e"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMDU0Ng==", "bodyText": "After a bit more thought, I think we should get rid of hbase-site.xml altogether. Settings things like com.google.cloud.bigtable.hbase2_x.BigtableConnection should be considered an internal implementation detail of BigtableConfiguration. I'm in particular worried about newer versions needing other shims injected. For each if the hbase-connector decided to start using async apis then we would have to start setting the async connection class name as well. I'd rather this be maintained java-bigtable-hbase than in a customer's handwritten xml file.\nPlease remove the xml file altogether and only use BigtableConfiguration here", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472320546", "createdAt": "2020-08-18T16:21:00Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMTM0Nw==", "bodyText": "please remove these comments ( google.bigtable.{project,instance}.id )", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472321347", "createdAt": "2020-08-18T16:22:17Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMzY5MA==", "bodyText": "That issue is supposed to be fixed in spark 2.2.1 & 2.3.0...this example uses 2.4.x. So I'm not sure that is correct or we should reopened that bug", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472323690", "createdAt": "2020-08-18T16:25:48Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/resources/hbase-site.xml", "diffHunk": "@@ -0,0 +1,49 @@\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<!--\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+-->\n+<configuration>\n+    <property>\n+        <name>google.bigtable.project.id</name>\n+        <value>your-google-cloud-project</value>\n+    </property>\n+    <property>\n+        <name>google.bigtable.instance.id</name>\n+        <value>your-bigtable-instance</value>\n+    </property>\n+    <property>\n+        <name>hbase.client.connection.impl</name>\n+        <value>com.google.cloud.bigtable.hbase2_x.BigtableConnection</value>\n+    </property>\n+\n+    <!-- shc uses namespaces, which bigtable doesn't support. This has the\n+    Bigtable client log warns rather than throw -->\n+    <property>\n+        <name>google.bigtable.namespace.warnings</name>\n+        <value>true</value>\n+    </property>\n+\n+    <!--  workaround: https://issues.apache.org/jira/browse/SPARK-21549 -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNDUzOQ==", "bodyText": "HBaseTableCatalog.newTable -> \"5\", needs an explanation before this PR can be merged\nThe other options can be left as a FIXME for now", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472324539", "createdAt": "2020-08-18T16:27:07Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjA4Mw==", "bodyText": "Lets make this a bit more useful. As opposed to generating a sequence, writing it and then reading. Why not make the a CopyTable example which reads the wordcount table generated from the RDD example and writes it to a new table", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326083", "createdAt": "2020-08-18T16:29:28Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  val hbaseContext = new HBaseContext(spark.sparkContext, conf)\n+  val opts_nouse = opts.filterNot { case (k, _) => k == HBaseSparkConf.USE_HBASECONTEXT }\n+  // END\n+\n+  records\n+    .write\n+    .format(\"org.apache.hadoop.hbase.spark\")\n+    .options(opts_nouse)\n+    .save\n+\n+  println(s\"Writing to $table...DONE\")\n+\n+  println(s\"Loading $table\")\n+  spark\n+    .read\n+    .format(\"org.apache.hadoop.hbase.spark\")\n+    .options(opts_nouse)\n+    .load\n+    .show(truncate = false)\n+  println(s\"Loading $table...DONE\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjUxMg==", "bodyText": "javadoc explaining what this job does", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326512", "createdAt": "2020-08-18T16:30:05Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,55 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.hbase.client.Put\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+object Wordcount extends App {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjg4Ng==", "bodyText": "I thought we are using hbase 2.x?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326886", "createdAt": "2020-08-18T16:30:41Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,55 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.hbase.client.Put\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  import org.apache.spark.SparkConf\n+  val sparkConf = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat in HBase 1.6.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NjM5NjYz", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-469639663", "createdAt": "2020-08-18T16:46:10Z", "commit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0NjoxMVrOHCdJPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0NjoxMVrOHCdJPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNjcwMw==", "bodyText": "I'm confused by this line:\n\nwhy is it separate from the opts definition above?\nwhy is the option being removed rather then reset to true?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472336703", "createdAt": "2020-08-18T16:46:11Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  val hbaseContext = new HBaseContext(spark.sparkContext, conf)\n+  val opts_nouse = opts.filterNot { case (k, _) => k == HBaseSparkConf.USE_HBASECONTEXT }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NjQwMTIy", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-469640122", "createdAt": "2020-08-18T16:46:47Z", "commit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0Njo0N1rOHCdKrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0Njo0N1rOHCdKrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzA2OA==", "bodyText": "move data generation to after setup", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472337068", "createdAt": "2020-08-18T16:46:47Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NjQwODM4", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-469640838", "createdAt": "2020-08-18T16:47:44Z", "commit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0Nzo0NFrOHCdM2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0Nzo0NFrOHCdM2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzYyNg==", "bodyText": "please move the println to where the write is actually happening", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472337626", "createdAt": "2020-08-18T16:47:44Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b155d03315891f9a44a451177b69a63154acb6a"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNDk3Mzc0", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-470497374", "createdAt": "2020-08-19T14:10:32Z", "commit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMDozMlrOHDJPgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMDozMlrOHDJPgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTIwMg==", "bodyText": "nit: the path has to be an absolute path\nGOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473059202", "createdAt": "2020-08-19T14:10:32Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "originalPosition": 219}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNDk4NDAz", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-470498403", "createdAt": "2020-08-19T14:11:37Z", "commit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMTozN1rOHDJScg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMTozN1rOHDJScg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTk1NA==", "bodyText": "why overwrite the assignments in Configure Environment?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473059954", "createdAt": "2020-08-19T14:11:37Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=tuesday\n+BIGTABLE_SPARK_CLUSTER_ID=tuesday-c1\n+BIGTABLE_SPARK_CLUSTER_ZONE=europe-west3-a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNDk4NTU3", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-470498557", "createdAt": "2020-08-19T14:11:47Z", "commit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMTo0N1rOHDJS5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxMTo0N1rOHDJS5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2MDA3MA==", "bodyText": "why overwrite the assignments in Configure Environment?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473060070", "createdAt": "2020-08-19T14:11:47Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=tuesday\n+BIGTABLE_SPARK_CLUSTER_ID=tuesday-c1\n+BIGTABLE_SPARK_CLUSTER_ZONE=europe-west3-a\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "originalPosition": 244}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNTA1MzYx", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-470505361", "createdAt": "2020-08-19T14:18:51Z", "commit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxODo1MVrOHDJnKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNDoxODo1MVrOHDJnKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NTI1OQ==", "bodyText": "fix what? please be descriptive, so that we know what needs to be fixed", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473065259", "createdAt": "2020-08-19T14:18:51Z", "author": {"login": "igorbernstein2"}, "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,55 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+// Versions to match Dataproc\n+val sparkVersion = \"2.4.5\"\n+val hbaseVersion = \"2.2.3\"\n+val bigtableVersion = \"1.15.0\"\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % bigtableVersion\n+)\n+\n+val fixes = Seq(\n+  // Fix for Exception: Incompatible Jackson 2.9.2\n+  // Version conflict between HBase and Spark\n+  // Forcing the version to match Spark\n+  // FIXME Would that work with dependencyOverrides?\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // Fix for NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame\n+  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion % Provided,\n+  // Fix for NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % hbaseVersion,\n+  \"org.apache.hbase\" % \"hbase-client\" % hbaseVersion\n+)\n+libraryDependencies ++= fixes\n+\n+// Excluding duplicates for the uber-jar\n+// There are other deps to provide necessary packages\n+excludeDependencies ++= Seq(\n+  ExclusionRule(organization = \"asm\", \"asm\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils-core\"),\n+  ExclusionRule(organization = \"org.mortbay.jetty\", \"servlet-api\")\n+)\n+\n+assemblyMergeStrategy in assembly := {\n+  case PathList(\"META-INF\", \"io.netty.versions.properties\") => MergeStrategy.first\n+  case PathList(\"META-INF\", \"MANIFEST.MF\") => MergeStrategy.discard\n+  case PathList(\"mozilla\", \"public-suffix-list.txt\") => MergeStrategy.first\n+  case PathList(\"google\", xs @ _*) => xs match {\n+    case ps @ (x :: xs) if ps.last.endsWith(\".proto\") => MergeStrategy.first\n+    case _ => MergeStrategy.deduplicate\n+  }\n+  case x =>\n+    val oldStrategy = (assemblyMergeStrategy in assembly).value\n+    oldStrategy(x)\n+    // FIXME", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1MzcyNTk2", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-485372596", "createdAt": "2020-09-09T20:18:40Z", "commit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDoxODo0MFrOHPY1ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDoyMzoxM1rOHPY-nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NzU3OA==", "bodyText": "because this will be used as a sample, I'd prefer to keep this a little less verbose. I think it's good to mention it's a workaround, but I would leave out the stack trace.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485897578", "createdAt": "2020-09-09T20:18:40Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,61 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.client._\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+// FIXME Explain the purpose of the app\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  var hConf = BigtableConfiguration.configure(projectId, instanceId)\n+  hConf.set(TableOutputFormat.OUTPUT_TABLE, table)\n+\n+  import org.apache.hadoop.mapreduce.Job\n+  val job = Job.getInstance(hConf)\n+  job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n+  hConf = job.getConfiguration\n+\n+  import org.apache.spark.SparkConf\n+  val config = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat\n+  // See https://stackoverflow.com/a/51959451/1305344\n+  // Without the property:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5ODE1Mg==", "bodyText": "it may be hard to maintain this if we upgrade the hbase version, as well as the static line numbers. Is this link necessary?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485898152", "createdAt": "2020-09-09T20:19:50Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,61 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.client._\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+// FIXME Explain the purpose of the app\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  var hConf = BigtableConfiguration.configure(projectId, instanceId)\n+  hConf.set(TableOutputFormat.OUTPUT_TABLE, table)\n+\n+  import org.apache.hadoop.mapreduce.Job\n+  val job = Job.getInstance(hConf)\n+  job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n+  hConf = job.getConfiguration\n+\n+  import org.apache.spark.SparkConf\n+  val config = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat\n+  // See https://stackoverflow.com/a/51959451/1305344\n+  // Without the property:\n+  // org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.\n+  // at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:138)\n+  // at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.assertConf(SparkHadoopWriter.scala:393)\n+  // at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n+  // at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)\n+  config.set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n+\n+  val sc = SparkContext.getOrCreate(config)\n+  val wordCounts = sc\n+    .textFile(file)\n+    .flatMap(_.split(\"\\\\W+\"))\n+    .filter(!_.isEmpty)\n+    .map { word => (word, 1) }\n+    .reduceByKey(_ + _)\n+    .map { case (word, count) =>\n+      val put = new Put(Bytes.toBytes(word))\n+        .addColumn(ColumnFamilyBytes, ColumnNameBytes, Bytes.toBytes(count))\n+      // The KEY is ignored while the output value must be either a Put or a Delete instance\n+      // https://github.com/apache/hbase/blob/rel/2.2.3/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java#L46-L48", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5ODc2Ng==", "bodyText": "why --quiet?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485898766", "createdAt": "2020-09-09T20:21:00Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,392 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+Set the following environment variable to reference the assembly file.\n+\n+```\n+BIGTABLE_SPARK_ASSEMBLY_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+```\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+SPARK_HOME=your-spark-home\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance\n+\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+\n+BIGTABLE_SPARK_COPYTABLE_TABLE=copytable\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+### Create Tables\n+\n+Create the tables using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_COPYTABLE_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+copytable\n+wordcount\n+```\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+324\n+```\n+\n+**TIP** For details about using the `cbt` tool, including a list of available commands, see the [cbt Reference](https://cloud.google.com/bigtable/docs/cbt-reference).\n+\n+### CopyTable\n+\n+The following `spark-submit` uses [example.CopyTable](src/main/scala/example/CopyTable.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.CopyTable \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_COPYTABLE_TABLE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_COPYTABLE_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_COPYTABLE_TABLE\n+324\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Create Cloud Bigtable Instance\n+\n+Create a Cloud Bigtable instance using the Google Cloud Console (as described in the [Create a Cloud Bigtable instance](https://cloud.google.com/bigtable/docs/quickstart-cbt#create-instance)) or `gcloud beta bigtable instances`.\n+\n+```\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-zone-id\n+BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME=your-display-name\n+\n+gcloud beta bigtable instances \\\n+  create $BIGTABLE_SPARK_INSTANCE_ID \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+  --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+  --display-name=$BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME \\\n+  --instance-type=DEVELOPMENT\n+```\n+\n+Check out the available Cloud Bigtable instances using `gcloud beta bigtable instances list` command.\n+\n+```\n+gcloud beta bigtable instances list\n+```\n+\n+### Create Table\n+\n+Create a table using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table. There should be \n+324 rows.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+324\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+Use `cbt listinstances` to list existing Bigtable instances.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+There should be at least `BIGTABLE_SPARK_INSTANCE_ID` instance. Delete it using `cbt deleteinstance`.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/CopyTable.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=your-region\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  --quiet\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=your-instance-id\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-cluster-zone\n+\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE rowkey\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf1\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf2\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf3\n+```\n+\n+### Submit Wordcount Job\n+\n+Submit the Wordcount job to a Cloud Dataproc instance.\n+\n+```\n+gcloud dataproc jobs submit spark \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --class=$BIGTABLE_SPARK_CLASS \\\n+  --jars=$BIGTABLE_SPARK_JAR \\\n+  --properties=spark.jars.packages='org.apache.hbase.connectors.spark:hbase-spark:1.0.0' \\\n+  -- \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  read $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+### Clean Up\n+\n+Delete the Bigtable instance.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+Delete the Dataproc cluster.\n+\n+```\n+gcloud dataproc clusters delete $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  --quiet", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "originalPosition": 391}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5OTQ2OQ==", "bodyText": "same concern here with static links", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485899469", "createdAt": "2020-09-09T20:22:14Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+object CopyTable extends App {\n+\n+  val appName = this.getClass.getSimpleName.replace(\"$\", \"\")\n+  println(s\"$appName Spark application is starting up...\")\n+\n+  val (projectId, instanceId, fromTable, toTable) = parse(args)\n+  println(\n+    s\"\"\"\n+      |Parameters:\n+      |projectId: $projectId\n+      |instanceId: $instanceId\n+      |copy from $fromTable to $toTable\n+      |\"\"\".stripMargin)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().getOrCreate()\n+  println(s\"Spark version: ${spark.version}\")\n+\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  // Creating HBaseContext explicitly to use the conf above\n+  // That's how to use command-line arguments for projectId and instanceId\n+  // Otherwise, we'd have to use hbase-site.xml\n+  // See HBaseSparkConf.USE_HBASECONTEXT option in hbase-connectors project\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala#L44", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5OTkzNQ==", "bodyText": "I'd add some additional comments here as to what we're doing (specifically around createCatalogJSON)", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485899935", "createdAt": "2020-09-09T20:23:13Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+object CopyTable extends App {\n+\n+  val appName = this.getClass.getSimpleName.replace(\"$\", \"\")\n+  println(s\"$appName Spark application is starting up...\")\n+\n+  val (projectId, instanceId, fromTable, toTable) = parse(args)\n+  println(\n+    s\"\"\"\n+      |Parameters:\n+      |projectId: $projectId\n+      |instanceId: $instanceId\n+      |copy from $fromTable to $toTable\n+      |\"\"\".stripMargin)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().getOrCreate()\n+  println(s\"Spark version: ${spark.version}\")\n+\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  // Creating HBaseContext explicitly to use the conf above\n+  // That's how to use command-line arguments for projectId and instanceId\n+  // Otherwise, we'd have to use hbase-site.xml\n+  // See HBaseSparkConf.USE_HBASECONTEXT option in hbase-connectors project\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala#L44\n+  new HBaseContext(spark.sparkContext, conf)\n+\n+  def createCatalogJSON(table: String): String = {\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"word\",\n+       |\"columns\":{\n+       |  \"word\":{\"cf\":\"rowkey\", \"col\":\"word\", \"type\":\"string\"},\n+       |  \"count\":{\"cf\":\"cf\", \"col\":\"Count\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  }\n+\n+  // The HBaseTableCatalog options are described in the sources themselves only\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala\n+\n+  println(s\"Loading records from $fromTable\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7c4ee2a76bb1c8027df712999454efdfc72207"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2ODU4NTY1", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-486858565", "createdAt": "2020-09-11T14:30:03Z", "commit": {"oid": "167dc628b0ebb5f1855a125e3818f959d494df0e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNDozMDowM1rOHQhPBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNDozMDowM1rOHQhPBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzA4Mzc4MA==", "bodyText": "looks good! Only thing I think needs to be added is a cleanup function to remove the data added to the table after it's finished with the assertions.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r487083780", "createdAt": "2020-09-11T14:30:03Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/test/scala/example/IntegrationTest.scala", "diffHunk": "@@ -0,0 +1,43 @@\n+package example\n+\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.models.Query\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.{BigtableDataClient, BigtableDataSettings}\n+import org.scalatest.flatspec._\n+import org.scalatest.matchers._\n+\n+class IntegrationTest extends AnyFlatSpec\n+    with should.Matchers {\n+\n+  def getOrThrowException(envName: String): String = {\n+    sys.env.getOrElse(\n+      envName,\n+      throw new IllegalStateException(s\"Environment variable '$envName' is required to perform this integration test.\"))\n+  }\n+  val projectId = getOrThrowException(\"BIGTABLE_SPARK_PROJECT_ID\")\n+  val instanceId = getOrThrowException(\"BIGTABLE_SPARK_INSTANCE_ID\")\n+  val table_wordcount = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_TABLE\")\n+  val file = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_FILE\")\n+  val table_copytable = getOrThrowException(\"BIGTABLE_SPARK_COPYTABLE_TABLE\")\n+  val rowCount = getOrThrowException(\"BIGTABLE_SPARK_ROW_COUNT\").toInt\n+\n+  \"IntegrationTest\" should \"write records to Bigtable, copy them between tables\" in {\n+    import org.apache.spark.{SparkConf, SparkContext}\n+    val appName = getClass.getSimpleName.replace(\"$\", \"\")\n+    val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)\n+    SparkContext.getOrCreate(config)\n+\n+    val wordcountArgs = Array(projectId, instanceId, table_wordcount, file)\n+    Wordcount.main(wordcountArgs)\n+    val copytableArgs = Array(projectId, instanceId, table_wordcount, table_copytable)\n+    CopyTable.main(copytableArgs)\n+\n+    val settings =\n+      BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n+    val dataClient = BigtableDataClient.create(settings)\n+    import collection.JavaConverters._\n+    val wordcountRowCount = dataClient.readRows(Query.create(table_wordcount)).iterator().asScala.length\n+    val copytableRowCount = dataClient.readRows(Query.create(table_copytable)).iterator().asScala.length\n+    wordcountRowCount should be(rowCount)\n+    wordcountRowCount should be(copytableRowCount)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "167dc628b0ebb5f1855a125e3818f959d494df0e"}, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24f1ebda25fe6e6c66b57b04f9e4306045b1ee1a", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/24f1ebda25fe6e6c66b57b04f9e4306045b1ee1a", "committedDate": "2020-09-17T12:48:16Z", "message": "[WIP] Spark Applications for Cloud Bigtable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "226ec5b00ad8c860ce2c772fa9e3bda2771e935b", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/226ec5b00ad8c860ce2c772fa9e3bda2771e935b", "committedDate": "2020-09-17T12:48:16Z", "message": "Loading and saving using DataFrame API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2720e7176bc8a8495ee8e4af8421baa7514445a1", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2720e7176bc8a8495ee8e4af8421baa7514445a1", "committedDate": "2020-09-17T12:48:16Z", "message": "Writing data using RDD API to Bigtable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "326f31e087813f8e941ac29e52dbbfdf81a87d50", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/326f31e087813f8e941ac29e52dbbfdf81a87d50", "committedDate": "2020-09-17T12:48:16Z", "message": "Addressing comments after a code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5c505960de92c2b518767c7fb9faa52dda0b868", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b5c505960de92c2b518767c7fb9faa52dda0b868", "committedDate": "2020-09-17T12:48:16Z", "message": "Submit DataFrameDemo to Cloud Dataproc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0f61a24ad9c6755199640e5efa60fd61c1babbb", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b0f61a24ad9c6755199640e5efa60fd61c1babbb", "committedDate": "2020-09-17T12:48:16Z", "message": "After code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cad5dd859f72bc15157caf136d26c3f865751ec", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7cad5dd859f72bc15157caf136d26c3f865751ec", "committedDate": "2020-09-17T12:48:16Z", "message": "After code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "212becb859c29f308cea96790061bef22539c774", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/212becb859c29f308cea96790061bef22539c774", "committedDate": "2020-09-17T12:48:17Z", "message": "[Wordcount] Use Admin API to create tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f08e91054d204786b6f6ff98baa379ecace992b2", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/f08e91054d204786b6f6ff98baa379ecace992b2", "committedDate": "2020-09-17T12:48:17Z", "message": "Command-line options + testing framework"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55be0bf3d55f30ddbbbafc7e4c1530fa46ca2f22", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/55be0bf3d55f30ddbbbafc7e4c1530fa46ca2f22", "committedDate": "2020-09-17T12:48:17Z", "message": "Add the other cmd options + exclude tests in assembly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c76467280531af29c247ca810f1b89c2acfb46cd", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/c76467280531af29c247ca810f1b89c2acfb46cd", "committedDate": "2020-09-17T12:48:17Z", "message": "Tasts marked done (and removed)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46679884dae9a338c6701a852f9ae07522b5e763", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/46679884dae9a338c6701a852f9ae07522b5e763", "committedDate": "2020-09-17T12:48:17Z", "message": "No need for extra dep. Bye, bye scopt"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70657d61a871b9e213e74e7aa2729746003cadc3", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/70657d61a871b9e213e74e7aa2729746003cadc3", "committedDate": "2020-09-17T12:48:17Z", "message": "No need for xml config. Command-line arguments enough"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b269353d9222c19e013ec62b14890a831d1dbd21", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b269353d9222c19e013ec62b14890a831d1dbd21", "committedDate": "2020-09-17T12:48:17Z", "message": "Create Tables before running demos"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8761674d8b42194fffeb3f39defbc8cf430b76d1", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/8761674d8b42194fffeb3f39defbc8cf430b76d1", "committedDate": "2020-09-17T12:48:17Z", "message": "CopyTable - Copying tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ee3dfcce550907b8b2251e900b55801e965ddd9", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2ee3dfcce550907b8b2251e900b55801e965ddd9", "committedDate": "2020-09-17T12:48:17Z", "message": "Integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "409ca69856f4398021806002bf02c054dda31799", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/409ca69856f4398021806002bf02c054dda31799", "committedDate": "2020-09-17T12:48:17Z", "message": "Assert row count in integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3d096fee60175181d78a32b6595874f318b489b", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/a3d096fee60175181d78a32b6595874f318b489b", "committedDate": "2020-09-17T12:48:17Z", "message": "BIGTABLE_SPARK_ROW_COUNT for IT test to assert the row count"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21ff69dd42fbbd1d01db3512f9bf87f7ad0790d4", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/21ff69dd42fbbd1d01db3512f9bf87f7ad0790d4", "committedDate": "2020-09-17T12:48:17Z", "message": "Use \"stable\" file for integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20393ed52ba8d41397ac252bad7a85eb6f857f3d", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/20393ed52ba8d41397ac252bad7a85eb6f857f3d", "committedDate": "2020-09-17T12:48:17Z", "message": "Running Integration Test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe3c6e5371fe3f5fca88eaac37144841d350a6c9", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fe3c6e5371fe3f5fca88eaac37144841d350a6c9", "committedDate": "2020-09-17T12:48:17Z", "message": "License headers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fd8027b9f4767610c446f99ff0006d267ec0401c", "committedDate": "2020-09-17T12:48:17Z", "message": "Fix the last round of style comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b9cdc4e893f83aec1b0d037970edc750440ab95a", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b9cdc4e893f83aec1b0d037970edc750440ab95a", "committedDate": "2020-09-17T12:43:35Z", "message": "Fix the last round of style comments"}, "afterCommit": {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fd8027b9f4767610c446f99ff0006d267ec0401c", "committedDate": "2020-09-17T12:48:17Z", "message": "Fix the last round of style comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkyOTU3Njc5", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-492957679", "createdAt": "2020-09-21T20:31:23Z", "commit": {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQyMDozMTozOFrOHVhWzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQyMDozNDozM1rOHVhcdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMyODY1NQ==", "bodyText": "Please find the format for headers here (same comment for other files): https://github.com/GoogleCloudPlatform/java-docs-samples#source-code-headers", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r492328655", "createdAt": "2020-09-21T20:31:38Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,115 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMzMDEwMQ==", "bodyText": "this is not left as an instruction on what to set as - maybe switch this to BIGTABLE_SPARK_CLUSTER_ID, which is already defined elsewhere?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r492330101", "createdAt": "2020-09-21T20:34:33Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,458 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+Set the following environment variable to reference the assembly file.\n+\n+```\n+BIGTABLE_SPARK_ASSEMBLY_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+```\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+SPARK_HOME=your-spark-home\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance\n+\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount\n+BIGTABLE_SPARK_WORDCOUNT_FILE=src/test/resources/Romeo-and-Juliet-prologue.txt\n+\n+BIGTABLE_SPARK_COPYTABLE_TABLE=copytable\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+### Create Tables\n+\n+Create the tables using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_COPYTABLE_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+copytable\n+wordcount\n+```\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+88\n+```\n+\n+**TIP** For details about using the `cbt` tool, including a list of available commands, see the [cbt Reference](https://cloud.google.com/bigtable/docs/cbt-reference).\n+\n+### CopyTable\n+\n+The following `spark-submit` uses [example.CopyTable](src/main/scala/example/CopyTable.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.CopyTable \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_COPYTABLE_TABLE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_COPYTABLE_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_COPYTABLE_TABLE\n+88\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Create Cloud Bigtable Instance\n+\n+Create a Cloud Bigtable instance using the Google Cloud Console (as described in the [Create a Cloud Bigtable instance](https://cloud.google.com/bigtable/docs/quickstart-cbt#create-instance)) or `gcloud beta bigtable instances`.\n+\n+```\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-zone-id\n+BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME=your-display-name\n+\n+gcloud beta bigtable instances \\\n+  create $BIGTABLE_SPARK_INSTANCE_ID \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+  --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+  --display-name=$BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME \\\n+  --instance-type=DEVELOPMENT\n+```\n+\n+Check out the available Cloud Bigtable instances and make sure yours is listed.\n+\n+```\n+gcloud beta bigtable instances list\n+```\n+\n+### Create Table\n+\n+Create a table using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table. There should be \n+88 rows.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+88\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+Use `cbt listinstances` to list existing Bigtable instances.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+There should be at least `BIGTABLE_SPARK_INSTANCE_ID` instance. Delete it using `cbt deleteinstance`.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/CopyTable.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=your-region\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=your-instance-id\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-cluster-zone\n+\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE rowkey\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf1\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf2\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf3\n+```\n+\n+### Submit Wordcount Job\n+\n+Submit the Wordcount job to a Cloud Dataproc instance.\n+\n+```\n+gcloud dataproc jobs submit spark \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c"}, "originalPosition": 350}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c30372a2af6058b5bc6a375ab45d54f19003e46", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2c30372a2af6058b5bc6a375ab45d54f19003e46", "committedDate": "2020-09-21T20:37:03Z", "message": "Deps to match Dataproc 1.4"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ac6d5994a9ddfcbde0623ef09006da3e7912ad2", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/4ac6d5994a9ddfcbde0623ef09006da3e7912ad2", "committedDate": "2020-09-22T07:16:37Z", "message": "No need for supersafe"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6535386614043e901f45ee696ce90ca0345c7f69", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/6535386614043e901f45ee696ce90ca0345c7f69", "committedDate": "2020-09-22T07:29:26Z", "message": "Source Code Headers\n\nAs per the official doc at https://github.com/GoogleCloudPlatform/java-docs-samples#source-code-headers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed22800dffc4866a9bda93afda6e11f217fc778e", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/ed22800dffc4866a9bda93afda6e11f217fc778e", "committedDate": "2020-09-22T08:52:20Z", "message": "Run Wordcount with Cloud Bigtable verified to work"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "179badeab9fc212d909ccda649217386fa02fa56", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/179badeab9fc212d909ccda649217386fa02fa56", "committedDate": "2020-09-22T10:13:50Z", "message": "Run Wordcount with Cloud Dataproc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbc31e14cce818656ec096c888f95e409feb847d", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fbc31e14cce818656ec096c888f95e409feb847d", "committedDate": "2020-09-22T15:30:22Z", "message": "Removing HBase version dep"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTQ4MjI2", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#pullrequestreview-495148226", "createdAt": "2020-09-24T00:46:10Z", "commit": {"oid": "fbc31e14cce818656ec096c888f95e409feb847d"}, "state": "DISMISSED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo0NjoxMFrOHXF7Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo0Njo0NlrOHXF7vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NjM0Nw==", "bodyText": "please fix extra space here (after https://)", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r493976347", "createdAt": "2020-09-24T00:46:10Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/src/test/resources/log4j.properties", "diffHunk": "@@ -0,0 +1,18 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https:// www.apache.org/licenses/LICENSE-2.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbc31e14cce818656ec096c888f95e409feb847d"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NjUxMA==", "bodyText": "please fix extra space here (after https://)", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r493976510", "createdAt": "2020-09-24T00:46:46Z", "author": {"login": "kolea2"}, "path": "bigtable/spark/project/build.properties", "diffHunk": "@@ -0,0 +1,15 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https:// www.apache.org/licenses/LICENSE-2.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbc31e14cce818656ec096c888f95e409feb847d"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c2fff88b0e05faa57f3a01a5ca3701367e5d9c6", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/0c2fff88b0e05faa57f3a01a5ca3701367e5d9c6", "committedDate": "2020-09-24T08:27:16Z", "message": "Licence headers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3fd0ae2913ee8cba85a2ed19843e9f7785b1d4e", "author": {"user": {"login": "jaceklaskowski", "name": "Jacek Laskowski"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/e3fd0ae2913ee8cba85a2ed19843e9f7785b1d4e", "committedDate": "2020-09-24T08:29:14Z", "message": "README"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 532, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}