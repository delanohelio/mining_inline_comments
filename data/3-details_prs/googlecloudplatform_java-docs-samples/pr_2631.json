{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyMjMwMTE5", "number": 2631, "title": "Bigtable Keyviz Art Codelab", "bodyText": "", "createdAt": "2020-04-11T18:37:07Z", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631", "merged": true, "mergeCommit": {"oid": "0057fc9f03997459ded3608097902745436f132e"}, "closed": true, "closedAt": "2020-04-22T18:21:31Z", "author": {"login": "billyjacobson"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcTZslVAH2gAyNDAyMjMwMTE5OjNiY2Y5N2FkMjIwNjQwYjU5MmQ0NTZmNmE1NmI3MjViOGM0OGU0Mzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcYQoi_AFqTM5NDg3NTQ5NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3bcf97ad220640b592d456f6a56b725b8c48e438", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/3bcf97ad220640b592d456f6a56b725b8c48e438", "committedDate": "2020-04-01T15:46:58Z", "message": "working keyviz art"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3f74d676c7103ae850526e52194658d8d5fb8e9", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/d3f74d676c7103ae850526e52194658d8d5fb8e9", "committedDate": "2020-04-02T19:02:25Z", "message": "Cleanup ReadData with comments and private functions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfd8c0803622dbcb9cfc0931a0048a88a27599ac", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/bfd8c0803622dbcb9cfc0931a0048a88a27599ac", "committedDate": "2020-04-08T14:45:33Z", "message": "Add more comments and cleanup code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a584266e77cfc7de0002d6665599faac09b930ab", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/a584266e77cfc7de0002d6665599faac09b930ab", "committedDate": "2020-04-10T18:39:56Z", "message": "Updating types, working on tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7865d3a038cdfc7a6d5cee623f8c3f6f6ba412aa", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7865d3a038cdfc7a6d5cee623f8c3f6f6ba412aa", "committedDate": "2020-04-10T20:16:25Z", "message": "Package for code and working tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/ad9fc3c0e3737db6a014ad4dee880731838fb56e", "committedDate": "2020-04-13T15:34:15Z", "message": "Cleanup commented code and import"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyMjM3Nzgw", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#pullrequestreview-392237780", "createdAt": "2020-04-13T15:57:33Z", "commit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNTo1NzozM1rOGErIvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNjozMVrOGErbww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg==", "bodyText": "Not our preferred package name, but we seem to be moving in this direction.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407554236", "createdAt": "2020-04-13T15:57:33Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NTg5Mw==", "bodyText": "Should this be done outside the processElement() so it's cached?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407555893", "createdAt": "2020-04-13T16:00:36Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.CloudBigtableIO;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.util.Random;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * A Beam job that loads random data into Cloud Bigtable.\n+ */\n+public class LoadData {\n+\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  public static void main(String[] args) {\n+\n+    WriteDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(WriteDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    long rowSize = options.getMegabytesPerRow() * ONE_MB;\n+    final long max =\n+        (Math.round((options.getGigabytesWritten() * ONE_GB)) / rowSize);\n+\n+    p.apply(GenerateSequence.from(0).to(max))\n+        .apply(\n+            ParDo.of(\n+                new DoFn<Long, Mutation>() {\n+                  @ProcessElement\n+                  public void processElement(@Element Long rowkey, OutputReceiver<Mutation> out) {\n+                    // Make each number the same length by padding with 0s\n+                    int maxLength = (\"\" + max).length();\n+                    String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", rowkey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NjM3OQ==", "bodyText": "xtra line?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407556379", "createdAt": "2020-04-13T16:01:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NzQ5MA==", "bodyText": "Isn't there a better way to do this?  If not, perhaps an explanation of what's going on would be useful?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407557490", "createdAt": "2020-04-13T16:03:36Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODAzMg==", "bodyText": "Should this be more specific?  Can we mitigate issues?  Should we just let this happen?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558032", "createdAt": "2020-04-13T16:04:33Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODUyMQ==", "bodyText": "More specificity?\nSpecific causes?\nmitigation?\nDon't bother catching?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558521", "createdAt": "2020-04-13T16:05:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1OTEwNw==", "bodyText": "Should the concatenation really be in a loop?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407559107", "createdAt": "2020-04-13T16:06:31Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }\n+    }\n+\n+    /**\n+     * Generates an array with the rowkeys that were loaded into the specified Bigtable. This is\n+     * used to create the correct intervals for scanning equal sections of rowkeys. Since Bigtable\n+     * sorts keys lexicographically if we just used standard intervals, each section would have\n+     * different sizes.\n+     */\n+    private void generateRowkeys(long maxInput) {\n+      int maxLength = (\"\" + maxInput).length();\n+      for (int i = 0; i < maxInput; i++) {\n+        // Make each number the same length by padding with 0s.\n+        String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 166}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7d1e86e2441864bdacf5c5afef0b304391f58e55", "committedDate": "2020-04-13T19:46:15Z", "message": "Cache number format, comment on row counting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8a39ba96e8a0339048f69ee28ba6fd3178f8eb2", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/c8a39ba96e8a0339048f69ee28ba6fd3178f8eb2", "committedDate": "2020-04-13T19:46:52Z", "message": "empty line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628", "author": {"user": {"login": "billyjacobson", "name": "Billy Jacobson"}}, "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/578922c87135900f5a704bc435bd8272b84ce628", "committedDate": "2020-04-15T14:47:34Z", "message": "Fix filter for efficient row counting"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MTg3NTk0", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#pullrequestreview-394187594", "createdAt": "2020-04-15T22:49:26Z", "commit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMjo0OToyNlrOGGOUMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMjo0OToyNlrOGGOUMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE3OTE4Nw==", "bodyText": "This is good now.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409179187", "createdAt": "2020-04-15T22:49:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg=="}, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0ODc1NDk1", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#pullrequestreview-394875495", "createdAt": "2020-04-16T18:02:29Z", "commit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowMjozMFrOGGxErw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowMjozMFrOGGxErw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODY1NQ==", "bodyText": "just want to confirm this is the default, and that's why we're deleting it?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409748655", "createdAt": "2020-04-16T18:02:30Z", "author": {"login": "kolea2"}, "path": "bigtable/beam/helloworld/src/test/java/HelloWorldTest.java", "diffHunk": "@@ -104,8 +104,7 @@ public void testWrite() {\n         new String[] {\n           \"--bigtableProjectId=\" + projectId,\n           \"--bigtableInstanceId=\" + instanceId,\n-          \"--bigtableTableId=\" + TABLE_ID,\n-          \"--runner=DirectRunner\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 812, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}