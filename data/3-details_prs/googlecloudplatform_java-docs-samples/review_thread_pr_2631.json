{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyMjMwMTE5", "number": 2631, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNTo1NzozM1rODxSe6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowMjozMFrODynY8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDA5NjQzOnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNTo1NzozM1rOGErIvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMjo0OToyNlrOGGOUMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg==", "bodyText": "Not our preferred package name, but we seem to be moving in this direction.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407554236", "createdAt": "2020-04-13T15:57:33Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NjI0OQ==", "bodyText": "I actually wasn't going to even have a package but needed it due to the way I am testing. Can change if preferred?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407676249", "createdAt": "2020-04-13T19:40:49Z", "author": {"login": "billyjacobson"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg=="}, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE3OTE4Nw==", "bodyText": "This is good now.", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409179187", "createdAt": "2020-04-15T22:49:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg=="}, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDEwNjgxOnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowMDozNlrOGErPNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowMDozNlrOGErPNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NTg5Mw==", "bodyText": "Should this be done outside the processElement() so it's cached?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407555893", "createdAt": "2020-04-13T16:00:36Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.CloudBigtableIO;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.util.Random;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * A Beam job that loads random data into Cloud Bigtable.\n+ */\n+public class LoadData {\n+\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  public static void main(String[] args) {\n+\n+    WriteDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(WriteDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    long rowSize = options.getMegabytesPerRow() * ONE_MB;\n+    final long max =\n+        (Math.round((options.getGigabytesWritten() * ONE_GB)) / rowSize);\n+\n+    p.apply(GenerateSequence.from(0).to(max))\n+        .apply(\n+            ParDo.of(\n+                new DoFn<Long, Mutation>() {\n+                  @ProcessElement\n+                  public void processElement(@Element Long rowkey, OutputReceiver<Mutation> out) {\n+                    // Make each number the same length by padding with 0s\n+                    int maxLength = (\"\" + max).length();\n+                    String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", rowkey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDEwOTk4OnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowMToyNlrOGErRGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowMToyNlrOGErRGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NjM3OQ==", "bodyText": "xtra line?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407556379", "createdAt": "2020-04-13T16:01:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDExNjk2OnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowMzozNlrOGErVcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxOTo0MTo1OVrOGEynhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NzQ5MA==", "bodyText": "Isn't there a better way to do this?  If not, perhaps an explanation of what's going on would be useful?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407557490", "createdAt": "2020-04-13T16:03:36Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NjgwNw==", "bodyText": "Yeah, this is the way to do it. Can leave a comment", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407676807", "createdAt": "2020-04-13T19:41:59Z", "author": {"login": "billyjacobson"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NzQ5MA=="}, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDEyMDY2OnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNDozM1rOGErXkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxOTo0MjozMVrOGEyocw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODAzMg==", "bodyText": "Should this be more specific?  Can we mitigate issues?  Should we just let this happen?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558032", "createdAt": "2020-04-13T16:04:33Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NzA0Mw==", "bodyText": "I think it's fine to just let it happen for this codelab", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407677043", "createdAt": "2020-04-13T19:42:31Z", "author": {"login": "billyjacobson"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODAzMg=="}, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDEyMzc2OnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNToyNlrOGErZeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNToyNlrOGErZeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODUyMQ==", "bodyText": "More specificity?\nSpecific causes?\nmitigation?\nDon't bother catching?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558521", "createdAt": "2020-04-13T16:05:26Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDEyNzM0OnYy", "diffSide": "RIGHT", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNjozMVrOGErbww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxNjowNjozMVrOGErbww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1OTEwNw==", "bodyText": "Should the concatenation really be in a loop?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407559107", "createdAt": "2020-04-13T16:06:31Z", "author": {"login": "lesv"}, "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }\n+    }\n+\n+    /**\n+     * Generates an array with the rowkeys that were loaded into the specified Bigtable. This is\n+     * used to create the correct intervals for scanning equal sections of rowkeys. Since Bigtable\n+     * sorts keys lexicographically if we just used standard intervals, each section would have\n+     * different sizes.\n+     */\n+    private void generateRowkeys(long maxInput) {\n+      int maxLength = (\"\" + maxInput).length();\n+      for (int i = 0; i < maxInput; i++) {\n+        // Make each number the same length by padding with 0s.\n+        String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NDAwNzU0OnYy", "diffSide": "LEFT", "path": "bigtable/beam/helloworld/src/test/java/HelloWorldTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowMjozMFrOGGxErw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxODoyMDo0M1rOGKEk3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODY1NQ==", "bodyText": "just want to confirm this is the default, and that's why we're deleting it?", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409748655", "createdAt": "2020-04-16T18:02:30Z", "author": {"login": "kolea2"}, "path": "bigtable/beam/helloworld/src/test/java/HelloWorldTest.java", "diffHunk": "@@ -104,8 +104,7 @@ public void testWrite() {\n         new String[] {\n           \"--bigtableProjectId=\" + projectId,\n           \"--bigtableInstanceId=\" + instanceId,\n-          \"--bigtableTableId=\" + TABLE_ID,\n-          \"--runner=DirectRunner\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIxMzkxOA==", "bodyText": "correct", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r413213918", "createdAt": "2020-04-22T18:20:43Z", "author": {"login": "billyjacobson"}, "path": "bigtable/beam/helloworld/src/test/java/HelloWorldTest.java", "diffHunk": "@@ -104,8 +104,7 @@ public void testWrite() {\n         new String[] {\n           \"--bigtableProjectId=\" + projectId,\n           \"--bigtableInstanceId=\" + instanceId,\n-          \"--bigtableTableId=\" + TABLE_ID,\n-          \"--runner=DirectRunner\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODY1NQ=="}, "originalCommit": {"oid": "578922c87135900f5a704bc435bd8272b84ce628"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 880, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}