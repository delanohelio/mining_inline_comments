{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzMzY1OTY3", "number": 1436, "title": "Feature/Adds result types ArrowStream and ArrowFile", "bodyText": "", "createdAt": "2020-11-18T17:49:49Z", "url": "https://github.com/bakdata/conquery/pull/1436", "merged": true, "mergeCommit": {"oid": "b35c9337743d8243904b3a18c293ed369a797982"}, "closed": true, "closedAt": "2020-11-24T15:49:48Z", "author": {"login": "thoniTUB"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdbmswSAH2gAyNTIzMzY1OTY3OmQ3MzNlNDU0MDE2Zjk3YTAxYjc0ZWVlYTEwZDRmOTVjNDcwMTYzODY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdfr2l1AFqTUzNzY0NjMxMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d733e454016f97a01b74eeea10d4f95c47016386", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/d733e454016f97a01b74eeea10d4f95c47016386", "committedDate": "2020-11-11T23:38:28Z", "message": "wip arrow test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08b444adfcddb5ea01d90c23b5b0ea2dc51ab0f3", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/08b444adfcddb5ea01d90c23b5b0ea2dc51ab0f3", "committedDate": "2020-11-13T08:33:31Z", "message": "more work on the test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a52dcf10b18bf33b81caf8c79efec7a417cfc95d", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/a52dcf10b18bf33b81caf8c79efec7a417cfc95d", "committedDate": "2020-11-13T15:23:37Z", "message": "put writers into a pipeline for save fiel access"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e09526c7a6b1e020773dee3ff3b2bcc0b666c8a", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/7e09526c7a6b1e020773dee3ff3b2bcc0b666c8a", "committedDate": "2020-11-17T10:45:16Z", "message": "added support for all ResultTypes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d10ce157e9321c4b26ceea85ab321fcc7ba5a07a", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/d10ce157e9321c4b26ceea85ab321fcc7ba5a07a", "committedDate": "2020-11-17T15:52:16Z", "message": "adds vector generation for idmapping and query renderer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "893c0a6bdb2d6f1834d90bfa88210488d6ced15a", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/893c0a6bdb2d6f1834d90bfa88210488d6ced15a", "committedDate": "2020-11-17T17:42:30Z", "message": "more complete testing and cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a6a9bc00d51b22283020f4d06e1730d163a88eb", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/7a6a9bc00d51b22283020f4d06e1730d163a88eb", "committedDate": "2020-11-17T17:52:18Z", "message": "abstract the arrow writer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed32480b94170beb897d6a397ef7803ed68e4a6e", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/ed32480b94170beb897d6a397ef7803ed68e4a6e", "committedDate": "2020-11-18T17:48:29Z", "message": "add multiline result to test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb7c023789b2b369ebe08677af2b55a55083b5c2", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/eb7c023789b2b369ebe08677af2b55a55083b5c2", "committedDate": "2020-11-18T17:48:43Z", "message": "add endpoints"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8916b9fc06909b7698cbb6edfccb6c7d38240f43", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/8916b9fc06909b7698cbb6edfccb6c7d38240f43", "committedDate": "2020-11-18T18:03:11Z", "message": "make members explicitly static as the compiler maven uses has problems with @UtilityClass"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6058a202853514cc128ba7d9c0418f984aef031c", "author": {"user": {"login": "bakdata-bot", "name": null}}, "url": "https://github.com/bakdata/conquery/commit/6058a202853514cc128ba7d9c0418f984aef031c", "committedDate": "2020-11-18T18:05:36Z", "message": "automatic update to docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "committedDate": "2020-11-19T08:45:36Z", "message": "adds new endpoints to test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1MjA2MTgy", "url": "https://github.com/bakdata/conquery/pull/1436#pullrequestreview-535206182", "createdAt": "2020-11-20T08:22:24Z", "commit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyMjoyNFrOH3FHsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo1NTozNlrOH3GN4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxNzYxOA==", "bodyText": "Die sind nirgends in Arrow codiert?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527517618", "createdAt": "2020-11-20T08:22:24Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/apiv1/AdditionalMediaTypes.java", "diffHunk": "@@ -5,4 +5,8 @@\n \tstatic final String JSON = \"application/json; charset=utf-8\";\n \n \tstatic final String CSV = \"text/csv; charset=utf-8\";\n+\t\n+\t// As proposed in https://issues.apache.org/jira/browse/ARROW-7396", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODQxOQ==", "bodyText": "Das solltest du am besten auch von au\u00dfen reinreichen, und dann via DI an die richtigen Stellen injecten", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518419", "createdAt": "2020-11-20T08:24:01Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/ResultUtil.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package com.bakdata.conquery.io.result;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.dictionary.DirectDictionary;\n+import com.bakdata.conquery.models.identifiable.mapping.CsvEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.ExternalEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingConfig;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingState;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.worker.Namespace;\n+\n+public class ResultUtil {\n+\n+\tpublic static final IdMappingConfig ID_MAPPING = ConqueryConfig.getInstance().getIdMapping();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODgwMg==", "bodyText": "Config?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518802", "createdAt": "2020-11-20T08:24:43Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxOTY2Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tlog.info(\"Writing final batch\");\n          \n          \n            \n            \t\tlog.debug(\"Writing final batch\");", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527519662", "createdAt": "2020-11-20T08:26:13Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng==", "bodyText": "den null check kannst du mit komposition vorziehen, es w\u00fcrde sich denke ich auch anbieten die Methoden in einer Enum zu kapseln?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527521456", "createdAt": "2020-11-20T08:29:23Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDAyNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tif(vector instanceof IntVector) {\n          \n          \n            \n                                    //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n          \n          \n            \n            \t\t\tif(vector instanceof IntVector) {", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524025", "createdAt": "2020-11-20T08:34:23Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDk4OA==", "bodyText": "Ich kann mir vorstellen, dass dieses Method chaining sehr langsam ist, du k\u00f6nntest das invertieren indem du eine Liste an Funktionen pro Spalte hast.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524988", "createdAt": "2020-11-20T08:36:09Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjM1NQ==", "bodyText": "Ich glaube ich w\u00fcrde diesen switch in einer Map<Type,Function..> kapseln, bin mir aber nicht sicher, wie viel besser das ist. Es bietet sich aber auf jeden Fall an, das mapping in eine separate Funktion zu schieben.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526355", "createdAt": "2020-11-20T08:38:48Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjY1Mw==", "bodyText": "was ist denn der letzte Parameter, der immer null ist?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526653", "createdAt": "2020-11-20T08:39:28Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(ArrowType.Bool.INSTANCE), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CATEGORICAL:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase DATE:\n+\t\t\t\t\tfields.add(NAMED_FIELD_DATE_DAY.apply(info.getUniqueName(settings)));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase INTEGER:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase MONEY:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase NUMERIC:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)), null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNDYzOQ==", "bodyText": "yo-dawg", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527534639", "createdAt": "2020-11-20T08:53:58Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA==", "bodyText": "w\u00e4re das nicht die Aufgabe des callers?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535400", "createdAt": "2020-11-20T08:55:15Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTU4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t// TODO Auto-generated method stub", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535587", "createdAt": "2020-11-20T08:55:36Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);\n+\t\tauthorize(user, datasetId, Ability.READ);\n+\t\tauthorize(user, queryId, Ability.READ);\n+\n+\t\tManagedExecution<?> exec = datasetRegistry.getMetaStorage().getExecution(queryId);\n+\t\t\n+\t\t// Check if user is permitted to download on all datasets that were referenced by the query\n+\t\tauthorizeDownloadDatasets(user, exec);\n+\t\t\n+\t\tif(!(exec instanceof ManagedQuery)) {\n+\t\t\treturn Response.notAcceptable(null).build();\n+\t\t}\n+\t\tManagedQuery mquery = (ManagedQuery) exec;\n+\n+\t\t// Get the locale extracted by the LocaleFilter\n+\t\tPrintSettings settings = new PrintSettings(pretty, I18n.LOCALE.get(), datasetRegistry);\n+\t\t\n+\t\tIdMappingConfig idMappingConf = config.getIdMapping();\n+\t\tDirectDictionary primaryDict = datasetRegistry.get(datasetId).getStorage().getPrimaryDictionary();\n+\t\tPersistentIdMap idMapping = datasetRegistry.get(datasetId).getStorage().getIdMapping();\n+\t\t\n+\t\tStreamingOutput out = new StreamingOutput() {\n+\t\t\t\n+\t\t\t@Override\n+\t\t\tpublic void write(OutputStream output) throws IOException, WebApplicationException {\n+\t\t\t\t// TODO Auto-generated method stub", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f70d90eab9d3763fc134f6c248e130f6d05135a0", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/f70d90eab9d3763fc134f6c248e130f6d05135a0", "committedDate": "2020-11-20T11:10:30Z", "message": "inject idmapper from outside"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "committedDate": "2020-11-20T11:15:22Z", "message": "makes batch size configurable"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "56543feb4ac508e113513486359097fc354599ed", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/56543feb4ac508e113513486359097fc354599ed", "committedDate": "2020-11-20T11:02:50Z", "message": "makes batch size configurable"}, "afterCommit": {"oid": "edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "committedDate": "2020-11-20T11:15:22Z", "message": "makes batch size configurable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69d53f27064c077ffaf69f0aa20a11b3666d4407", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/69d53f27064c077ffaf69f0aa20a11b3666d4407", "committedDate": "2020-11-20T11:21:40Z", "message": "clean up logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "033357b80c1b61a5fd5e4adf8652d30c8e1847c8", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/033357b80c1b61a5fd5e4adf8652d30c8e1847c8", "committedDate": "2020-11-20T11:31:57Z", "message": "extract switch statement"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65c8104710f88c2d519fef84e8878f35def16674", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/65c8104710f88c2d519fef84e8878f35def16674", "committedDate": "2020-11-20T11:41:14Z", "message": "fix batchwrite invocation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0317efe3c3478138a763cf2b7a79acbf990f908", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/a0317efe3c3478138a763cf2b7a79acbf990f908", "committedDate": "2020-11-20T12:51:14Z", "message": "Update backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java\n\nCo-authored-by: awildturtok <1553491+awildturtok@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ecff18ec12248938920f13e18574e00f2445265", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/1ecff18ec12248938920f13e18574e00f2445265", "committedDate": "2020-11-20T12:51:35Z", "message": "dont chain cell fillers but have them in a list."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb60a08f1cd224cfb1946520cd23ed46626af72b", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/cb60a08f1cd224cfb1946520cd23ed46626af72b", "committedDate": "2020-11-20T13:07:11Z", "message": "Merge branch 'develop' into feature/arrow-result-output\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java\n#\tbackend/src/main/java/com/bakdata/conquery/models/query/ManagedQuery.java\n#\tbackend/src/main/java/com/bakdata/conquery/resources/api/ResultCSVResource.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "author": {"user": {"login": "bakdata-bot", "name": null}}, "url": "https://github.com/bakdata/conquery/commit/ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "committedDate": "2020-11-20T13:10:05Z", "message": "automatic update to docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1NDQzMDMw", "url": "https://github.com/bakdata/conquery/pull/1436#pullrequestreview-535443030", "createdAt": "2020-11-20T13:50:55Z", "commit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "state": "DISMISSED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMzo1MDo1NVrOH3Qbmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNDowNjowNFrOH3Q_Cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwMjkzOQ==", "bodyText": "Ich denke, dass du das sogar als Stream behalten kannst, dann sparst du dir die potentiell riesige allokation", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527702939", "createdAt": "2020-11-20T13:50:55Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNDMxMw==", "bodyText": "ouh, das ist nat\u00fcrlich nervig :/", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527704313", "createdAt": "2020-11-20T13:53:13Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNTQ0MA==", "bodyText": "Intellij sagt mit, dass du dir hier alle for(int ... zu for ( : ) machen kannst", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527705440", "createdAt": "2020-11-20T13:55:02Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwODM2MQ==", "bodyText": "Die kommen von BaseFixed-/BaseVariableWidthVector, vlt kannst du da in einer parent klasse einen check drauf machen? aber ja sch\u00f6n ists nicht", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527708361", "createdAt": "2020-11-20T13:59:53Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMDQ1MQ==", "bodyText": "public static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n\t\treturn  infos.stream()\n\t\t\t\t\t .map(info -> getFieldForResultInfo(info, settings))\n\t\t\t\t\t .collect(Collectors.toUnmodifiableList());\n\t\t\t\t\n\t}", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527710451", "createdAt": "2020-11-20T14:03:21Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+                        //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tbuilder[resultPos]  = intVectorFiller((IntVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tbuilder[resultPos]  = varCharVectorFiller((VarCharVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tbuilder[resultPos]  = bitVectorFiller((BitVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tbuilder[resultPos]  = float4VectorFiller((Float4Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tbuilder[resultPos]  = float8VectorFiller((Float8Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tbuilder[resultPos]  = dateDayVectorFiller((DateDayVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn builder;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMjAxMA==", "bodyText": "jo hatte gedacht, dass du das dispatch in einer method machst, aber du hast unterschiedliche klassen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527712010", "createdAt": "2020-11-20T14:06:04Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a45364b11191a5ab60b67c171133c52d0ac92650", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/a45364b11191a5ab60b67c171133c52d0ac92650", "committedDate": "2020-11-20T15:28:18Z", "message": "fix wrong parameterization in arrow result url path"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b21684803eab64fdee7f5f8c43e221cd29632fb", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/7b21684803eab64fdee7f5f8c43e221cd29632fb", "committedDate": "2020-11-20T16:08:49Z", "message": "review changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ffbf06f457458e0804da722f149f186071852b0", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/1ffbf06f457458e0804da722f149f186071852b0", "committedDate": "2020-11-23T15:51:53Z", "message": "better ErrorCode, when executtion cannot be rendered"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6423b628d97243b145987cf7f208cf1c14ec237", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/a6423b628d97243b145987cf7f208cf1c14ec237", "committedDate": "2020-11-23T15:53:11Z", "message": "call toString on any object that is flagged as string"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "207be338c3e486fce054446f8559b24101e1d233", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/207be338c3e486fce054446f8559b24101e1d233", "committedDate": "2020-11-23T15:53:35Z", "message": "add render support for Forms"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NjUzMzIy", "url": "https://github.com/bakdata/conquery/pull/1436#pullrequestreview-536653322", "createdAt": "2020-11-23T16:40:16Z", "commit": {"oid": "207be338c3e486fce054446f8559b24101e1d233"}, "state": "DISMISSED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNjo0MDoxNlrOH4WDKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNjo0MDoxNlrOH4WDKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODg0MzU2Mw==", "bodyText": "in Methode kapseln, das collectResultInfos m\u00fcsstest du doch eigentlich nicht so umst\u00e4ndlich benutzen?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r528843563", "createdAt": "2020-11-23T16:40:16Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,285 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results;\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "207be338c3e486fce054446f8559b24101e1d233"}, "originalPosition": 56}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54358b1388d08bd462e18a584c62f2967001137a", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/54358b1388d08bd462e18a584c62f2967001137a", "committedDate": "2020-11-24T13:46:12Z", "message": "review changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "author": {"user": {"login": "thoniTUB", "name": "MT"}}, "url": "https://github.com/bakdata/conquery/commit/3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "committedDate": "2020-11-24T15:10:02Z", "message": "Merge branch 'develop' into feature/arrow-result-output\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3NjQ2MzEz", "url": "https://github.com/bakdata/conquery/pull/1436#pullrequestreview-537646313", "createdAt": "2020-11-24T15:45:11Z", "commit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0NToxMVrOH5H2HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0OTowNlrOH5ILDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTQyMA==", "bodyText": "Kannst du nicht direkt returnen?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659420", "createdAt": "2020-11-24T15:45:11Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTc4Mw==", "bodyText": "return reinziehen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659783", "createdAt": "2020-11-24T15:45:25Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tint vecI = vectorOffset, resultPos = 0; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++, resultPos++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = resultPos;\n          \n          \n            \n            \t\t\tint vecI = vectorOffset; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = vecI - vectorOffset;", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529663685", "createdAt": "2020-11-24T15:48:18Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2NDc4MQ==", "bodyText": "ah, das ist nicht ganz richtig, aber w\u00fcrde nur eine laufvariable verwenden, gerade weil die voneinander abh\u00e4ngen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529664781", "createdAt": "2020-11-24T15:49:06Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4752, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}