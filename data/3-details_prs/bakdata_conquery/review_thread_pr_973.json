{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NzAzMjE2", "number": 973, "reviewThreads": {"totalCount": 53, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODoxNTozMFrODV6Pig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDozMDoyOFrOEH_wRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzAwOTM4OnYy", "diffSide": "RIGHT", "path": "autodoc/src/main/java/com/bakdata/conquery/Constants.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODoxNTozMFrOFaXkyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODoxNTozMFrOFaXkyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5MzU0Ng==", "bodyText": "K\u00f6nntest du java importe nach vor javax machen?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363193546", "createdAt": "2020-01-06T08:15:30Z", "author": {"login": "thoniTUB"}, "path": "autodoc/src/main/java/com/bakdata/conquery/Constants.java", "diffHunk": "@@ -16,6 +8,14 @@\n import javax.ws.rs.core.Context;\n import javax.ws.rs.core.Response;\n \n+import java.net.InetAddress;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzAxODE4OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODoyMDo0OFrOFaXpzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODoyMDo0OFrOFaXpzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5NDgyOQ==", "bodyText": "umbenennen Jobs->Descriptiors", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363194829", "createdAt": "2020-01-06T08:20:48Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,8 +111,8 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<ImportDescriptor> findPreprocessingJobs(Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzA0MTkzOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODozNDowM1rOFaX3zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wN1QxMTowODo0NlrOFa2cZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg==", "bodyText": "Muss die Klasse ein Threadlocal sein?\nKann man\nprivate final Set<DateTimeFormatter> formats = new HashSet<>();\nprivate DateTimeFormatter lastFormat;\t\tprivate DateTimeFormatter lastFormat;\n\nnicht auch static und threadsafe machen und dann ein Singleton machen?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363198412", "createdAt": "2020-01-06T08:34:03Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java", "diffHunk": "@@ -11,27 +11,19 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.bakdata.conquery.models.config.ConqueryConfig;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n \n public class DateFormats {\n \n-\tprivate static ThreadLocal<DateFormats> INSTANCE;\n-\tprivate static String[] ADDITIONAL_FORMATS;\n+\tprivate static ThreadLocal<DateFormats> INSTANCE = ThreadLocal.withInitial(() -> new DateFormats(ConqueryConfig.getInstance().getAdditionalFormats()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzY5OTMwMg==", "bodyText": "Ich habs \u00fcberarbeitet. Das Threadlocal ist schon sinnvoll, dass es keine Contention darauf gibt. Synchronisierung darauf w\u00e4re nicht gut, weil das extra daf\u00fcr da ist schnell zu sein und nicht pr\u00e4zise", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363699302", "createdAt": "2020-01-07T11:08:46Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java", "diffHunk": "@@ -11,27 +11,19 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.bakdata.conquery.models.config.ConqueryConfig;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n \n public class DateFormats {\n \n-\tprivate static ThreadLocal<DateFormats> INSTANCE;\n-\tprivate static String[] ADDITIONAL_FORMATS;\n+\tprivate static ThreadLocal<DateFormats> INSTANCE = ThreadLocal.withInitial(() -> new DateFormats(ConqueryConfig.getInstance().getAdditionalFormats()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg=="}, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzA1MjA3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODozOTowNVrOFaX9mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODozOTowNVrOFaX9mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5OTg5OA==", "bodyText": "Doku und im namen verdeutlichen, dass die Klasse auf eine Column abzielt.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363199898", "createdAt": "2020-01-06T08:39:05Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java", "diffHunk": "@@ -0,0 +1,56 @@\n+package com.bakdata.conquery.models.preproc.outputs;\n+\n+import java.io.Serializable;\n+import java.util.InputMismatchException;\n+import java.util.StringJoiner;\n+\n+import com.bakdata.conquery.io.cps.CPSBase;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.bakdata.conquery.models.preproc.ColumnDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.bakdata.conquery.models.types.parser.Parser;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonTypeInfo;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+@Data", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzA1NTIzOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODo0MDo0MlrOFaX_Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODo0MDo0MlrOFaX_Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDM1OQ==", "bodyText": "Klammern", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200359", "createdAt": "2020-01-06T08:40:42Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -69,39 +68,49 @@ public boolean checkAutoOutput() {\n \t}\n \n \t@JsonIgnore\n-\t@ValidationMethod(message=\"The primary column must be of type STRING\")\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n \tpublic boolean isPrimaryString() {\n-\t\treturn primary.getResultType()==MajorTypeId.STRING;\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n \t}\n-\t\n-\tpublic boolean filter(String[] row) {\n-\t\tif(filter == null) {\n-\t\t\treturn true;\n-\t\t}\n-\t\telse {\n-\t\t\tif(script==null) {\n-\t\t\t\ttry {\n-\t\t\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n-\t\t\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n-\t\t\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n-\t\t\t\t\tGroovyShell groovy = new GroovyShell(config);\n-\t\t\t\t\t\n-\t\t\t\t\tscript = (GroovyPredicate) groovy.parse(filter);\n-\t\t\t\t} catch(Exception|Error e) {\n-\t\t\t\t\tthrow new RuntimeException(\"Failed to compile filter '\" + filter + \"'\", e);\n-\t\t\t\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzA1NzQwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODo0MTo0NlrOFaYAnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwODo0MTo0NlrOFaYAnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDY3MQ==", "bodyText": "InputTableDescriptor?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200671", "createdAt": "2020-01-06T08:41:46Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -1,60 +1,59 @@\n package com.bakdata.conquery.models.preproc;\n \n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n import java.io.File;\n import java.io.Serializable;\n import java.time.LocalDate;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n-import org.codehaus.groovy.control.CompilerConfiguration;\n-import org.codehaus.groovy.control.customizers.ImportCustomizer;\n-\n import com.bakdata.conquery.models.common.Range;\n import com.bakdata.conquery.models.exceptions.validators.ExistingFile;\n import com.bakdata.conquery.models.preproc.outputs.AutoOutput;\n-import com.bakdata.conquery.models.preproc.outputs.Output;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n import com.bakdata.conquery.models.types.MajorTypeId;\n import com.fasterxml.jackson.annotation.JsonIgnore;\n-\n import groovy.lang.GroovyShell;\n import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n \n @Data\n public class Input implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzExODY1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOToxMzoyM1rOFaYlNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOToxMzoyM1rOFaYlNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDAzOQ==", "bodyText": "Schau mal ob die Checks nicht schon von der CDateRange Klasse gemacht werden", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210039", "createdAt": "2020-01-06T09:13:23Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -1,59 +1,58 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"EPOCH_DATE_RANGE\", base=Output.class)\n-public class EpochDateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"EPOCH_DATE_RANGE\", base = OutputDescription.class)\n+public class EpochDateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MzEyMTcyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQwOToxNDo0NVrOFaYm_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wN1QxNDoyNjowM1rOFa612g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw==", "bodyText": "Wie werden offene ranges dargestellt?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210493", "createdAt": "2020-01-06T09:14:45Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java", "diffHunk": "@@ -1,60 +1,60 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.preproc.DateFormats;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"DATE_RANGE\", base=Output.class)\n-public class DateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"DATE_RANGE\", base = OutputDescription.class)\n+public class DateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc3MTM1NA==", "bodyText": "bisher gar nicht", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363771354", "createdAt": "2020-01-07T14:26:03Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java", "diffHunk": "@@ -1,60 +1,60 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.preproc.DateFormats;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"DATE_RANGE\", base=Output.class)\n-public class DateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"DATE_RANGE\", base = OutputDescription.class)\n+public class DateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw=="}, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0Mzk4Njc4OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozMToxMFrOFagtHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozMToxMFrOFagtHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0MzEzMg==", "bodyText": "copyOutput von QueryTest.java wiederverwenden", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363343132", "createdAt": "2020-01-06T15:31:10Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -142,9 +142,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\t}\n \t}\n \n-\tprivate Output copyOutput(int columnPosition, RequiredColumn column) {\n+\tprivate OutputDescription copyOutput(RequiredColumn column) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0Mzk5NTc1OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozNDowNFrOFagykg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozNDowNFrOFagykg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDUzMA==", "bodyText": "Werden die settings gebraucht?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344530", "createdAt": "2020-01-06T15:34:04Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -135,7 +135,7 @@ public void importTableContents(StandaloneSupport support, Collection<RequiredTa\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0Mzk5Njc3OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozNDoyOFrOFagzSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxNTozNDoyOFrOFagzSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDcxMg==", "bodyText": "Again: Werden die settings gebraucht?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344712", "createdAt": "2020-01-06T15:34:28Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -106,7 +104,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NzgxMjY5OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDowMzo0M1rOF4njoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjo1MToxOVrOF4s-nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw==", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen.\nOder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394912673", "createdAt": "2020-03-19T10:03:43Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(new Preprocessor(config, descr));\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDk1MA==", "bodyText": "Oder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.\n\nGef\u00e4llt mir!", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395000950", "createdAt": "2020-03-19T12:50:22Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(new Preprocessor(config, descr));\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTUwMA==", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen\n\nFrage ist wie man das kommuniziert, wir kriegen ja selber auch nicht so die guten Exceptions. Ich k\u00f6nnte eine Map machen, die einfach z\u00e4hlt wie of welche Exception kam und dann das loggen?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395001500", "createdAt": "2020-03-19T12:51:19Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(new Preprocessor(config, descr));\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NzgyMjQ2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMDowNjoyOVrOF4np8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjo1MzowNlrOF4tCRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA==", "bodyText": "Dieses stille \u00dcberspringen kann zu sneaky Fehlern f\u00fchren.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394914288", "createdAt": "2020-03-19T10:06:29Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMjQzOQ==", "bodyText": "Ja, das Problem ist so ein bisschen wie wir weiter gehen wollen. An und f\u00fcr sich ist das okay, weil das einfach ein contract ist. Aber ich w\u00fcnsche mir f\u00fcr diese stelle auch sowas wie globbing (wie oben angemerkt) dann k\u00f6nnen wir n\u00e4mlich in zukunft deutlich selektiver preprocessen ohne immer den ganzen stand ordentlich vorhalten zu m\u00fcssen etc. Bin mir noch nicht sicher ob das ne gute idee ist aber sieht man dann ja.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395002439", "createdAt": "2020-03-19T12:53:06Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODE2NjA4OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMTo1MDo0MFrOF4rDiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjo1NTowOVrOF4tGmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw==", "bodyText": "Das ist etwas gef\u00e4hrlich davon auszugehen, dass nicht schon vorher punkte drinne sind", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394969993", "createdAt": "2020-03-19T11:50:40Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMzU0NA==", "bodyText": "hm, was w\u00e4re dein vorschlag? Eigentlich sollte es das vorletzte sein\nalso table.tag.csv.gz oder table.dead.tag.csv.gz aber nicht so einfach.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395003544", "createdAt": "2020-03-19T12:55:09Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODE4OTk2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMTo1ODo0NVrOF4rS5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMTo1ODo0NVrOF4rS5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3MzkyNg==", "bodyText": "Macht ein Builder hier Sinn? Funktioniert das auch wenn man einen Member nicht setzen w\u00fcrde?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394973926", "createdAt": "2020-03-19T11:58:45Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {\n+\t\t// Write content to file\n+\t\tImport imp = Import.createForPreprocessing(descriptor.getTable(), descriptor.getName(), columns);\n+\n+\t\ttry (Output out = new Output(outFile.writeContent())) {\n+\t\t\tfor(int entityId = 0; entityId < entries.size(); entityId++) {\n+\t\t\t\tList<Object[]> events = (List<Object[]>) entries.get(entityId);\n+\n+\t\t\t\tif(!events.isEmpty()) {\n+\t\t\t\t\twriteRowsToFile(out, imp, entityId, events);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Then write headers.\n+\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\tint hash = descriptor.calculateValidityHash();\n+\n+\t\t\tPreprocessedHeader header = PreprocessedHeader.builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODIxODQ3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjowODoyNlrOF4rlTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjowODoyNlrOF4rlTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3ODYzOQ==", "bodyText": "Wird das noch gebraucht?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394978639", "createdAt": "2020-03-19T12:08:26Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n+\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\tlineId++;\n+\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n+\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n+\n+\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n+\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n+\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n \t\t\t\t\t}\n \n-\t\t\t\t\tif (input.checkAutoOutput()) {\n-\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n-\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n-\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\n+\t\t\t\t\tString[] row;\n+\n+\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n+\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n+\t\t\t\t\t\ttry {\n+\n+\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n+\n+\t\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n+\t\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n+\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n+\n+\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n+\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n+\n+\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\n+\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}. Errors={}\", e.getSource().getDescription(), lineId, Arrays.toString(row), errors, e.getCause());\n+\n+//\n+//\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 261}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODIyNDczOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjoxMDozN1rOF4rpbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjoxMDozN1rOF4rpbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3OTY5NQ==", "bodyText": "Cool!", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394979695", "createdAt": "2020-03-19T12:10:37Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"pid\", MajorTypeId.STRING);\n+\t@Valid\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODI5OTczOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjozNDoxNFrOF4sZzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDoxOToyNVrOGJIVBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw==", "bodyText": "Nach Au\u00dfen sieht es aus als w\u00fcrde die Funktion immer erfolgreich sein. Ist das nicht ein Problem?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394992077", "createdAt": "2020-03-19T12:34:14Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.bakdata.conquery.util;\n+\n+import java.time.LocalDate;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.format.DateTimeParseException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Locale;\n+import java.util.Set;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.google.common.base.Strings;\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import lombok.experimental.UtilityClass;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Utility class for parsing multiple dateformats. Parsing is cached in two ways: First parsed values are cached. Second, the last used parser is cached since it's likely that it will be used again, we therefore try to use it first, then try all others.\n+ */\n+@UtilityClass\n+@Slf4j\n+public class DateFormats {\n+\n+\t/**\n+\t * All available formats for parsing.\n+\t */\n+\tprivate static Set<DateTimeFormatter> formats;\n+\n+\t/**\n+\t * Last successfully parsed dateformat.\n+\t */\n+\tprivate static ThreadLocal<DateTimeFormatter> lastFormat = new ThreadLocal<>();\n+\n+\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n+\n+\t/**\n+\t * Parsed values cache.\n+\t */\n+\tprivate static final LoadingCache<String, LocalDate> DATE_CACHE = CacheBuilder.newBuilder()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .weakKeys().weakValues()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  // TODO: 07.01.2020 fk: Tweak this number?\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .concurrencyLevel(10)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .initialCapacity(64000)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .build(CacheLoader.from(DateFormats::tryParse));\n+\n+\t/**\n+\t * Try parsing the String value to a LocalDate.\n+\t */\n+\tpublic static LocalDate parseToLocalDate(String value) throws ParsingException {\n+\t\tif(Strings.isNullOrEmpty(value)) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\treturn DATE_CACHE.getUnchecked(value);\n+\t}\n+\n+\t/**\n+\t * Try and parse with the last successful parser. If not successful try and parse with other parsers and update the last successful parser.\n+\t *\n+\t * Method is private as it is only directly accessed via the Cache.\n+\t */\n+\tprivate static LocalDate tryParse(String value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNTExNw==", "bodyText": "Kein Kommentar von dir \ud83d\ude22", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412225117", "createdAt": "2020-04-21T14:17:31Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.bakdata.conquery.util;\n+\n+import java.time.LocalDate;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.format.DateTimeParseException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Locale;\n+import java.util.Set;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.google.common.base.Strings;\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import lombok.experimental.UtilityClass;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Utility class for parsing multiple dateformats. Parsing is cached in two ways: First parsed values are cached. Second, the last used parser is cached since it's likely that it will be used again, we therefore try to use it first, then try all others.\n+ */\n+@UtilityClass\n+@Slf4j\n+public class DateFormats {\n+\n+\t/**\n+\t * All available formats for parsing.\n+\t */\n+\tprivate static Set<DateTimeFormatter> formats;\n+\n+\t/**\n+\t * Last successfully parsed dateformat.\n+\t */\n+\tprivate static ThreadLocal<DateTimeFormatter> lastFormat = new ThreadLocal<>();\n+\n+\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n+\n+\t/**\n+\t * Parsed values cache.\n+\t */\n+\tprivate static final LoadingCache<String, LocalDate> DATE_CACHE = CacheBuilder.newBuilder()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .weakKeys().weakValues()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  // TODO: 07.01.2020 fk: Tweak this number?\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .concurrencyLevel(10)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .initialCapacity(64000)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .build(CacheLoader.from(DateFormats::tryParse));\n+\n+\t/**\n+\t * Try parsing the String value to a LocalDate.\n+\t */\n+\tpublic static LocalDate parseToLocalDate(String value) throws ParsingException {\n+\t\tif(Strings.isNullOrEmpty(value)) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\treturn DATE_CACHE.getUnchecked(value);\n+\t}\n+\n+\t/**\n+\t * Try and parse with the last successful parser. If not successful try and parse with other parsers and update the last successful parser.\n+\t *\n+\t * Method is private as it is only directly accessed via the Cache.\n+\t */\n+\tprivate static LocalDate tryParse(String value) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNjgyMA==", "bodyText": "Was macht das ErrorDate in den Daten? Taucht das Event dann nur auf wenn man quasi keine Datumsbeschr\u00e4nkung gesetzt hat?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412226820", "createdAt": "2020-04-21T14:19:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.bakdata.conquery.util;\n+\n+import java.time.LocalDate;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.format.DateTimeParseException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Locale;\n+import java.util.Set;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.google.common.base.Strings;\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import lombok.experimental.UtilityClass;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Utility class for parsing multiple dateformats. Parsing is cached in two ways: First parsed values are cached. Second, the last used parser is cached since it's likely that it will be used again, we therefore try to use it first, then try all others.\n+ */\n+@UtilityClass\n+@Slf4j\n+public class DateFormats {\n+\n+\t/**\n+\t * All available formats for parsing.\n+\t */\n+\tprivate static Set<DateTimeFormatter> formats;\n+\n+\t/**\n+\t * Last successfully parsed dateformat.\n+\t */\n+\tprivate static ThreadLocal<DateTimeFormatter> lastFormat = new ThreadLocal<>();\n+\n+\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n+\n+\t/**\n+\t * Parsed values cache.\n+\t */\n+\tprivate static final LoadingCache<String, LocalDate> DATE_CACHE = CacheBuilder.newBuilder()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .weakKeys().weakValues()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  // TODO: 07.01.2020 fk: Tweak this number?\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .concurrencyLevel(10)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .initialCapacity(64000)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .build(CacheLoader.from(DateFormats::tryParse));\n+\n+\t/**\n+\t * Try parsing the String value to a LocalDate.\n+\t */\n+\tpublic static LocalDate parseToLocalDate(String value) throws ParsingException {\n+\t\tif(Strings.isNullOrEmpty(value)) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\treturn DATE_CACHE.getUnchecked(value);\n+\t}\n+\n+\t/**\n+\t * Try and parse with the last successful parser. If not successful try and parse with other parsers and update the last successful parser.\n+\t *\n+\t * Method is private as it is only directly accessed via the Cache.\n+\t */\n+\tprivate static LocalDate tryParse(String value) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODMwOTM5OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjozNzoxMFrOF4sgCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjozNzoxMFrOF4sgCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MzY3NQ==", "bodyText": "Die Funktionen sehen aus wie Util methoden. K\u00f6nnen die nicht in die LoadingUtil?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394993675", "createdAt": "2020-03-19T12:37:10Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -59,8 +86,97 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \n \t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n+\t\timportIdMapping(support);\n+\t\timportPreviousQueries(support);\n+\t}\n+\n+\tpublic void importIdMapping(StandaloneSupport support) throws JSONException, IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODMxMjQ4OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjozODowNFrOF4siEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxMjozODowNFrOF4siEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDE5NA==", "bodyText": "Warum wird das gemacht?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394994194", "createdAt": "2020-03-19T12:38:04Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -99,12 +96,9 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \t}\n \n \tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n-\t\tDateFormats.initialize(ArrayUtils.EMPTY_STRING_ARRAY);\n+\n+\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NjExMzgzOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDo1NTo1OFrOGIYq6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNDo1NTo1OFrOGIYq6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ0NTk5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n          \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411445993", "createdAt": "2020-04-20T14:55:58Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NjE1NjQ4OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNTowNDowMVrOGIZEUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNTowNjozOVrOGIZMmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw==", "bodyText": "Hier w\u00fcrde ich den Member von description umbenennen.  Sodass man im code ein Hint welcher Dateityp dahinter steht: import/table ...", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411452497", "createdAt": "2020-04-20T15:04:01Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1NDYxOA==", "bodyText": "Oder in dem fall, dass es ein Pfad ist", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411454618", "createdAt": "2020-04-20T15:06:39Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MTI2Njc2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDowMjozMFrOGJHbhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDo1MDo1OVrOGMc7xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA==", "bodyText": "Die Methoden Signatur ist hier unsauber, weil man der Methode auch ein HCFile im READ mode \u00fcbergeben kann.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412212100", "createdAt": "2020-04-21T14:02:30Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcxMDE1MQ==", "bodyText": "Ich hab hier einen Test eingebaut ob es in read ge\u00f6ffnet ist. Die alte variante war mMn noch bl\u00f6der aber ich sehe hier keinen trivialen Weg das zu garantieren.  Ich hab auch ein todo in die HCFile klasse geschrieben aber so wie sie aktuell ist gef\u00e4llt sie mir eigentlich nicht", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415710151", "createdAt": "2020-04-27T10:50:59Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MTMyMTY1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDoxMjowN1rOGJH8HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOToxMDoyNVrOGNKchg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA==", "bodyText": "Ich glaube hier ist Map<Integer, List<Object[]>> besser, da wir Die zweite Interpretation aus der Multimap-Doku benutzt wird und f\u00fcr diese eine Multimap nicht empfohlen wird", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412220444", "createdAt": "2020-04-21T14:12:07Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMTE0Ng==", "bodyText": "A collection that maps keys to values, similar to Map, but in which each key may beassociated with multiple values. You can visualize the contents of a multimap either as amap from keys to nonempty collections of values:\n\u2022a \u00e2\u2020\u2019 1, 2\n\u2022b \u00e2\u2020\u2019 3\n... or as a single \"flattened\" collection of key-value pairs: \u2022a \u00e2\u2020\u2019 1\n\u2022a \u00e2\u2020\u2019 2\n\u2022b \u00e2\u2020\u2019 3\n\n\nImportant: although the first interpretation resembles how most multimaps are implemented, the design of the Multimap API is based on the second form.So, using the multimap shown above as an example, the size is 3, not 2,and the values collection is [1, 2, 3], not [[1, 2], [3]]. For thosetimes when the first style is more useful, use the multimap's asMap view (or create a Map<K, Collection> in the first place).", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412221146", "createdAt": "2020-04-21T14:13:00Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMjQ1NQ==", "bodyText": "Dann sparst du dir auch den cast in Zeile 67", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412222455", "createdAt": "2020-04-21T14:14:28Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1NTgxNA==", "bodyText": "Was sind deine Gedanken hierzu?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416455814", "createdAt": "2020-04-28T09:10:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MTM5MzMzOnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDoyNDo1MlrOGJInVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDoyNDo1MlrOGJInVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzMTUwOQ==", "bodyText": "Mache ambesten aus dem table Array in RequiredData gleich eine Liste, dann musst du hier nichts wrappen", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412231509", "createdAt": "2020-04-21T14:24:52Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java", "diffHunk": "@@ -81,18 +82,18 @@\n \t@Override\n \tpublic void importRequiredData(StandaloneSupport support) throws Exception {\n \n-\t\tLoadingUtil.importTables(support, content);\n+\t\tIntegrationUtils.importTables(support, content);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT TABLES\", getLabel());\n \n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT CONCEPTS\", getLabel());\n \n-\t\tLoadingUtil.importTableContents(support, content);\n+\t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MTQxMDY4OnYy", "diffSide": "RIGHT", "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNDoyODowMlrOGJIx_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMjoxNzo0MlrOGNRG3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA==", "bodyText": "Gibt es auch einen Test in dem tag != null ist?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412234238", "createdAt": "2020-04-21T14:28:02Z", "author": {"login": "thoniTUB"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "diffHunk": "@@ -177,29 +176,13 @@ public void execute(String name, TestConquery testConquery) throws Exception {\n \t\t\tFileUtils.copyInputStreamToFile(In.resource(path.substring(0, path.lastIndexOf(\"/\")) + \"/\" + \"content2.2.csv\")\n \t\t\t\t\t\t\t\t\t\t\t  .asStream(), new File(conquery.getTmpDir(), import2Table.getCsv().getName()));\n \n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(conquery.getConfig().getPreprocessor().getDirectories()[0], importId.getTag());\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(import2Table.getName() + \"_import\");\n-\t\t\tdesc.setTable(import2Table.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(IntegrationUtils.copyOutput(0, import2Table.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), import2Table.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new Output[import2Table.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < import2Table.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = IntegrationUtils.copyOutput(i + 1, import2Table.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n \n \t\t\t//preprocess\n \t\t\tconquery.preprocessTmp();\n \n \t\t\t//import preprocessedFiles\n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), inputFile.getPreprocessedFile());\n+\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgwMzkwMA==", "bodyText": "Daf\u00fcr m\u00fcssten wir so tief in die Tests eingreifen f\u00fcr ein convenience feature", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415803900", "createdAt": "2020-04-27T13:17:26Z", "author": {"login": "awildturtok"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "diffHunk": "@@ -177,29 +176,13 @@ public void execute(String name, TestConquery testConquery) throws Exception {\n \t\t\tFileUtils.copyInputStreamToFile(In.resource(path.substring(0, path.lastIndexOf(\"/\")) + \"/\" + \"content2.2.csv\")\n \t\t\t\t\t\t\t\t\t\t\t  .asStream(), new File(conquery.getTmpDir(), import2Table.getCsv().getName()));\n \n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(conquery.getConfig().getPreprocessor().getDirectories()[0], importId.getTag());\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(import2Table.getName() + \"_import\");\n-\t\t\tdesc.setTable(import2Table.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(IntegrationUtils.copyOutput(0, import2Table.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), import2Table.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new Output[import2Table.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < import2Table.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = IntegrationUtils.copyOutput(i + 1, import2Table.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n \n \t\t\t//preprocess\n \t\t\tconquery.preprocessTmp();\n \n \t\t\t//import preprocessedFiles\n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), inputFile.getPreprocessedFile());\n+\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU2NDk1OA==", "bodyText": "aber ich sollte die methode selber unit testen", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416564958", "createdAt": "2020-04-28T12:17:42Z", "author": {"login": "awildturtok"}, "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "diffHunk": "@@ -177,29 +176,13 @@ public void execute(String name, TestConquery testConquery) throws Exception {\n \t\t\tFileUtils.copyInputStreamToFile(In.resource(path.substring(0, path.lastIndexOf(\"/\")) + \"/\" + \"content2.2.csv\")\n \t\t\t\t\t\t\t\t\t\t\t  .asStream(), new File(conquery.getTmpDir(), import2Table.getCsv().getName()));\n \n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(conquery.getConfig().getPreprocessor().getDirectories()[0], importId.getTag());\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(import2Table.getName() + \"_import\");\n-\t\t\tdesc.setTable(import2Table.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(IntegrationUtils.copyOutput(0, import2Table.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), import2Table.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new Output[import2Table.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < import2Table.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = IntegrationUtils.copyOutput(i + 1, import2Table.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n \n \t\t\t//preprocess\n \t\t\tconquery.preprocessTmp();\n \n \t\t\t//import preprocessedFiles\n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), inputFile.getPreprocessedFile());\n+\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}, "originalCommit": {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODEzMjY3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxODo1NzoyNVrOGKGKXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMTo0MjowOFrOGNP6lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ==", "bodyText": "Der Cast macht nichts", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r413239901", "createdAt": "2020-04-22T18:57:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -72,23 +71,27 @@ public Integer transform(CDateRange value) {\n \t\t\t\ttype\n \t\t\t);\n \t\t}\n-\t\tif(maxValue - minValue <PackedUnsigned1616.MAX_VALUE) {\n+\t\t// min or max can be Integer.MIN/MAX_VALUE when this happens, the left expression overflows causing it to be true when it is not.\n+\t\tif ((long) maxValue - (long) minValue < (long) PackedUnsigned1616.MAX_VALUE) {\n \t\t\tDateRangeTypePacked type = new DateRangeTypePacked();\n \t\t\ttype.setMinValue(minValue);\n \t\t\ttype.setMaxValue(maxValue);\n+\n+\t\t\tlog.debug(\"Decided for Packed: min={}, max={}\", minValue, maxValue);\n+\n \t\t\treturn new Decision<>(\n-\t\t\t\tnew Transformer<CDateRange, Integer>() {\n-\t\t\t\t\t@Override\n-\t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\tCDateRange v = (CDateRange) value;\n-\t\t\t\t\t\tif(v.getMaxValue()>Integer.MAX_VALUE || v.getMinValue()<Integer.MIN_VALUE) {\n-\t\t\t\t\t\t\tthrow new IllegalArgumentException(value+\" is out of range\");\n+\t\t\t\t\tnew Transformer<CDateRange, Integer>() {\n+\t\t\t\t\t\t@Override\n+\t\t\t\t\t\tpublic Integer transform(CDateRange value) {\n+\t\t\t\t\t\t\tCDateRange v = (CDateRange) value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0513d63a2d8f91ae75f3521777b9617da5b18ba3"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NTQzMQ==", "bodyText": "hast recht, das sollte auch weg.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416545431", "createdAt": "2020-04-28T11:42:08Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -72,23 +71,27 @@ public Integer transform(CDateRange value) {\n \t\t\t\ttype\n \t\t\t);\n \t\t}\n-\t\tif(maxValue - minValue <PackedUnsigned1616.MAX_VALUE) {\n+\t\t// min or max can be Integer.MIN/MAX_VALUE when this happens, the left expression overflows causing it to be true when it is not.\n+\t\tif ((long) maxValue - (long) minValue < (long) PackedUnsigned1616.MAX_VALUE) {\n \t\t\tDateRangeTypePacked type = new DateRangeTypePacked();\n \t\t\ttype.setMinValue(minValue);\n \t\t\ttype.setMaxValue(maxValue);\n+\n+\t\t\tlog.debug(\"Decided for Packed: min={}, max={}\", minValue, maxValue);\n+\n \t\t\treturn new Decision<>(\n-\t\t\t\tnew Transformer<CDateRange, Integer>() {\n-\t\t\t\t\t@Override\n-\t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\tCDateRange v = (CDateRange) value;\n-\t\t\t\t\t\tif(v.getMaxValue()>Integer.MAX_VALUE || v.getMinValue()<Integer.MIN_VALUE) {\n-\t\t\t\t\t\t\tthrow new IllegalArgumentException(value+\" is out of range\");\n+\t\t\t\t\tnew Transformer<CDateRange, Integer>() {\n+\t\t\t\t\t\t@Override\n+\t\t\t\t\t\tpublic Integer transform(CDateRange value) {\n+\t\t\t\t\t\t\tCDateRange v = (CDateRange) value;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ=="}, "originalCommit": {"oid": "0513d63a2d8f91ae75f3521777b9617da5b18ba3"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MDk0ODcwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwODo1MTozOVrOGNJsTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwODo1MTozOVrOGNJsTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0MzQ3MA==", "bodyText": "Ist in dieser Datei einfach nur ein Import dazugekommen?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416443470", "createdAt": "2020-04-28T08:51:39Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java", "diffHunk": "@@ -1,5 +1,7 @@\n package com.bakdata.conquery.apiv1;\n \n+import static com.bakdata.conquery.models.auth.AuthorizationHelper.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MDk1NzAyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwODo1Mzo0NVrOGNJxgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwODo1Mzo0NVrOGNJxgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0NDgwMQ==", "bodyText": "Ah nice! Insgeheim ein lang ersehntes Feature f\u00fcr mich ;)", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416444801", "createdAt": "2020-04-28T08:53:45Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java", "diffHunk": "@@ -91,6 +91,10 @@ public void run(ConqueryConfig config, Environment environment) {\n \t\t\n \t\tenvironment.lifecycle().manage(this);\n \n+\t\tif(config.getStorage().getDirectory().mkdirs()){\n+\t\t\tlog.warn(\"Had to create Storage Dir at `{}`\", config.getStorage().getDirectory());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MDk4NjQ0OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTowMDo0OVrOGNKD3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTowMDo0OVrOGNKD3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0OTUwMw==", "bodyText": "Hier w\u00fcrde ich eine Message zusammenbauen und die als einzelnen Error absetzten. Das machst die Logs besser durchsuchbar und lesbarer, da die Zusammenfassung als ein Block erkennbar ist.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416449503", "createdAt": "2020-04-28T09:00:49Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MDk5MDQ5OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTowMTozOFrOGNKGPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzo0ODozM1rOGNzcBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA==", "bodyText": "Hier kollidierst du potentiell mit speziellen StatusCodes: http://tldp.org/LDP/abs/html/exitcodes.html", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416450110", "createdAt": "2020-04-28T09:01:38Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MTQ3OA==", "bodyText": "Zwar gilt das nur f\u00fcr bash, aber wo werden die StatusCodes sonst so eifrig benutzt?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416451478", "createdAt": "2020-04-28T09:03:46Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODc1MQ==", "bodyText": "https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\nHabe mich hier von inspirieren lassen, das ist eher ein easter egg, es geht f\u00fcr mich nur um != 0 aber ichw erde es auf 100 truncaten so wie parallel die wissen was sie machen", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468751", "createdAt": "2020-04-28T09:30:25Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNzQyOQ==", "bodyText": "Okay :D so kann man es nat\u00fcrlich auch machen und einen Gro\u00dfteil der codes erhalten", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417127429", "createdAt": "2020-04-29T07:48:33Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MTAwODE0OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTowNjowMFrOGNKRSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTowNjowMFrOGNKRSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MjkzOA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416452938", "createdAt": "2020-04-28T09:06:00Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java", "diffHunk": "@@ -3,15 +3,13 @@\n import javax.validation.Valid;\n import javax.validation.constraints.Min;\n \n-import org.hibernate.validator.constraints.NotEmpty;\n-\n import lombok.Getter;\n import lombok.Setter;\n import lombok.ToString;\n \n @Getter @Setter @ToString\n public class PreprocessingConfig {\n-\t@NotEmpty @Valid\n+\t@Valid", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MTA0MzUyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOToxNDoyOVrOGNKnOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzo1MzozMFrOGNzmWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg==", "bodyText": "Hast du das mit Lennart abgesprochen?, ich wei\u00df nicht wann die OpenRanges auftreten aber potentiell k\u00f6nnen dadurch dann Daten in den Formularen fehlen.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416458552", "createdAt": "2020-04-28T09:14:29Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -320,6 +320,11 @@ public boolean hasLowerBound() {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n+\t\tif(isOpen()){\n+\t\t\t// TODO: 22.04.2020 throw exceptiopn?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODIzMA==", "bodyText": "wenn wir es nicht testen ist das eine endlosschleife.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468230", "createdAt": "2020-04-28T09:29:35Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -320,6 +320,11 @@ public boolean hasLowerBound() {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n+\t\tif(isOpen()){\n+\t\t\t// TODO: 22.04.2020 throw exceptiopn?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzMDA3NQ==", "bodyText": "Ok, ich habe gerade auch nochmal nachgeschaut in dem Context, sollten eigentlich auch keine offnen Ranges auf treten", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417130075", "createdAt": "2020-04-29T07:53:30Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -320,6 +320,11 @@ public boolean hasLowerBound() {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n+\t\tif(isOpen()){\n+\t\t\t// TODO: 22.04.2020 throw exceptiopn?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MTA2ODE5OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOToyMDo0M1rOGNK2lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMTo0MDo1NlrOGNP3oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ==", "bodyText": "Angenommen es w\u00e4ren nur komplett offene DateRanges vorhanden, dann ist das ganz sch\u00f6n teuer die null abzuspeichern, aber ein Edgecase", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416462485", "createdAt": "2020-04-28T09:20:43Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -57,6 +67,15 @@ public static CDateRange parseISORange(String value) throws ParsingException {\n \t\n \t@Override\n \tprotected Decision<CDateRange, ?, ? extends CType<CDateRange, ?>> decideType() {\n+\t\t// We cannot yet do meaningful compression for open dateranges.\n+\t\t// TODO: 27.04.2020 consider packed compression with extra value as null value.\n+\t\tif(!onlyClosed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NDY3Mg==", "bodyText": "ganz offene meinst du -inf/+inf, das werden wir nicht zulassen, das sind m\u00fcll daten. Bzw die werden wir so hoffentlich nie zulassen, da w\u00fcrden sehr viele komische sachen bei rauskommen.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416544672", "createdAt": "2020-04-28T11:40:56Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -57,6 +67,15 @@ public static CDateRange parseISORange(String value) throws ParsingException {\n \t\n \t@Override\n \tprotected Decision<CDateRange, ?, ? extends CType<CDateRange, ?>> decideType() {\n+\t\t// We cannot yet do meaningful compression for open dateranges.\n+\t\t// TODO: 27.04.2020 consider packed compression with extra value as null value.\n+\t\tif(!onlyClosed) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ=="}, "originalCommit": {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTM2MTk5OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzo0NjowMVrOGNzWdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNzo0NjowMVrOGNzWdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNjAwNQ==", "bodyText": "Hier verstehe ich die Logik gerade nicht.\nM\u00fcsstest du nicht erst pr\u00fcfen ob beides  NullorEmpty ist und offene ranges erlaubt sind und CDateRange.of() zur\u00fcckgeben anstatt null.\nDie nachfolgende If-Bedingung, sollte die nicht als erstes gepr\u00fcft werden?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417126005", "createdAt": "2020-04-29T07:46:01Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -39,14 +40,13 @@ public Output createForHeaders(Object2IntArrayMap<String> headers) {\n \t\treturn new Output() {\n \t\t\t@Override\n \t\t\tprotected Object parseLine(String[] row, Parser<?> type, long sourceLine) throws ParsingException {\n-\t\t\t\tif (!allowOpen && (row[startIndex] == null || row[endIndex] == null)) {\n-\t\t\t\t\tthrow new IllegalArgumentException(\"Open Ranges are not allowed.\");\n-\t\t\t\t}\n-\n-\t\t\t\tif (row[startIndex] == null && row[endIndex] == null) {\n+\t\t\t\tif (Strings.isNullOrEmpty(row[startIndex]) && Strings.isNullOrEmpty(row[endIndex])) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed515d582a2f8a3d6709d52528d8836ea79bdbb4"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzMzOTEwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNDozODo0NlrOGW6LXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNDozODo0NlrOGW6LXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTAzOA==", "bodyText": "Dieses und das n\u00e4chste if k\u00f6nnen weg wenn  ein max-Value == Integer.MAX_VALUE dann kannst einfach Math#min drauf aufrufen. Das ist dann auch egal", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675038", "createdAt": "2020-05-18T14:38:46Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzM0MTg1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNDozOToyNlrOGW6NJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNDozOToyNlrOGW6NJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTQ5NA==", "bodyText": "Auch hier k\u00f6nnen dieses und das n\u00e4chste if  weg", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675494", "createdAt": "2020-05-18T14:39:26Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (min == Integer.MAX_VALUE) {\n+\t\t\t\tmin = Integer.MIN_VALUE;\n+\t\t\t}\n+\t\t}\n+\n+\t\tint max = Integer.MIN_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzQ4Mzg1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToxMToyMFrOGW7nuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToxMToyMFrOGW7nuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODY4Mw==", "bodyText": "Error hat mich etwas verwundert:\n\nAn Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426698683", "createdAt": "2020-05-18T15:11:20Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);\n+\n+\t\t\tfor (int col = 0; col < headers.length; col++) {\n+\t\t\t\tgroovy.setVariable(headers[col], col);\n+\t\t\t}\n+\n+\t\t\treturn  (GroovyPredicate) groovy.parse(filter);\n+\t\t} catch (Exception | Error e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzQ5ODU0OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToxNDozOVrOGW7xIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMDoxNjo1MlrOGZ8W6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ==", "bodyText": "Das sieht so aus, als k\u00f6nnte es auch statisch sein.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426701091", "createdAt": "2020-05-18T15:14:39Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NjQ5MA==", "bodyText": "Wir setzen \u00fcber die Skripte Properties der Shell, dann w\u00fcrden die sich gegenseitig reingr\u00e4tschen.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429856490", "createdAt": "2020-05-25T10:16:52Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ=="}, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzUyMjUyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyMDoyNVrOGW8AhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyMDoyNVrOGW8AhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTAyOA==", "bodyText": "Ich sehe hier das Upgrade zum BigMultiSelect irgendwie nicht", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705028", "createdAt": "2020-05-18T15:20:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java", "diffHunk": "@@ -58,20 +60,22 @@\n \tpublic void configureFrontend(FEFilter f) throws ConceptConfigurationException {\n \t\tf.setTemplate(getTemplate());\n \t\tf.setType(filterType);\n-\t\t// TODO: 20.11.2019 Upgrade to BigMultiSelect if more than maximumSize values are found.\n-\t\tif (values != null) {\n-\t\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n-\t\t\t\tthrow new ConceptConfigurationException(getConnector(),\n-\t\t\t\t\tString.format(\"Too many possible values (%d of %d in filter %s).\", values.size(), maximumSize, this.getId()));\n-\t\t\t}\n-\t\t\tif(this.filterType != FEFilterType.BIG_MULTI_SELECT) {\n-\t\t\t\tf.setOptions(\n-\t\t\t\t\tvalues\n-\t\t\t\t\t\t.stream()\n-\t\t\t\t\t\t.map(v->new FEValue(getLabelFor(v), v))\n-\t\t\t\t\t\t.collect(Collectors.toList())\n-\t\t\t\t);\n-\t\t\t}\n+\n+\t\tif (values == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n+\t\t\tlog.warn(\"Too many possible values ({} of {} in Filter[{}]). Upgrading to BigMultiSelect\", values.size(), maximumSize, getId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzUyODI3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyMTo1MVrOGW8ESg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyMTo1MVrOGW8ESg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTk5NA==", "bodyText": "Das war die Syntax die nicht funktioniert", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705994", "createdAt": "2020-05-18T15:21:51Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java", "diffHunk": "@@ -78,7 +77,7 @@ public void execute() throws Exception {\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (Exception e) {\n-\t\t\t\tlog.error(\"Failed to collect the matching stats for CBlock \" + cBlock.getId(), e);\n+\t\t\t\tlog.error(\"Failed to collect the matching stats for {}\", cBlock, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzUzODc3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyNDowMVrOGW8Kkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyNDowMVrOGW8Kkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzYwMg==", "bodyText": "Nice", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426707602", "createdAt": "2020-05-18T15:24:01Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "diffHunk": "@@ -1,41 +1,50 @@\n package com.bakdata.conquery.models.preproc;\n \n-import com.bakdata.conquery.models.common.daterange.CDateRange;\n+import java.util.StringJoiner;\n+\n import com.bakdata.conquery.models.datasets.Table;\n import lombok.AllArgsConstructor;\n-import lombok.Builder;\n import lombok.Data;\n import lombok.NoArgsConstructor;\n+import lombok.extern.slf4j.Slf4j;\n \n-@Data @Builder @NoArgsConstructor @AllArgsConstructor\n+@Data @NoArgsConstructor @AllArgsConstructor @Slf4j\n public class PreprocessedHeader {\n-\tprivate int validityHash;\n \tprivate String name;\n \tprivate String table;\n+\tprivate String suffix;\n+\n \tprivate long rows;\n \tprivate long groups;\n-\tprivate CDateRange eventRange;\n \tprivate PPColumn primaryColumn;\n \tprivate PPColumn[] columns;\n-\tprivate String suffix;\n+\n+\tprivate int validityHash;\n+\n \n \t/**\n \t * Verify that the supplied table matches the preprocessed' data in shape.\n \t */\n-\tpublic boolean matches(Table table) {\n-\t\tif(!table.getPrimaryColumn().matches(getPrimaryColumn())) {\n-\t\t\treturn false;\n+\tpublic void assertMatch(Table table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzU0NjM2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyNTo0OFrOGW8PdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToyNTo0OFrOGW8PdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwODg1Mw==", "bodyText": "Den Reader k\u00f6nntest du schon mal statisch vorbereiten mit Jackson.BINARY_MAPPER.readFor(PreprocessedHeader.class)", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426708853", "createdAt": "2020-05-18T15:25:48Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzU3MzA1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTozMTo1OVrOGW8gOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTozMTo1OVrOGW8gOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE0Ng==", "bodyText": "Hier w\u00fcrde doch auch ein int reichen", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426713146", "createdAt": "2020-05-18T15:31:59Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzU4Mjk0OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTozNDoyNVrOGW8mXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMDoxOTozM1rOGZ8bVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw==", "bodyText": "Du z\u00e4hlst hier keine Errors vom GroovyScript mit, nur die von den Outputs", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426714717", "createdAt": "2020-05-18T15:34:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n+\n+\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n+\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n+\t\texceptions.defaultReturnValue(0);\n+\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NzYyMQ==", "bodyText": "Ja, weil die skripte nicht abst\u00fcrzen d\u00fcrfen, und wenn sie es doch tun den ganzen preprocess der datei killen sollen", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429857621", "createdAt": "2020-05-25T10:19:33Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n+\n+\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n+\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n+\t\texceptions.defaultReturnValue(0);\n+\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw=="}, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzY0MzU5OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo0ODoxNlrOGW9L3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo0ODoxNlrOGW9L3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDMxOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMaxValue() > maxValue) {\n          \n          \n            \n            \t\t\tmaxValue = v.getMaxValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t  maxValue = Math.max(maxValue,v.getMaxValue());", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724319", "createdAt": "2020-05-18T15:48:16Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzY0NjczOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo0ODo1OFrOGW9Nyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo0ODo1OFrOGW9Nyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDgxMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMinValue() < minValue) {\n          \n          \n            \n            \t\t\tminValue = v.getMinValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n                minValue = Math.min(minValue,v.getMinValue());", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724811", "createdAt": "2020-05-18T15:48:58Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}\n+\n \t\tif(v.getMinValue() < minValue) {\n \t\t\tminValue = v.getMinValue();\n \t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzY5NTMyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo1OToxMlrOGW9roA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo1OToxMlrOGW9roA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczMjQ0OA==", "bodyText": "Ich bin mir nicht ganz sicher warum die Map jetzt synchronisiert ist, aber m\u00fcsstest du  dann hier nicht explizit synchronisieren? Du \u00e4nderst du die Referenz von string.", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426732448", "createdAt": "2020-05-18T15:59:12Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java", "diffHunk": "@@ -94,8 +94,8 @@ public Boolean transform(@NonNull Integer value) {\n \t\t//remove prefix and suffix\n \t\tif(!StringUtils.isEmpty(prefix) || !StringUtils.isEmpty(suffix)) {\n \t\t\tlog.debug(\"Reduced strings by the '{}' prefix and '{}' suffix\", prefix, suffix);\n-\t\t\tLinkedHashMap<String, Integer> oldStrings = strings;\n-\t\t\tstrings = new LinkedHashMap<>(oldStrings.size());\n+\t\t\tMap<String, Integer> oldStrings = strings;\n+\t\t\tstrings = Collections.synchronizedMap(new LinkedHashMap<>(oldStrings.size()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNjg1MjAwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzowODo1NlrOGebAqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzoyNToxMlrOGebqDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA==", "bodyText": "onlyQuarters = onlyQuarters && v.isSingleQuarter()", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553000", "createdAt": "2020-06-03T13:08:56Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +34,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46ec8ec578fd70849a0725d832e9aecde16cbd36"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1Mzk5NA==", "bodyText": "Ich wei\u00df nicht ob das assignment hier overhead machen w\u00fcrde", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553994", "createdAt": "2020-06-03T13:10:33Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +34,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}, "originalCommit": {"oid": "46ec8ec578fd70849a0725d832e9aecde16cbd36"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU2MzU5Nw==", "bodyText": "branching ist teurer als assignment vermutlich", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434563597", "createdAt": "2020-06-03T13:25:12Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +34,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}, "originalCommit": {"oid": "46ec8ec578fd70849a0725d832e9aecde16cbd36"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTQ1NDQ1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNToyMTowNFrOGlK8ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNToyMTowNFrOGlK8ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYyOTg1NA==", "bodyText": "Autoformat?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441629854", "createdAt": "2020-06-17T15:21:04Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -56,7 +57,10 @@ public PreprocessorCommand(ExecutorService pool) {\n \tpublic void configure(Subparser subparser) {\n \t\tsuper.configure(subparser);\n \n-\t\tfinal ArgumentGroup group = subparser.addArgumentGroup(\"Preprocessing CLI Config\").description(\"Optional arguments to do a single import step by hand. Overrides json configuration.\");\n+\t\tfinal ArgumentGroup\n+\t\t\t\tgroup =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTQ4NTkzOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNToyODoxM1rOGlLQ0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNToyODoxM1rOGlLQ0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNTAyNw==", "bodyText": "Ist das ! richtig? mein Brain macht gerade nicht mehr so richtig mit", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441635027", "createdAt": "2020-06-17T15:28:13Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MTQ5MzA3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNToyOTo1OVrOGlLViA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwODoxMjo1MlrOGneNsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg==", "bodyText": "Und hier h\u00e4tte ich ein !v.isOpen() erwartet", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636232", "createdAt": "2020-06-17T15:29:59Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyClosed = onlyClosed && v.isOpen();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjcwNw==", "bodyText": "Hast du einen Test hier f\u00fcr?", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636707", "createdAt": "2020-06-17T15:30:42Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyClosed = onlyClosed && v.isOpen();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}, "originalCommit": {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA0MjY3Mg==", "bodyText": "Gute Idee!", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444042672", "createdAt": "2020-06-23T08:12:52Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyClosed = onlyClosed && v.isOpen();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}, "originalCommit": {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODIwMDM2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDozMDoyOFrOGnsEMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDozMDoyOFrOGnsEMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2OTYxNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(!v.isAtLeast()) {\n          \n          \n            \n            \t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \n          \n          \n            \n            \t\tif(!v.isAtMost()) {\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t\tif(!anyOpen) {\n          \n          \n            \n            \t\t        maxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444269616", "createdAt": "2020-06-23T14:30:28Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -36,11 +36,16 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n \n-\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n-\t\tonlyClosed = onlyClosed && v.isOpen();\n+\t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n+\t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\tif(!v.isAtLeast()) {\n+\t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\t}\n+\n+\t\tif(!v.isAtMost()) {\n+\t\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\t}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dffa8a30d2be61dfbc16002904a701b3dc33b8ef"}, "originalPosition": 26}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1467, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}