{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzMzY1OTY3", "number": 1436, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyMjoyNFrOE7bxMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0ODoxOFrOE8yuHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzU2NDAxOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/apiv1/AdditionalMediaTypes.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyMjoyNFrOH3FHsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMDoxMTowMlrOH3JWWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxNzYxOA==", "bodyText": "Die sind nirgends in Arrow codiert?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527517618", "createdAt": "2020-11-20T08:22:24Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/apiv1/AdditionalMediaTypes.java", "diffHunk": "@@ -5,4 +5,8 @@\n \tstatic final String JSON = \"application/json; charset=utf-8\";\n \n \tstatic final String CSV = \"text/csv; charset=utf-8\";\n+\t\n+\t// As proposed in https://issues.apache.org/jira/browse/ARROW-7396", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4NjkwNg==", "bodyText": "Ist ja erstmal nur ein vorschlag. Das ist eine politische Entscheidung den Typ zu standardisieren", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527586906", "createdAt": "2020-11-20T10:11:02Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/apiv1/AdditionalMediaTypes.java", "diffHunk": "@@ -5,4 +5,8 @@\n \tstatic final String JSON = \"application/json; charset=utf-8\";\n \n \tstatic final String CSV = \"text/csv; charset=utf-8\";\n+\t\n+\t// As proposed in https://issues.apache.org/jira/browse/ARROW-7396", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxNzYxOA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzU2OTMyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/ResultUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyNDowMVrOH3FK0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyNDowMVrOH3FK0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODQxOQ==", "bodyText": "Das solltest du am besten auch von au\u00dfen reinreichen, und dann via DI an die richtigen Stellen injecten", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518419", "createdAt": "2020-11-20T08:24:01Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/ResultUtil.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package com.bakdata.conquery.io.result;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.dictionary.DirectDictionary;\n+import com.bakdata.conquery.models.identifiable.mapping.CsvEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.ExternalEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingConfig;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingState;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.worker.Namespace;\n+\n+public class ResultUtil {\n+\n+\tpublic static final IdMappingConfig ID_MAPPING = ConqueryConfig.getInstance().getIdMapping();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzU3MTg1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyNDo0M1rOH3FMUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMToxMzozNVrOH3LffA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODgwMg==", "bodyText": "Config?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518802", "createdAt": "2020-11-20T08:24:43Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyMjAxMg==", "bodyText": "kann jetzt in der config eingestellt werden", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527622012", "createdAt": "2020-11-20T11:13:35Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODgwMg=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzU3NzU3OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyNjoxM1rOH3FPrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyNjoxM1rOH3FPrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxOTY2Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tlog.info(\"Writing final batch\");\n          \n          \n            \n            \t\tlog.debug(\"Writing final batch\");", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527519662", "createdAt": "2020-11-20T08:26:13Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzU4OTU1OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODoyOToyM1rOH3FWsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNTo1NjoyN1rOH3Vj2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng==", "bodyText": "den null check kannst du mit komposition vorziehen, es w\u00fcrde sich denke ich auch anbieten die Methoden in einer Enum zu kapseln?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527521456", "createdAt": "2020-11-20T08:29:23Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzY3MDU3MA==", "bodyText": "Da die methoden vector.setNull(..) vector.setSafe(..) immer nur auf dem konkreten FieldVector definiert sind und nicht vererbt werden f\u00e4llt mir das schwer. Hast du eine Idee?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527670570", "createdAt": "2020-11-20T12:51:04Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNDMxMw==", "bodyText": "ouh, das ist nat\u00fcrlich nervig :/", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527704313", "createdAt": "2020-11-20T13:53:13Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwODM2MQ==", "bodyText": "Die kommen von BaseFixed-/BaseVariableWidthVector, vlt kannst du da in einer parent klasse einen check drauf machen? aber ja sch\u00f6n ists nicht", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527708361", "createdAt": "2020-11-20T13:59:53Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc4Njk3MQ==", "bodyText": "Bei vector.setNull(..) stimmt das bei setSafe(..) leider nicht. Ich w\u00fcrde es erstmal so lassen.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527786971", "createdAt": "2020-11-20T15:56:27Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzYwNjc2OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODozNDoyM1rOH3FguQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODozNDoyM1rOH3FguQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDAyNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tif(vector instanceof IntVector) {\n          \n          \n            \n                                    //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n          \n          \n            \n            \t\t\tif(vector instanceof IntVector) {", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524025", "createdAt": "2020-11-20T08:34:23Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzYxMjk4OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODozNjowOVrOH3FkfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMjoyNToyM1rOH3NqIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDk4OA==", "bodyText": "Ich kann mir vorstellen, dass dieses Method chaining sehr langsam ist, du k\u00f6nntest das invertieren indem du eine Liste an Funktionen pro Spalte hast.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524988", "createdAt": "2020-11-20T08:36:09Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzY1NzUwNQ==", "bodyText": "Ich benutze jetzt ein Array anstatt der Verkettung", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527657505", "createdAt": "2020-11-20T12:25:23Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDk4OA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzYyMTkwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODozODo0OFrOH3Fp0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMToyOTo0N1rOH3L_gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjM1NQ==", "bodyText": "Ich glaube ich w\u00fcrde diesen switch in einer Map<Type,Function..> kapseln, bin mir aber nicht sicher, wie viel besser das ist. Es bietet sich aber auf jeden Fall an, das mapping in eine separate Funktion zu schieben.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526355", "createdAt": "2020-11-20T08:38:48Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzMDIxMQ==", "bodyText": "Ich extrahiere es aber lasse es erstmal beim switch. Kann dann auch durch Switch Expressions in java 12 ersetzt werden.", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527630211", "createdAt": "2020-11-20T11:29:47Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjM1NQ=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzYyMzgwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODozOToyOFrOH3Fq_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMDoxNDoxNFrOH3Jdyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjY1Mw==", "bodyText": "was ist denn der letzte Parameter, der immer null ist?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526653", "createdAt": "2020-11-20T08:39:28Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(ArrowType.Bool.INSTANCE), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CATEGORICAL:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase DATE:\n+\t\t\t\t\tfields.add(NAMED_FIELD_DATE_DAY.apply(info.getUniqueName(settings)));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase INTEGER:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase MONEY:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase NUMERIC:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)), null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4ODgxMQ==", "bodyText": "Das sind die Felder von nested/complex Feldern. Wir haben aber erstmal noch keine Listen Typen oder Objekte", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527588811", "createdAt": "2020-11-20T10:14:14Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(ArrowType.Bool.INSTANCE), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CATEGORICAL:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase DATE:\n+\t\t\t\t\tfields.add(NAMED_FIELD_DATE_DAY.apply(info.getUniqueName(settings)));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase INTEGER:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase MONEY:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase NUMERIC:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)), null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjY1Mw=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzY3Mzc4OnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo1Mzo1OFrOH3GKLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMTo0MTo1MVrOH3MWbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNDYzOQ==", "bodyText": "yo-dawg", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527534639", "createdAt": "2020-11-20T08:53:58Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzNjA3OQ==", "bodyText": "mini Abstraktion ;)", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527636079", "createdAt": "2020-11-20T11:41:51Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNDYzOQ=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzY3ODcwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo1NToxNVrOH3GNKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNDowNjowNFrOH3Q_Cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA==", "bodyText": "w\u00e4re das nicht die Aufgabe des callers?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535400", "createdAt": "2020-11-20T08:55:15Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4OTE4MA==", "bodyText": "Jap kann es hochziehen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527589180", "createdAt": "2020-11-20T10:14:51Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzNjk3Ng==", "bodyText": "Ist doch besser wenn es hier ist, da die Methoden potenziell aus verschiedenen Dateien aufgerufen werden", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527636976", "createdAt": "2020-11-20T11:43:43Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMjAxMA==", "bodyText": "jo hatte gedacht, dass du das dispatch in einer method machst, aber du hast unterschiedliche klassen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527712010", "createdAt": "2020-11-20T14:06:04Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNzY4MDAwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo1NTozNlrOH3GN4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo1NTozNlrOH3GN4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTU4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t// TODO Auto-generated method stub", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535587", "createdAt": "2020-11-20T08:55:36Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);\n+\t\tauthorize(user, datasetId, Ability.READ);\n+\t\tauthorize(user, queryId, Ability.READ);\n+\n+\t\tManagedExecution<?> exec = datasetRegistry.getMetaStorage().getExecution(queryId);\n+\t\t\n+\t\t// Check if user is permitted to download on all datasets that were referenced by the query\n+\t\tauthorizeDownloadDatasets(user, exec);\n+\t\t\n+\t\tif(!(exec instanceof ManagedQuery)) {\n+\t\t\treturn Response.notAcceptable(null).build();\n+\t\t}\n+\t\tManagedQuery mquery = (ManagedQuery) exec;\n+\n+\t\t// Get the locale extracted by the LocaleFilter\n+\t\tPrintSettings settings = new PrintSettings(pretty, I18n.LOCALE.get(), datasetRegistry);\n+\t\t\n+\t\tIdMappingConfig idMappingConf = config.getIdMapping();\n+\t\tDirectDictionary primaryDict = datasetRegistry.get(datasetId).getStorage().getPrimaryDictionary();\n+\t\tPersistentIdMap idMapping = datasetRegistry.get(datasetId).getStorage().getIdMapping();\n+\t\t\n+\t\tStreamingOutput out = new StreamingOutput() {\n+\t\t\t\n+\t\t\t@Override\n+\t\t\tpublic void write(OutputStream output) throws IOException, WebApplicationException {\n+\t\t\t\t// TODO Auto-generated method stub", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwODc2MjgwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMzo1MDo1NVrOH3Qbmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNjowNjoxMlrOH3V83w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwMjkzOQ==", "bodyText": "Ich denke, dass du das sogar als Stream behalten kannst, dann sparst du dir die potentiell riesige allokation", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527702939", "createdAt": "2020-11-20T13:50:55Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc5MzM3NQ==", "bodyText": "Ich spare mir das filtern hier und \u00fcberspringe beim schreiben einfach die die notContained sind.\nIch sortiere das ergebnis jetzt noch nicht, meinst du das ist wichtig?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527793375", "createdAt": "2020-11-20T16:06:12Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwMjkzOQ=="}, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwODc3OTQyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxMzo1NTowMlrOH3QlYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNjowNzoyMVrOH3V_uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNTQ0MA==", "bodyText": "Intellij sagt mit, dass du dir hier alle for(int ... zu for ( : ) machen kannst", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527705440", "createdAt": "2020-11-20T13:55:02Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc5NDEwNw==", "bodyText": "Da hats recht", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527794107", "createdAt": "2020-11-20T16:07:21Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNTQ0MA=="}, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwODgxMTgwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNDowMzoyMVrOH3Q48w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNDowMzoyMVrOH3Q48w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMDQ1MQ==", "bodyText": "public static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n\t\treturn  infos.stream()\n\t\t\t\t\t .map(info -> getFieldForResultInfo(info, settings))\n\t\t\t\t\t .collect(Collectors.toUnmodifiableList());\n\t\t\t\t\n\t}", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527710451", "createdAt": "2020-11-20T14:03:21Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+                        //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tbuilder[resultPos]  = intVectorFiller((IntVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tbuilder[resultPos]  = varCharVectorFiller((VarCharVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tbuilder[resultPos]  = bitVectorFiller((BitVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tbuilder[resultPos]  = float4VectorFiller((Float4Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tbuilder[resultPos]  = float8VectorFiller((Float8Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tbuilder[resultPos]  = dateDayVectorFiller((DateDayVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn builder;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxNjY2NjYyOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNjo0MDoxNlrOH4WDKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwODozMDoxOFrOH4xNFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODg0MzU2Mw==", "bodyText": "in Methode kapseln, das collectResultInfos m\u00fcsstest du doch eigentlich nicht so umst\u00e4ndlich benutzen?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r528843563", "createdAt": "2020-11-23T16:40:16Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,285 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results;\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "207be338c3e486fce054446f8559b24101e1d233"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI4ODQ2OA==", "bodyText": "okay ich mache split phase. Aber ja die Result infos sind nur auf der ManagedQuery definiert", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529288468", "createdAt": "2020-11-24T08:30:18Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,285 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results;\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODg0MzU2Mw=="}, "originalCommit": {"oid": "207be338c3e486fce054446f8559b24101e1d233"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMTc4NTgwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0NToxMVrOH5H2HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0NToxMVrOH5H2HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTQyMA==", "bodyText": "Kannst du nicht direkt returnen?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659420", "createdAt": "2020-11-24T15:45:11Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMTc4NzkzOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0NToyNVrOH5H3hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0NToyNVrOH5H3hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTc4Mw==", "bodyText": "return reinziehen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659783", "createdAt": "2020-11-24T15:45:25Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMTgxMDIwOnYy", "diffSide": "RIGHT", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNTo0ODoxOFrOH5IGxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNjowNzo1MlrOH5JzLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tint vecI = vectorOffset, resultPos = 0; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++, resultPos++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = resultPos;\n          \n          \n            \n            \t\t\tint vecI = vectorOffset; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = vecI - vectorOffset;", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529663685", "createdAt": "2020-11-24T15:48:18Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2NDc4MQ==", "bodyText": "ah, das ist nicht ganz richtig, aber w\u00fcrde nur eine laufvariable verwenden, gerade weil die voneinander abh\u00e4ngen", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529664781", "createdAt": "2020-11-24T15:49:06Z", "author": {"login": "awildturtok"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY4NTkyMw==", "bodyText": "Was meinst du mit, dass es nicht ganz richtig ist?", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529685923", "createdAt": "2020-11-24T16:03:25Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY5MTQzOQ==", "bodyText": "neuer PR #1436", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529691439", "createdAt": "2020-11-24T16:07:52Z", "author": {"login": "thoniTUB"}, "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, "originalCommit": {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0"}, "originalPosition": 224}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1543, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}