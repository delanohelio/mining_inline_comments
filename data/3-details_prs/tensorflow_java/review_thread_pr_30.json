{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NjI3MTUx", "number": 30, "reviewThreads": {"totalCount": 70, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTowODo1MVrODm6GMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMTozMTo0OFrOD3cX8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI0MzM2OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Output.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTowODo1MVrOF0mJ4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTowODo1MVrOF0mJ4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTM5NQ==", "bodyText": "Maybe \"... cast from output of .... to output of.... \"?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390695395", "createdAt": "2020-03-11T01:08:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Output.java", "diffHunk": "@@ -52,6 +52,24 @@ public Shape shape() {\n     return (DataType<T>)operation.dtype(index);\n   }\n \n+    /**\n+   * Returns this Output object with the type {@code Output<U>}. This method is useful when given a\n+   * value of type {@code Output<?>}.\n+   *\n+   * @param dt any supported tensor data type\n+   * @throws IllegalArgumentException if the actual data type of this object does not match the type\n+   *     {@code U}.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public <U extends TType> Output<U> expect(DataType<U> dt) {\n+    if (!dt.equals(this.dataType())) {\n+      throw new IllegalArgumentException(\n+          \"Cannot cast from tensor of \" + this.dataType() + \" to tensor of \" + dt);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI0Njk0OnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": true, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToxMToyMlrOF0mMFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQyMTo0ODowNlrOGH9YMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng==", "bodyText": "I think datasets are mostly used for training, aren't they? If so, then they might fit well as part of the tensorflow-training artifact, aside with the new optimizers?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390695956", "createdAt": "2020-03-11T01:11:22Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NjY0OA==", "bodyText": "What do you think guys, should we move the dataset API to tensorflow-training? For me it make sense but I might not have all use cases in mind", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407066648", "createdAt": "2020-04-11T13:53:22Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMTY5Mw==", "bodyText": "We could do that! I was thinking that it would be good to have the distinct frameworks exposed as separate artifacts, so that the artifact names mirror known pieces of the python framework... But that's not a strong preference, I could move it to tensorflow-training", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407111693", "createdAt": "2020-04-11T21:14:04Z", "author": {"login": "dhruvrajan"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI1OTk5MA==", "bodyText": "The users might not understand why they have to include so many different artifacts in their application, unless one is transitive from the other (e.g. tensorflow-training might depend on tensorflow-data).\nSo I'm tempted to think to move everything in tensorflow-training right away, @Craigacp what do you think?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407259990", "createdAt": "2020-04-12T22:25:26Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzQ3NTI3Mw==", "bodyText": "@karllessard, I think that's reasonable. We can always split it out again if necessary.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407475273", "createdAt": "2020-04-13T13:22:42Z", "author": {"login": "Craigacp"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzQ5MDc5OA==", "bodyText": "Sounds good, I'll move this tensorflow-training.\nA question on the naming though\u2014is it named tensorflow-training to make a hard distinction between training and inference? If so, datasets (and other features we add in the future, like Keras layers, etc.) will be used for both training and inference, so further down the road maybe it makes sense to consider the naming of this package...\nAdding the ... .training. ... in the package name might also be a little cumbersome, what do you think about keeping the package name org.tensorflow.data?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407490798", "createdAt": "2020-04-13T13:56:52Z", "author": {"login": "dhruvrajan"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2MDM5OQ==", "bodyText": "is it named tensorflow-training to make a hard distinction between training and inference?\n\nThat was the original idea, yes. We want tensorflow-core-api to be as closest to the TF runtime as we can, and move higher-level APIs in other artifacts.\nI was foreseeing having Keras in its own artifact (tensorflow-keras) but having other API that fits more \u00ab\u00a0in the middle\u00a0\u00bb either in tensorflow-training or tensorflow-inference, so it is easy for a user to pick the right artifact based on the purpose of his application (having too many artifacts might confuse them more than helping them).\nNow if you say that this Data API will be used as much for inference (outside Keras) than for training, the name might be wrong...  maybe then we should have all middle-level APIs in the same artifact (tensorflow-framework)? So packages would be org.tensorflow.framework.training and org.tensorflow.framework.data?\nIn the new proto packages though, there is already org.tensorflow.proto.framework, so it might be confusing if they do not relate on the same features", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407760399", "createdAt": "2020-04-13T22:38:33Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ5MjMyNg==", "bodyText": "This topic is is still left opened, what do you think @Craigacp ?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408492326", "createdAt": "2020-04-14T23:23:39Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ5NDI1Ng==", "bodyText": "I think I'm ok with training moving into framework along with this data api. The proto package is a mess already so I'm happy documenting it as its own namespace and saying people shouldn't draw inferences from it's layout to the rest of the library.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408494256", "createdAt": "2020-04-14T23:29:39Z", "author": {"login": "Craigacp"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg2MTk2Nw==", "bodyText": "So @dhruvrajan , I think you are good to rename tensorflow-training by tensorflow-framework and change the package names so that they all begin with org.tensorflow.framework in that artifact.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408861967", "createdAt": "2020-04-15T13:56:41Z", "author": {"login": "karllessard"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk5ODgzNA==", "bodyText": "This is done!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410998834", "createdAt": "2020-04-19T21:48:06Z", "author": {"login": "dhruvrajan"}, "path": "pom.xml", "diffHunk": "@@ -32,6 +32,7 @@\n     <module>tensorflow-tools</module>\n     <module>tensorflow-core</module>\n     <module>tensorflow-training</module>\n+    <module>tensorflow-frameworks</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTk1Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI1NDQ2OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToxNjozMFrOF0mQyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjowMjowM1rOGC2HSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NzE2Mw==", "bodyText": "Recommended usage would be to use NdArrays here, as using standard arrays requires an extra copy. It's ok for small constants like this but since tensors used in training in a real scenario can be significantly larger, maybe the example here should show how to handle these correctly.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390697163", "createdAt": "2020-03-11T01:16:30Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzNjkzNg==", "bodyText": "I updated all the tests to use NdArray; it worked quite well!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405636936", "createdAt": "2020-04-08T16:02:03Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NzE2Mw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI2MTc1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToyMToxOVrOF0mVIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwNjo1NjoxNFrOGChZ_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5ODI3NQ==", "bodyText": "I guess this makeIterator op can be added to the list of graph initializers?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390698275", "createdAt": "2020-03-11T01:21:19Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI5NzY2MQ==", "bodyText": "Yup, it now uses the tf.init() mechanism.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405297661", "createdAt": "2020-04-08T06:56:14Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5ODI3NQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI2NDg5OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToyMzoyMFrOF0mXCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjowMjoxOFrOGC2H4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5ODc2MA==", "bodyText": "These tensors need to be closed as well", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390698760", "createdAt": "2020-03-11T01:23:20Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzNzA4OA==", "bodyText": "Thanks, I updated this!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405637088", "createdAt": "2020-04-08T16:02:18Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5ODc2MA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI2NzMzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToyNDo1OVrOF0mYew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjowMzowM1rOGC2KDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTEzMQ==", "bodyText": "It is preferable to close tensors returned by Output.tensor() to avoid potential memory leaks.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390699131", "createdAt": "2020-03-11T01:24:59Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzNzY0NQ==", "bodyText": "I updated the readme, the examples aren't fetching tensors anymore.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405637645", "createdAt": "2020-04-08T16:03:03Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTEzMQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI2ODQyOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMToyNTo0OFrOF0mZIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMDoyNDoyOFrOGC_lvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng==", "bodyText": "So the exception is part of the normal logic flow? Any way to avoid this?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390699296", "createdAt": "2020-03-11T01:25:48Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzOTE1Mw==", "bodyText": "I agree with Karl, but moreso, I don't think that we should expose any API where catching a RuntimeException is the expected way to use it.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393039153", "createdAt": "2020-03-16T13:55:35Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzOTkzNg==", "bodyText": "See the example here: https://www.tensorflow.org/api_docs/python/tf/compat/v1/data/Iterator#from_structure\nI don't know of any way to avoid this nicely in graph mode. It doesn't really make sense to make it an Iterable since we don't actually want to fetch dataset batches at the beginning of the loop; we want to run the graph each iteration of the loop and get things like losses (all of which are constructed before opening a loop).\nI think this API is ok for now; since it matches Python, it will be similar for users. Do you see any way of improving it from here?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405639936", "createdAt": "2020-04-08T16:06:24Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY2Mzg3Nw==", "bodyText": "I agree that's how Python does it, but that's because Python's iteration idiom involves catching StopIteration when it's done, so the TF Python way mirrors that. Java's iteration idiom is not like that so we should try to make it idiomatically Java.\nWhere does the IndexOutOfBoundsException come from? Wherever it's generated we could modify the code to set a flag on the session that could be used as a condition in the while loop. Or we could convert the output of session.runner().run() to be a results object that was closeable, and monitor a boolean set in that (that's got ugly scoping issues though). Or it could add a node which produces a scalar Tensor containing true or false that the user could monitor.\nThose all assume there's only a single iterator on any given session (or a single root iterator), but I'm not sure from the code if multiple iterators are supported anyway. I'm not sure that any of these are particularly good solutions, but I really don't like catching runtime exceptions as the way users have to use the library.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405663877", "createdAt": "2020-04-08T16:42:43Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2MjU4Nw==", "bodyText": "The IndexOutOfBoundsException is thrown when session.run() is called on a graph with dataset \"getNext\" components which are exhausted; so that'd not be easy to change, I think.\nI think propagating the exception to the user is reasonable; hiding the exception can cause the user to misuse the abstraction (i.e. improperly tracking a boolean flag), and deal with undefined behavior...\nAlso, if the user already knows the size of the dataset (# batches), they can use a for-loop.\nMultiple and re-initializeable iterators are supported; yup it does make things a little more complicated.\n@karllessard what are your thoughts?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405762587", "createdAt": "2020-04-08T19:29:48Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5MjE4OQ==", "bodyText": "But the exception must be thrown by some code we control, as the TF C library isn't going to throw IndexOutOfBoundsException. So if you can point me at the place where it's thrown then we can figure out what information is available then, and what we could do to make this behave like Java code.\nDo you have an example somewhere of a Graph training loop with multiple epochs where we'd have to reinitialise the iterator? I've not seen one so far and I'd like to see how it's supposed to be used. Ditto for multiple iterators, as at the moment we won't know which iterator is exhausted, and AFAICT there's no way for the user to know that either?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405792189", "createdAt": "2020-04-08T20:24:28Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides\n+a simple API for configuring and iterating over datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+Creation\n+-\n+A dataset can be constructed from a list of constant tensors\n+using `Dataset.fromTensorSlices( ... )` as follows:\n+\n+```java\n+// Declare dataset components as arrays.\n+// NOTE: All components in a dataset must share the first \"batch\" dimension.\n+\n+int[][] m1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+int[][] m2 = new int[][]{\n+    {1}, {0}, {1}, {1}\n+};\n+\n+\n+Ops tf = // ... TensorFlow Ops Accessor (either graph or eager).\n+\n+// Construct dataset with two components, batchSize=2.\n+Dataset dataset = Dataset.fromTensorSlices(tf,\n+    // List of array components\n+    Arrays.asList(tf.constant(m1), tf.constant(m2)),\n+    // List of each component's dtype\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+)\n+```\n+\n+Iteration\n+--\n+\n+In eager mode, the dataset can be iterated through using a standard \n+\"for-each\" loop, to receive the tensor values of each component:\n+\n+```java\n+int BATCH_SIZE = 2;\n+for (List<Output<?>> components : dataset.batch(BATCH_SIZE)) {\n+      Tensor<TInt32> XBatch = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> yBatch = components.get(1).tensor().expect(TInt32.DTYPE);\n+      \n+      // ... use batch tensors\n+}\n+```\n+\n+In graph mode, the dataset can be iterated through using the `OneShotIterator` abstraction, and a while loop, as follows:\n+\n+```java\n+OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+Operation makeIterator = oneShotIterator.getMakeIteratorOp();\n+List<Output<?>> components = oneShotIterator.getComponents();\n+\n+try (Session session = new Session(graph)) {\n+    // Run MakeIterator Op to set iterator position\n+    session.runner()\n+        .addTarget(makeIterator)\n+        .run();\n+    \n+    while (true) {\n+        try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(components.get(0))\n+                .fetch(components.get(1))\n+                .run();\n+\n+            Tensor<TInt32> matrix1 = outputs.get(0).expect(TInt32.DTYPE);\n+            Tensor<TInt32> matrix2 = outputs.get(1).expect(TInt32.DTYPE);\n+\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5OTI5Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI3OTM4OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTozMzoxMVrOF0mfxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjoyMzoyN1rOGE7znA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMDk5OA==", "bodyText": "Google Java Style Guide doesn't recommend the usage of wildcard imports (I personally think it is fine for static imports though)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390700998", "createdAt": "2020-03-11T01:33:11Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgyNzM1Ng==", "bodyText": "I removed all the wildcard imports.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407827356", "createdAt": "2020-04-14T02:23:27Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMDk5OA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI4MDc2OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTozNDowNVrOF0mgjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjoyODoxN1rOGE74pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMTE5OA==", "bodyText": "no need of else if you throw in the previous block", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390701198", "createdAt": "2020-03-11T01:34:05Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgyODY0NA==", "bodyText": "updated!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407828644", "createdAt": "2020-04-14T02:28:17Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMTE5OA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTI4MjQxOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTozNTozNFrOF0mhqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxNjoyNzo0NVrOGH51iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMTQ4MA==", "bodyText": "Are there benefits to use Objects.isNull if it is not as a filter predicate, instead of just tf == null?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390701480", "createdAt": "2020-03-11T01:35:34Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA0MDQ4Mg==", "bodyText": "I think it was added to the JDK to exist as a method reference for a null check (so people didn't have to make a lambda), not sure if it has any optimization benefits (probably not as if it's not inlined then it will be more expensive).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393040482", "createdAt": "2020-03-16T13:57:37Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMTQ4MA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MDgwOQ==", "bodyText": "Using == null here. There's no benefits, the implementation of isNull calls == null.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410940809", "createdAt": "2020-04-19T16:27:45Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwMTQ4MA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMwNzgzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTo1MzowM1rOF0mxaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjozODoyMlrOGE8DAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNTUxMw==", "bodyText": "Maybe we could add a isEager() method in ExecutionEnvironment to avoid these instanceof", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390705513", "createdAt": "2020-03-11T01:53:03Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAyNDI4NQ==", "bodyText": "Do we expect there to be more environments in the future? Maybe an enum that's returned from sessionType?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393024285", "createdAt": "2020-03-16T13:32:25Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNTUxMw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA1NzAwNg==", "bodyText": "I suspect that there will always be only these two but who knows what will come out of those Google Brains in the future, so I'm ok with an enum too.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r394057006", "createdAt": "2020-03-18T01:20:44Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNTUxMw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzMTI5OQ==", "bodyText": "I added isEager() and isGraph() methods to the ExecutionEnvironment interface, using an enum.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407831299", "createdAt": "2020-04-14T02:38:22Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNTUxMw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMxMjI1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMTo1NTo0OVrOF0mz_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjowOTozNVrOGC2bPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNjE3Mg==", "bodyText": "Method return type as Dataset instead, to preserve the abstraction?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390706172", "createdAt": "2020-03-11T01:55:49Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r\n+      throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+    }\r\n+\r\n+    Operand<?> dataset = getVariant();\r\n+    AnonymousIterator anonymousIterator = tf.data.anonymousIterator(getOutputTypes(), getOutputShapes());\r\n+\r\n+    tf.data.makeIterator(dataset, anonymousIterator.handle());\r\n+\r\n+    return new Iterator<List<Output<?>>>() {\r\n+      private List<Output<?>> tryNext = getNext();\r\n+\r\n+      private List<Output<?>> getNext() {\r\n+        try {\r\n+          return tf.data.iteratorGetNext(anonymousIterator.handle(), getOutputTypes(), getOutputShapes()).components();\r\n+        } catch (IndexOutOfBoundsException e) {\r\n+          return null;\r\n+        }\r\n+      }\r\n+\r\n+      @Override\r\n+      public boolean hasNext() {\r\n+        return tryNext != null;\r\n+      }\r\n+\r\n+      @Override\r\n+      public List<Output<?>> next() {\r\n+        List<Output<?>> result = tryNext;\r\n+        tryNext = getNext();\r\n+        return result;\r\n+      }\r\n+    };\r\n+  }\r\n+\r\n+  /**\r\n+   * Return the necessary components to iterate through batches of this\r\n+   * dataset in Graph mode.\r\n+   * <p>\r\n+   * This method returns a Pair whose first element is a MakeIterator operation\r\n+   * that must be run first in its own session to create the iterator internally.\r\n+   * <p>\r\n+   * The second element in the pair is a list of Output objects. In sequential\r\n+   * calls to session.run() in which these (or child) nodes are fetched, the batches\r\n+   * are already loaded into these objects.\r\n+   *\r\n+   * @return A Pair whose first element is a MakeIterator Operation, and whose\r\n+   * second element is a list batch components.\r\n+   */\r\n+  public OneShotIterator makeOneShotIterator() {\r\n+    if (!(tf.scope().env() instanceof Graph)) {\r\n+      throw new UnsupportedOperationException(\"OneShotIterator should only be used in Graph mode.\");\r\n+    }\r\n+    List<DataType<?>> outputTypes = getOutputTypes();\r\n+    List<Shape> outputShapes = getOutputShapes();\r\n+    Operand<?> iterator = tf.data.iterator(\"null\", \"null\", outputTypes, outputShapes);\r\n+\r\n+    MakeIterator makeIterator = tf.data.makeIterator(getVariant(), iterator);\r\n+    List<Output<?>> components = tf.data.iteratorGetNext(iterator, outputTypes, outputShapes).components();\r\n+\r\n+    return new OneShotIterator(makeIterator, components);\r\n+  }\r\n+\r\n+  public static TensorSliceDataset fromTensorSlices(Ops tf, List<Operand<?>> slices, List<DataType<?>> outputTypes) {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0MjA0NQ==", "bodyText": "Thanks, I updated this!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405642045", "createdAt": "2020-04-08T16:09:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r\n+      throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+    }\r\n+\r\n+    Operand<?> dataset = getVariant();\r\n+    AnonymousIterator anonymousIterator = tf.data.anonymousIterator(getOutputTypes(), getOutputShapes());\r\n+\r\n+    tf.data.makeIterator(dataset, anonymousIterator.handle());\r\n+\r\n+    return new Iterator<List<Output<?>>>() {\r\n+      private List<Output<?>> tryNext = getNext();\r\n+\r\n+      private List<Output<?>> getNext() {\r\n+        try {\r\n+          return tf.data.iteratorGetNext(anonymousIterator.handle(), getOutputTypes(), getOutputShapes()).components();\r\n+        } catch (IndexOutOfBoundsException e) {\r\n+          return null;\r\n+        }\r\n+      }\r\n+\r\n+      @Override\r\n+      public boolean hasNext() {\r\n+        return tryNext != null;\r\n+      }\r\n+\r\n+      @Override\r\n+      public List<Output<?>> next() {\r\n+        List<Output<?>> result = tryNext;\r\n+        tryNext = getNext();\r\n+        return result;\r\n+      }\r\n+    };\r\n+  }\r\n+\r\n+  /**\r\n+   * Return the necessary components to iterate through batches of this\r\n+   * dataset in Graph mode.\r\n+   * <p>\r\n+   * This method returns a Pair whose first element is a MakeIterator operation\r\n+   * that must be run first in its own session to create the iterator internally.\r\n+   * <p>\r\n+   * The second element in the pair is a list of Output objects. In sequential\r\n+   * calls to session.run() in which these (or child) nodes are fetched, the batches\r\n+   * are already loaded into these objects.\r\n+   *\r\n+   * @return A Pair whose first element is a MakeIterator Operation, and whose\r\n+   * second element is a list batch components.\r\n+   */\r\n+  public OneShotIterator makeOneShotIterator() {\r\n+    if (!(tf.scope().env() instanceof Graph)) {\r\n+      throw new UnsupportedOperationException(\"OneShotIterator should only be used in Graph mode.\");\r\n+    }\r\n+    List<DataType<?>> outputTypes = getOutputTypes();\r\n+    List<Shape> outputShapes = getOutputShapes();\r\n+    Operand<?> iterator = tf.data.iterator(\"null\", \"null\", outputTypes, outputShapes);\r\n+\r\n+    MakeIterator makeIterator = tf.data.makeIterator(getVariant(), iterator);\r\n+    List<Output<?>> components = tf.data.iteratorGetNext(iterator, outputTypes, outputShapes).components();\r\n+\r\n+    return new OneShotIterator(makeIterator, components);\r\n+  }\r\n+\r\n+  public static TensorSliceDataset fromTensorSlices(Ops tf, List<Operand<?>> slices, List<DataType<?>> outputTypes) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNjE3Mg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMyMDMwOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/OneShotIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjowMDowM1rOF0m4Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxMDo0NFrOGC2eVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNzMwMg==", "bodyText": "Maybe we can move this class as a @Operator available in Ops? e.g. tf.data.oneShotIterator(makeIterator, components)? I understand we don't have access to this op because  it uses a func attribute but that could replace it?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390707302", "createdAt": "2020-03-11T02:00:03Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/OneShotIterator.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.data.MakeIterator;\n+\n+import java.util.List;\n+\n+public class OneShotIterator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0MjgzOA==", "bodyText": "I refactored the iteration mechanisms to capture both eager and graph iteration in DatasetIterator; I think the new logic is much cleaner (See the README / RFC)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405642838", "createdAt": "2020-04-08T16:10:44Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/OneShotIterator.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.data.MakeIterator;\n+\n+import java.util.List;\n+\n+public class OneShotIterator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNzMwMg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMyMzg1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjowMjoxNlrOF0m6aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTozNjozMlrOGEQPQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNzgxNg==", "bodyText": "It would be worth it to check which of the following utilities could be added directly to the Shape class instead.\nFor example, array(...) could be replaced by something like shape.extend(batchSize, 0), where 0 is the dimension index", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390707816", "createdAt": "2020-03-11T02:02:16Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+public class Utils {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMzUzNw==", "bodyText": "That makes sense; I removed this class, and added Shape.head() and Shape.tail() to the Shape class, and a new constructor for Shape(int, int[]).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407113537", "createdAt": "2020-04-11T21:36:32Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+public class Utils {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwNzgxNg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMyODE1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjowNToxM1rOF0m88w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxNjozMDowOFrOGH53aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwODQ2Nw==", "bodyText": "That could be replaced by Arrays.copyOfRange(shape.asArray(), 1, shape.numDimensions)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390708467", "createdAt": "2020-03-11T02:05:13Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+public class Utils {\r\n+\r\n+    private static Shape head(Shape shape) {\r\n+        return Shape.of(shape.size(0));\r\n+    }\r\n+\r\n+    public static Shape tail(Shape shape) {\r\n+        long[] tail = new long[shape.numDimensions() - 1];\r\n+        for (int i = 1; i < shape.numDimensions(); i++) {\r\n+            tail[i - 1] = shape.size(i);\r\n+        }\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MTI4OQ==", "bodyText": "Yup, I'm using copyOfRange in the Shape.tail implementation.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410941289", "createdAt": "2020-04-19T16:30:08Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+public class Utils {\r\n+\r\n+    private static Shape head(Shape shape) {\r\n+        return Shape.of(shape.size(0));\r\n+    }\r\n+\r\n+    public static Shape tail(Shape shape) {\r\n+        long[] tail = new long[shape.numDimensions() - 1];\r\n+        for (int i = 1; i < shape.numDimensions(); i++) {\r\n+            tail[i - 1] = shape.size(i);\r\n+        }\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwODQ2Nw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMyODUwOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjowNToyOVrOF0m9KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjowNToyOVrOF0m9KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwODUyMQ==", "bodyText": "that could be replaced simply by shape.asArray(), unless you need a copy, which Arrays.copyOf can provide you", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390708521", "createdAt": "2020-03-11T02:05:29Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Utils.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+public class Utils {\r\n+\r\n+    private static Shape head(Shape shape) {\r\n+        return Shape.of(shape.size(0));\r\n+    }\r\n+\r\n+    public static Shape tail(Shape shape) {\r\n+        long[] tail = new long[shape.numDimensions() - 1];\r\n+        for (int i = 1; i < shape.numDimensions(); i++) {\r\n+            tail[i - 1] = shape.size(i);\r\n+        }\r\n+\r\n+        return Shape.of(tail);\r\n+    }\r\n+\r\n+    public static long[] shapeArray(Shape shape) {\r\n+        long[] arr = new long[shape.numDimensions()];\r\n+        for (int i = 0; i < arr.length; i++) {\r\n+            arr[i] = shape.size(i);\r\n+        }\r\n+        return arr;\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTMzNjE3OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoxMDoyM1rOF0nByQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxNjozMTowN1rOGH54Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwOTcwNQ==", "bodyText": "I don't know how tf.data.batchDataset works but I find it strange that we need to add the batchSize as a new dimension for all individual shapes. Just saying in case this is a mistake", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390709705", "createdAt": "2020-03-11T02:10:23Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MTQ5MQ==", "bodyText": "This isn't a mistake, it does seem to be needed here, since it's the outputShapes that need to be specified.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410941491", "createdAt": "2020-04-19T16:31:07Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcwOTcwNQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM1MDg0OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/impl/TensorSliceDataset.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoxOToyNFrOF0nKbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxNjozMzowMVrOGH55Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMTkxNg==", "bodyText": "Why do we need to remove the first dimension of each tensor shapes?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390711916", "createdAt": "2020-03-11T02:19:24Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/impl/TensorSliceDataset.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package org.tensorflow.data.impl;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.data.Dataset;\n+import org.tensorflow.data.Utils;\n+import org.tensorflow.op.Ops;\n+\n+public class TensorSliceDataset extends Dataset {\n+  private org.tensorflow.op.data.TensorSliceDataset dataset;\n+\n+  public TensorSliceDataset(Ops tf, List<Operand<?>> components, List<DataType<?>> outputTypes) {\n+    super(tf, outputTypes,\n+        components.stream()\n+            .map(c -> Utils.tail(c.asOutput().shape()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MTc5NQ==", "bodyText": "The outputSize is supposed to be the size for the returned elements from the dataset. in a standard 2-D TensorSliceDataset, for example, it's the \"length of each vector\" that counts as the element shape, not the \"number of vectors\"", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410941795", "createdAt": "2020-04-19T16:33:01Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/impl/TensorSliceDataset.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package org.tensorflow.data.impl;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.data.Dataset;\n+import org.tensorflow.data.Utils;\n+import org.tensorflow.op.Ops;\n+\n+public class TensorSliceDataset extends Dataset {\n+  private org.tensorflow.op.data.TensorSliceDataset dataset;\n+\n+  public TensorSliceDataset(Ops tf, List<Operand<?>> components, List<DataType<?>> outputTypes) {\n+    super(tf, outputTypes,\n+        components.stream()\n+            .map(c -> Utils.tail(c.asOutput().shape()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMTkxNg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM1NTUxOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyMjo0NFrOF0nNcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTozODowMFrOGEQPwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjY5MQ==", "bodyText": "testMatrix1 and testMatrix2 can be of the type IntNdArray that you initialize with: StdArrays.copyOf(new int[][]{ ... })", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390712691", "createdAt": "2020-03-11T02:22:44Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.tensorflow.data;\n+import org.junit.Before;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TInt32;\n+\n+import java.nio.IntBuffer;\n+\n+public class DatasetTestBase {\n+  int[][] testMatrix1;\n+  int[][] testMatrix2;\n+\n+\n+  @Before\n+  public void setUp() {\n+    testMatrix1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+    testMatrix2 = new int[][]{\n+        {1}, {0}, {1}, {1}\n+    };", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMzY2NA==", "bodyText": "Yup, I updated these!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407113664", "createdAt": "2020-04-11T21:38:00Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.tensorflow.data;\n+import org.junit.Before;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TInt32;\n+\n+import java.nio.IntBuffer;\n+\n+public class DatasetTestBase {\n+  int[][] testMatrix1;\n+  int[][] testMatrix2;\n+\n+\n+  @Before\n+  public void setUp() {\n+    testMatrix1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+    testMatrix2 = new int[][]{\n+        {1}, {0}, {1}, {1}\n+    };", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMjY5MQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM1ODk1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyNTowMFrOF0nPmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTozODozNVrOGEQP2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMzI0MQ==", "bodyText": "Use intTensor.data() here to return the tensor as a TInt32/IntNdArray directly", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390713241", "createdAt": "2020-03-11T02:25:00Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.tensorflow.data;\n+import org.junit.Before;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TInt32;\n+\n+import java.nio.IntBuffer;\n+\n+public class DatasetTestBase {\n+  int[][] testMatrix1;\n+  int[][] testMatrix2;\n+\n+\n+  @Before\n+  public void setUp() {\n+    testMatrix1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+    testMatrix2 = new int[][]{\n+        {1}, {0}, {1}, {1}\n+    };\n+  }\n+\n+  static int[] concat(int[] first, int[] second) {\n+    int[] concatenated = new int[first.length + second.length];\n+    System.arraycopy(first, 0, concatenated, 0, first.length);\n+    System.arraycopy(second, 0, concatenated, first.length, second.length);\n+    return concatenated;\n+  }\n+\n+  static int[] getIntTensorAsArray(Tensor<TInt32> intTensor) {\n+    IntBuffer buffer = IntBuffer.allocate((int) intTensor.shape().size());\n+    intTensor.writeTo(buffer);\n+    return buffer.array();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMzY4OA==", "bodyText": "I removed that method, and updated the tests!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407113688", "createdAt": "2020-04-11T21:38:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetTestBase.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.tensorflow.data;\n+import org.junit.Before;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TInt32;\n+\n+import java.nio.IntBuffer;\n+\n+public class DatasetTestBase {\n+  int[][] testMatrix1;\n+  int[][] testMatrix2;\n+\n+\n+  @Before\n+  public void setUp() {\n+    testMatrix1 = new int[][]{\n+        {1, 2, 3, 4, 5},\n+        {2, 4, 6, 8, 10},\n+        {3, 6, 8, 12, 15},\n+        {4, 8, 12, 16, 20}\n+    };\n+\n+    testMatrix2 = new int[][]{\n+        {1}, {0}, {1}, {1}\n+    };\n+  }\n+\n+  static int[] concat(int[] first, int[] second) {\n+    int[] concatenated = new int[first.length + second.length];\n+    System.arraycopy(first, 0, concatenated, 0, first.length);\n+    System.arraycopy(second, 0, concatenated, first.length, second.length);\n+    return concatenated;\n+  }\n+\n+  static int[] getIntTensorAsArray(Tensor<TInt32> intTensor) {\n+    IntBuffer buffer = IntBuffer.allocate((int) intTensor.shape().size());\n+    intTensor.writeTo(buffer);\n+    return buffer.array();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMzI0MQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM2MDU5OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyNTo1MlrOF0nQlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTozODo1MFrOGEQP6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMzQ5Mg==", "bodyText": "As said before, tensors must be released with try-with-resources blocks", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390713492", "createdAt": "2020-03-11T02:25:52Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.Output;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TInt32;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class BatchDatasetTest extends DatasetTestBase {\n+\n+  @Test\n+  public void testEagerBatchDataset() {\n+    Ops tf = Ops.create();\n+\n+    // EVEN BATCH SIZES\n+    Dataset dataset = Dataset\n+        .fromTensorSlices(tf,\n+            Arrays.asList(\n+                tf.val(testMatrix1),\n+                tf.val(testMatrix2)),\n+            Arrays.asList(TInt32.DTYPE, TInt32.DTYPE))\n+        .batch(2);\n+\n+    int count = 0;\n+\n+    for (List<Output<?>> components : dataset) {\n+      Tensor<TInt32> batch1 = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> batch2 = components.get(1).tensor().expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMzcwNA==", "bodyText": "Thanks, updated!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407113704", "createdAt": "2020-04-11T21:38:50Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.Output;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TInt32;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class BatchDatasetTest extends DatasetTestBase {\n+\n+  @Test\n+  public void testEagerBatchDataset() {\n+    Ops tf = Ops.create();\n+\n+    // EVEN BATCH SIZES\n+    Dataset dataset = Dataset\n+        .fromTensorSlices(tf,\n+            Arrays.asList(\n+                tf.val(testMatrix1),\n+                tf.val(testMatrix2)),\n+            Arrays.asList(TInt32.DTYPE, TInt32.DTYPE))\n+        .batch(2);\n+\n+    int count = 0;\n+\n+    for (List<Output<?>> components : dataset) {\n+      Tensor<TInt32> batch1 = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> batch2 = components.get(1).tensor().expect(TInt32.DTYPE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxMzQ5Mg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM2MzgzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyNzo1M1rOF0nSkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxMjowM1rOGC2hrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDAwMA==", "bodyText": "This PR will allow you to test equality of NdArray instances, if you make that change (see other comments)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390714000", "createdAt": "2020-03-11T02:27:53Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.Output;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TInt32;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class BatchDatasetTest extends DatasetTestBase {\n+\n+  @Test\n+  public void testEagerBatchDataset() {\n+    Ops tf = Ops.create();\n+\n+    // EVEN BATCH SIZES\n+    Dataset dataset = Dataset\n+        .fromTensorSlices(tf,\n+            Arrays.asList(\n+                tf.val(testMatrix1),\n+                tf.val(testMatrix2)),\n+            Arrays.asList(TInt32.DTYPE, TInt32.DTYPE))\n+        .batch(2);\n+\n+    int count = 0;\n+\n+    for (List<Output<?>> components : dataset) {\n+      Tensor<TInt32> batch1 = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> batch2 = components.get(1).tensor().expect(TInt32.DTYPE);\n+\n+      assertArrayEquals(concat(testMatrix1[count], testMatrix1[count + 1]), getIntTensorAsArray(batch1));\n+      assertArrayEquals(concat(testMatrix2[count], testMatrix2[count + 1]), getIntTensorAsArray(batch2));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0MzY5Mg==", "bodyText": "Thanks Karl, I updated the tests to check for NdArray equality; it's much better now, that API works really well!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405643692", "createdAt": "2020-04-08T16:12:03Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/BatchDatasetTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.Output;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TInt32;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class BatchDatasetTest extends DatasetTestBase {\n+\n+  @Test\n+  public void testEagerBatchDataset() {\n+    Ops tf = Ops.create();\n+\n+    // EVEN BATCH SIZES\n+    Dataset dataset = Dataset\n+        .fromTensorSlices(tf,\n+            Arrays.asList(\n+                tf.val(testMatrix1),\n+                tf.val(testMatrix2)),\n+            Arrays.asList(TInt32.DTYPE, TInt32.DTYPE))\n+        .batch(2);\n+\n+    int count = 0;\n+\n+    for (List<Output<?>> components : dataset) {\n+      Tensor<TInt32> batch1 = components.get(0).tensor().expect(TInt32.DTYPE);\n+      Tensor<TInt32> batch2 = components.get(1).tensor().expect(TInt32.DTYPE);\n+\n+      assertArrayEquals(concat(testMatrix1[count], testMatrix1[count + 1]), getIntTensorAsArray(batch1));\n+      assertArrayEquals(concat(testMatrix2[count], testMatrix2[count + 1]), getIntTensorAsArray(batch2));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDAwMA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM2NjcyOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DataInterfaceTester.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjoyOTo0OVrOF0nUVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxMjoyNlrOGC2irg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDQ1NA==", "bodyText": "In this PR again, I dropped the StaticApi class, so better not using it :) Meaning that would be NdArrays.ofInts(Shape.of(3, 4))\nBut everything in this class seems to test more tensorflow-tools functionalities than tensorflow-data, could we simply remove it? Or move it directly to tensorflow-tools if you think they are worth it.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390714454", "createdAt": "2020-03-11T02:29:49Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DataInterfaceTester.java", "diffHunk": "@@ -0,0 +1,31 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.tools.ndarray.IntNdArray;\n+\n+import java.util.Arrays;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.tensorflow.tools.StaticApi.*;\n+\n+public class DataInterfaceTester {\n+  @Test\n+  public void testSomething() {\n+    IntNdArray matrix2d = ndArrayOfInts(shapeOf(3, 4));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0Mzk1MA==", "bodyText": "I agree, I just removed it.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405643950", "createdAt": "2020-04-08T16:12:26Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DataInterfaceTester.java", "diffHunk": "@@ -0,0 +1,31 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.tools.ndarray.IntNdArray;\n+\n+import java.util.Arrays;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.tensorflow.tools.StaticApi.*;\n+\n+public class DataInterfaceTester {\n+  @Test\n+  public void testSomething() {\n+    IntNdArray matrix2d = ndArrayOfInts(shapeOf(3, 4));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNDQ1NA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3MTAxOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozMjo1M1rOF0nXFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxMzoyN1rOGC2lYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTE1OA==", "bodyText": "Are TensorSliceDataset, BatchDataset, etc. meant to be accessed directly or this is only for testing purposes?\nIf they are, then you'll probably want to move these out of the impl package", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390715158", "createdAt": "2020-03-11T02:32:53Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NDY0Mw==", "bodyText": "They shouldn't be accessed directly. This file was just for testing, so I removed it.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405644643", "createdAt": "2020-04-08T16:13:27Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTE1OA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3MjE4OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozMzozNFrOF0nXww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxMzo0MVrOGC2mFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTMzMQ==", "bodyText": "handle tensor close", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390715331", "createdAt": "2020-03-11T02:33:34Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NDgyMg==", "bodyText": "Thanks, updated!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405644822", "createdAt": "2020-04-08T16:13:41Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTMzMQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3MzM2OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozNDoyNVrOF0nYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjo1NToyM1rOGE8VSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTUzNQ==", "bodyText": "again, any way to avoid catching an exception as part of the normal process? I think I already asked you this question before and you told me that is how Python is doing it... but still, can we do better? :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390715535", "createdAt": "2020-03-11T02:34:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NTUwNg==", "bodyText": "I can't think of a way to improve this, without restricting what can be done within the loop (e.g. arbitrary session runs / fetches / etc. that depend on the loop iteration).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405645506", "createdAt": "2020-04-08T16:14:43Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTUzNQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgyNjcxNw==", "bodyText": "Will resolve this since we have other threads on this topic.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407826717", "createdAt": "2020-04-14T02:21:10Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTUzNQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNTk3OA==", "bodyText": "Will resolve this thread as we have others on this topic.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407835978", "createdAt": "2020-04-14T02:55:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTUzNQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3NjMwOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozNjozOFrOF0naVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxNTozMlrOGC2q2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTk4OQ==", "bodyText": "tensor.writeTo might be remove soon, you might want to use tensor.data() instead and maybe enhanced NdArray with what is missing (like ndArray.print()?)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390715989", "createdAt": "2020-03-11T02:36:38Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {\n+          System.out.println(\"finished iterating.\");\n+          break;\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testCleanEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      // // Try running TensorDataset\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+\n+      Dataset dataset = Dataset\n+          .fromTensorSlices(tf, tensors, outputTypes)\n+          .batch(50)\n+          .take(3)\n+          .skip(1);\n+      for (List<Output<?>> output : dataset) {\n+        Tensor<?> XBatch = output.get(0).tensor();\n+        Tensor<?> yBatch = output.get(1).tensor();\n+\n+        System.out.println(\"New Batch: \");\n+        System.out.print(\"   X is\");\n+        printIntTensor(XBatch);\n+        System.out.print(\"   y is\");\n+        printIntTensor(yBatch);\n+      }\n+    }\n+  }\n+\n+\n+  @Test\n+  public  void testGraphBatching() {\n+    try (Graph graph = new Graph()) {\n+\n+      Ops tf = Ops.create(graph);\n+      try (Session session = new Session(graph)) {\n+        long BATCH_SIZE = 2L;\n+\n+        Constant<TInt32> X = tf.val(\n+            new int[][]{\n+                {1, 2, 3},\n+                {4, 5, 6},\n+                {7, 8, 9},\n+                {10, 11, 12}\n+            }\n+        );\n+\n+        Constant<TInt32> y = tf.val(\n+            new int[][]{\n+                {1},\n+                {4},\n+                {7},\n+                {10}\n+            }\n+        );\n+\n+        // // Try running TensorDataset\n+        List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+        List<Shape> outputShapes = Arrays.asList(\n+            Shape.of(3),\n+            Shape.of(1));\n+\n+        TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(),\n+            Arrays.asList(X, y),\n+            outputShapes);\n+\n+        BatchDataset batchDataset = BatchDataset.create(\n+            tf.scope(),\n+            tensorDataset,\n+            tf.val(BATCH_SIZE),\n+            tf.val(true),\n+            outputTypes,\n+            Arrays.asList(\n+                Shape.of(BATCH_SIZE, 3),\n+                Shape.of(BATCH_SIZE, 1))\n+        );\n+\n+        Iterator anonymousIter = Iterator.create(tf.scope(), null, null, outputTypes, Arrays.asList(\n+            Shape.of(BATCH_SIZE, 3),\n+            Shape.of(BATCH_SIZE, 1)));\n+\n+        MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter);\n+\n+        session.runner()\n+            .addTarget(makeIterator.op())\n+            .run();\n+\n+        IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter, outputTypes, outputShapes);\n+        Operand<?> XOp = getNext.components().get(0);\n+        Operand<?> yOp = getNext.components().get(1);\n+        while (true) {\n+          try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .addTarget(getNext.op())\n+                .fetch(XOp)\n+                .fetch(yOp)\n+                .run();\n+            System.out.println(\"BATCH: \");\n+            printIntTensor(outputs.get(0));\n+            printIntTensor(outputs.get(1));\n+            System.out.println();\n+          } catch (IndexOutOfBoundsException e) {\n+            System.out.println(\"finished iterating.\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testCleanGraphBatching() {\n+    try (Graph graph = new Graph()) {\n+\n+      Ops tf = Ops.create(graph);\n+      try (Session session = new Session(graph)) {\n+        long BATCH_SIZE = 2L;\n+\n+        Constant<TInt32> X = tf.val(\n+            new int[][]{\n+                {1, 2, 3},\n+                {4, 5, 6},\n+                {7, 8, 9},\n+                {10, 11, 12}\n+            }\n+        );\n+\n+        Constant<TInt32> y = tf.val(\n+            new int[][]{\n+                {1},\n+                {4},\n+                {7},\n+                {10}\n+            }\n+        );\n+\n+        // // Try running TensorDataset\n+        List<Operand<?>> tensors = Arrays.asList(X, y);\n+        List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+\n+        Dataset dataset = Dataset\n+            .fromTensorSlices(tf, tensors, outputTypes)\n+            .batch(2);\n+\n+        OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+        List<Output<?>> components = oneShotIterator.getComponents();\n+        Operand<?> XOp = components.get(0);\n+        Operand<?> yOp = components.get(1);\n+\n+        // Run MakeIterator Op\n+        session.runner()\n+            .addTarget(oneShotIterator.getMakeIteratorOp())\n+            .run();\n+\n+        while (true) {\n+          try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(XOp)\n+                .fetch(yOp)\n+                .run();\n+\n+            System.out.println(\"BATCH: \");\n+            printIntTensor(outputs.get(0));\n+            printIntTensor(outputs.get(1));\n+            System.out.println();\n+          } catch (IndexOutOfBoundsException e) {\n+            System.out.println(\"finished iterating.\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  public static void printIntTensor(Tensor<?> tensor) {\n+    IntBuffer buffer = IntBuffer.allocate((int) tensor.shape().size());\n+    tensor.writeTo(buffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjA0MQ==", "bodyText": "I removed the print statements; but it would be great to have an print statement for NdArrays!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405646041", "createdAt": "2020-04-08T16:15:32Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/test/java/org/tensorflow/data/DatasetOpTester.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.tensorflow.data;\n+\n+import org.junit.Test;\n+import org.tensorflow.*;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.data.*;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.utils.Tuple2;\n+\n+import java.nio.IntBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class DatasetOpTester {\n+\n+  @Test\n+  public void testEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+\n+      // // Try running TensorDataset\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+      List<Shape> outputShapes = Arrays.asList(\n+          Shape.of(3),\n+          Shape.of(1));\n+\n+      TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(), tensors, outputShapes);\n+\n+      BatchDataset batchDataset = BatchDataset.create(\n+          tf.scope(),\n+          tensorDataset,\n+          tf.val(2L),\n+          tf.val(true),\n+          outputTypes,\n+          Arrays.asList(\n+              Shape.of(2, 3),\n+              Shape.of(2, 1))\n+      );\n+\n+\n+      AnonymousIterator anonymousIter = AnonymousIterator.create(tf.scope(), outputTypes, Arrays.asList(\n+          Shape.of(2, 3),\n+          Shape.of(2, 1)));\n+\n+      MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter.handle());\n+\n+      while (true) {\n+        try {\n+          IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter.handle(), outputTypes, outputShapes);\n+          List<Output<?>> outputs = getNext.components();\n+          System.out.println(\"BATCH: \");\n+          printIntTensor(outputs.get(0).tensor());\n+          printIntTensor(outputs.get(1).tensor());\n+          System.out.println();\n+        } catch (IndexOutOfBoundsException e) {\n+          System.out.println(\"finished iterating.\");\n+          break;\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testCleanEagerBatching() {\n+    try (EagerSession session = EagerSession.create()) {\n+      Ops tf = Ops.create(session);\n+\n+      Constant<TInt32> X = tf.val(\n+          new int[][]{\n+              {1, 2, 3},\n+              {4, 5, 6},\n+              {7, 8, 9},\n+              {10, 11, 12}\n+          }\n+      );\n+\n+      Constant<TInt32> y = tf.val(\n+          new int[][]{\n+              {1},\n+              {4},\n+              {7},\n+              {10}\n+          }\n+      );\n+\n+\n+      // // Try running TensorDataset\n+      List<Operand<?>> tensors = Arrays.asList(X, y);\n+      List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+\n+      Dataset dataset = Dataset\n+          .fromTensorSlices(tf, tensors, outputTypes)\n+          .batch(50)\n+          .take(3)\n+          .skip(1);\n+      for (List<Output<?>> output : dataset) {\n+        Tensor<?> XBatch = output.get(0).tensor();\n+        Tensor<?> yBatch = output.get(1).tensor();\n+\n+        System.out.println(\"New Batch: \");\n+        System.out.print(\"   X is\");\n+        printIntTensor(XBatch);\n+        System.out.print(\"   y is\");\n+        printIntTensor(yBatch);\n+      }\n+    }\n+  }\n+\n+\n+  @Test\n+  public  void testGraphBatching() {\n+    try (Graph graph = new Graph()) {\n+\n+      Ops tf = Ops.create(graph);\n+      try (Session session = new Session(graph)) {\n+        long BATCH_SIZE = 2L;\n+\n+        Constant<TInt32> X = tf.val(\n+            new int[][]{\n+                {1, 2, 3},\n+                {4, 5, 6},\n+                {7, 8, 9},\n+                {10, 11, 12}\n+            }\n+        );\n+\n+        Constant<TInt32> y = tf.val(\n+            new int[][]{\n+                {1},\n+                {4},\n+                {7},\n+                {10}\n+            }\n+        );\n+\n+        // // Try running TensorDataset\n+        List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+        List<Shape> outputShapes = Arrays.asList(\n+            Shape.of(3),\n+            Shape.of(1));\n+\n+        TensorSliceDataset tensorDataset = TensorSliceDataset.create(tf.scope(),\n+            Arrays.asList(X, y),\n+            outputShapes);\n+\n+        BatchDataset batchDataset = BatchDataset.create(\n+            tf.scope(),\n+            tensorDataset,\n+            tf.val(BATCH_SIZE),\n+            tf.val(true),\n+            outputTypes,\n+            Arrays.asList(\n+                Shape.of(BATCH_SIZE, 3),\n+                Shape.of(BATCH_SIZE, 1))\n+        );\n+\n+        Iterator anonymousIter = Iterator.create(tf.scope(), null, null, outputTypes, Arrays.asList(\n+            Shape.of(BATCH_SIZE, 3),\n+            Shape.of(BATCH_SIZE, 1)));\n+\n+        MakeIterator makeIterator = tf.data.makeIterator(batchDataset, anonymousIter);\n+\n+        session.runner()\n+            .addTarget(makeIterator.op())\n+            .run();\n+\n+        IteratorGetNext getNext = tf.data.iteratorGetNext(anonymousIter, outputTypes, outputShapes);\n+        Operand<?> XOp = getNext.components().get(0);\n+        Operand<?> yOp = getNext.components().get(1);\n+        while (true) {\n+          try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .addTarget(getNext.op())\n+                .fetch(XOp)\n+                .fetch(yOp)\n+                .run();\n+            System.out.println(\"BATCH: \");\n+            printIntTensor(outputs.get(0));\n+            printIntTensor(outputs.get(1));\n+            System.out.println();\n+          } catch (IndexOutOfBoundsException e) {\n+            System.out.println(\"finished iterating.\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testCleanGraphBatching() {\n+    try (Graph graph = new Graph()) {\n+\n+      Ops tf = Ops.create(graph);\n+      try (Session session = new Session(graph)) {\n+        long BATCH_SIZE = 2L;\n+\n+        Constant<TInt32> X = tf.val(\n+            new int[][]{\n+                {1, 2, 3},\n+                {4, 5, 6},\n+                {7, 8, 9},\n+                {10, 11, 12}\n+            }\n+        );\n+\n+        Constant<TInt32> y = tf.val(\n+            new int[][]{\n+                {1},\n+                {4},\n+                {7},\n+                {10}\n+            }\n+        );\n+\n+        // // Try running TensorDataset\n+        List<Operand<?>> tensors = Arrays.asList(X, y);\n+        List<DataType<?>> outputTypes = Arrays.asList(TInt32.DTYPE, TInt32.DTYPE);\n+\n+        Dataset dataset = Dataset\n+            .fromTensorSlices(tf, tensors, outputTypes)\n+            .batch(2);\n+\n+        OneShotIterator oneShotIterator = dataset.makeOneShotIterator();\n+        List<Output<?>> components = oneShotIterator.getComponents();\n+        Operand<?> XOp = components.get(0);\n+        Operand<?> yOp = components.get(1);\n+\n+        // Run MakeIterator Op\n+        session.runner()\n+            .addTarget(oneShotIterator.getMakeIteratorOp())\n+            .run();\n+\n+        while (true) {\n+          try {\n+            List<Tensor<?>> outputs = session.runner()\n+                .fetch(XOp)\n+                .fetch(yOp)\n+                .run();\n+\n+            System.out.println(\"BATCH: \");\n+            printIntTensor(outputs.get(0));\n+            printIntTensor(outputs.get(1));\n+            System.out.println();\n+          } catch (IndexOutOfBoundsException e) {\n+            System.out.println(\"finished iterating.\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  public static void printIntTensor(Tensor<?> tensor) {\n+    IntBuffer buffer = IntBuffer.allocate((int) tensor.shape().size());\n+    tensor.writeTo(buffer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNTk4OQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM3ODk3OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-utils/pom.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjozODozMlrOF0ncHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxNjowNVrOGC2sOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNjQ0Ng==", "bodyText": "You can move classes in this artifact directly in tensorflow-tools, its main purpose is to provide such generic utilities.\nBut looking again at your PR, it seems that those classes (Pair and Tuple2) are not being used anywhere, so I suggest we simple drop them.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390716446", "createdAt": "2020-03-11T02:38:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-utils/pom.xml", "diffHunk": "@@ -0,0 +1,62 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-frameworks</artifactId>\n+        <version>0.1.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-utils</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjEyNjU0Mw==", "bodyText": "Agreed", "url": "https://github.com/tensorflow/java/pull/30#discussion_r396126543", "createdAt": "2020-03-22T18:37:06Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-utils/pom.xml", "diffHunk": "@@ -0,0 +1,62 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-frameworks</artifactId>\n+        <version>0.1.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-utils</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNjQ0Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjM5Mg==", "bodyText": "I removed tensorflow-utils", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405646392", "createdAt": "2020-04-08T16:16:05Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-utils/pom.xml", "diffHunk": "@@ -0,0 +1,62 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-frameworks</artifactId>\n+        <version>0.1.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-utils</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNjQ0Ng=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMTM4NDI3OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwMjo0MjowOFrOF0nfag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxNjoxOVrOGC2s3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNzI5MA==", "bodyText": "It seems that it does not return a Pair anymore?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r390717290", "createdAt": "2020-03-11T02:42:08Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r\n+      throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+    }\r\n+\r\n+    Operand<?> dataset = getVariant();\r\n+    AnonymousIterator anonymousIterator = tf.data.anonymousIterator(getOutputTypes(), getOutputShapes());\r\n+\r\n+    tf.data.makeIterator(dataset, anonymousIterator.handle());\r\n+\r\n+    return new Iterator<List<Output<?>>>() {\r\n+      private List<Output<?>> tryNext = getNext();\r\n+\r\n+      private List<Output<?>> getNext() {\r\n+        try {\r\n+          return tf.data.iteratorGetNext(anonymousIterator.handle(), getOutputTypes(), getOutputShapes()).components();\r\n+        } catch (IndexOutOfBoundsException e) {\r\n+          return null;\r\n+        }\r\n+      }\r\n+\r\n+      @Override\r\n+      public boolean hasNext() {\r\n+        return tryNext != null;\r\n+      }\r\n+\r\n+      @Override\r\n+      public List<Output<?>> next() {\r\n+        List<Output<?>> result = tryNext;\r\n+        tryNext = getNext();\r\n+        return result;\r\n+      }\r\n+    };\r\n+  }\r\n+\r\n+  /**\r\n+   * Return the necessary components to iterate through batches of this\r\n+   * dataset in Graph mode.\r\n+   * <p>\r\n+   * This method returns a Pair whose first element is a MakeIterator operation\r\n+   * that must be run first in its own session to create the iterator internally.\r\n+   * <p>\r\n+   * The second element in the pair is a list of Output objects. In sequential\r\n+   * calls to session.run() in which these (or child) nodes are fetched, the batches\r\n+   * are already loaded into these objects.\r\n+   *\r\n+   * @return A Pair whose first element is a MakeIterator Operation, and whose\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjEyNjU3NQ==", "bodyText": "Good catch, yup this returns OneShotIterator now.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r396126575", "createdAt": "2020-03-22T18:37:27Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r\n+      throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+    }\r\n+\r\n+    Operand<?> dataset = getVariant();\r\n+    AnonymousIterator anonymousIterator = tf.data.anonymousIterator(getOutputTypes(), getOutputShapes());\r\n+\r\n+    tf.data.makeIterator(dataset, anonymousIterator.handle());\r\n+\r\n+    return new Iterator<List<Output<?>>>() {\r\n+      private List<Output<?>> tryNext = getNext();\r\n+\r\n+      private List<Output<?>> getNext() {\r\n+        try {\r\n+          return tf.data.iteratorGetNext(anonymousIterator.handle(), getOutputTypes(), getOutputShapes()).components();\r\n+        } catch (IndexOutOfBoundsException e) {\r\n+          return null;\r\n+        }\r\n+      }\r\n+\r\n+      @Override\r\n+      public boolean hasNext() {\r\n+        return tryNext != null;\r\n+      }\r\n+\r\n+      @Override\r\n+      public List<Output<?>> next() {\r\n+        List<Output<?>> result = tryNext;\r\n+        tryNext = getNext();\r\n+        return result;\r\n+      }\r\n+    };\r\n+  }\r\n+\r\n+  /**\r\n+   * Return the necessary components to iterate through batches of this\r\n+   * dataset in Graph mode.\r\n+   * <p>\r\n+   * This method returns a Pair whose first element is a MakeIterator operation\r\n+   * that must be run first in its own session to create the iterator internally.\r\n+   * <p>\r\n+   * The second element in the pair is a list of Output objects. In sequential\r\n+   * calls to session.run() in which these (or child) nodes are fetched, the batches\r\n+   * are already loaded into these objects.\r\n+   *\r\n+   * @return A Pair whose first element is a MakeIterator Operation, and whose\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNzI5MA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjU1Nw==", "bodyText": "... And now usees DatasetIterator", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405646557", "createdAt": "2020-04-08T16:16:19Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r\n+    return batch(batchSize, true);\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` skips `count` initial elements from this dataset\r\n+   *\r\n+   * @param count The number of elements to `skip` to form the new dataset.\r\n+   * @return A new Dataset with `count` elements removed.\r\n+   */\r\n+  public final Dataset skip(long count) {\r\n+    return new SkipDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates new `Dataset` with the first `count` elements from this dataset.\r\n+   *\r\n+   * @param count The number of elements to \"take\" from this dataset.\r\n+   * @return A new Dataset containing the first `count` elements from this dataset.\r\n+   */\r\n+  public final Dataset take(long count) {\r\n+    return new TakeDataset(tf, this.getVariant(), tf.val(count), this.getOutputTypes(), this.getOutputShapes());\r\n+  }\r\n+\r\n+  /**\r\n+   * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+   * Each batch is a list of components, returned as `Output` objects.\r\n+   * <p>\r\n+   * This method enables for-each iteration through batches when running\r\n+   * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+   *\r\n+   * @return an Iterator through batches of this dataset.\r\n+   */\r\n+  @Override\r\n+  public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+    if (!(tf.scope().env() instanceof EagerSession)) {\r\n+      throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+    }\r\n+\r\n+    Operand<?> dataset = getVariant();\r\n+    AnonymousIterator anonymousIterator = tf.data.anonymousIterator(getOutputTypes(), getOutputShapes());\r\n+\r\n+    tf.data.makeIterator(dataset, anonymousIterator.handle());\r\n+\r\n+    return new Iterator<List<Output<?>>>() {\r\n+      private List<Output<?>> tryNext = getNext();\r\n+\r\n+      private List<Output<?>> getNext() {\r\n+        try {\r\n+          return tf.data.iteratorGetNext(anonymousIterator.handle(), getOutputTypes(), getOutputShapes()).components();\r\n+        } catch (IndexOutOfBoundsException e) {\r\n+          return null;\r\n+        }\r\n+      }\r\n+\r\n+      @Override\r\n+      public boolean hasNext() {\r\n+        return tryNext != null;\r\n+      }\r\n+\r\n+      @Override\r\n+      public List<Output<?>> next() {\r\n+        List<Output<?>> result = tryNext;\r\n+        tryNext = getNext();\r\n+        return result;\r\n+      }\r\n+    };\r\n+  }\r\n+\r\n+  /**\r\n+   * Return the necessary components to iterate through batches of this\r\n+   * dataset in Graph mode.\r\n+   * <p>\r\n+   * This method returns a Pair whose first element is a MakeIterator operation\r\n+   * that must be run first in its own session to create the iterator internally.\r\n+   * <p>\r\n+   * The second element in the pair is a list of Output objects. In sequential\r\n+   * calls to session.run() in which these (or child) nodes are fetched, the batches\r\n+   * are already loaded into these objects.\r\n+   *\r\n+   * @return A Pair whose first element is a MakeIterator Operation, and whose\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcxNzI5MA=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNjI1ODUzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxMzo0OToxOFrOF208mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjoxODo1MVrOGC2ziw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDkwNw==", "bodyText": "Does the bit about C++ need to be at the top of the readme? It's not relevant to most users who come across this library. The rest of it is fine, I just don't want to put off people who might think they need to know anything about TF's C++ API before using this.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393034907", "createdAt": "2020-03-16T13:49:18Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjEyNjUyNQ==", "bodyText": "I agree, that makes sense, that's my operational understanding but hopefully the users doing have to think that way!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r396126525", "createdAt": "2020-03-22T18:36:51Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDkwNw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0ODI2Nw==", "bodyText": "I added simpler wording here.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405648267", "createdAt": "2020-04-08T16:18:51Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides simple APIs for loading data of various formats, and preparing\n+datasets for use in training and using deep learning models.\n+\n+TensorFlow Java's implementation simplifies the use of the C++ `data` ops, and provides", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDkwNw=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNjMwODA3OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNDowMToxOFrOF21cKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTo0MDoxN1rOGEQQcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA0Mjk4NQ==", "bodyText": "Why do we want the default behaviour to drop the last batch? Most code should be agnostic to the batch size right?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393042985", "createdAt": "2020-03-16T14:01:18Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMzg0MQ==", "bodyText": "I updated this, thanks for catching! The default in python is to not drop the last batch.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407113841", "createdAt": "2020-04-11T21:40:17Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.*;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.op.data.AnonymousIterator;\r\n+import org.tensorflow.op.data.MakeIterator;\r\n+import org.tensorflow.tools.Shape;\r\n+import org.tensorflow.utils.Tuple2;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+  protected Ops tf;\r\n+  private List<DataType<?>> outputTypes;\r\n+  private List<Shape> outputShapes;\r\n+\r\n+  public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+    if (Objects.isNull(tf)) {\r\n+      throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+    } else if (outputTypes.size() != outputShapes.size()) {\r\n+      throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+    }\r\n+\r\n+    this.tf = tf;\r\n+    this.outputTypes = outputTypes;\r\n+    this.outputShapes = outputShapes;\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   *\r\n+   * @param batchSize     The number of desired elements per batch\r\n+   * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+   *                      than `batchSize` elements.\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+    List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+        .map(s -> Shape.of(Utils.array(batchSize, s.asArray())))\r\n+        .collect(Collectors.toList());\r\n+    return new BatchDataset(tf, this.getVariant(), tf.val(batchSize),\r\n+        tf.val(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+  }\r\n+\r\n+  /**\r\n+   * Groups elements of this dataset into batches.\r\n+   * Leaves out the last batch if it has fewer than `batchSize` elements.\r\n+   *\r\n+   * @param batchSize The number of desired elements per batch\r\n+   * @return A batched Dataset\r\n+   */\r\n+  public final Dataset batch(long batchSize) {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA0Mjk4NQ=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNjM2ODE1OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-utils/src/main/java/org/tensorflow/utils/Pair.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNDoxNDo1MVrOF22CHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQxODozNDozNFrOF5xoWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA1MjcwMg==", "bodyText": "Do we need both Pair and Tuple2? Or indeed either of them?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r393052702", "createdAt": "2020-03-16T14:14:51Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-utils/src/main/java/org/tensorflow/utils/Pair.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.tensorflow.utils;\r\n+\r\n+public class Pair<T> extends Tuple2<T, T> {\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA1NzM3OQ==", "bodyText": "I had the same comment, I think Dhruv can safely discard them", "url": "https://github.com/tensorflow/java/pull/30#discussion_r394057379", "createdAt": "2020-03-18T01:22:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-utils/src/main/java/org/tensorflow/utils/Pair.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.tensorflow.utils;\r\n+\r\n+public class Pair<T> extends Tuple2<T, T> {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA1MjcwMg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjEyNjI5Ng==", "bodyText": "Yup I think for now we can remove this; I originally used a Pair instead of OneShotIterator. I always miss tuples in Java :) but the Java pattern of creating bundler classes instead is pretty useful.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r396126296", "createdAt": "2020-03-22T18:34:34Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-utils/src/main/java/org/tensorflow/utils/Pair.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.tensorflow.utils;\r\n+\r\n+public class Pair<T> extends Tuple2<T, T> {\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzA1MjcwMg=="}, "originalCommit": {"oid": "a69cfd9d8117fe6158a25e23133364b3b59b29ce"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxODE4MzE4OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMDoyNjowOFrOGC_psw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxOToyNTowM1rOGEDmDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5MzIwMw==", "bodyText": "This class is package private, yet returned by a public method on Dataset. Should it be public?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405793203", "createdAt": "2020-04-08T20:26:08Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+class DatasetIterator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjkwNjM4Mw==", "bodyText": "Yup good catch, updated!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r406906383", "createdAt": "2020-04-10T19:25:03Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+class DatasetIterator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5MzIwMw=="}, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxODE4NDQ5OnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMDoyNjozNFrOGC_qkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxOToyMzo1OFrOGEDkmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5MzQyNQ==", "bodyText": "There should be an explicit notice in this javadoc that DatasetIterator is not a java.util.Iterator. It's unfortunate that we can't really change the name entirely, but I guess it's best to go with what the TF Python functions are called when they mirror the functionality.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405793425", "createdAt": "2020-04-08T20:26:34Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjkwNjAwOA==", "bodyText": "Good point, I added some explanation.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r406906008", "createdAt": "2020-04-10T19:23:58Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5MzQyNQ=="}, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxODIwMzExOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMDozMjoxMVrOGC_1_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxOToyNDo0NVrOGEDlqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5NjM1MA==", "bodyText": "Should these constructors be public? They don't validate that the iteratorResource is actually an iterator.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r405796350", "createdAt": "2020-04-08T20:32:11Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+class DatasetIterator {\n+    public static final String EMPTY_SHARED_NAME = \"\";\n+\n+    private Ops tf;\n+\n+    private Operand<?> iteratorResource;\n+    private Op initializer;\n+\n+    private List<DataType<?>> outputTypes;\n+    private List<Shape> outputShapes;\n+\n+    /**\n+     * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+     *                         as the `iteratorResource`.\n+     * @param iteratorResource An Operand representing the iterator\n+     *                         (e.g. constructed from `tf.data.iterator` or\n+     *                         `tf.data.anonymousIterator`)\n+     * @param initializer      An `Op` that should be run to initialize this iterator\n+     * @param outputTypes      A list of `DataType` objects corresponding to the\n+     *                         types of each component of a dataset element.\n+     * @param outputShapes     A list of `Shape` objects corresponding to the\n+     *                         shapes of each componenet of a dataset element.\n+     */\n+    public DatasetIterator(Ops tf, Operand<?> iteratorResource,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjkwNjI4Mw==", "bodyText": "I agree, makes sense to make them private for now, and require the use of makeInitializer. Updated those.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r406906283", "createdAt": "2020-04-10T19:24:45Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+package org.tensorflow.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset.\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+class DatasetIterator {\n+    public static final String EMPTY_SHARED_NAME = \"\";\n+\n+    private Ops tf;\n+\n+    private Operand<?> iteratorResource;\n+    private Op initializer;\n+\n+    private List<DataType<?>> outputTypes;\n+    private List<Shape> outputShapes;\n+\n+    /**\n+     * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+     *                         as the `iteratorResource`.\n+     * @param iteratorResource An Operand representing the iterator\n+     *                         (e.g. constructed from `tf.data.iterator` or\n+     *                         `tf.data.anonymousIterator`)\n+     * @param initializer      An `Op` that should be run to initialize this iterator\n+     * @param outputTypes      A list of `DataType` objects corresponding to the\n+     *                         types of each component of a dataset element.\n+     * @param outputShapes     A list of `Shape` objects corresponding to the\n+     *                         shapes of each componenet of a dataset element.\n+     */\n+    public DatasetIterator(Ops tf, Operand<?> iteratorResource,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5NjM1MA=="}, "originalCommit": {"oid": "3b193cddfe94d332d29e4962ac3b40c2776d9cb1"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU0NTY3OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Output.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxMzo1NDoxMFrOGENYmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxNjozNToxM1rOGH57Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2Njc3Ng==", "bodyText": "The starting comment tag is offset of one character (at least in GitHub review)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407066776", "createdAt": "2020-04-11T13:54:10Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Output.java", "diffHunk": "@@ -47,6 +47,24 @@ public Shape shape() {\n     return (DataType<T>)operation.dtype(index);\n   }\n \n+    /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MjIzNA==", "bodyText": "thanks, fixed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410942234", "createdAt": "2020-04-19T16:35:13Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Output.java", "diffHunk": "@@ -47,6 +47,24 @@ public Shape shape() {\n     return (DataType<T>)operation.dtype(index);\n   }\n \n+    /**", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2Njc3Ng=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU0ODAyOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxMzo1NzowM1rOGENZyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTo1MToxMlrOGEQT5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NzA4MA==", "bodyText": "@dhruvrajan , there is a few typos in that README but before going through them, is this PR ready to be merged or it is still work in progress?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407067080", "createdAt": "2020-04-11T13:57:03Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExNDY5NA==", "bodyText": "I think it is ready to be merged, since it provides most of the structural features we want.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407114694", "createdAt": "2020-04-11T21:50:45Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NzA4MA=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExNDcyNg==", "bodyText": "I scanned the README; I hope it's a little freer of typos :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407114726", "createdAt": "2020-04-11T21:51:12Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NzA4MA=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU0ODkzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxMzo1Nzo1M1rOGENaNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQyMTo1MToyM1rOGEQT8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NzE4OQ==", "bodyText": "In case of simple vectors, you probably want to go with NdArrays.vectorOf(0.0f, 1.0f, 1.0f, 0.0f);", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407067189", "createdAt": "2020-04-11T13:57:53Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExNDczNw==", "bodyText": "Makes sense!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407114737", "createdAt": "2020-04-11T21:51:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NzE4OQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU3NDkzOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxNDoyODoyNlrOGENmaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjo1NjoyM1rOGE8WNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3MDMxMw==", "bodyText": "I think we talked previously to add a method in the Ops or Scope class telling if we are in graph mode or not without the need of instanceof (returning either a boolean or an enum to support future execution environments). Should we do it in this PR as well?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407070313", "createdAt": "2020-04-11T14:28:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,210 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.BatchDataset;\r\n+import org.tensorflow.data.impl.SkipDataset;\r\n+import org.tensorflow.data.impl.TakeDataset;\r\n+import org.tensorflow.data.impl.TensorSliceDataset;\r\n+import org.tensorflow.op.Op;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+    protected Ops tf;\r\n+    private List<DataType<?>> outputTypes;\r\n+    private List<Shape> outputShapes;\r\n+\r\n+    public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+        if (Objects.isNull(tf)) {\r\n+            throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+        } else if (outputTypes.size() != outputShapes.size()) {\r\n+            throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+        }\r\n+\r\n+        this.tf = tf;\r\n+        this.outputTypes = outputTypes;\r\n+        this.outputShapes = outputShapes;\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     *\r\n+     * @param batchSize     The number of desired elements per batch\r\n+     * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+     *                      than `batchSize` elements.\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+        List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+                .map(s -> Shape.of(batchSize, s.asArray()))\r\n+                .collect(Collectors.toList());\r\n+\r\n+\r\n+        return new BatchDataset(tf, this.getVariant(), tf.constant(batchSize),\r\n+                tf.constant(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     * Includes the last batch, even if it has fewer than `batchSize` elements.\r\n+     *\r\n+     * @param batchSize The number of desired elements per batch\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize) {\r\n+        return batch(batchSize, false);\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` which skips `count` initial elements from this\r\n+     * dataset\r\n+     *\r\n+     * @param count The number of elements to `skip` to form the new dataset.\r\n+     * @return A new Dataset with `count` elements removed.\r\n+     */\r\n+    public final Dataset skip(long count) {\r\n+        return new SkipDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` with only the first `count` elements from this\r\n+     * dataset.\r\n+     *\r\n+     * @param count The number of elements to \"take\" from this dataset.\r\n+     * @return A new Dataset containing the first `count` elements from this dataset.\r\n+     */\r\n+    public final Dataset take(long count) {\r\n+        return new TakeDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+     * Each batch is a list of components, returned as `Output` objects.\r\n+     * <p>\r\n+     * This method enables for-each iteration through batches when running\r\n+     * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+     *\r\n+     * @return an Iterator through batches of this dataset.\r\n+     */\r\n+    @Override\r\n+    public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+        if (!(tf.scope().env() instanceof EagerSession)) {\r\n+            throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+        }\r\n+\r\n+        DatasetIterator iterator = makeOneShotIterator();\r\n+\r\n+        return new Iterator<List<Output<?>>>() {\r\n+            private List<Output<?>> tryNext = getNext();\r\n+\r\n+            private List<Output<?>> getNext() {\r\n+                try {\r\n+                    return iterator.getNext();\r\n+                } catch (IndexOutOfBoundsException e) {\r\n+                    return null;\r\n+                }\r\n+            }\r\n+\r\n+            @Override\r\n+            public boolean hasNext() {\r\n+                return tryNext != null;\r\n+            }\r\n+\r\n+            @Override\r\n+            public List<Output<?>> next() {\r\n+                List<Output<?>> result = tryNext;\r\n+                tryNext = getNext();\r\n+                return result;\r\n+            }\r\n+        };\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate\r\n+     * over elements of this dataset.\r\n+     *\r\n+     * This iterator will have to be initialized with a call\r\n+     * to `iterator.makeInitializer(Dataset)` before elements\r\n+     * can be retreived in a loop.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeInitializeableIterator() {\r\n+        return DatasetIterator\r\n+                .fromStructure(tf, outputTypes, outputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate over\r\n+     * elements of this dataset. Using `makeOneShotIterator` ensures\r\n+     * that the iterator is\r\n+     * automatically initialized on this dataset.\r\n+     *skips\r\n+     * In graph mode, the initializer op will be added to the Graph's\r\n+     * intitializer list, which must be run via `tf.init()`:\r\n+     *\r\n+     * Ex:\r\n+     * <pre>\r\n+     *     try (Session session = new Session(graph) {\r\n+     *         // Immediately run initializers\r\n+     *         session.run(tf.init());\r\n+     *     }\r\n+     * </pre>\r\n+     *\r\n+     * In eager mode, the initializer will be run automatically as a result\r\n+     * of this call.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeOneShotIterator() {\r\n+        DatasetIterator iterator = makeInitializeableIterator();\r\n+        Op initializer = iterator.makeInitializer(this);\r\n+        if (tf.scope().env() instanceof Graph) tf.initAdd(initializer);\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExNTYyNQ==", "bodyText": "I can add this to the Ops class! Which file do I change? tensorflow-core/tensorflow-core-api/gen/annotations/org/tensorflow/op/Ops.java is generated, right?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407115625", "createdAt": "2020-04-11T22:02:04Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,210 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.BatchDataset;\r\n+import org.tensorflow.data.impl.SkipDataset;\r\n+import org.tensorflow.data.impl.TakeDataset;\r\n+import org.tensorflow.data.impl.TensorSliceDataset;\r\n+import org.tensorflow.op.Op;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+    protected Ops tf;\r\n+    private List<DataType<?>> outputTypes;\r\n+    private List<Shape> outputShapes;\r\n+\r\n+    public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+        if (Objects.isNull(tf)) {\r\n+            throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+        } else if (outputTypes.size() != outputShapes.size()) {\r\n+            throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+        }\r\n+\r\n+        this.tf = tf;\r\n+        this.outputTypes = outputTypes;\r\n+        this.outputShapes = outputShapes;\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     *\r\n+     * @param batchSize     The number of desired elements per batch\r\n+     * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+     *                      than `batchSize` elements.\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+        List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+                .map(s -> Shape.of(batchSize, s.asArray()))\r\n+                .collect(Collectors.toList());\r\n+\r\n+\r\n+        return new BatchDataset(tf, this.getVariant(), tf.constant(batchSize),\r\n+                tf.constant(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     * Includes the last batch, even if it has fewer than `batchSize` elements.\r\n+     *\r\n+     * @param batchSize The number of desired elements per batch\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize) {\r\n+        return batch(batchSize, false);\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` which skips `count` initial elements from this\r\n+     * dataset\r\n+     *\r\n+     * @param count The number of elements to `skip` to form the new dataset.\r\n+     * @return A new Dataset with `count` elements removed.\r\n+     */\r\n+    public final Dataset skip(long count) {\r\n+        return new SkipDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` with only the first `count` elements from this\r\n+     * dataset.\r\n+     *\r\n+     * @param count The number of elements to \"take\" from this dataset.\r\n+     * @return A new Dataset containing the first `count` elements from this dataset.\r\n+     */\r\n+    public final Dataset take(long count) {\r\n+        return new TakeDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+     * Each batch is a list of components, returned as `Output` objects.\r\n+     * <p>\r\n+     * This method enables for-each iteration through batches when running\r\n+     * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+     *\r\n+     * @return an Iterator through batches of this dataset.\r\n+     */\r\n+    @Override\r\n+    public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+        if (!(tf.scope().env() instanceof EagerSession)) {\r\n+            throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+        }\r\n+\r\n+        DatasetIterator iterator = makeOneShotIterator();\r\n+\r\n+        return new Iterator<List<Output<?>>>() {\r\n+            private List<Output<?>> tryNext = getNext();\r\n+\r\n+            private List<Output<?>> getNext() {\r\n+                try {\r\n+                    return iterator.getNext();\r\n+                } catch (IndexOutOfBoundsException e) {\r\n+                    return null;\r\n+                }\r\n+            }\r\n+\r\n+            @Override\r\n+            public boolean hasNext() {\r\n+                return tryNext != null;\r\n+            }\r\n+\r\n+            @Override\r\n+            public List<Output<?>> next() {\r\n+                List<Output<?>> result = tryNext;\r\n+                tryNext = getNext();\r\n+                return result;\r\n+            }\r\n+        };\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate\r\n+     * over elements of this dataset.\r\n+     *\r\n+     * This iterator will have to be initialized with a call\r\n+     * to `iterator.makeInitializer(Dataset)` before elements\r\n+     * can be retreived in a loop.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeInitializeableIterator() {\r\n+        return DatasetIterator\r\n+                .fromStructure(tf, outputTypes, outputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate over\r\n+     * elements of this dataset. Using `makeOneShotIterator` ensures\r\n+     * that the iterator is\r\n+     * automatically initialized on this dataset.\r\n+     *skips\r\n+     * In graph mode, the initializer op will be added to the Graph's\r\n+     * intitializer list, which must be run via `tf.init()`:\r\n+     *\r\n+     * Ex:\r\n+     * <pre>\r\n+     *     try (Session session = new Session(graph) {\r\n+     *         // Immediately run initializers\r\n+     *         session.run(tf.init());\r\n+     *     }\r\n+     * </pre>\r\n+     *\r\n+     * In eager mode, the initializer will be run automatically as a result\r\n+     * of this call.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeOneShotIterator() {\r\n+        DatasetIterator iterator = makeInitializeableIterator();\r\n+        Op initializer = iterator.makeInitializer(this);\r\n+        if (tf.scope().env() instanceof Graph) tf.initAdd(initializer);\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3MDMxMw=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNjIxNA==", "bodyText": "I added this method to the ExecutionEnvironment class; so we can use tf.scope().env().isEager()  or isGraph() instead.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407836214", "createdAt": "2020-04-14T02:56:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/src/main/java/org/tensorflow/data/Dataset.java", "diffHunk": "@@ -0,0 +1,210 @@\n+package org.tensorflow.data;\r\n+\r\n+import org.tensorflow.*;\r\n+import org.tensorflow.data.impl.BatchDataset;\r\n+import org.tensorflow.data.impl.SkipDataset;\r\n+import org.tensorflow.data.impl.TakeDataset;\r\n+import org.tensorflow.data.impl.TensorSliceDataset;\r\n+import org.tensorflow.op.Op;\r\n+import org.tensorflow.op.Ops;\r\n+import org.tensorflow.tools.Shape;\r\n+\r\n+import java.util.Iterator;\r\n+import java.util.List;\r\n+import java.util.Objects;\r\n+import java.util.stream.Collectors;\r\n+\r\n+/**\r\n+ * Represents a potentially large list of independent elements (samples), and\r\n+ * allows iteration and transformations to be performed across these elements.\r\n+ */\r\n+public abstract class Dataset implements Iterable<List<Output<?>>> {\r\n+    protected Ops tf;\r\n+    private List<DataType<?>> outputTypes;\r\n+    private List<Shape> outputShapes;\r\n+\r\n+    public Dataset(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes) {\r\n+        if (Objects.isNull(tf)) {\r\n+            throw new IllegalArgumentException(\"Ops accessor cannot be null.\");\r\n+        } else if (outputTypes.size() != outputShapes.size()) {\r\n+            throw new IllegalArgumentException(\"`outputTypes` and `outputShapes` must have the same size.\");\r\n+        }\r\n+\r\n+        this.tf = tf;\r\n+        this.outputTypes = outputTypes;\r\n+        this.outputShapes = outputShapes;\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     *\r\n+     * @param batchSize     The number of desired elements per batch\r\n+     * @param dropLastBatch Whether to leave out the final batch if it has fewer\r\n+     *                      than `batchSize` elements.\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize, boolean dropLastBatch) {\r\n+        List<Shape> batchOutputShapes = getOutputShapes().stream()\r\n+                .map(s -> Shape.of(batchSize, s.asArray()))\r\n+                .collect(Collectors.toList());\r\n+\r\n+\r\n+        return new BatchDataset(tf, this.getVariant(), tf.constant(batchSize),\r\n+                tf.constant(dropLastBatch), this.getOutputTypes(), batchOutputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Groups elements of this dataset into batches.\r\n+     * Includes the last batch, even if it has fewer than `batchSize` elements.\r\n+     *\r\n+     * @param batchSize The number of desired elements per batch\r\n+     * @return A batched Dataset\r\n+     */\r\n+    public final Dataset batch(long batchSize) {\r\n+        return batch(batchSize, false);\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` which skips `count` initial elements from this\r\n+     * dataset\r\n+     *\r\n+     * @param count The number of elements to `skip` to form the new dataset.\r\n+     * @return A new Dataset with `count` elements removed.\r\n+     */\r\n+    public final Dataset skip(long count) {\r\n+        return new SkipDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Returns a new `Dataset` with only the first `count` elements from this\r\n+     * dataset.\r\n+     *\r\n+     * @param count The number of elements to \"take\" from this dataset.\r\n+     * @return A new Dataset containing the first `count` elements from this dataset.\r\n+     */\r\n+    public final Dataset take(long count) {\r\n+        return new TakeDataset(tf, this.getVariant(), tf.constant(count), this.getOutputTypes(), this.getOutputShapes());\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates an iterator which iterates through all batches of this Dataset in an eager fashion.\r\n+     * Each batch is a list of components, returned as `Output` objects.\r\n+     * <p>\r\n+     * This method enables for-each iteration through batches when running\r\n+     * in eager mode. For Graph mode batch iteration, see `makeOneShotIterator`.\r\n+     *\r\n+     * @return an Iterator through batches of this dataset.\r\n+     */\r\n+    @Override\r\n+    public Iterator<List<Output<?>>> iterator() {\r\n+\r\n+        if (!(tf.scope().env() instanceof EagerSession)) {\r\n+            throw new UnsupportedOperationException(\"Cannot iterate through a dataset in graph mode.\");\r\n+        }\r\n+\r\n+        DatasetIterator iterator = makeOneShotIterator();\r\n+\r\n+        return new Iterator<List<Output<?>>>() {\r\n+            private List<Output<?>> tryNext = getNext();\r\n+\r\n+            private List<Output<?>> getNext() {\r\n+                try {\r\n+                    return iterator.getNext();\r\n+                } catch (IndexOutOfBoundsException e) {\r\n+                    return null;\r\n+                }\r\n+            }\r\n+\r\n+            @Override\r\n+            public boolean hasNext() {\r\n+                return tryNext != null;\r\n+            }\r\n+\r\n+            @Override\r\n+            public List<Output<?>> next() {\r\n+                List<Output<?>> result = tryNext;\r\n+                tryNext = getNext();\r\n+                return result;\r\n+            }\r\n+        };\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate\r\n+     * over elements of this dataset.\r\n+     *\r\n+     * This iterator will have to be initialized with a call\r\n+     * to `iterator.makeInitializer(Dataset)` before elements\r\n+     * can be retreived in a loop.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeInitializeableIterator() {\r\n+        return DatasetIterator\r\n+                .fromStructure(tf, outputTypes, outputShapes);\r\n+    }\r\n+\r\n+    /**\r\n+     * Creates a `DatasetIterator` that can be used to iterate over\r\n+     * elements of this dataset. Using `makeOneShotIterator` ensures\r\n+     * that the iterator is\r\n+     * automatically initialized on this dataset.\r\n+     *skips\r\n+     * In graph mode, the initializer op will be added to the Graph's\r\n+     * intitializer list, which must be run via `tf.init()`:\r\n+     *\r\n+     * Ex:\r\n+     * <pre>\r\n+     *     try (Session session = new Session(graph) {\r\n+     *         // Immediately run initializers\r\n+     *         session.run(tf.init());\r\n+     *     }\r\n+     * </pre>\r\n+     *\r\n+     * In eager mode, the initializer will be run automatically as a result\r\n+     * of this call.\r\n+     *\r\n+     * @return A new `DatasetIterator` based on this dataset's structure.\r\n+     */\r\n+    public DatasetIterator makeOneShotIterator() {\r\n+        DatasetIterator iterator = makeInitializeableIterator();\r\n+        Op initializer = iterator.makeInitializer(this);\r\n+        if (tf.scope().env() instanceof Graph) tf.initAdd(initializer);\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3MDMxMw=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU3OTMzOnYy", "diffSide": "RIGHT", "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxNDozMzoxNVrOGENocA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxNDozMzoxNVrOGENocA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3MDgzMg==", "bodyText": "Oops, I don't think this reformatting was meant to be part of the PR", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407070832", "createdAt": "2020-04-11T14:33:15Z", "author": {"login": "karllessard"}, "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "diffHunk": "@@ -21,122 +21,159 @@\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjY0MjIyOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": false, "comments": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxNTo0Nzo0NVrOGEOHXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMDowNjo0OVrOGMMFVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ==", "bodyText": "I think we can get avoid having this exception as part of the normal workflow if we use tf.data.iteratorGetNextAsOptional instead of tf.data.iteratorGetNext, but that might require other changes as well in the graph so it is handle properly.\nMaybe the framework can automatically add what is required to do it? I did not went very far in my analysis, I just dropped the idea here so you can check if it make sense or not.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407078751", "createdAt": "2020-04-11T15:47:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTkxNA==", "bodyText": "Another solution could be to add a method called runAllBatches() in the Session or the SessionRunner, which accept a lambda from the user so he can intercept the output tensors after each run if desired. The runAllBatches() method would hide these intrinsic details of the loop that are not common for a framework library like TF Java (e.g. the while (true) and the try/catch statements)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407099914", "createdAt": "2020-04-11T19:11:35Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDUxNA==", "bodyText": "What would be interesting about that idea of accepting a lambda to consume the output tensors is that the framework could take care of freeing them after the lambda call is completed (or the lambda could return true or false, depending if the tensors reference should remain valid or not).\nThis way, we would avoid this annoying try-with-resources block when fetching the output tensors. So we could add it to other run methods as well. Wdyt? and @Craigacp ?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407100514", "createdAt": "2020-04-11T19:17:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExMTM1Mg==", "bodyText": "Thanks for the suggestion Karl! I experimented with the iteratorGetNextAsOptional; it does make eager-mode iteration cleaner!\nUsing a DatasetIterator directly for eager-mode iteration, the syntax looks like\nDataset dataset = Dataset.fromTensorSlices(tf, tensors, dataTypes);\nDatasetIterator iterator = dataset.makeOneShotIterator();\n        \nDatasetOptional nextOptional = iterator.getNextAsOptional();\nwhile (nextOptional.hasValue().data().getBoolean()) {\n    List<Output<?>> outputs = nextOptional.getValue();\n    // use outputs in some computation.\n    nextOptional = iterator.getNextAsOptional();\n}\n\nthough this is still wrapped by the Dataset Iterable, so that the for-loop can be used:\nfor (List<Output<?>> outputs : dataset) { ... }\nIn graph mode, this will get tougher. In a graph session, calling tf.data.optionalGetValue on an optional that doesn't have a value throws an IllegalArgumentException. In the Python tests for iteratorGetNextAsOptional, you can see that this exception is expected there also (see the test here). Looks like they are stuck with the same issue :).\nIn graph mode, there isn't an easy way of making sure that we only make the iteratorGetNextAsOptional call if tf.data.optionalHasValue returns true.\nFetching hasValue in it's own session.run doesn't work either, as every session.run will advance the iterator position.\nThe only other thing I can think of is to use control flow ops. The two options there would be to\n\n\nUse a switch to introduce a branch on hasValue; I looked into this but I think there are some constraints that make this harder than it looks.\n\n\nUse a graph whileLoop to run the entire iteration as a single sesion.run()... I think that may be a nice way of implementing the runAllBatches you mentioned, Karl; but still within a single session.run(), but until we have a working graph while loop, this will be very hard.\n\n\nI'd love to look into both of these; but I think they are complexities we may not want to put on the user yet. I'm inclined to stick with the IndexOutOfBoundsException in graph mode for now. Even if it's sub optimal Java iteration style, the pattern is familiar to tensorflow data users.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407111352", "createdAt": "2020-04-11T21:10:07Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI1Njk4MQ==", "bodyText": "Ok, the idea behind the runAllBatches method would be to continue using IndexOutOfBoundException to stop the epoch iteration but to do it internally, something like this:\nIn Session.Runner:\n// consumer receives in parameter the batch number and the result of the run for this batch,\n// and returns a boolean telling if if it is safe to release the fetched tensors (true) or not\nvoid runAllBatches(BiFunction<Integer, Session.Run, Boolean> consumer) {\n  int batch = 0;\n  while (true) {\n    try {\n      Run run = run();\n      if (consumer.apply(batch++, run)) {\n        run.outputs.forEach(Tensor::close);  \n      }\n    } catch (IndexOutOfBoundException e) {\n      break; // end of the loop   \n    }\n  }  \n}\nIn user space:\nvoid trainGraph(Graph g) {\n  try (Session s = new Session(g)) {\n    s.runner().addTarget(trainOp).fetch(loss).runAllBatches((batch, run) -> {\n      // print loss, etc.\n      return true;\n    });\n  }\n}\nAnd don't have to restrict this signature for dataset batches only, we could add a consumer parameter to other run method as well. What I like is that we remove from the user the burden of releasing the fetched tensors himself, plus the new one of catching an exception to stop the training loop. I just feel that we are expecting too much knowledge from the user to be able to use our framework properly, while Java developers are more used with \"don't-worry-we'll-take-care-of-it\" libraries :).\nIn comparison, the user code without the proposed changes would look like this.\nvoid trainGraph(Graph g) {\n  try (Session s = new Session(g)) {\n    int batch = 0;\n    while (true) {\n      List<Tensor> tensors;\n      try {\n        tensors = s.runner().addTarget(trainOp).fetch(loss).run().outputs;\n        // print loss, etc.\n        ++batch;\n      } catch (IndexOutOfBoundException e) {\n        break;\n      } finally {\n        tensors.forEach(Tensor::close());\n      }\n  }\n}", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407256981", "createdAt": "2020-04-12T21:56:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI4MzQzOA==", "bodyText": "Hey @karllessard , this could be quite interesting! I think the mechanics will get quite difficult, though. The primary operation users will want to do with an iteration through all batches is to aggregate a value emitted from the batch (e.g. batchLoss, batchAccuracy) over the entire epoch.\nWith a lambda on each batch, it becomes more complicated to perform a \"epoch loss update\" like this, since within a lambda you can't modify non-final variables. So something like this won't be possible without additional machinery:\ntry (Session session = new Session(g)) {\n    for (int epoch = 0; epoch < 10; epoch++) {\n        int epochLoss = 0;\n        while (true) {\n            session.runner()\n                .addTarget(trainOp)\n                .fetch(loss)\n                .runAllBatches((batch, run) -> {\n                   epochLoss += run.loss; // not easily possible\n                   return true;\n            });\n        } catch (IndexOutOfBoundsException e) {\n            break;\n        }\n    }\n}\n\nIn the interest of simplifying things where possible, I had an idea for introducing a Session.Results (like the Session.Run you mention) to wrap the List<Tensor<?>> output from a run() call. See a potential implementation here.\nWe can reduce the boilerplate involved in managing / unwrapping this list, so that in a standard training loop, we get:\ntry (Session session = new Session(g)) {\n    for (int epoch = 0; epoch < 10; epoch++) {\n        int epochLoss = 0;\n        int epochAccuracy = 0;\n\n        while (true) {\n            try (Session.Results results = session.runner()\n                        .addTarget(trainOp)\n                        .fetch(loss)\n                        .fetch(accuracy)\n                        .fetch(prediction)\n                        .run()) {\n            \n                // Outputs are retrieved from `results` with a sequence \n                // of pop() calls matching the order of fetch() calls.\n\n                // Simple helper functions to fetch scalar primitive values.\n                float batchLoss = results.popFloat();\n                float batchAccuracy = results.popFloat();\n\n                // Just an example, in case we want to retrieve a large tensor\n                Tensor<TFloat32> batchPrediction = results.pop(TFloat32.DTYPE);\n\n                epochLoss += batchLoss;\n                epochAccuracy += batchAccuracy;\n            }\n      } catch (IndexOutOfBoundsException e) {\n           break;\n      }\n}\n\nThis adds savings in a few places:\n\nSimplifies the arguments to the try-with-resources block; only one statement is needed, since the entire Session.Result object is Autocloseable\nSimplifies the call chain required to extract typed tensors (and even more for scalar values)\nEliminates the need to access individual indices of an output list; calling pop in the same order as items were fetched is easier than keeping track of indices.\n\nWith the current setup we'd need to have\ntry (Session session = new Session(g)) {\n    for (int epoch = 0; epoch < 10; epoch++) {\n        int epochLoss = 0;\n        int epochAccuracy = 0;\n        while (true) {\n           List<Tensor<?>> outputs = session.runner()\n                        .addTarget(trainOp)\n                        .fetch(loss)\n                        .fetch(accuracy)\n                        .fetch(prediction)\n                        .run();\n\n            try (Tensor<TFloat32> lossTensor = outputs.get(0).expect(TFloat32.DTYPE);\n                  Tensor<TFloat32> accuracyTensor = outputs.get(1).expect(TFloat32.DTYPE);\n                   Tensor<TFloat32> predictionTensor = outputs.get(2).expect(TFloat32.DTYPE);) {\n\n                float batchLoss = lossTensor.data().getFloat();\n                float batchAccuracy = accuracyTensor.data().getFloat();\n\n                epochLoss += batchLoss;\n                epochAccuracy += batchAccuracy;\n        }  catch (IndexOutOfBoundsException e) {\n            break;\n        }\n    }\n}\n\nWhat do you think?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407283438", "createdAt": "2020-04-13T01:53:42Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI5MDkzOQ==", "bodyText": "I was afraid that trying to modify final variables in the lambda could be a problem, your example is a good one... like you said, there are ways to go around this (e.g. using AtomicInteger) but maybe not as trivial as we would like.\nOverall, I like your proposition (I think we should keep Session.Run and just make it auto-closeable and add the pop* methods, as this class already exists). For sure, it looks better than it is right now. The \"expected exception\" mechanism is still required though... I'd be curious to know what other members think about this.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407290939", "createdAt": "2020-04-13T02:37:08Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzMwNDgwNQ==", "bodyText": "Ahh, yes I didn't realize Session.Run already exists; yup it makes sense to use it!\n\nThe \"expected exception\" mechanism is still required though\n\nYup it will be for now ... Definitely would love to keep exploring alternative mechanisms here! If everyone uses Keras model fitting, no one will be the wiser! ;)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407304805", "createdAt": "2020-04-13T03:55:15Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzQ4ODI3NA==", "bodyText": "I've done something similar to Session.Run in the onnx-runtime Java API (https://github.com/microsoft/onnxruntime/blob/54bbbb78ae3eefe69bd27f71dcc5affa977842c2/java/src/main/java/ai/onnxruntime/OrtSession.java#L613) which works fairly well, though it still feels a bit suboptimal (which I think is mainly because I'm fighting the type system a bit).\nGiven the purpose of the run all batches code would be to iterate a dataset then we should try to make it safer. What happens if a DatasetIterator isn't defined on the graph? Also I think that if we go down this path then we could modify the function to return either T and let the user unpack it how they wish, or some sort of ComputationResult<T> which is a tuple of a T and a boolean if we want to persist the tensors outside the run. I think that not closing the tensors in this kind of use case for training wouldn't be that likely, so I'm not sure we'd need to support it.\nDoes the IndexOutOfBoundsException come from here (\n  \n    \n      java/tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java\n    \n    \n         Line 80\n      in\n      9fe9d87\n    \n    \n    \n    \n\n        \n          \n           throw new IndexOutOfBoundsException(TF_Message(s).getString()); \n        \n    \n  \n\n)? I've not had chance to rebuild TF with your branch yet, so I can't quickly tell.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407488274", "createdAt": "2020-04-13T13:51:36Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2MzA3NQ==", "bodyText": "It must come from there yes, that\u2019s where TF statuses returned by the C API are mapped to exceptions.\nIt now sounds to me @dhruvrajan that we should go with your original proposal for now and work to improve the Session API as a separate topic that requires more thinking on how to manage native resources in general. It will probably end up wrapping existing mechanisms so it shouldn\u2019t break too much the backward compatibility with what we are doing here. What do you guys think?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407763075", "createdAt": "2020-04-13T22:46:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODE4MQ==", "bodyText": "That sounds good to me, it's a great idea to have a specific investigation into improving the Session / other native resources APIs. A helpful thing to focus on there might be some form of (potentially unified) management of \"results lists\": List<Tensor<?>> (graph mode) and List<Output<?>> (eager mode). The close() and pop() methods in Session.Run I proposed above help some for Graph Sessions; it'd be awesome to find a way to do the same (maybe duplicate those?) for eager mode.\nWe can move forward with the current proposal, and hopefully update it once we think of something more user-friendly!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r407838181", "createdAt": "2020-04-14T03:03:34Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE1MDc5OQ==", "bodyText": "I would prefer that this API be disabled for Graphs rather than have anything that caught a runtime exception in it. The IndexOutOfBoundsException can be thrown in multiple places from the session.run() call, as it returns a List<Tensor<?>> which throws IndexOutOfBoundsException if the list.get() call is out of bounds. This means while a user is developing their code it will just silently do nothing if they forget to add a fetch target, as the exception thrown from list.get(0) will cause the loop to break. Similarly if the graph ever returns a different number of tensors than requested, it will cause the loop to silently break out.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408150799", "createdAt": "2020-04-14T13:49:37Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE4NDkzMw==", "bodyText": "Hmm... A possible solution (though it's high touch) could be to create a class of \"TensorFlow Exceptions\" in Java, and map to those instead of standard Java Exceptions in Abstract_TF_Status.\nThis way, we'd know that by catching a standard Java exception, we're handling a problem resulting from Java code, and by catching a TensorFlowException, we're handling an issue potentially caught at the C level (like a tf data out of range error)\nEven without this though, I don't think the ambiguity IndexOutOfBounds introduces is a huge problem, since it's nearly the same ambiguity Python users have, unless I'm misunderstanding...\nWhat do you think @Craigacp ?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408184933", "createdAt": "2020-04-14T14:33:43Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5MTgyMg==", "bodyText": "Actually, in Python, the error that is caught is a tf.errors.OutOfRangeError\nAdding something like this would at least resolve the ambiguity problem, right?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408191822", "createdAt": "2020-04-14T14:42:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODIyNDIyOQ==", "bodyText": "I stand by my previous statement on runtime exceptions, and it's really that exceptions shouldn't be control flow (e.g. Effective Java 3rd Ed Item 69). I'm a bit less worried by it being a checked exception out of the run method that we force users to deal with, but that's horrible for everyone who doesn't use the dataset api. If we do change the exception name then we need to figure out what other cases can throw that kind of exception out of the TF runtime, as if it's anything other than dataset iteration exhaustion then in my view that's a non-starter.\nOverall I think this should be a Java version of TF, and this is a python-ism that goes against how Java libraries should work.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408224229", "createdAt": "2020-04-14T15:22:47Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODk5MQ==", "bodyText": "I agree with Adam that the Java solution shouldn\u2019t be based on the Python one since they are two very different languages with different profile of users. And in Java, an exception is normally an unexpected error.\nSo it might make sense for users jumping from TF Python to Java to see this mechanism in place but for someone who just starts from Java, it looks like a flaw in the library design.\nWhat about Runner.runNextBatch then which returns true if the next batch did not throw (internally) an exception and false if it did?Then you can retrieve the last Run metadata by getting Runner.lastRun() with a try-with-resources block. i.e.\nRunner runner = s.runner().fetch(loss);\nwhile (runner.runNextBatch()) {\n  try (Run r = runner.lastRun()) {\n    // play with data\n  }\n}\nor something like that (I\u2019m on my phone so it\u2019s not the best tool for writing a complete solution...)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408318991", "createdAt": "2020-04-14T17:41:47Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODA0OA==", "bodyText": "That's fine by me. I think we'd need to hammer out some scoping issues (i.e. when the tensors get closed that the runner holds), and also what the semantics of that method is when there is no datasetiterator defined. I was also considering making the datasetiterator on the Java side add a boolean flag node to the TF graph saying when it wasn't exhausted, and we could check for that.\nMore generally what are the semantics of the iterator if you save the graph to disk and load it back up again? And do we want it to work on iterators that were embedded by python graphs (as those wouldn't have the boolean flag).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408328048", "createdAt": "2020-04-14T17:56:08Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc4NDU5MA==", "bodyText": "I agree @karllessard, catching the exception internally is probably the best way to limit the effect of this on users.\nThe runNextBatch() / lastRun flow can work, but needs some slightly complex state management in Runner. Instead of keeping track of the state of previous results in the Runner class, we could have it return an Optional<Run>:\nRunner runner = session.runner().fetch(loss);\n\nOptional<Run> nextBatch;\nwhile ((nextBatch = session.runner().runNextBatch()).isPresent()) {\n   try (Run run = nextBatch.get()) {\n       // play with data\n    }\n}\nAfter experimenting more, though, I think that we can do much better using streams, and allowing the ability to \"take\" from, \"limit\", and \"map\" over a stream of session.run() calls.\nWe can introduce a method repeat() in Session.Runner that returns an \"infinite\" stream of Session.Run. To handle the iteration, the stream will end once we catch an IndexOutOfBoundsException.\n/**\n * Creates a stream of `Session.Run` objects which can be\n * processed to retrieve the results of repeated calls\n * to Session.Runner.run().\n *\n * Each item in the stream contains the results corresponding\n * to \"fetched\" tensors from this Runner.\n *\n * When the graph contains a dataset iterator, the stream\n * will end once the iterator has reached the end of\n * its iteration.\n *\n * To process only a prefix of this stream, a limit can manually\n * specified using Stream.limit( ... ). Other Stream operators\n * can also be used.\n *\n * @return A Stream of Session.Run items containing results\n *         of repeated calls to Session.Runner.run()\n */\npublic Stream<Session.Run> repeat();\n\nWe can then use map() with a lambda to process batch updates, similarly to @karllessard's earlier proposal. While we still can't update primitive values, by creating a Logs object\ncontaining the fields we want to extract, users can quite cleanly retrieve values that they\nwant to save.\nclass EpochLogs \n    float loss = 0;\n    float accuracy = 0;\n    int batches = 0;\n}\n\nWith such a class, the training loop becomes quite simple:\n// Run training loop\nfor (int i = 0; i < EPOCHS; i++) {\n    // reset iterator object\n    session.run(initTrainIterator);\n    \n    EpochLogs epoch = new EpochLogs();\n    session.runner()\n        .addTarget(optimizerTargets)\n        .fetch(trainLoss)\n        .fetch(trainAccuracy)\n        .repeat() // Create a stream that ends once the iterator is exhausted.\n        .forEach(outputs -> {\n            epoch.loss += outputs.popFloat();  // here imagine output.get(...).expect(...).data().getFloat() . \n            epoch.accuracy += outputs.popFloat();\n            epoch.batches++;\n        });\n\n    System.out.println(\"Epoch Accuracy \" + i \n        + \": \" + epoch.accuracy / epoch.batches);\n}\nThere's a couple nice things about this:\n\nRunner.repeat() can be used for ANY tensorflow graph, even if it doesn't explicitly use\nDataset ops.\nThe stream iteration takes care of closing the tensors returned each batch.\nUsers can use the extensive set of Stream operators to control their loops.\nLogical distinction between Session and Dataset is maintained.\n\nIt's a little toubling that we have to catch an IndexOutOfBoundsException to handle exception from tensorflow-core, I think it'd be a great idea in a follow-up PR to add a set of exceptions extending TensorFlowException , and map to these in TF_Abstract_Status.java, so that we at least know when we are catching an issue from tensorflow-core.\nWhat do you think? I've quite come around to liking lambda-style iteration here. See MNISTBasicGraphClassifier.java for a more complete example.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410784590", "createdAt": "2020-04-19T01:07:56Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDgwMzk2Nw==", "bodyText": "More generally what are the semantics of the iterator if you save the graph to disk and load it back up again?\n\n@Craigacp that's a pretty interesting point, I haven't tried it (not familiar with the graph-to-disk saving and loading). But since the iterator state is saved in the graph, saving to disk and re-loading should save the state of the iterator.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410803967", "createdAt": "2020-04-19T03:30:52Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDkyNjI0Mw==", "bodyText": "Hey @dhruvrajan , I like it! I\u2019m not too sure if we should provide the EpochLogs class or just let the user create it based on his needs but that\u2019s a detail.\nFor auto-closing all tensors after each batch, that can do to begin with but at some point, we might allow the user to also retain a reference to one or more of them if he wants to. Reference counting added lately to JavaCPP might be helpful here.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410926243", "createdAt": "2020-04-19T15:13:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDkyNzgxMA==", "bodyText": "@karllessard Cool! Yeah, that makes sense, that'll be quite cool if we can let the user maintain the tensor objects when useful.\nAs part of the Keras library, we'll actually have to add these Logs classes anyway, for now I think it's ok to leave that to the user, since that's the only way to allow full flexibility with the fields.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410927810", "createdAt": "2020-04-19T15:21:26Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2MDY4NQ==", "bodyText": "To clarify the proposed semantics for session.runner().repeat() are that it repeats until it hits an IndexOutOfBoundsException, which it silently swallows and the stream terminates?\nIf so in addition to the case where it works for the dataset iterator, there are two more cases, one where it goes forever executing identical operations (i.e. feeding the same tensors and receiving the same output), and another where TF_OUT_OF_RANGE is thrown by something else in the TF code which silently ends the execution (possibly in the middle of a dataset iteration due to a coding error). It looks like TF_OUT_OF_RANGE (or equivalently OUT_OF_RANGE) get thrown from a variety of different places in the TF runtime, not just from exhausted dataset iterators. This latter case is worrying to me as it could lead to situations which are very difficult to debug as we expect it to do one thing and silently eat the error message when it died for some other reason.\nAlso, the stream API removes control from the user of when the computation happens. This is usually not an issue as the laziness usually helps the compiler orchestrate things better, but TF computations are big and not as thread safe as we'd like them to be, because they modify mutable state. The spliterator built out of the iterator should have the IMMUTABLE and ORDERED characteristics set on it, which should prevent some of the oddities. Fortunately the IteratorSpliterator.trySplit() method will buffer elements of the underlying iterator into an ArraySpliterator, triggering TF computation sequentially, so if someone does call stream.parallel() on the output it should have the correct semantics. This computation happens up front when the stream is polled, rather than when the elements of the stream are actually processed by the end user, so that might be a little weird for them.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410960685", "createdAt": "2020-04-19T18:16:48Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2Njc4OA==", "bodyText": "Yup that's correct, the stream will terminate when the IndexOutOfBoundsException is caught.\n\none where it goes forever executing identical operations (i.e. feeding the same tensors and receiving the same output)\n\nTo handle this case, users can specify a limit on the number of iterations with Stream.limit. In Java11, they can also use Stream.takeWhile, which is much more flexible.\n\nand another where TF_OUT_OF_RANGE is thrown by something else in the TF code which silently ends the execution\n\nDefinitely, this is an unfortunate property of the TF runtime. I think the best we can do is make sure we treat these separately from other Java exceptions (i.e. not map this error to IndexOutOfBounds). This will a problem that exists across every TensorFlow system that uses the standard TF runtime.\nI think that's why the \"safest\" thing to do is to surface the error to the user,  allow them to handle it, look at the actual error message (which states that the iterator is exhausted), etc, to distinguish it from other errors while debugging. Users can still use the while loop and catch syntax if they prefer.\nBut there doesn't seem to be a both safe and easy way of providing an API which hides the exception, so that's the tradeoff to make here. It sounds like we are clear that we should provide an API which doesn't require exception-handling on the user side; I think this would be a decent way of making that tradeoff, while providing additional benefits to the user.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410966788", "createdAt": "2020-04-19T18:50:51Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk3NDM5MA==", "bodyText": "Ahh these are really interesting points, we will have to pay close attention to the stream semantics.\n\nthe stream API removes control from the user of when the computation happens\n\nIt's true that the user won't be making the individual session.run() calls; but apart from this, but I don't think it would cause any major problems / slowdowns? Do you know of cases where this could cause an inadvertent performance issue?\n\nthis computation happens up front when the stream is polled, rather than when the elements of the stream are actually processed by the end user\n\nHmm... I'm not too familiar with Stream internals, what do you mean by polling vs processing by the user? Is it that all batch session.run() calls will occur up front, before any of the user lambdas are run as a separate stage over the Run objects? I think that would be ok, as far as I know that's expected stream behavior and hopefully doesn't result in too much confusion.\nDo you see any cases where it would be useful to run some tests, to make sure that this doesn't result in actual bugs?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410974390", "createdAt": "2020-04-19T19:31:44Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTAyOTc2NQ==", "bodyText": "If they call stream.parallel() and then some terminal operation, all the computation will happen in the single thread dividing up the work for the parallel workers, as it happens on the call to next in the iterator and that's called repeatedly by IteratorSpliterator.trySplit() as it builds up a chunk to hand to an ArraySpliterator. Usually stream creation doesn't involve this kind of overhead and it's not really expected behaviour that the computation happens while the stream is being chunked, as it tends to mess with the sizing internals of the Spliterator (e.g. it uses an increasingly large buffer before chunking off the work, and this can cause issues with IO operations. I had to implement my own Spliterator to get around this in my Java word2vec implementation). I'm referring to this as polling the stream, where it chunks the work up, rather than processing (i.e. when the individual workers start executing the chain of operations leading to the terminal one). It's not quite the right terminology, but to talk about it precisely requires going through the internals in some detail.\nI feel this is trying to paper over an API that doesn't really work in Graph mode, especially in Java. Unless we know for sure that this code can iterate over a graph saved out from python using Python's dataset iterator, then I'm not sure what the benefit is of forcing Java to behave like this. Maybe we should build our own iteration operation that exposes the necessary checks that we can tightly monitor from the Java side? Either by wrapping the current dataset iterator ops with something that can grab the OUT_OF_RANGE status exactly where it occurs in only the iterator exhausted case, or by building something that slices the tensors by hand and serves up another one each time entirely from primitive ops we control (with appropriate teaching of the Session API so it has appropriate entry points for pumping an iterator, that are strongly typed and prevent users from making mistakes).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411029765", "createdAt": "2020-04-20T00:43:38Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA3MjkzOA==", "bodyText": "wrapping the current dataset iterator ops with something that can grab the OUT_OF_RANGE status exactly where it occurs in only the iterator exhausted case\n\nThis would certainly be great; but it requires a huge amount of effort, not just on the Java side, but within the C++ runtime. That seems too complicated for the scope of this PR.\nWe have considered a few alternatives; I think it'd be best to choose one, move forward with it, and revisit the semantics of graph-mode dataset iterators when we have better iterator support in Java.\n\n\nStandard tf.data syntax: Propagate the IndexOutOfBounds exception to the user so it can be caught in a while(true) loop. The user handles success checking (catching the error), as well as closing the returned result tensors.\n\n\nCatch the IndexOutOfBounds error internally, and return an Optional (or boolean) indicating success at each iteration to the user. The user has to manage closing the result tensors.\n\n\nMake repeated calls to session.run() internally, and allow the user to process results with a lambda, catching the error internally, and closing tensors automatically. This can be either a single function in Session, or accomplished via streams (I think, more flexible and usable).\n\n\nLet's pick one of these and revisit this later, as appropriate? I prefer the first, as it's the simplest, despite the obvious shortcomings.\n@karllessard if you think it would be helpful, I can consolidate each of these, and send out a poll in the mailing list.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411072938", "createdAt": "2020-04-20T03:43:30Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA3NDAxNw==", "bodyText": "Re: Stream.parallel, those are great points, and if we go that route, we should restrict the stream to run sequentially (return null from trySplit, or override parallel(), perhaps).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411074017", "createdAt": "2020-04-20T03:47:36Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQxNDkxNg==", "bodyText": "If we go with option 1, then at least we should make a new subclass of RuntimeException called TFOutOfRangeException and modify the TFStatus check to throw that rather than IndexOutOfBoundsException along with the necessary documentation changes. We should not throw IndexOutOfBoundsException in this case as it's semantically different to the TF_OUT_OF_RANGE \"error\".", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411414916", "createdAt": "2020-04-20T14:16:59Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQxOTExNA==", "bodyText": "Also any of these approaches should be documented with big warning flags around them saying this is going to change when we figure out a better way of doing it. I really really don't want to be stuck supporting this API for Graphs for a long time.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411419114", "createdAt": "2020-04-20T14:22:09Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgzNTQ0NQ==", "bodyText": "Option 1 is for sure the ugliest but the one having less impact on the Session API (in fact, it has none), therefore if we want to move that PR forward, we can go with this one and build something nicer on top of it at a later time.\nIn my initial proposal for handling the result in a lambda and auto-closing the tensors, there were no streams involve therefore no possible awkward behaviour with Stream like @Craigacp described (and which I did my best to understand :) ). Maybe we should go back to something like this then?\nBut again, if we are not ready to do it, this could be done later if we go with Option 1 first, without breaking backward compatibility.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411835445", "createdAt": "2020-04-21T03:09:17Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgzNzMyNw==", "bodyText": "...quite a thread guys, only to go back to the original solution \ud83d\ude04", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411837327", "createdAt": "2020-04-21T03:15:23Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg0NTY3OQ==", "bodyText": "Option 1 is for sure the ugliest but the one having less impact on the Session API (in fact, it has none)\n\nYup I totally agree. This seems to be safest, as it doesn't run the risk of hiding other errors from the user, unrelated to dataset iteration (like Adam mentioned, even if we introduce additional TensorFlow exceptions), which could make things pretty near impossible for the user to debug. That's my thinking here; better to not take this risk, until we are sure we have a better solution.\n\nMaybe we should go back to something like [lambdas] then?\n\nSyntax-wise, I do actually really like the lambda-iteration approaches; both with and without streams, the resulting code is quite clean. Would love start introducing this separately! Maybe we can attack this without the initial need to end iteration by catching the iterator exception :).\nThe thing that isn't yet (but can be) solved here is having an exit condition definable in the lambda. In runAllBatches we wait for the IndexOutOfBoundsException; in Java 11 we have takeWhile which can help solve some of these issues. Starting from the repeat abstraction we could add methods like repeatWhile(predicate), repeatFor(iterations), or repeatUntilFailure(consumer) etc... Even if we implement it without streams, we can get some nice syntax.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411845679", "createdAt": "2020-04-21T03:43:13Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQyMDU1OQ==", "bodyText": "I think we should break compatibility in that we change what exception is thrown by TF_OUT_OF_RANGE to a new thing subclassing RuntimeException. Remember one of the issues with the blind catching of the IndexOutOfBoundsException is that if the user does list.get(1) on the List<Tensor<?>> returned by session.runner().run() when there is only a single Tensor returned it will silently swallow the IndexOutOfBoundsException thrown by list.get().", "url": "https://github.com/tensorflow/java/pull/30#discussion_r412420559", "createdAt": "2020-04-21T19:10:38Z", "author": {"login": "Craigacp"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQzNjQwNw==", "bodyText": "@Craigacp totally agree here, like I mentioned before, I think this will be a great safety improvement.\nI can investigate this; I wonder how many places in our code are expecting IndexOutOfBoundsException; hopefully not too many.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r412436407", "createdAt": "2020-04-21T19:35:27Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMzY1Mg==", "bodyText": "I can investigate this; I wonder how many places in our code are expecting IndexOutOfBoundsException; hopefully not too many.\n\nI guess none? All IndexOutOfBoundException that we might catch in the core API are the real runtime exceptions, I don't think there is any coming from TF_Status mapping... but I didn't look neither.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r412633652", "createdAt": "2020-04-22T03:02:35Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTAwNw==", "bodyText": "We need to save that epic thread somewhere before closing it :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415399007", "createdAt": "2020-04-26T20:56:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzNDA2OA==", "bodyText": "Man, this truly is an epic thread. I'll save the link! :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415434068", "createdAt": "2020-04-27T00:06:49Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,220 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIsfor loading data of various formats\n+, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` abstraction represents a sequence of elements, where each element in the sequence is a collection (`List`) of tensors (or, \"components\").\n+\n+\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = StdArrays.ndCopyOf(\n+    new float[] {\n+        0,\n+        1,\n+        1,\n+        0\n+});\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Dataset transformations alter both the values and shapes of the original elements, and\n+return a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this elements can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of `Session`s\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in an `IndexOutOfBounds` exception.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODc1MQ=="}, "originalCommit": {"oid": "c8c50b8c61b52a4431a32488908d6d66563abd6f"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzODQyNzk4OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMzo1NToyOFrOGF65Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwMzoxMzoxOFrOGIwhLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg2MTAwMw==", "bodyText": "Mmh, I was expecting those changes to come in, from our conversation with @Craigacp , I thought we were more heading on just finding a way to prevent to user to catch an exception as part of the control flow.\nI don't discard this idea of \"popping\" the tensors out of the Run object but there might be more thinking to do around this and it is not required for supporting the Data API.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408861003", "createdAt": "2020-04-15T13:55:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +484,51 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public <T extends TType> Tensor<T> pop(DataType<T> dtype) {\n+      return outputs.get(index++).expect(dtype);\n+    }\n+\n+    public int popInt(long... coordinates) {\n+      return pop(TInt32.DTYPE).data().getInt(coordinates);\n+    }\n+\n+    public long popLong(long... coordinates) {\n+      return pop(TInt64.DTYPE).data().getLong(coordinates);\n+    }\n+\n+    public float popFloat16(long... coordinates) {\n+      return pop(TFloat16.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public float popFloat(long... coordinates) {\n+      return pop(TFloat32.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public double popDouble(long... coordinates) {\n+      return pop(TFloat64.DTYPE).data().getDouble(coordinates);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f3ae9f00eef133a3bf24fa95b82f8296f5c150"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg3OTkwOQ==", "bodyText": "Ah, I was playing around with this, since it sounds like we will require some extra machinery, for graph mode datasets, but I'll move it to a separate branch.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r408879909", "createdAt": "2020-04-15T14:20:37Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +484,51 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public <T extends TType> Tensor<T> pop(DataType<T> dtype) {\n+      return outputs.get(index++).expect(dtype);\n+    }\n+\n+    public int popInt(long... coordinates) {\n+      return pop(TInt32.DTYPE).data().getInt(coordinates);\n+    }\n+\n+    public long popLong(long... coordinates) {\n+      return pop(TInt64.DTYPE).data().getLong(coordinates);\n+    }\n+\n+    public float popFloat16(long... coordinates) {\n+      return pop(TFloat16.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public float popFloat(long... coordinates) {\n+      return pop(TFloat32.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public double popDouble(long... coordinates) {\n+      return pop(TFloat64.DTYPE).data().getDouble(coordinates);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg2MTAwMw=="}, "originalCommit": {"oid": "88f3ae9f00eef133a3bf24fa95b82f8296f5c150"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk0MDQyNQ==", "bodyText": "I removed the pop() methods from Session.Run for this PR; but later on it would be great to find a way to simplify the data-extraction-chain.\nRun.outputs.get(...).expect(...).data().getFloat(...) etc.\nis pretty lengthy, any way of simplifying this would be quite useful. I replaced the pops with a get(int index) method to at least remove the need to reference Run.outputs.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r410940425", "createdAt": "2020-04-19T16:25:18Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +484,51 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public <T extends TType> Tensor<T> pop(DataType<T> dtype) {\n+      return outputs.get(index++).expect(dtype);\n+    }\n+\n+    public int popInt(long... coordinates) {\n+      return pop(TInt32.DTYPE).data().getInt(coordinates);\n+    }\n+\n+    public long popLong(long... coordinates) {\n+      return pop(TInt64.DTYPE).data().getLong(coordinates);\n+    }\n+\n+    public float popFloat16(long... coordinates) {\n+      return pop(TFloat16.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public float popFloat(long... coordinates) {\n+      return pop(TFloat32.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public double popDouble(long... coordinates) {\n+      return pop(TFloat64.DTYPE).data().getDouble(coordinates);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg2MTAwMw=="}, "originalCommit": {"oid": "88f3ae9f00eef133a3bf24fa95b82f8296f5c150"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgzNjcxOA==", "bodyText": "Yes we'll come back to this as well. I would like to reduce the chain too but at the same time, I would like to avoid having all possible variations of popFloat(), popInt(), ... presented to the user to let him pick the right one, as we are trying to keep the client type-safe as much as possible. Not to forget that as we are adding more datatypes, you will need more accessors (popFloat16, popBfloat16, ...).", "url": "https://github.com/tensorflow/java/pull/30#discussion_r411836718", "createdAt": "2020-04-21T03:13:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +484,51 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public <T extends TType> Tensor<T> pop(DataType<T> dtype) {\n+      return outputs.get(index++).expect(dtype);\n+    }\n+\n+    public int popInt(long... coordinates) {\n+      return pop(TInt32.DTYPE).data().getInt(coordinates);\n+    }\n+\n+    public long popLong(long... coordinates) {\n+      return pop(TInt64.DTYPE).data().getLong(coordinates);\n+    }\n+\n+    public float popFloat16(long... coordinates) {\n+      return pop(TFloat16.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public float popFloat(long... coordinates) {\n+      return pop(TFloat32.DTYPE).data().getFloat(coordinates);\n+    }\n+\n+    public double popDouble(long... coordinates) {\n+      return pop(TFloat64.DTYPE).data().getDouble(coordinates);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg2MTAwMw=="}, "originalCommit": {"oid": "88f3ae9f00eef133a3bf24fa95b82f8296f5c150"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzYwNzQ2OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/ExecutionEnvironment.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDoxNjoxNlrOGMJfXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzozMjowN1rOGMLrEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MTU4Mw==", "bodyText": "Maybe rename this enum to Type? (so it is referenced as ExecutionEnvironment.Type instead of ExecutionEnvironment.Environments)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415391583", "createdAt": "2020-04-26T20:16:16Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/ExecutionEnvironment.java", "diffHunk": "@@ -18,6 +18,11 @@\n /** Defines an environment for creating and executing TensorFlow {@link Operation}s. */\n public interface ExecutionEnvironment {\n \n+  enum Environments {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyNzM0Ng==", "bodyText": "That sounds good, renamed Environments to Types", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415427346", "createdAt": "2020-04-26T23:32:07Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/ExecutionEnvironment.java", "diffHunk": "@@ -18,6 +18,11 @@\n /** Defines an environment for creating and executing TensorFlow {@link Operation}s. */\n public interface ExecutionEnvironment {\n \n+  enum Environments {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MTU4Mw=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzYxNDMxOnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDoyMDoyMlrOGMJiaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzozMjozNVrOGMLrSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MjM2MA==", "bodyText": "We shouldn't use wildcards in non-static imports, but looking again it seems that these new imports are not required anymore (probably a left over from the pop* methods you added before), can you please cleanup your list of imports?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415392360", "createdAt": "2020-04-26T20:20:22Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -41,6 +48,8 @@\n import org.tensorflow.proto.framework.ConfigProto;\n import org.tensorflow.proto.framework.RunMetadata;\n import org.tensorflow.proto.framework.RunOptions;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyNzQwMA==", "bodyText": "Good point, went through and removed wildcard imports", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415427400", "createdAt": "2020-04-26T23:32:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -41,6 +48,8 @@\n import org.tensorflow.proto.framework.ConfigProto;\n import org.tensorflow.proto.framework.RunMetadata;\n import org.tensorflow.proto.framework.RunOptions;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TType;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MjM2MA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzYxOTQxOnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDoyMzowM1rOGMJkqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzozNTowN1rOGMLtFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MjkzOA==", "bodyText": "Just to double check, all those reformatting commits, are they the result of applying the Google Java Style formatting settings over the code or just personal settings?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415392938", "createdAt": "2020-04-26T20:23:03Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -135,21 +148,21 @@ public void close() {\n    * Run {@link Operation}s and evaluate {@link Tensor Tensors}.\n    *\n    * <p>A Runner runs the necessary graph fragments to execute every {@link Operation} required to\n-   * evaluate the {@link Tensor Tensors} to fetch. The {@link #feed(String,int,Tensor)} call allows\n+   * evaluate the {@link Tensor Tensors} to fetch. The {@link #feed(String, int, Tensor)} call allows\n    * callers to override the value of {@link Tensor Tensors} in the graph by substituting the\n    * provided {@link Tensor Tensors} for the outputs of the operations provided to {@link\n-   * #feed(String,int,Tensor)}.\n+   * #feed(String, int, Tensor)}.\n    */\n   public final class Runner {\n     /**\n      * Avoid evaluating {@code operation} and substitute {@code t} for the value it produces.\n      *\n      * @param operation Is either the string name of the operation, in which case this method is a\n-     *     shorthand for {@code feed(operation, 0)}, or it is a string of the form\n-     *     <tt>operation_name:output_index</tt> , in which case this method acts like {@code\n-     *     feed(operation_name, output_index)}. These colon-separated names are commonly used in the\n-     *     {@code SignatureDef} protocol buffer messages that are included in {@link\n-     *     SavedModelBundle#metaGraphDef()}.\n+     *                  shorthand for {@code feed(operation, 0)}, or it is a string of the form\n+     *                  <tt>operation_name:output_index</tt> , in which case this method acts like {@code\n+     *                  feed(operation_name, output_index)}. These colon-separated names are commonly used in the\n+     *                  {@code SignatureDef} protocol buffer messages that are included in {@link\n+     *                  SavedModelBundle#metaGraphDef()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyNzg2MA==", "bodyText": "I installed the standard google-java formatter and applied them to each file I changed, hopefully the formats look more standard now.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415427860", "createdAt": "2020-04-26T23:35:07Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -135,21 +148,21 @@ public void close() {\n    * Run {@link Operation}s and evaluate {@link Tensor Tensors}.\n    *\n    * <p>A Runner runs the necessary graph fragments to execute every {@link Operation} required to\n-   * evaluate the {@link Tensor Tensors} to fetch. The {@link #feed(String,int,Tensor)} call allows\n+   * evaluate the {@link Tensor Tensors} to fetch. The {@link #feed(String, int, Tensor)} call allows\n    * callers to override the value of {@link Tensor Tensors} in the graph by substituting the\n    * provided {@link Tensor Tensors} for the outputs of the operations provided to {@link\n-   * #feed(String,int,Tensor)}.\n+   * #feed(String, int, Tensor)}.\n    */\n   public final class Runner {\n     /**\n      * Avoid evaluating {@code operation} and substitute {@code t} for the value it produces.\n      *\n      * @param operation Is either the string name of the operation, in which case this method is a\n-     *     shorthand for {@code feed(operation, 0)}, or it is a string of the form\n-     *     <tt>operation_name:output_index</tt> , in which case this method acts like {@code\n-     *     feed(operation_name, output_index)}. These colon-separated names are commonly used in the\n-     *     {@code SignatureDef} protocol buffer messages that are included in {@link\n-     *     SavedModelBundle#metaGraphDef()}.\n+     *                  shorthand for {@code feed(operation, 0)}, or it is a string of the form\n+     *                  <tt>operation_name:output_index</tt> , in which case this method acts like {@code\n+     *                  feed(operation_name, output_index)}. These colon-separated names are commonly used in the\n+     *                  {@code SignatureDef} protocol buffer messages that are included in {@link\n+     *                  SavedModelBundle#metaGraphDef()}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MjkzOA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzYyMDgzOnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDoyMzo1OVrOGMJlRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzozNToyM1rOGMLtRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MzA5NA==", "bodyText": "This is not used anymore.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415393094", "createdAt": "2020-04-26T20:23:59Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +499,31 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyNzkxMQ==", "bodyText": "Thanks, removed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415427911", "createdAt": "2020-04-26T23:35:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +499,31 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    /**\n+     * Current `pop` index in the `outputs` list.\n+     */\n+    private int index = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MzA5NA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 192}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzYyMzQ4OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/TensorFlowException.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDoyNTo1NVrOGMJmjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzozNTozNlrOGMLtdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MzQyMA==", "bodyText": "while we are at it, should we move this guy to the exceptions package as well?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415393420", "createdAt": "2020-04-26T20:25:55Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/TensorFlowException.java", "diffHunk": "@@ -15,11 +15,14 @@\n \n package org.tensorflow;\n \n-/** Unchecked exception thrown by TensorFlow core classes */\n-public final class TensorFlowException extends RuntimeException {\n+/**\n+ * Unchecked exception thrown by TensorFlow core classes\n+ */\n+public class TensorFlowException extends RuntimeException {\n   public TensorFlowException(String message) {\n     super(message);\n   }\n+\n   public TensorFlowException(String message, Throwable cause) {\n     super(message, cause);\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyNzk1Ng==", "bodyText": "Good idea, moved!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415427956", "createdAt": "2020-04-26T23:35:36Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/TensorFlowException.java", "diffHunk": "@@ -15,11 +15,14 @@\n \n package org.tensorflow;\n \n-/** Unchecked exception thrown by TensorFlow core classes */\n-public final class TensorFlowException extends RuntimeException {\n+/**\n+ * Unchecked exception thrown by TensorFlow core classes\n+ */\n+public class TensorFlowException extends RuntimeException {\n   public TensorFlowException(String message) {\n     super(message);\n   }\n+\n   public TensorFlowException(String message, Throwable cause) {\n     super(message, cause);\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5MzQyMA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY0MDc1OnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDozNjoxNFrOGMJuPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNzoyOTo0MVrOGOJcjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTM4OA==", "bodyText": "Oh I didn't know we were changing all exceptions here, I thought we were only doing it for OutOfRangeException, @Craigacp  was it what you had in mind as well?\nChanging all of them might be tougher for our clients migrating for 1.* to 2.* as they are all thrown as runtime exception... another idea would have been then to extends each of these TF errors to known runtime exceptions (i.e. TFOutOfRangeException would extends from IndexOutOfBoundsException instead of TensorFlowException).\nIf we are keeping this new mapping, then I suggest that we have all distinct exceptions for each possible error (i.e. there should be  a TFResourceExhaustedException and a TFUnauthenticatedException as well)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415395388", "createdAt": "2020-04-26T20:36:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "diffHunk": "@@ -69,17 +75,17 @@ public void throwExceptionIfNotOK() {\n         case TF_OK:\n           break;\n         case TF_INVALID_ARGUMENT:\n-          throw new IllegalArgumentException(TF_Message(s).getString());\n+          throw new TFInvalidArgumentException(TF_Message(s).getString());\n         case TF_UNAUTHENTICATED:\n         case TF_PERMISSION_DENIED:\n-          throw new SecurityException(TF_Message(s).getString());\n+          throw new TFPermissionDeniedException(TF_Message(s).getString());\n         case TF_RESOURCE_EXHAUSTED:\n         case TF_FAILED_PRECONDITION:\n-          throw new IllegalStateException(TF_Message(s).getString());\n+          throw new TFFailedPreconditionException(TF_Message(s).getString());\n         case TF_OUT_OF_RANGE:\n-          throw new IndexOutOfBoundsException(TF_Message(s).getString());\n+          throw new TFOutOfRangeException(TF_Message(s).getString());\n         case TF_UNIMPLEMENTED:\n-          throw new UnsupportedOperationException(TF_Message(s).getString());\n+          throw new TFUnimplementedException(TF_Message(s).getString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMDY2MQ==", "bodyText": "Good point, I had created TFUnauthenticatedException and TFResourceExhaustedException as well but hadn't mapped to them. Just added them to this mapping.\nI think it's a good idea to change it for all exceptions. The ambiguity issue we found with IndexOutOfBoundsException applies to all of these exceptions. There weren't too many issues in tests to address so within our codebase, this is a pretty low-touch change.\nAre there other major cases where current users might be catching errors from tensorflow-core? My guess is that not too many users will depend strongly on this. If we document this change well during our alpha releases, I think it can be adjusted to.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415430661", "createdAt": "2020-04-26T23:49:52Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "diffHunk": "@@ -69,17 +75,17 @@ public void throwExceptionIfNotOK() {\n         case TF_OK:\n           break;\n         case TF_INVALID_ARGUMENT:\n-          throw new IllegalArgumentException(TF_Message(s).getString());\n+          throw new TFInvalidArgumentException(TF_Message(s).getString());\n         case TF_UNAUTHENTICATED:\n         case TF_PERMISSION_DENIED:\n-          throw new SecurityException(TF_Message(s).getString());\n+          throw new TFPermissionDeniedException(TF_Message(s).getString());\n         case TF_RESOURCE_EXHAUSTED:\n         case TF_FAILED_PRECONDITION:\n-          throw new IllegalStateException(TF_Message(s).getString());\n+          throw new TFFailedPreconditionException(TF_Message(s).getString());\n         case TF_OUT_OF_RANGE:\n-          throw new IndexOutOfBoundsException(TF_Message(s).getString());\n+          throw new TFOutOfRangeException(TF_Message(s).getString());\n         case TF_UNIMPLEMENTED:\n-          throw new UnsupportedOperationException(TF_Message(s).getString());\n+          throw new TFUnimplementedException(TF_Message(s).getString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTM4OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgzODg0OQ==", "bodyText": "I've gone back and forth on this, and I don't have strong opinions about if the others should be TF exceptions or not. The main reason that I want the TFOutOfRangeException is because we've made that part of the API that the user has to know about and catch. As far as I can tell the rest of the errors are programmer errors that are reasonable to leave as the standard exceptions, but they are probably also fine as wrapped TF exceptions so we have more of an idea of the provenance. I agree with Karl that if we're going to map most of them we should map all of them for completeness.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415838849", "createdAt": "2020-04-27T14:01:58Z", "author": {"login": "Craigacp"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "diffHunk": "@@ -69,17 +75,17 @@ public void throwExceptionIfNotOK() {\n         case TF_OK:\n           break;\n         case TF_INVALID_ARGUMENT:\n-          throw new IllegalArgumentException(TF_Message(s).getString());\n+          throw new TFInvalidArgumentException(TF_Message(s).getString());\n         case TF_UNAUTHENTICATED:\n         case TF_PERMISSION_DENIED:\n-          throw new SecurityException(TF_Message(s).getString());\n+          throw new TFPermissionDeniedException(TF_Message(s).getString());\n         case TF_RESOURCE_EXHAUSTED:\n         case TF_FAILED_PRECONDITION:\n-          throw new IllegalStateException(TF_Message(s).getString());\n+          throw new TFFailedPreconditionException(TF_Message(s).getString());\n         case TF_OUT_OF_RANGE:\n-          throw new IndexOutOfBoundsException(TF_Message(s).getString());\n+          throw new TFOutOfRangeException(TF_Message(s).getString());\n         case TF_UNIMPLEMENTED:\n-          throw new UnsupportedOperationException(TF_Message(s).getString());\n+          throw new TFUnimplementedException(TF_Message(s).getString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTM4OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyMzkyOA==", "bodyText": "Ok guys, sorry but I think I changed a bit my mind on this again... I think it is good to have distinct exceptions per possible errors thrown by TensorFlow, like you did @dhruvrajan. But I think those exceptions should extend from their matching runtime exception (the one that were thrown before) rather than from the generic TensorFlowException.\nMy reasoning is that it does not really matter for a user to know that, for instance, a parameter is detected to be invalid at the JVM level or at the native level. So if we throw a IllegalArgumentException when validating user parameters in the JVM, we should throw a IllegalArgumentException too (or in this case an exception that extends from it) if the validation failed in the native library.\nStill, having concrete type like TFOutOfRangeException will allow us to catch explicitly this exception without catching all IndexOutOfBoundException, which was the original purpose here.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417023928", "createdAt": "2020-04-29T01:47:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "diffHunk": "@@ -69,17 +75,17 @@ public void throwExceptionIfNotOK() {\n         case TF_OK:\n           break;\n         case TF_INVALID_ARGUMENT:\n-          throw new IllegalArgumentException(TF_Message(s).getString());\n+          throw new TFInvalidArgumentException(TF_Message(s).getString());\n         case TF_UNAUTHENTICATED:\n         case TF_PERMISSION_DENIED:\n-          throw new SecurityException(TF_Message(s).getString());\n+          throw new TFPermissionDeniedException(TF_Message(s).getString());\n         case TF_RESOURCE_EXHAUSTED:\n         case TF_FAILED_PRECONDITION:\n-          throw new IllegalStateException(TF_Message(s).getString());\n+          throw new TFFailedPreconditionException(TF_Message(s).getString());\n         case TF_OUT_OF_RANGE:\n-          throw new IndexOutOfBoundsException(TF_Message(s).getString());\n+          throw new TFOutOfRangeException(TF_Message(s).getString());\n         case TF_UNIMPLEMENTED:\n-          throw new UnsupportedOperationException(TF_Message(s).getString());\n+          throw new TFUnimplementedException(TF_Message(s).getString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTM4OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4ODAxMw==", "bodyText": "I guess the thing here is, when a user tries to catch an IndexOutOfBounds exception to catch they expect from a Java array, for example (in their own code), do we want it to catch other errors thrown from tensorflow-core? Granted, these cases should be pretty infrequent, but it's still a very confusing semantic.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417488013", "createdAt": "2020-04-29T17:29:41Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Status.java", "diffHunk": "@@ -69,17 +75,17 @@ public void throwExceptionIfNotOK() {\n         case TF_OK:\n           break;\n         case TF_INVALID_ARGUMENT:\n-          throw new IllegalArgumentException(TF_Message(s).getString());\n+          throw new TFInvalidArgumentException(TF_Message(s).getString());\n         case TF_UNAUTHENTICATED:\n         case TF_PERMISSION_DENIED:\n-          throw new SecurityException(TF_Message(s).getString());\n+          throw new TFPermissionDeniedException(TF_Message(s).getString());\n         case TF_RESOURCE_EXHAUSTED:\n         case TF_FAILED_PRECONDITION:\n-          throw new IllegalStateException(TF_Message(s).getString());\n+          throw new TFFailedPreconditionException(TF_Message(s).getString());\n         case TF_OUT_OF_RANGE:\n-          throw new IndexOutOfBoundsException(TF_Message(s).getString());\n+          throw new TFOutOfRangeException(TF_Message(s).getString());\n         case TF_UNIMPLEMENTED:\n-          throw new UnsupportedOperationException(TF_Message(s).getString());\n+          throw new TFUnimplementedException(TF_Message(s).getString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTM4OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY0MzkyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDozODoyNVrOGMJvuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1MDoxMFrOGML4Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTc2OA==", "bodyText": "This is no more only for training. So maybe more something like High-level abstractions for training and serving TensorFlow models?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415395768", "createdAt": "2020-04-26T20:38:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -24,12 +24,13 @@\n     <artifactId>tensorflow-java</artifactId>\n     <version>0.1.0-SNAPSHOT</version>\n   </parent>\n-  <artifactId>tensorflow-training</artifactId>\n+  <artifactId>tensorflow-framework</artifactId>\n   <packaging>jar</packaging>\n \n-  <name>TensorFlow Training Library</name>\n+  <name>TensorFlow Framework Library</name>\n   <description>\n-    Operations for training Tensorflow models.\n+    Abstractions to help train deep learning\n+    models using TensorFlow Java", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMDcxOQ==", "bodyText": "I like that wording better, I updated this.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415430719", "createdAt": "2020-04-26T23:50:10Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -24,12 +24,13 @@\n     <artifactId>tensorflow-java</artifactId>\n     <version>0.1.0-SNAPSHOT</version>\n   </parent>\n-  <artifactId>tensorflow-training</artifactId>\n+  <artifactId>tensorflow-framework</artifactId>\n   <packaging>jar</packaging>\n \n-  <name>TensorFlow Training Library</name>\n+  <name>TensorFlow Framework Library</name>\n   <description>\n-    Operations for training Tensorflow models.\n+    Abstractions to help train deep learning\n+    models using TensorFlow Java", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTc2OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY0NDg3OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDozOTowMVrOGMJwKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1MDoyM1rOGML4Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTg4Mg==", "bodyText": "version is inherited from parent and scope should remain test", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415395882", "createdAt": "2020-04-26T20:39:01Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -41,7 +42,7 @@\n     <dependency>\n       <groupId>junit</groupId>\n       <artifactId>junit</artifactId>\n-      <scope>test</scope>\n+      <version>${junit.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMDc1NQ==", "bodyText": "Thanks, fixed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415430755", "createdAt": "2020-04-26T23:50:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -41,7 +42,7 @@\n     <dependency>\n       <groupId>junit</groupId>\n       <artifactId>junit</artifactId>\n-      <scope>test</scope>\n+      <version>${junit.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTg4Mg=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY0NTExOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDozOToxMVrOGMJwRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1MDozNVrOGML4gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTkwOA==", "bodyText": "oops :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415395908", "createdAt": "2020-04-26T20:39:11Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -64,7 +65,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n-          <argLine>-Xmx2G -XX:MaxPermSize=256m</argLine>\n+          <argLine>-Xmx2G -XX:MaxPermSize=256m -Djava.library.path=/home/dhruv/git/tensorflow-java/tensorflow-core/tensorflow-core-api/target/native/org/tensorflow/internal/c_api/linux-x86_64/</argLine>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMDc4Ng==", "bodyText": "Thanks, removed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415430786", "createdAt": "2020-04-26T23:50:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -64,7 +65,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n-          <argLine>-Xmx2G -XX:MaxPermSize=256m</argLine>\n+          <argLine>-Xmx2G -XX:MaxPermSize=256m -Djava.library.path=/home/dhruv/git/tensorflow-java/tensorflow-core/tensorflow-core-api/target/native/org/tensorflow/internal/c_api/linux-x86_64/</argLine>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NTkwOA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY0OTE1OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0MTozNVrOGMJyEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1MToxMlrOGML5DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjM2OQ==", "bodyText": "It looks to me that your line wrapping is less that 100 characters.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415396369", "createdAt": "2020-04-26T20:41:35Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMDkyNQ==", "bodyText": "Adding the TF Java formatting, looks like the line wrapping everywhere has been updated to 100 characters.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415430925", "createdAt": "2020-04-26T23:51:12Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjM2OQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY1MDE2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0MjoxMlrOGMJyfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1MTo0MlrOGML5iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjQ3Nw==", "bodyText": "should be TFOutOfRangeException now. Also, where is the loop?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415396477", "createdAt": "2020-04-26T20:42:12Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMTA0OA==", "bodyText": "Right on both counts, updated the exception and added a while(true) loop", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415431048", "createdAt": "2020-04-26T23:51:42Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjQ3Nw=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY1MjMzOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0NDowMFrOGMJzgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyNDo1N1rOGMpFfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjczOQ==", "bodyText": "where are defined X and y? Also, variables should start with lowercase, so xTensor & cie are a better pick", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415396739", "createdAt": "2020-04-26T20:44:00Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMTgxNQ==", "bodyText": "These should be XBatch and yBatch\nNever really thought about it, in a lot of the ML code I've seen (in Python), the examples tensor X is capital and the features y is lowercase. Pretty sure this started with the scikit-learn code base.\nProbably a good idea to stick with Java conventions here; maybe a better choice would be featureBatch and labelBatch...", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415431815", "createdAt": "2020-04-26T23:55:48Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjczOQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0MDY4Mg==", "bodyText": "Actually I think this comes from maths notation where matrices are usually capital letters and vectors are usually lower case. But either way it's in strong conflict with how Java does it, and we should respect Java idioms.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415840682", "createdAt": "2020-04-27T14:04:21Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjczOQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkwOTI0NA==", "bodyText": "I've changed the docs to use features, labels, featureBatch, labelBatch, etc.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415909244", "createdAt": "2020-04-27T15:24:57Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NjczOQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY1NjUwOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0NjozMFrOGMJ1Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyNToxOFrOGMpGnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzIxMA==", "bodyText": "should be List<Operand<?>> according to the documentation", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415397210", "createdAt": "2020-04-26T20:46:30Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjEzMA==", "bodyText": "These should be List<Output<?>>. We could change them all to List<Operand<?>> if we modify the IteratorGetNext classes; since these are generated I wasn't sure how to do that.\nThink that change would be worth it?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432130", "createdAt": "2020-04-26T23:57:17Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzIxMA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkwOTUzMg==", "bodyText": "These all have been changed to List<Operand<?>>", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415909532", "createdAt": "2020-04-27T15:25:18Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzIxMA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY1ODE2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0NzozN1rOGMJ2Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1NzozNFrOGML95w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzQwNg==", "bodyText": "Wrong doc, this is a copy of getNext()", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415397406", "createdAt": "2020-04-26T20:47:37Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {\n+    return tf.data.iteratorGetNext(getIteratorResource(),\n+        getOutputTypes(), getOutputShapes()).components();\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjE2Nw==", "bodyText": "Added DatasetOptional docs here.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432167", "createdAt": "2020-04-26T23:57:34Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {\n+    return tf.data.iteratorGetNext(getIteratorResource(),\n+        getOutputTypes(), getOutputShapes()).components();\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzQwNg=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY1OTEzOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0ODozMlrOGMJ2hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMToyOTowMFrOGNs3Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzUxMQ==", "bodyText": "extra line", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415397511", "createdAt": "2020-04-26T20:48:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {\n+    return tf.data.iteratorGetNext(getIteratorResource(),\n+        getOutputTypes(), getOutputShapes()).components();\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public DatasetOptional getNextAsOptional() {\n+    Operand<?> optionalVariant = tf.data.iteratorGetNextAsOptional(\n+        getIteratorResource(),\n+        getOutputTypes(),\n+        getOutputShapes()).optional();\n+    return new DatasetOptional(tf, optionalVariant, outputTypes,\n+        outputShapes);\n+  }\n+\n+  /**\n+   * Creates and returns a TF `Op` that can be run to initialize\n+   * this iterator on a dataset. The dataset must have a structure\n+   * (outputTypes, outputShapes) that match this iterator, and\n+   * share the same ExecutionEnvironment as this iterator.\n+   * <p>\n+   * When this `Op` is run, this iterator will be \"re-initialized\" at\n+   * the first element of the input dataset.\n+   * <p>\n+   * In eager mode, the op will be run automatically as part of\n+   * a call to `makeIterator`.\n+   *\n+   * @param dataset An `org.tensorflow.data.Dataset` to initialize this\n+   *                iterator on.\n+   * @return A TF `Op` that can be used to initialize this iterator on the\n+   * dataset.\n+   * @throws IllegalArgumentException if the dataset's ExecutionEnvironment or\n+   *                                  structure doesn't match this iterator.\n+   */\n+  public Op makeInitializer(Dataset dataset) {\n+    if (tf.scope().env() != dataset.tf.scope().env()) {\n+      throw new IllegalArgumentException(\"Dataset must share the same\" +\n+          \"ExecutionEnvironment as this iterator.\");\n+    }\n+\n+    if (!dataset.getOutputShapes().equals(getOutputShapes())\n+        || !dataset.getOutputTypes().equals(getOutputTypes())) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjIxOA==", "bodyText": "Fixed, I think with google java formatting.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432218", "createdAt": "2020-04-26T23:57:52Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {\n+    return tf.data.iteratorGetNext(getIteratorResource(),\n+        getOutputTypes(), getOutputShapes()).components();\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public DatasetOptional getNextAsOptional() {\n+    Operand<?> optionalVariant = tf.data.iteratorGetNextAsOptional(\n+        getIteratorResource(),\n+        getOutputTypes(),\n+        getOutputShapes()).optional();\n+    return new DatasetOptional(tf, optionalVariant, outputTypes,\n+        outputShapes);\n+  }\n+\n+  /**\n+   * Creates and returns a TF `Op` that can be run to initialize\n+   * this iterator on a dataset. The dataset must have a structure\n+   * (outputTypes, outputShapes) that match this iterator, and\n+   * share the same ExecutionEnvironment as this iterator.\n+   * <p>\n+   * When this `Op` is run, this iterator will be \"re-initialized\" at\n+   * the first element of the input dataset.\n+   * <p>\n+   * In eager mode, the op will be run automatically as part of\n+   * a call to `makeIterator`.\n+   *\n+   * @param dataset An `org.tensorflow.data.Dataset` to initialize this\n+   *                iterator on.\n+   * @return A TF `Op` that can be used to initialize this iterator on the\n+   * dataset.\n+   * @throws IllegalArgumentException if the dataset's ExecutionEnvironment or\n+   *                                  structure doesn't match this iterator.\n+   */\n+  public Op makeInitializer(Dataset dataset) {\n+    if (tf.scope().env() != dataset.tf.scope().env()) {\n+      throw new IllegalArgumentException(\"Dataset must share the same\" +\n+          \"ExecutionEnvironment as this iterator.\");\n+    }\n+\n+    if (!dataset.getOutputShapes().equals(getOutputShapes())\n+        || !dataset.getOutputTypes().equals(getOutputTypes())) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzUxMQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxOTY3OQ==", "bodyText": "extra-line is still there but whatever, not important  :)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417019679", "createdAt": "2020-04-29T01:29:00Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.tools.Shape;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents the state of an iteration through\n+ * a tf.data Datset. DatasetIterator is not\n+ * a java.util.Iterator. In eager mode, `Dataset`\n+ * can be used as an Iterable, returning dataset\n+ * elements each iteration.\n+ *\n+ * <p>\n+ * Example: Iteration in graph mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor);\n+ *          .batch(BATCH_SIZE);\n+ *\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *  List<Output<?>> components = iterator.getNext();\n+ *  Operand<?> XBatch = components.get(0);\n+ *  Operand<?> yBatch = components.get(1);\n+ *\n+ *  // Build a TensorFlow graph that does something on each element.\n+ *  loss = computeModelLoss(X, y);\n+ *\n+ *  optimizer = ... // create an optimizer\n+ *  trainOp = optimizer.minimize(loss);\n+ *\n+ *  try (Session session = new Session(graph) {\n+ *      try {\n+ *          session.run(trainOp);\n+ *          ...\n+ *      } catch (IndexOutOfBoundsException e) {\n+ *          System.out.println(\"finished iterating.\");\n+ *          break;\n+ *      }\n+ *  }\n+ *\n+ * }</pre>\n+ * <p>\n+ * Example: Iteration in eager mode.\n+ *\n+ * <pre>{@code\n+ *  // Create input tensors\n+ *  Operand<?> XTensor = tf.constant( ... );\n+ *  Operand<?> yTensor = tf.constant( ... );\n+ *\n+ *  int BATCH_SIZE = ...\n+ *\n+ *  Dataset dataset = Dataset\n+ *          .fromTensorSlices(XTensor, yTensor)\n+ *          .batch(BATCH_SIZE);\n+ *  DatasetIterator iterator = dataset.makeIterator();\n+ *\n+ *  Optimizer optimizer = ... // create an optimizer\n+ *\n+ *  for (List<Output<?>> components : dataset) {\n+ *      Operand<?> XBatch = components.get(0);\n+ *      Operand<?> yBatch = components.get(1);\n+ *\n+ *      loss = computeModelLoss(X, y);\n+ *      trainOp = optimizer.minimize(loss);\n+ *  }\n+ * }</pre>\n+ */\n+public class DatasetIterator {\n+  public static final String EMPTY_SHARED_NAME = \"\";\n+\n+  private Ops tf;\n+\n+  private Operand<?> iteratorResource;\n+  private Op initializer;\n+\n+  private List<DataType<?>> outputTypes;\n+  private List<Shape> outputShapes;\n+\n+  /**\n+   * @param tf               Ops accessor corresponding to the same `ExecutionEnvironment`\n+   *                         as the `iteratorResource`.\n+   * @param iteratorResource An Operand representing the iterator\n+   *                         (e.g. constructed from `tf.data.iterator` or\n+   *                         `tf.data.anonymousIterator`)\n+   * @param initializer      An `Op` that should be run to initialize this iterator\n+   * @param outputTypes      A list of `DataType` objects corresponding to the\n+   *                         types of each component of a dataset element.\n+   * @param outputShapes     A list of `Shape` objects corresponding to the\n+   *                         shapes of each componenet of a dataset element.\n+   */\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          Op initializer,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.initializer = initializer;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  private DatasetIterator(Ops tf, Operand<?> iteratorResource,\n+                          List<DataType<?>> outputTypes,\n+                          List<Shape> outputShapes) {\n+    this.tf = tf;\n+    this.iteratorResource = iteratorResource;\n+    this.outputTypes = outputTypes;\n+    this.outputShapes = outputShapes;\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public List<Output<?>> getNext() {\n+    return tf.data.iteratorGetNext(getIteratorResource(),\n+        getOutputTypes(), getOutputShapes()).components();\n+  }\n+\n+  /**\n+   * Returns a list of `Operand<?>` representing the components of the\n+   * next dataset element.\n+   * <p>\n+   * In graph mode, call this method once, and use its result as input\n+   * to another computation. Then in the training loop, on successive calls\n+   * to session.run(), successive dataset elements will be retrieved through\n+   * these components.\n+   * <p>\n+   * In eager mode, each time this method is called, the next dataset\n+   * element will be returned. (This is done automatically by iterating\n+   * through `Dataset` as a Java `Iterable`).\n+   *\n+   * @return A `List<Operand<?>>` representing dataset element components.\n+   */\n+  public DatasetOptional getNextAsOptional() {\n+    Operand<?> optionalVariant = tf.data.iteratorGetNextAsOptional(\n+        getIteratorResource(),\n+        getOutputTypes(),\n+        getOutputShapes()).optional();\n+    return new DatasetOptional(tf, optionalVariant, outputTypes,\n+        outputShapes);\n+  }\n+\n+  /**\n+   * Creates and returns a TF `Op` that can be run to initialize\n+   * this iterator on a dataset. The dataset must have a structure\n+   * (outputTypes, outputShapes) that match this iterator, and\n+   * share the same ExecutionEnvironment as this iterator.\n+   * <p>\n+   * When this `Op` is run, this iterator will be \"re-initialized\" at\n+   * the first element of the input dataset.\n+   * <p>\n+   * In eager mode, the op will be run automatically as part of\n+   * a call to `makeIterator`.\n+   *\n+   * @param dataset An `org.tensorflow.data.Dataset` to initialize this\n+   *                iterator on.\n+   * @return A TF `Op` that can be used to initialize this iterator on the\n+   * dataset.\n+   * @throws IllegalArgumentException if the dataset's ExecutionEnvironment or\n+   *                                  structure doesn't match this iterator.\n+   */\n+  public Op makeInitializer(Dataset dataset) {\n+    if (tf.scope().env() != dataset.tf.scope().env()) {\n+      throw new IllegalArgumentException(\"Dataset must share the same\" +\n+          \"ExecutionEnvironment as this iterator.\");\n+    }\n+\n+    if (!dataset.getOutputShapes().equals(getOutputShapes())\n+        || !dataset.getOutputTypes().equals(getOutputTypes())) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzUxMQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY2MTIxOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo0OTozMlrOGMJ3Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1ODoxNlrOGML-cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzczNA==", "bodyText": "Missing copyrights (and for all new files in this PR)", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415397734", "createdAt": "2020-04-26T20:49:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjMwNA==", "bodyText": "Added copyrights to all the files! Let me know if it's done right", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432304", "createdAt": "2020-04-26T23:58:16Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/DatasetIterator.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package org.tensorflow.framework.data;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5NzczNA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY2ODM4OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo1MzozNVrOGMJ6fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1ODoyM1rOGML-gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODUyNg==", "bodyText": "There is no reference to the RFC, but I think you can simply remove this line", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415398526", "createdAt": "2020-04-26T20:53:35Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjMyMg==", "bodyText": "Removed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432322", "createdAt": "2020-04-26T23:58:23Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODUyNg=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY3MDIyOnYy", "diffSide": "RIGHT", "path": "tensorflow-frameworks/tensorflow-data/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo1NDozM1rOGMJ7Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1ODozMFrOGML-jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODczMQ==", "bodyText": "tensorflow-frameworks should not be present anymore", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415398731", "createdAt": "2020-04-26T20:54:33Z", "author": {"login": "karllessard"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,209 @@\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjMzMw==", "bodyText": "Removed this!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432333", "createdAt": "2020-04-26T23:58:30Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-frameworks/tensorflow-data/README.md", "diffHunk": "@@ -0,0 +1,209 @@\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODczMQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY3MTI0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo1NTowNVrOGMJ7wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo1ODo1N1rOGML-9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODg0OA==", "bodyText": "or is it TFOutOfRangeException?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415398848", "createdAt": "2020-04-26T20:55:05Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of\n+ `Session` instances\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in a `TFOutOfRangeException`.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (TFOutOfRangeError e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMjQzNw==", "bodyText": "Yup (in Python, it's Error so it gets confusing :) ). Updated this", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415432437", "createdAt": "2020-04-26T23:58:57Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods\n+provided in the `Dataset` class (examples to follow).\n+\n+To construct an iterator for a dataset of a specific structure, use\n+the static method `Iterator.fromStructure(Ops tf, List<DataType<?>> outputTypes, List<Shape> outputShapes)`. This creates a `DatasetIterator` object\n+which can be used with any dataset of a matching structure.\n+\n+Once a `DatasetIterator` is created, it can be initialized on a `Dataset` intsance using `Iterator.makeInitializer(Dataset dataset)`. This will initialize (or re-initialize) the iterator to start at the beginning\n+of this dataset.\n+\n+The `Iterator.getNext()` method can be used to retrieve dataset elements.\n+In eager mode, each call to `getNext()` will return the next dataset element as\n+as `List<Output<?>>`. In graph mode, this method should be called just once\n+to retrieve the components. These can be fed into additional operations as\n+a computation Graph is built. On successive `session.run` operations, the\n+successive dataset elements will be automatically passed through the graph.\n+\n+\n+#### Eager Mode: Iterable\n+The `Dataset` class implements the `Iterable` interface, so in\n+eager mode, iteration over dataset elements is possible using a standard for-each loop (this is a wrapper around `DatasetIterator` constructs).\n+\n+Using the same example dataset from above, dataset elements can be extracted and\n+used as follows:\n+```java\n+// Use default EagerSession\n+Ops tf = Ops.create()\n+\n+// Dataset of (features, labels) from above\n+Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+// batch dataset elements into batches of size 2\n+dataset = dataset.batch(2);\n+\n+Optmizer optimizer = ... // TF Optimizer\n+\n+for (List<Output<?>> batch : dataset) {\n+    Operand<?> featureBatch = element.get(0);\n+    Operand<?> labelBatch = element.get(1);\n+\n+    // Perform batch-wise computations on featureBatch and labelBatch\n+    // e.g. computing model losses, running optimizers.\n+\n+    Operand<TFloat32> loss = myModelLoss(featureBatch, labelBatch);\n+\n+    optimizer.minimize(loss);\n+    \n+    ...\n+}   \n+\n+```\n+\n+#### Graph Mode: OneShotIterator\n+\n+The above code will not work in graph mode, which requires the use of\n+ `Session` instances\n+to run the computations. In graph mode, datasets can be iterated over using the `DatasetIterator` abstraction, and a while loop.\n+\n+Once the iterator is initialized, repeated calls to `Session.run` will populate the components with new values, until all elements have\n+been retrieved. After this, `Session.run` will result in a `TFOutOfRangeException`.\n+\n+Note that the make-iterator operation can be re-run to re-initialize\n+the iterator, to iterate over the dataset a second time.\n+\n+```java\n+try (Graph graph = new Graph()) {\n+    // Graph mode Ops accessor\n+    Ops tf = Ops.create(graph)\n+\n+    // Dataset of (features, labels) from above\n+    Dataset dataset = Dataset.fromTensorSlices(tf, ... );\n+\n+    // batch dataset elements into batches of size 2\n+    dataset = dataset.batch(2); \n+\n+    // makeOneShotIterator() automatically adds the \n+    // iterator initializer (MakeIterator) Op to the Graph\n+    // initializers list. Make sure to run `session.run(tf.init())`\n+    // first!\n+    DatasetIterator iterator = dataset.makeOneShotIterator();\n+    List<Output<?>> batch = iterator.getNext();\n+\n+    Operand<?> features = batch.get(0);\n+    Operand<?> labels = batch.get(1);\n+\n+    // Run additional computations on `features` and `labels`,\n+    // e.g. computing model losses, instantiating Optimizers\n+\n+    Optimizer optimizer = ... // TF Optimizer \n+    Operand<TFloat32> loss = myModelLoss(features, labels);\n+    \n+    Op trainOp = optimizer.minimize(loss)\n+\n+    // instantiate graph-mode session\n+    try (Session session = new Session(graph)) {\n+        // Run graph initializers (and the iterator initializer)\n+        session.run(tf.init());\n+\n+        // Iterate over dataset elements\n+        while (true) {\n+            try {\n+                // Run training ops / fetch loss\n+                List<Tensor<?>> outputs = session.runner()\n+                    .addTarget(trainOp)\n+                    .fetch(loss)\n+                    .run();\n+\n+                ...\n+\n+            } catch (TFOutOfRangeError e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5ODg0OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY3NTEyOnYy", "diffSide": "RIGHT", "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo1NzoxN1rOGMJ9eQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNzoyNDo0MlrOGOJQqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTI4OQ==", "bodyText": "Where in the code do you use this new constructor? is it really required?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415399289", "createdAt": "2020-04-26T20:57:17Z", "author": {"login": "karllessard"}, "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "diffHunk": "@@ -61,6 +65,15 @@ public static Shape of(long... dimensionSizes) {\n     return new Shape(dimensionSizes);\n   }\n \n+  public static Shape of(long firstDimensionSize, long[] otherDimensionSizes) {\n+    long[] dimensionSizes = new long[otherDimensionSizes.length + 1];\n+    dimensionSizes[0] = firstDimensionSize;\n+    System.arraycopy(\n+        otherDimensionSizes, 0, dimensionSizes, 1, otherDimensionSizes.length);\n+\n+    return Shape.of(dimensionSizes);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzNTMxNg==", "bodyText": "I'm using it in Dataset.batch. It's just a bit of a convenience so that you can append to the beginning of an existing Shape.\nI currently have this statement:\nList<Shape> batchOutputShapes =\n        outputShapes.stream().map(s -> Shape.of(-1, s.asArray())).collect(Collectors.toList());\n\nwhich we could replace with something like this, if we remove the constructor:\nList<Shape> batchOutputShapes =\n        outputShapes.stream()\n            .map(\n                shape -> {\n                  long[] newShape = new long[shape.numDimensions() + 1];\n                  newShape[0] = -1;\n                  System.arraycopy(shape.asArray(), 0, newShape, 1, shape.numDimensions());\n                  return Shape.of(newShape);\n                })\n            .collect(Collectors.toList());", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415435316", "createdAt": "2020-04-27T00:13:12Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "diffHunk": "@@ -61,6 +65,15 @@ public static Shape of(long... dimensionSizes) {\n     return new Shape(dimensionSizes);\n   }\n \n+  public static Shape of(long firstDimensionSize, long[] otherDimensionSizes) {\n+    long[] dimensionSizes = new long[otherDimensionSizes.length + 1];\n+    dimensionSizes[0] = firstDimensionSize;\n+    System.arraycopy(\n+        otherDimensionSizes, 0, dimensionSizes, 1, otherDimensionSizes.length);\n+\n+    return Shape.of(dimensionSizes);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTI4OQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyMTE4Mg==", "bodyText": "But if the main reason is to add an extra dimension to an existing shape, I would prefer having a method called Shape prepend(dimensionSizes) or something like that than a new constructor.\nWe should be able also to \"reshape\" a shape ultimately (i.e. changing [2, 3, 2] to [2, 1, 2, 3]) with a utility called Shape reshape(dimensionsSizes)... but it sounds that in your case, prepend would be a better pick? Like you did with head and tail?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417021182", "createdAt": "2020-04-29T01:35:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "diffHunk": "@@ -61,6 +65,15 @@ public static Shape of(long... dimensionSizes) {\n     return new Shape(dimensionSizes);\n   }\n \n+  public static Shape of(long firstDimensionSize, long[] otherDimensionSizes) {\n+    long[] dimensionSizes = new long[otherDimensionSizes.length + 1];\n+    dimensionSizes[0] = firstDimensionSize;\n+    System.arraycopy(\n+        otherDimensionSizes, 0, dimensionSizes, 1, otherDimensionSizes.length);\n+\n+    return Shape.of(dimensionSizes);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTI4OQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4NDk2OA==", "bodyText": "Makes sense, prepend() is pretty nice, I replaced the constructor.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417484968", "createdAt": "2020-04-29T17:24:42Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-tools/src/main/java/org/tensorflow/tools/Shape.java", "diffHunk": "@@ -61,6 +65,15 @@ public static Shape of(long... dimensionSizes) {\n     return new Shape(dimensionSizes);\n   }\n \n+  public static Shape of(long firstDimensionSize, long[] otherDimensionSizes) {\n+    long[] dimensionSizes = new long[otherDimensionSizes.length + 1];\n+    dimensionSizes[0] = firstDimensionSize;\n+    System.arraycopy(\n+        otherDimensionSizes, 0, dimensionSizes, 1, otherDimensionSizes.length);\n+\n+    return Shape.of(dimensionSizes);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTI4OQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY3Njg2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMDo1ODoyOVrOGMJ-Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMDowMzo1NFrOGMMDBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTQ5OQ==", "bodyText": "With dash - or without is fine, but let's be consistent :) Since this API is now part of the framework, maybe we don't need to refer to it as a distinct framework. What about simply call this document \"Datasets\"?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415399499", "createdAt": "2020-04-26T20:58:29Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMzQ3OQ==", "bodyText": "The API is pretty heavily based on tf.data (for which I think the official name is tf.data). Figured I'd try and stick with naming this accordingly.\nThe benefit of keeping the framework name is to call out that it's using the same ops as tf.data... But maybe that's obvious anyway? If so I don't mind changing the name.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415433479", "createdAt": "2020-04-27T00:03:54Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM5OTQ5OQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY4MTc2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMTowMToyOVrOGMKAhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyNjo1MlrOGMpLog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDA2OA==", "bodyText": "My English is not great but I think something is off in this sentence... maybe:\n\"The Datasets provides an abstraction for loading data of various formats and preparing it for training of deep learning models\"?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415400068", "createdAt": "2020-04-26T21:01:29Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMzUyNg==", "bodyText": "It is a little long-winded, I changed it.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415433526", "createdAt": "2020-04-27T00:04:11Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDA2OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0MjEyMA==", "bodyText": "I'd say preparing it for training machine learning models rather than deep learning models, as you can train other things in TF. And our first example is a logistic regression which is anything but deep.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415842120", "createdAt": "2020-04-27T14:06:10Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDA2OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkwOTgzMA==", "bodyText": "Good point, changed this.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415909830", "createdAt": "2020-04-27T15:25:40Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDA2OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxMDgxOA==", "bodyText": "Good point, changed this.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415910818", "createdAt": "2020-04-27T15:26:52Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDA2OA=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY4Mjc4OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMTowMjowMVrOGMKA8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyNjozNVrOGMpK0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDE3Ng==", "bodyText": "again, is it List<Output<?>> or List<Operand<?>>?", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415400176", "createdAt": "2020-04-26T21:02:01Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMzU4NA==", "bodyText": "It's List<Output<?>>", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415433584", "createdAt": "2020-04-27T00:04:29Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDE3Ng=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg5ODA0MA==", "bodyText": "Now it's List<Operand<?>>", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415898040", "createdAt": "2020-04-27T15:11:31Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDE3Ng=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxMDYwOA==", "bodyText": "Now it's List<Operand<?>>", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415910608", "createdAt": "2020-04-27T15:26:35Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDE3Ng=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzY4NjMxOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMTowNDoxMVrOGMKChQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNToyOTozMVrOGMpUZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDU4MQ==", "bodyText": "I think many reference to Iterator should be replaced by DatasetIterator here.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415400581", "createdAt": "2020-04-26T21:04:11Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQzMzYwMw==", "bodyText": "Thanks, changed these!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415433603", "createdAt": "2020-04-27T00:04:39Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDU4MQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg0MzA4Nw==", "bodyText": "Once we get a stable docs location, we should probably go through this file and make the class names link to the javadoc for that class. Nothing to worry about in this PR but something to keep in mind for later.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415843087", "createdAt": "2020-04-27T14:07:21Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDU4MQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg5NzY3Mw==", "bodyText": "Yes, definitely!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415897673", "createdAt": "2020-04-27T15:11:12Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDU4MQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTkxMzA2MA==", "bodyText": "Yup, definitely!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r415913060", "createdAt": "2020-04-27T15:29:31Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,209 @@\n+\n+NOTE: This readme follows the discussion of this [RFC]()\n+\n+\n+Tensorflow-Data (Java)\n+==\n+\n+TensorFlow Data provides utilities and APIs for loading data of various formats, and preparing datasets for use in training and using deep learning models\n+. This package\n+ provides a\n+ simple API for configuring and iterating over\n+ datasets in both \"graph\" and \"eager\" mode.\n+\n+Usage\n+--\n+\n+The `Dataset` class represents a sequence of elements which can be iterated over and\n+transformed. Each element is a list of \"output\" operands, represented by the type `List<Output<?>>`. \n+\n+Note: An `Output` is a symbolic handle to a tensor produced by a TensorFlow op. In graph\n+mode, `Output` objects will not have a concrete `Tensor` value unless all dependent operations\n+are run in a `Session` (this is done \"in-real-time\" in eager mode).\n+\n+### Construction\n+\n+Datasets can be constructed either directly from a data source (e.g. a list of tensors representing the components of the dataset), or as a transformation on an existing dataset.\n+\n+#### From Data Source\n+\n+To construct a dataset from a list of tensor components, use \n+`Dataset.fromTensorSlices( ... )`. For example, say we are working\n+with a standard feature/label dataset which has 4 elements.\n+\n+```java\n+FloatNdArray features = StdArrays.ndCopyOf(\n+        new float[][] {\n+        {1, 2, 3},\n+        {4, 5, 6},\n+        {7, 8, 9},\n+        {10, 11, 12}\n+});\n+\n+FloatNdArray labels = NdArrays.vectorOf(0, 1, 1, 0);\n+```\n+\n+A dataset can be constructed from a list of the constant `Operand`s generated\n+from this dataset, and a list of `DataType` objects corresponding\n+to the type of each component:\n+\n+Note: Each of the input components must share the same first \"batch\" dimension.\n+\n+```java\n+Ops tf = // ... TensorFlow Ops accessor (either graph or eager)\n+Dataset dataset = Dataset.fromTensorSlices(\n+    Arrays.asList(tf.constant(features), tf.constant(labels)),\n+    Arrays.asList(TInt32.DTYPE, TInt32.DTYPE)\n+);\n+```\n+\n+\n+Other data sources are also possible, using `tf.data` ops; these include TFRecord files, CSV files, and more.\n+\n+#### Transformations on Existing Datasets\n+\n+Once a dataset has been created from a data source it can be transformed by calling\n+methods on the `Dataset` object. For example, to group elements in the above dataset into batches of size `2`, use `Dataset.batch(int batchSize)`:\n+\n+```java\n+dataset = dataset.batch(2)\n+```\n+\n+Each dataset transformation alters both the values and shapes of the original\n+ elements, and returns a *new* `Dataset` object.\n+\n+In this case, the original dataset had 4 elements of shape `[features: (3,) labels: (1,)]`.\n+Once the `.batch` transformation is applied, the new dataset has 2 elements (batches) of shape `[features: (2, 3), labels: (2, 1)]`.\n+\n+Similar transformations include `.skip`, `.take`, `.map`, `.filter`, etc.\n+\n+\n+### Iterating over Dataset Elements\n+\n+The primary use of a dataset is for iteration over its elements.\n+Each row (or batch) element is represented as a list of tensor components, with\n+type `List<Output<?>>`. The tensor components of this element can be accessed using `List.get(int index)`.\n+\n+It is recommended to use `Tensor.expect(DataType<?> dtype)` to restore types\n+to the retrieved tensors.\n+\n+#### Using DatastetIterator\n+The `DatasetIterator` class provides abstractions for creating and using\n+iterators in graph and eager mode. These will be explained here; however\n+end-users should only interact with `Iterator` objects through the methods", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQwMDU4MQ=="}, "originalCommit": {"oid": "41d1b563c857107b549c80bc16a3c2cdf52a275f"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDYxOTgxOnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMToyNDo0OFrOGNszLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjoxMDoxMlrOGOGTMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxODY3MQ==", "bodyText": "Just noticed this, if we auto-close tensors resulting from a run, we should allow the user to retain a reference to one or more of them so the tensor will survive beyond the life of Run if needed.\nThis could be done by adding a utility method like Tensor.retain() which will call tensorHandle.retainReference() internally... or we remove this change from this actual PR and do it later, your choice.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417018671", "createdAt": "2020-04-29T01:24:48Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +475,26 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public Tensor<?> get(int index) {\n+      return this.outputs.get(index);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQzNjQ2Nw==", "bodyText": "Great point! I think it makes sense to handle this outside this PR, as this functionality isn't used here. I reverted Session.java to the (new) master.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417436467", "createdAt": "2020-04-29T16:10:12Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -485,6 +475,26 @@ public void run(Op op) {\n      * this field may be replaced by more type-safe equivalents at any time.\n      */\n     public RunMetadata metadata;\n+\n+    Run(List<Tensor<?>> outputs, RunMetadata metadata) {\n+      this.outputs = outputs;\n+      this.metadata = metadata;\n+    }\n+\n+    Run(List<Tensor<?>> outputs) {\n+      this.outputs = outputs;\n+    }\n+\n+    public Tensor<?> get(int index) {\n+      return this.outputs.get(index);\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (Tensor<?> tensor : this.outputs) {\n+        tensor.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxODY3MQ=="}, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDYyMjE5OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMToyNjowOFrOGNs0Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjoxMDoyMVrOGOGTjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxODk4Mw==", "bodyText": "This shouldn't be there as well.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417018983", "createdAt": "2020-04-29T01:26:08Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -64,7 +65,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n-          <argLine>-Xmx2G -XX:MaxPermSize=256m</argLine>\n+          <argLine>-Xmx2G -XX:MaxPermSize=256m -Djava.library.path=${basedir}/../tensorflow-core/tensorflow-core-api/target/native/org/tensorflow/internal/c_api/linux-x86_64/</argLine>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQzNjU1OA==", "bodyText": "Removed!", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417436558", "createdAt": "2020-04-29T16:10:21Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/pom.xml", "diffHunk": "@@ -64,7 +65,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n-          <argLine>-Xmx2G -XX:MaxPermSize=256m</argLine>\n+          <argLine>-Xmx2G -XX:MaxPermSize=256m -Djava.library.path=${basedir}/../tensorflow-core/tensorflow-core-api/target/native/org/tensorflow/internal/c_api/linux-x86_64/</argLine>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxODk4Mw=="}, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDYyNDU3OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/Dataset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMToyNzozMFrOGNs1sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjoxMjo0MFrOGOGZ_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxOTMxMw==", "bodyText": "AFAIK, you don't work at Oracle :)\nI suggest you change all copyrights to Copyright 2020 The TensorFlow Authors. All Rights Reserved., like most of the files in the core module, that is what I'm using too.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417019313", "createdAt": "2020-04-29T01:27:30Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/Dataset.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\r\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\r", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQzODIwNA==", "bodyText": "Thanks for catching! I'll make that change.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417438204", "createdAt": "2020-04-29T16:12:40Z", "author": {"login": "dhruvrajan"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/data/Dataset.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\r\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\r", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAxOTMxMw=="}, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDYzMTU0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/tensorflow-data.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMTozMTo0OFrOGNs5zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMTozMTo0OFrOGNs5zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyMDM2NQ==", "bodyText": "I'm still not sure if we should have individual .md files per package or just one big README.md that is correctly split into sections... but I think that's fine for now.", "url": "https://github.com/tensorflow/java/pull/30#discussion_r417020365", "createdAt": "2020-04-29T01:31:48Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/tensorflow-data.md", "diffHunk": "@@ -0,0 +1,201 @@\n+Tensorflow Data (Java)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7d4d0e992c6ff0525f2cada08d34cd4b20d3cdc"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1591, "cost": 1, "resetAt": "2021-11-13T14:23:39Z"}}}