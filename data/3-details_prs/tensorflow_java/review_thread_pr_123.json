{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2NDgxNzI2", "number": 123, "reviewThreads": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoxNDo1OVrOEpngJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMDowMTo0OFrOEwi68g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDc0Mjc4OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoxNDo1OVrOHbZYrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQxNTo1MDowMFrOHnyntA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4OTUxNg==", "bodyText": "How does this emit negative zero? Could we catch that and emit a positive zero?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498489516", "createdAt": "2020-10-01T20:14:59Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUzMTc1Ng==", "bodyText": "I am not sure who is the culprit here. It seems to be the result of the multiply operation of a negative number * zero always produces -0, maybe this is a Pyhtonism.\nHere is the same operation in just Numpy:\na = np.array([-10., -5., 0.0, 5., 10.], dtype=np.float32)\nc = np.greater(a, 5.)\nprint(c);\n[False False False False  True]\n\nc = np.cast['float32'](c)\nprint(c)\n[0. 0. 0. 0. 1.]\n\nd = a * c\nprint(d)\n[-0. -0.  0.  0. 10.]\n\nz = -1. * 0.\nprint(z)\n-0.0\n\n\nTensorFlow Python and TensorFlow Java produce the same -0 result.\nIf you don't want this behavior, we could add zero to the final result, because -0 + 0 = 0.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498531756", "createdAt": "2020-10-01T21:52:56Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4OTUxNg=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyNzU3Ng==", "bodyText": "@Craigacp , do you have any other comment about this?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509827576", "createdAt": "2020-10-22T01:32:38Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4OTUxNg=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ4NTg3Ng==", "bodyText": "If TF python is doing it then I guess it's fine.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r511485876", "createdAt": "2020-10-24T15:50:00Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4OTUxNg=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDc2Mzk3OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoyMjo0NFrOHbZm1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0Njo0NVrOHbwe6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5MzE0MA==", "bodyText": "Does this work on a vector now the check has been removed?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498493140", "createdAt": "2020-10-01T20:22:44Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.reduce_sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();\n+    int numDimensions = shape.numDimensions();\n+    if (numDimensions == 2) {\n+      return tf.nn.softmax(input);\n+    } else {\n+      Operand<T> e =\n+          tf.math.exp(\n+              tf.math.sub(input, tf.reduceMax(input, tf.constant(axis), ReduceMax.keepDims(true))));\n+      Operand<T> s = tf.reduceSum(input, tf.constant(axis), ReduceSum.keepDims(true));\n+      return tf.math.div(e, s);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2Nzk0NA==", "bodyText": "Yes it works, I have added tests for 1D and 3D to make both path are covered.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498867944", "createdAt": "2020-10-02T14:46:45Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.reduce_sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();\n+    int numDimensions = shape.numDimensions();\n+    if (numDimensions == 2) {\n+      return tf.nn.softmax(input);\n+    } else {\n+      Operand<T> e =\n+          tf.math.exp(\n+              tf.math.sub(input, tf.reduceMax(input, tf.constant(axis), ReduceMax.keepDims(true))));\n+      Operand<T> s = tf.reduceSum(input, tf.constant(axis), ReduceSum.keepDims(true));\n+      return tf.math.div(e, s);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5MzE0MA=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDc3MDIxOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Linear.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoyNTowOVrOHbZq9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0NzoxNlrOHbwgQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5NDE5OQ==", "bodyText": "This should probably note that the linear activation function is the identity function.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498494199", "createdAt": "2020-10-01T20:25:09Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Linear.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Linear activation function.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2ODI4OQ==", "bodyText": "I have updated the JavaDoc to indicate that this is also known as Identity activation.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498868289", "createdAt": "2020-10-02T14:47:16Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Linear.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Linear activation function.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5NDE5OQ=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDgyMDY5OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftplusTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDo0Mzo0MlrOHbaLwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0ODozNlrOHbwjhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMjU5Mw==", "bodyText": "This should test that a negative or near zero value behaves appropriately rather than just the linear region.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498502593", "createdAt": "2020-10-01T20:43:42Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftplusTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftplusTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftplusTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Int() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softplus<TInt32> instance = new Softplus<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Float() {\n+    float[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    float[] expected = {\n+      1.3132616F, 2.126928F, 3.0485873F, 4.01815F, 5.0067153F, 6.0024757F, 7.0009117F, 8.000336F\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softplus<TFloat32> instance = new Softplus<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(expected, result);\n+      }\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Double() {\n+    double[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    double[] expected = {\n+      1.3132616875182228, 2.1269280110429727, 3.048587351573742,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2OTEyNw==", "bodyText": "I have modified the input to use a combination of positive and negative values.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498869127", "createdAt": "2020-10-02T14:48:36Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftplusTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftplusTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftplusTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Int() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softplus<TInt32> instance = new Softplus<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Float() {\n+    float[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    float[] expected = {\n+      1.3132616F, 2.126928F, 3.0485873F, 4.01815F, 5.0067153F, 6.0024757F, 7.0009117F, 8.000336F\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softplus<TFloat32> instance = new Softplus<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(expected, result);\n+      }\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Double() {\n+    double[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    double[] expected = {\n+      1.3132616875182228, 2.1269280110429727, 3.048587351573742,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMjU5Mw=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDgyNTQ5OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDo0NToyNlrOHbaO1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0OTowMlrOHbwkpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMzM4Mg==", "bodyText": "It would be nice to have a test for the 3-tensor and a vector input as they have different code paths.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498503382", "createdAt": "2020-10-01T20:45:26Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftmaxTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftmaxTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testIntThrowsIAE() {\n+    int[][] input = {{1, -2, 3, -4}, {-1, 2, -3, 4}};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softmax<TInt32> instance = new Softmax<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Float() {\n+    float[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};\n+    float[][] expected = {\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f},\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f}\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softmax<TFloat32> instance = new Softmax<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(tf.constant(expected), result);\n+      }\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Double() {\n+    double[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2OTQxMw==", "bodyText": "I have added a test for 3D input", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498869413", "createdAt": "2020-10-02T14:49:02Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftmaxTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftmaxTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testIntThrowsIAE() {\n+    int[][] input = {{1, -2, 3, -4}, {-1, 2, -3, 4}};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softmax<TInt32> instance = new Softmax<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Float() {\n+    float[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};\n+    float[][] expected = {\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f},\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f}\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softmax<TFloat32> instance = new Softmax<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(tf.constant(expected), result);\n+      }\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Double() {\n+    double[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMzM4Mg=="}, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzcxODMyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ReLUTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODowMDowNFrOHb2qlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMDo1NjowMVrOHb7YhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTIzNw==", "bodyText": "Still snake_case, looks like the find replace didn't get all of them.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498969237", "createdAt": "2020-10-02T18:00:04Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ReLUTest.java", "diffHunk": "@@ -82,7 +82,7 @@ public void testCall__Long() {\n \n   /** Test of ReLU call method */\n   @Test\n-  public void testCall__Float16() {\n+  public void testCall_Float16() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA0NjUzMg==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499046532", "createdAt": "2020-10-02T20:56:01Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ReLUTest.java", "diffHunk": "@@ -82,7 +82,7 @@ public void testCall__Long() {\n \n   /** Test of ReLU call method */\n   @Test\n-  public void testCall__Float16() {\n+  public void testCall_Float16() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTIzNw=="}, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzcxOTI1OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODowMDoyM1rOHb2rMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMDo1NjoxMVrOHb7YwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTM5Mw==", "bodyText": "Swap the snake_case for camelCase.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498969393", "createdAt": "2020-10-02T18:00:23Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -109,4 +111,38 @@ public void testSoftmax_Ops_Operand_Double_Negative() {\n         session.evaluate(tf.constant(expected), result);\n       }\n   }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_1D() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA0NjU5Mg==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499046592", "createdAt": "2020-10-02T20:56:11Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -109,4 +111,38 @@ public void testSoftmax_Ops_Operand_Double_Negative() {\n         session.evaluate(tf.constant(expected), result);\n       }\n   }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_1D() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTM5Mw=="}, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTAzMjI2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoxMTo1OVrOHcCVtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMDowMDoyNFrOHmL25w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ==", "bodyText": "Is it ok for the caller to provide null in this constructor and then use setTf(Ops) later? How do we feel about using  nullability annotations?\nI'd argue that at minimum, for our public APIs, we should always document whether null is acceptable or not (unless utterly clear from the context), and that the annotations are an easier and more useful way of doing that. Here's someone who landed on using JSR 305 annotations and provides a careful explanation of why.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499160501", "createdAt": "2020-10-03T16:11:59Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMTUzMg==", "bodyText": "We've already discussed about adding the @Nullable annotation but this is not part of the standard JDK and we need to bring additional dependencies, which we try to avoid as much as possible to avoid conflict with other version of that same dependency that might have been imported by the user.\nThat's being said, when we discussed about it, it was related to the Core API. I'm OK though to bring more dependencies in the framework if we need to.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505121532", "createdAt": "2020-10-15T02:07:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU4MDExMw==", "bodyText": "The present @Nullable is in a private Java api and will break after Java 8.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509580113", "createdAt": "2020-10-21T18:53:19Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjIxNQ==", "bodyText": "Ok I suggest we keep the @Nullable introduction for the future, outside this PR", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509802215", "createdAt": "2020-10-22T00:00:24Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTAzODczOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoyMTo1M1rOHcCY2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMTozNTo0MFrOHmNdEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg==", "bodyText": "Here, for example, is where it seems clear enough to me that input shouldn't be nullable, that I wouldn't care as much about documenting or annotating it. That said, my personal favorite would be to thoroughly annotate, which could be partly done through package-level defaults.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161306", "createdAt": "2020-10-03T16:21:53Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMjYxOQ==", "bodyText": "I agree with annotating @Nullable fields in the framework, especially that other JVM languages like Kotlin handle nullable values in a very specific way. Should we do it in this PR or go through all our classes later?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505122619", "createdAt": "2020-10-15T02:11:57Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjQzNTg5NQ==", "bodyText": "My leaning would be to do it as a separate complete pass that results in a new invariant in the build, like tool X with configuration Y gives no warnings. Also, that would let us agree on conventions and then bootstrap following them throughout the framework API.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r506435895", "createdAt": "2020-10-16T13:41:59Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU4MTQ4Nw==", "bodyText": "OK", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509581487", "createdAt": "2020-10-21T18:54:19Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc1NjM3Mw==", "bodyText": "BTW, we can use Lombok for that kind of stuff. It needs a dependency that's used only by the compiler, it's not required at runtime.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509756373", "createdAt": "2020-10-21T22:25:05Z", "author": {"login": "saudet"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyNzAyNg==", "bodyText": "But like I was explaining, @Nullable has some other benefits, e.g. this is what Kotlin compiler uses to know if a value from an external Java library is nullable or not (since it handles these two differently).\nAnyway as I suggested previously, we should probably keep this for another PR.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509827026", "createdAt": "2020-10-22T01:30:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyODM2OQ==", "bodyText": "Hum, from what I understand that's what Optional is for and that's in Java SE 8. Wouldn't that do? We can make it such that anything that isn't Optional can't be null. Anyway, something for a future PR, yes.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509828369", "createdAt": "2020-10-22T01:35:40Z", "author": {"login": "saudet"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTAzOTQ1OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoyMzoyOVrOHcCZOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxODo1NTozMlrOHl-fOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTQwMA==", "bodyText": "&gt -> &lt", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161400", "createdAt": "2020-10-03T16:23:29Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU4MzE2Mw==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509583163", "createdAt": "2020-10-21T18:55:32Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTQwMA=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA0MTc0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoyNzowN1rOHcCaSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxOToyMDoyNFrOHcJmxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTY3Mw==", "bodyText": "ELU -> Operand", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161673", "createdAt": "2020-10-03T16:27:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI3OTU1Nw==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499279557", "createdAt": "2020-10-04T19:20:24Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTY3Mw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA1MTU0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjo0MzoyMVrOHcCfGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOToxNzowN1rOHl_q4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg==", "bodyText": "This will come up a lot! Define a helper somewhere?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499162906", "createdAt": "2020-10-03T16:43:21Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMzY4MA==", "bodyText": "Ultimately, we would have a TFloating family that can the be bound to the T parameter of that method (this family is present in PR #92 ).", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505123680", "createdAt": "2020-10-15T02:15:41Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjQzNzIwMA==", "bodyText": "Good point, so this is fine for now.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r506437200", "createdAt": "2020-10-16T13:43:32Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODEwNjk0MA==", "bodyText": "I would even say that if you feel adding TFloating right now @JimClarke5 , go ahead, just extend TNumber from it and implement it in all floating point tensor types we have (TFloat32, TFloat16, ...). Then you can remove all these type checks at runtime by binding T to TFloating, i.e. Exponential<T extends TFloating> { ... }", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508106940", "createdAt": "2020-10-19T22:49:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5MjA3Ng==", "bodyText": "@karllessard  Ok, did we want to also add TInteger while we're at it?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509592076", "createdAt": "2020-10-21T19:03:31Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwMjUyOA==", "bodyText": "Added TFloating extends TNumber and modified TBfloat16, TFloat16, TFloat32, and TFloat64 to extend TFloating. Activation classes have been modified for TFloating where appropriate.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509602528", "createdAt": "2020-10-21T19:17:07Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA2MjYzOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzowMjo1MFrOHcCkmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMToyNjoxNlrOHcYvYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw==", "bodyText": "What would you think about moving both the declaration and the initialization of negativePart down to immediately before it is used? I realize that would require a change to keep input as the original input instead of reusing it for intermediate calculations, but personally I'd see that as a feature not a bug.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499164313", "createdAt": "2020-10-03T17:02:50Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTU4MA==", "bodyText": "Perhaps rename negativePart -> amountBelowThreshold? For one thing, negativePart is vague to me. For another, it's always positive.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499165580", "createdAt": "2020-10-03T17:19:01Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MDMwNw==", "bodyText": "I am not sure if this is used here as a math term, but I found Wolfram MathWorld - Negative Part. Perhaps @Craigacp  and @karllessard  might want to comment.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499280307", "createdAt": "2020-10-04T19:28:27Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUyNzUyMw==", "bodyText": "Good point, that Wolfram definition does show why negativePart was used in the first place. That said, I'd still advocate that in the presence of threshold, it's a confusing name.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499527523", "createdAt": "2020-10-05T11:26:16Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA3MDA2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzoxNDo1MlrOHcCoTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMToyNzoyM1rOHcYxvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTI2MQ==", "bodyText": "Could we adapt this part of the Keras documentation?\nalpha: A float that governs the slope for values lower than the threshold.\nmax_value: A float that sets the saturation threshold (the largest value the function will return).\nthreshold: A float giving the threshold value of the activation function below which values will be damped or set to zero.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499165261", "createdAt": "2020-10-03T17:14:52Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MDY5Mg==", "bodyText": "I am not sure there is an advantage in Java to specify a \"A float\" in the JavaDoc comment, as it is already defined as a float in the method signature. I could see in Python why this may be necessary.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499280692", "createdAt": "2020-10-04T19:32:20Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTI2MQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUyODEyNw==", "bodyText": "For sure! But the text like \"the threshold value of the activation function below which values will be damped or set to zero\" may be helpful.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499528127", "createdAt": "2020-10-05T11:27:23Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTI2MQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA4MzMwOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzozODowNVrOHcCvAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo0MTozMFrOHcZN1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2Njk3Ng==", "bodyText": "I'm finding it extremely difficult to think through the interactions of all these options. E.g. if we calculate negativePart before applying clipMax, and then we clip the input, and then we subtract a fraction of negativePart from the input, did we do the right thing? How about if there was also a threshold?\nPerhaps either there's some reasoning we could document up front, or somehow there's a way to order the computation and name the intermediate results so it's clearly correct?\nPerhaps the problem is that there are neither documentation, nor examples, nor tests that show how these parameters are supposed to interact.\nOr if all of this accurately mimics the Python implementation, perhaps we leave it for now while opening a ticket for some newbie like me to come back and tidy it up?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499166976", "createdAt": "2020-10-03T17:38:05Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);\n+    }\n+\n+    if (alpha != 0.) {\n+      input =\n+          tf.math.sub(\n+              input, tf.math.mul(tf.dtypes.cast(tf.constant(alpha), dataType), negativePart));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MTM1Mg==", "bodyText": "This was ported mainly line by line from the Python ReLU method in Keras backend. Java does not have operator overload, so it is a little more cumbersome code than the original python. For example, Python\nx -= alpha * negative_part\ntranslates to TF Java as:\ninput =\n          tf.math.sub(\n              input, tf.math.mul(tf.dtypes.cast(tf.constant(alpha), dataType), negativePart));\n\nThere may be some benefit to breaking this into individual statements, but TF Java in  Graph mode is very difficult to actually debug, so I don't see an advantage there. When I have to debug it, I use special code to print the values using a try(Tensor...) logic.  Perhaps though, we could leave the Python equation in as comments.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499281352", "createdAt": "2020-10-04T19:40:11Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);\n+    }\n+\n+    if (alpha != 0.) {\n+      input =\n+          tf.math.sub(\n+              input, tf.math.mul(tf.dtypes.cast(tf.constant(alpha), dataType), negativePart));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2Njk3Ng=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUzNTMxOA==", "bodyText": "My difficulty with the code isn't with this sort of tactic. It's with the overall flow of the computation and the variable names used. Given that it's transliterated from the Python and that the same tests pass, seems good to leave it for another day.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499535318", "createdAt": "2020-10-05T11:41:30Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);\n+    }\n+\n+    if (alpha != 0.) {\n+      input =\n+          tf.math.sub(\n+              input, tf.math.mul(tf.dtypes.cast(tf.constant(alpha), dataType), negativePart));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2Njk3Ng=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTA4MzY0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzozODo1MlrOHcCvLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOToyMTo1MVrOHl_1PA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NzAyMA==", "bodyText": "Is zero still the right lower clip if there's a threshold?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499167020", "createdAt": "2020-10-03T17:38:52Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwNTE4MA==", "bodyText": "The numbers less than the threshold get converted to zero before the clipMax is applied.\nSo, there is no need to move the lower bound on the clipByValue operation.\nif (threshold != 0) {\n      // computes input for input > threshold else 0\n      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509605180", "createdAt": "2020-10-21T19:21:51Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NzAyMA=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTcyNTAyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMzozNDoxMFrOHcHpRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxOTo1NDowNVrOHcJywQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0NzQzMA==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499247430", "createdAt": "2020-10-04T13:34:10Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MjYyNQ==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499282625", "createdAt": "2020-10-04T19:54:05Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0NzQzMA=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTcyOTU3OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMzo0MDoxOFrOHcHryA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxOTo1NToyN1rOHcJzQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODA3Mg==", "bodyText": "> -> &gt;", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499248072", "createdAt": "2020-10-04T13:40:18Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4Mjc1Mg==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499282752", "createdAt": "2020-10-04T19:55:27Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODA3Mg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTczMzgyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Sigmoid.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMzo0NToyNVrOHcHt2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxOTo1NjoxNlrOHcJzgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODYwMA==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499248600", "createdAt": "2020-10-04T13:45:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Sigmoid.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Sigmoid activation. <code>sigmoid(x) = 1 / (1 + exp(-x))</code>.\n+ *\n+ * <p>Applies the sigmoid activation function. For small values (<-5), <code>sigmoid</code> returns\n+ * a value close to zero, and for large values (>5) the result of the function gets close to 1.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MjgxNg==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499282816", "createdAt": "2020-10-04T19:56:16Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Sigmoid.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Sigmoid activation. <code>sigmoid(x) = 1 / (1 + exp(-x))</code>.\n+ *\n+ * <p>Applies the sigmoid activation function. For small values (<-5), <code>sigmoid</code> returns\n+ * a value close to zero, and for large values (>5) the result of the function gets close to 1.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODYwMA=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgwNjUyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNToyNjowNFrOHcITnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxOTo1OTowOVrOHcJ0bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODI2OQ==", "bodyText": "Sadly, the trailing period freaks out Maven's javadoc generation.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258269", "createdAt": "2020-10-04T15:26:04Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;\n+ *     ELU&lt;TFloat32&gt;elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MzA1Mg==", "bodyText": "Removed trailing periods on @see", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283052", "createdAt": "2020-10-04T19:59:09Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;\n+ *     ELU&lt;TFloat32&gt;elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODI2OQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgwNzQ0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/HardSigmoid.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNToyNzowN1rOHcIUCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDowMDozMlrOHcJ03g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODM3Nw==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258377", "createdAt": "2020-10-04T15:27:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/HardSigmoid.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hard sigmoid activation.\n+ *\n+ * <p>A faster approximation of the sigmoid activation.\n+ *\n+ * <p>Defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x < -2.5: return 0</code>\n+ *   <li><code>if x > 2.5: return 1</code>\n+ *   <li><code>if -2.5 <= x <= 2.5: return 0.2 * x + 0.5</code>\n+ * </ul>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MzE2Ng==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283166", "createdAt": "2020-10-04T20:00:32Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/HardSigmoid.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hard sigmoid activation.\n+ *\n+ * <p>A faster approximation of the sigmoid activation.\n+ *\n+ * <p>Defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x < -2.5: return 0</code>\n+ *   <li><code>if x > 2.5: return 1</code>\n+ *   <li><code>if -2.5 <= x <= 2.5: return 0.2 * x + 0.5</code>\n+ * </ul>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODM3Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgxMTMyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNTozMjozNVrOHcIV8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDowMzoyN1rOHcJ15w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODg2NQ==", "bodyText": "trailing period freaks out Maven's javadoc generation", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258865", "createdAt": "2020-10-04T15:32:35Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MzQzMQ==", "bodyText": "Removed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283431", "createdAt": "2020-10-04T20:03:27Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODg2NQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgxOTEzOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNTo0MjoyMlrOHcIZ0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOToyMjo1MlrOHl_3SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTg1Nw==", "bodyText": "This is boilerplate for us. Although it could be greatly reduced with a helper for the isFloating check, I wonder whether it's worth creating a subclass FloatingActivation that does the isFloating check and invokes a protected Operand<T extends TNumber> callFloating(Operand<T extends TNumber> input)? (Where TNumber would be replaced by TFloating as we build out our type family support.)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499259857", "createdAt": "2020-10-04T15:42:22Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.\n+ */\n+public class SELU<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Scaled Exponential Linear Unit (SELU) activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public SELU(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwNTcwNA==", "bodyText": "This is no longer the case after implementing TFloating.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509605704", "createdAt": "2020-10-21T19:22:52Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.\n+ */\n+public class SELU<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Scaled Exponential Linear Unit (SELU) activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public SELU(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTg1Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgyMDAxOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNTo0Mzo1M1rOHcIaVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDowNDo0MlrOHcJ2Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTk5MA==", "bodyText": "extra right angle bracket", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499259990", "createdAt": "2020-10-04T15:43:53Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MzU1NQ==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283555", "createdAt": "2020-10-04T20:04:42Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTk5MA=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgyNzE1OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNTo1NDowN1rOHcIeMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMTozNToxNFrOHmNcng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MDk3Nw==", "bodyText": "Perhaps worth defining an Operand.shape()?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499260977", "createdAt": "2020-10-04T15:54:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4Mzc2OQ==", "bodyText": "There are 2 shapes, which one, asOutput() or asTensor()?   They may be different at different times.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283769", "createdAt": "2020-10-04T20:06:45Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MDk3Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUyOTM1Ng==", "bodyText": ":-) oh!", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499529356", "createdAt": "2020-10-05T11:29:39Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MDk3Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyODI1NA==", "bodyText": "I think it could be reasonable still to have a Operand.shape() operation... let's keep that anyway for another PR", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509828254", "createdAt": "2020-10-22T01:35:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MDk3Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTgzMDg0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Swish.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNjowMDoxN1rOHcIgGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDowNzoxM1rOHcJ3Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MTQ2Nw==", "bodyText": "trailing period freaks out Maven's javadoc generation", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499261467", "createdAt": "2020-10-04T16:00:17Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Swish.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Swish activation function. <code>swish(x) = x * sigmoid(x)</code>.\n+ *\n+ * <p>Swish activation function which returns <code>x*sigmoid(x)</code>. It is a smooth,\n+ * non-monotonic function that consistently matches or outperforms <code>ReLU</code> on deep\n+ * networks, it is unbounded above and bounded below.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-20, -1.0, 0.0, 1.0, 20});\n+ *     Swish&lt;TFloat32&gt; swish = new Swish&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = swish.call(input);\n+ *     // result = [-4.1223075e-08f, -2.6894143e-01f,  0.0000000e+00f,\n+ *     //          7.3105860e-01f,  2.0000000e+01f ]\n+ *\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1710.05941\">Ramachandran et al., 2017</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4MzgwNw==", "bodyText": "Removed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499283807", "createdAt": "2020-10-04T20:07:13Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Swish.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Swish activation function. <code>swish(x) = x * sigmoid(x)</code>.\n+ *\n+ * <p>Swish activation function which returns <code>x*sigmoid(x)</code>. It is a smooth,\n+ * non-monotonic function that consistently matches or outperforms <code>ReLU</code> on deep\n+ * networks, it is unbounded above and bounded below.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-20, -1.0, 0.0, 1.0, 20});\n+ *     Swish&lt;TFloat32&gt; swish = new Swish&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = swish.call(input);\n+ *     // result = [-4.1223075e-08f, -2.6894143e-01f,  0.0000000e+00f,\n+ *     //          7.3105860e-01f,  2.0000000e+01f ]\n+ *\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1710.05941\">Ramachandran et al., 2017</a>.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MTQ2Nw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzcxOTAzOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Tanh.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo0ODo1N1rOHcZd9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjoyOFrOHkjYBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUzOTQ0NA==", "bodyText": "I like how this eliminates a chunk of (comment) boilerplate! But it doesn't document the IllegalArgumentException on !isFloating. Might be additional motivation to add a FloatingActivation subclass?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499539444", "createdAt": "2020-10-05T11:48:57Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Tanh.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hyperbolic tangent activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-3.0f,-1.0f, 0.0f, 1.0f, 3.0f});\n+ *     Tanh&lt;TFloat32&gt; tanh = new Tanh&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = tanh.call(input);\n+ *     // result = [-0.9950547f, -0.7615942f,  0.f,  0.7615942f,  0.9950547f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Tanh<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Hyperbolic tangent activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Tanh(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /** {@inheritDoc} */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDM3Mg==", "bodyText": "Similar to my previous comment, when we will bind T to TFloating, no doc will be required as no exception will be thrown", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508090372", "createdAt": "2020-10-19T22:06:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Tanh.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hyperbolic tangent activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-3.0f,-1.0f, 0.0f, 1.0f, 3.0f});\n+ *     Tanh&lt;TFloat32&gt; tanh = new Tanh&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = tanh.call(input);\n+ *     // result = [-0.9950547f, -0.7615942f,  0.f,  0.7615942f,  0.9950547f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Tanh<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Hyperbolic tangent activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Tanh(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /** {@inheritDoc} */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUzOTQ0NA=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzcyOTEyOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo1MTo0MFrOHcZjzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNDo1NzowM1rOHmmCdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0MDk0MQ==", "bodyText": "This test will be boilerplate. Perhaps create a helper class or superclass for tests of floating activations?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499540941", "createdAt": "2020-10-05T11:51:40Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class ELUTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public ELUTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallInt() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMTE1OA==", "bodyText": "This test is obsolete after implementing TFloating as it will no longer compile.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r510231158", "createdAt": "2020-10-22T14:57:03Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class ELUTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public ELUTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallInt() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0MDk0MQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzc2MDQ5OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMjowMDoyNVrOHcZ2cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMjowMDoyNVrOHcZ2cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NTcxNQ==", "bodyText": "Perhaps also consolidate this sort of test logic into a helper class? E.g.\nassertFloat32Output(tf -> new ELU<Float32>(tf),\n            {1f, -0.86466473f, 3f, -0.9816844f, -0.63212055f, 2f, -0.95021296f, 4f},\n            {1, -2, 3, -4, -1, 2, -3, 4})", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499545715", "createdAt": "2020-10-05T12:00:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class ELUTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public ELUTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallInt() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Operand<TInt32> input = tf.constant(new int[] {1, 2, 3, 4, 5});\n+              ELU<TInt32> instance = new ELU<>(tf);\n+              Operand<TInt32> result = instance.call(input);\n+            }\n+          });\n+  }\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallFloat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NDA2NTQxOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjowODo1NVrOHhuL9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjowODo1NVrOHhuL9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMTc4MQ==", "bodyText": "Probably a way to avoid these setTF/getTF would be to push created Ops instances on a stack somewhere... but this is out of scope of this PR for sure", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505121781", "createdAt": "2020-10-15T02:08:55Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NDA3NTIwOnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjoxNDoxM1rOHhuRuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxMzo1NDo1NFrOHmi_Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMzI1Nw==", "bodyText": "Nit: no need of a else block if you return unconditionally in the if one", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505123257", "createdAt": "2020-10-15T02:14:13Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &lt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = &#46;&#46;&#46;;\n+ *     ELU&lt;TFloat32&gt; elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>\n+ */\n+public class ELU<T extends TNumber> extends Activation<T> {\n+\n+  private static final double ALPHA_DEFAULT = 1.0;\n+\n+  /** A scalar, slope of negative section. */\n+  private final double alpha;\n+\n+  /**\n+   * Creates a new ELU with alpha={@link #ALPHA_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ELU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ELU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha A scalar, slope of negative section. It controls the value to which an ELU\n+   *     saturates for negative net inputs.\n+   */\n+  public ELU(Ops tf, double alpha) {\n+    super(tf);\n+    this.alpha = alpha;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Operand<T> result = tf.nn.elu(input);\n+    if (alpha == 1.0) {\n+      return result;\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE4MTE0Mw==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r510181143", "createdAt": "2020-10-22T13:54:54Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &lt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = &#46;&#46;&#46;;\n+ *     ELU&lt;TFloat32&gt; elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>\n+ */\n+public class ELU<T extends TNumber> extends Activation<T> {\n+\n+  private static final double ALPHA_DEFAULT = 1.0;\n+\n+  /** A scalar, slope of negative section. */\n+  private final double alpha;\n+\n+  /**\n+   * Creates a new ELU with alpha={@link #ALPHA_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ELU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ELU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha A scalar, slope of negative section. It controls the value to which an ELU\n+   *     saturates for negative net inputs.\n+   */\n+  public ELU(Ops tf, double alpha) {\n+    super(tf);\n+    this.alpha = alpha;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Operand<T> result = tf.nn.elu(input);\n+    if (alpha == 1.0) {\n+      return result;\n+    } else {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMzI1Nw=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NDA5NTc5OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjoyNToyN1rOHhudZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDo1NDoyNlrOHmzcWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNjI0NQ==", "bodyText": "LeakyRelu is not add to tf.nn because it is set as not visible in the main API def proto. To add it to tf.nn, you can override its visibility by setting it to VISIBLE in our proto", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505126245", "createdAt": "2020-10-15T02:25:27Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1Mjk4OA==", "bodyText": "@karllessard should we do this or just leave it as is for internal use? In Python it is available as tf.nn.leaky_relu.\nIf we want to add this, it may be better as a separate PR because of the generated classes being changed.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509652988", "createdAt": "2020-10-21T20:14:16Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNjI0NQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMzQyOQ==", "bodyText": "I suggest we make it visible in this PR, even if the ops will be regenerated, only two files (NnOps and LeakyRelu) should end up being different", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509803429", "createdAt": "2020-10-22T00:04:44Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNjI0NQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MDc3OA==", "bodyText": "I have made the changes to add LeakyRelu to NnOps and generate tf.nn.leakyRelu()", "url": "https://github.com/tensorflow/java/pull/123#discussion_r510450778", "createdAt": "2020-10-22T20:54:26Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNjI0NQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NDEwMTY0OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjoyOTowNFrOHhuhAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOTozMTowOVrOHmAJQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNzE2OQ==", "bodyText": "I would try to avoid changing the value of a method parameter, better use a new variable.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505127169", "createdAt": "2020-10-15T02:29:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxMDMwNA==", "bodyText": "OK", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509610304", "createdAt": "2020-10-21T19:31:09Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNzE2OQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjYzMDE2OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMTo1NjoxOFrOHkjHMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMDoxNDowNVrOHmCu2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4NjA2NQ==", "bodyText": "missing space after </code>", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508086065", "createdAt": "2020-10-19T21:56:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>argument sets which axis of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1MjY5OA==", "bodyText": "Fixed", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509652698", "createdAt": "2020-10-21T20:14:05Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>argument sets which axis of the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4NjA2NQ=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzAxNzg4OnYy", "diffSide": "RIGHT", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SwishTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMDo1MjoyNlrOHkmq-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOTozMzowNFrOHmAP9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0NDM3OA==", "bodyText": "Where were the expected values computed? I would've thought we'd be using the examples provided in the Python API docs: https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508144378", "createdAt": "2020-10-20T00:52:26Z", "author": {"login": "KartikChugh"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SwishTest.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SwishTest {\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SwishTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallInt() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Swish<TInt32> instance = new Swish<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallFloat() {\n+    float[] input = {1, -2, 3, -4, -5, 6, -7, 8};\n+    float[] expected = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxMjAyMQ==", "bodyText": "For activations, the expected values where determined by running the same inputs against Python TF. The Unit test cases in Python made heavy use of numpy and this is too much to duplicate in Java.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509612021", "createdAt": "2020-10-21T19:33:04Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SwishTest.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SwishTest {\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SwishTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallInt() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Swish<TInt32> instance = new Swish<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallFloat() {\n+    float[] input = {1, -2, 3, -4, -5, 6, -7, 8};\n+    float[] expected = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0NDM3OA=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzM5MjUwOnYy", "diffSide": "RIGHT", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMDowMTo0OFrOHmL4gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTowNDo1NlrOHmmaJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg==", "bodyText": "Provably that the TNumber import is not required anymore?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509802626", "createdAt": "2020-10-22T00:01:48Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwODUwMw==", "bodyText": "@karllessard I don\u2019t fully understand your comment. Did you mean TNumber is not required for most Activations or not required at all? TFloating works well with those activations that only work with floating point. But there is still use for TNumber when any number type is accepted.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509808503", "createdAt": "2020-10-22T00:20:59Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg=="}, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyNTc1Mw==", "bodyText": "No, I just meant that in these type classes TFloat32, TFloat16, etc., the TNumber seems not to be used anymore since you replaced it by TFloating therefore there is no need to import it :)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509825753", "createdAt": "2020-10-22T01:25:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg=="}, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgyNjIyOQ==", "bodyText": "actually my comment should have been put on the line below... sorry, that is kind of things that happens when you review a PR on a iPad", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509826229", "createdAt": "2020-10-22T01:27:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg=="}, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzNzIyMA==", "bodyText": "Interesting the google-java-format did not remove the TNumber import, I had to remove them all by hand.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r510237220", "createdAt": "2020-10-22T15:04:56Z", "author": {"login": "JimClarke5"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg=="}, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1637, "cost": 1, "resetAt": "2021-11-13T14:23:39Z"}}}