{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2NDgxNzI2", "number": 123, "title": "Add Activations", "bodyText": "This is a redone PR to make sure the Activations branch did not rely on the Initializers branch.\nI have removed all the no-arg CTORs. The original motivation was that I found previously in looking at Keras layers is that  objects like \u201cactivations\u201d, \u201closs\u201d and \u201cmetric\"  most likely would  need to be constructed\nbefore the TensorFlow Ops is available within the model, hence I make constructors without Ops and used setTF() so the Model could set it later. The model itself hides Ops tf from the user. We have decided to address this requirement later when we start looking at Model.\nI fixed HardSigmoid, as I forgot to apply a clipByValue operation on the final result. Also fixed the Unit test.\nI have verified all the Unit tests against their TF Python corresponding methods.\nFor Softmax, we eliminated the IllegalArgmentException on 1D input that was in the Keras version.\nIn Swish, there is a comment on not handling the grad argument from the corresponding Python class. This is a Python optimization and will not be implemented in Java, at least at this time.", "createdAt": "2020-10-01T19:01:28Z", "url": "https://github.com/tensorflow/java/pull/123", "merged": true, "mergeCommit": {"oid": "6f3ec0f3f2399b707947fd47d4afe897012091e8"}, "closed": true, "closedAt": "2020-10-24T16:20:04Z", "author": {"login": "JimClarke5"}, "timelineItems": {"totalCount": 67, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5ca-JAH2gAyNDk2NDgxNzI2OmVmMGNlNjdhMmU3OTYyODM4MDcyOWNmMTZmYjBjZDRkOTljYmU2YTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdVtnyhAFqTUxNjI4OTA1Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "committedDate": "2020-07-28T20:26:34Z", "message": "Initial checkin of Keras Optimzers and helper classes.\nFixed dependencies in pom.xml"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c113a7e6dfcc98c0e57bede8dcb46de56125320", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/9c113a7e6dfcc98c0e57bede8dcb46de56125320", "committedDate": "2020-08-20T12:12:27Z", "message": "Added static final NAME to replace hardcoded String in the create method. This allows the NAME to be used elsewhere instead of hardcoding the string."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "824d4872257a5a614a53c7dff579748b819c800e", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/824d4872257a5a614a53c7dff579748b819c800e", "committedDate": "2020-08-20T12:14:26Z", "message": "Changed of method to use the DataType NAME attribute rather than hardcoding the string.\nadded methods isFloating(), isInteger(), isNUmeric(), isBoolean() and isString()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07a83a5aef5a25f385aa762e3bb929e22c8052e3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/07a83a5aef5a25f385aa762e3bb929e22c8052e3", "committedDate": "2020-08-20T12:17:40Z", "message": "Added method WriteFieldWithInitializer to output a \"final static String OP_NAME\" to each generated operation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d26831bab64cc4fdd2340403cafa229f0fb7099", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/3d26831bab64cc4fdd2340403cafa229f0fb7099", "committedDate": "2020-08-20T13:52:13Z", "message": "Added tf.nn.softmaxCrossEntropyWitLogits() and tf.nn.raw.softmaxCrossEntropyWitLogits()\nAdded tf.nn.sparesSoftmaxCrossEntropyWithLogits() and\ntf.nn.raw.sparesSoftmaxCrossEntropyWithLogits()\n\nAdded tf.nn.sigmoidCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11cda5fde99a6bec21fb04ed1465934ec1889485", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/11cda5fde99a6bec21fb04ed1465934ec1889485", "committedDate": "2020-08-20T13:58:05Z", "message": "Moved SoftmaxCrossEntropyWithLogits and  SparseSoftmaxCrossEntropyWithLogits to org.tensorflow.op.nn.raw"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "committedDate": "2020-08-20T14:00:25Z", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84f49db3fb6c085befabfcb8356ab77589facf5d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/84f49db3fb6c085befabfcb8356ab77589facf5d", "committedDate": "2020-08-20T14:03:38Z", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "208b84a1a09d38ab9821d66572c1328326685c5a", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/208b84a1a09d38ab9821d66572c1328326685c5a", "committedDate": "2020-08-20T14:42:22Z", "message": "fix dependencies for other Tensorflow Java modules"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "committedDate": "2020-08-20T14:46:20Z", "message": "formatting fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "committedDate": "2020-08-20T14:48:23Z", "message": "Fix ctors with name to properly pass the name to the the super ctor."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcba0a525d4712fca5210db71c141bc1bce87307", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/fcba0a525d4712fca5210db71c141bc1bce87307", "committedDate": "2020-08-20T17:04:27Z", "message": "change asserts to IllegalArgumentException\nfix javadoc, fix casts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "committedDate": "2020-08-20T17:05:22Z", "message": "change asserts to IllegalArgumentException"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d37298a6b9dcb206cdff985b96e340f7f93a075c", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d37298a6b9dcb206cdff985b96e340f7f93a075c", "committedDate": "2020-08-20T17:06:58Z", "message": "Moved back to tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c68812cc9c8815b0ea186f273748078c4907fbaa", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c68812cc9c8815b0ea186f273748078c4907fbaa", "committedDate": "2020-08-20T17:08:44Z", "message": "Moved SoftmaxCrossEntropyWithLogits.java and SparseSoftmaxCrossEntropyWithLogits.java to nn.raw,\nadded new versions of these to NnOps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "committedDate": "2020-08-20T17:50:01Z", "message": "Deleted files that are not necessary yet"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6515c248ddd8f6911f267bee0972d44ca20f0038", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/6515c248ddd8f6911f267bee0972d44ca20f0038", "committedDate": "2020-08-20T17:51:04Z", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76d0fe553559176a33040b4bdb8e07d8e033b08e", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/76d0fe553559176a33040b4bdb8e07d8e033b08e", "committedDate": "2020-08-20T18:30:16Z", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d2201df3a78e82c79b142fc7d7c3a461afa63444", "committedDate": "2020-08-20T19:43:35Z", "message": "Merge branch 'master' into master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "committedDate": "2020-09-03T14:21:27Z", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "889d67e11ec7154605a0cb235097e30c53a5704a", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/889d67e11ec7154605a0cb235097e30c53a5704a", "committedDate": "2020-09-03T14:21:33Z", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "515b799bf793fbb38a47b5aa92d948dad052187b", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/515b799bf793fbb38a47b5aa92d948dad052187b", "committedDate": "2020-09-03T14:34:57Z", "message": "Reformatted code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "committedDate": "2020-09-03T14:35:19Z", "message": "Added sub scope"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d21dd7266e48a4df36813ed3fb9dc1adee58915", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/8d21dd7266e48a4df36813ed3fb9dc1adee58915", "committedDate": "2020-09-03T16:30:10Z", "message": "Miscellaneous fixes based on review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c3cc78a999505240be35040b8865d86b955e1af", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/4c3cc78a999505240be35040b8865d86b955e1af", "committedDate": "2020-09-03T22:52:11Z", "message": "Fixed op_generator.cc to remove a spurious new line in the generated Java files for some Ops. This also  resulted in new generated  source that are also committed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44f530f292fdba34164de696fb454b30108064d3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/44f530f292fdba34164de696fb454b30108064d3", "committedDate": "2020-09-03T22:53:44Z", "message": "Changed back to non-generic Operand until we resolve how to handle generics."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b8d3ac2d001251c95bd55ed9cd902431108468bd", "committedDate": "2020-09-03T22:55:55Z", "message": "Regenerated due to creation of SoftmaxCrossEntropyWithLogits.java,  SigmoidCrossEntropyWithLogits.java, and SparseSoftmaxCrossEntropyWithLogits.java under package org.tensorflow.op.nn in"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c32fc5be951166ffde8d8763dcc99dd7e5879e86", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c32fc5be951166ffde8d8763dcc99dd7e5879e86", "committedDate": "2020-09-07T18:15:30Z", "message": "change snake case to camel case. format code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "committedDate": "2020-09-07T18:38:12Z", "message": "clean upd warning,  format code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9c3134742e9155e457be157515d67b5c0bb7ac4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/e9c3134742e9155e457be157515d67b5c0bb7ac4", "committedDate": "2020-09-09T20:14:20Z", "message": "Added Adamax, Ftrl, and Nadam Optimizers. Added Optimizers enum for easy inclusion of a default optimizer. Cleaned up JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c30a72fa335f338727358b4299e4796a211403d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/5c30a72fa335f338727358b4299e4796a211403d", "committedDate": "2020-09-09T20:17:16Z", "message": "Removed optimize classes from tensorflow-keras, moved optimizer test cases to framework. Created Tests for GradientDescent and Momentum"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "committedDate": "2020-09-09T20:17:37Z", "message": "Fixed generics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7915e6309e9db7a536cda24eac8264578cdbfe31", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/7915e6309e9db7a536cda24eac8264578cdbfe31", "committedDate": "2020-09-09T23:03:09Z", "message": "Fixed from Unit test results"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec4f6790ff666a4c23f60e1b4764874d6167d392", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ec4f6790ff666a4c23f60e1b4764874d6167d392", "committedDate": "2020-09-09T23:08:41Z", "message": "added @SuppressWarnings(\"unchecked\") on Variable array"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c86d09b74a4a02b173604f206161ed37b804d0a9", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c86d09b74a4a02b173604f206161ed37b804d0a9", "committedDate": "2020-09-18T23:51:10Z", "message": "Merge pull request #1 from tensorflow/master\n\nSync Clarke fork"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a670ec9eef5a2843dca1cd505d1c5589e3de6f8", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/1a670ec9eef5a2843dca1cd505d1c5589e3de6f8", "committedDate": "2020-09-30T19:27:47Z", "message": "Added Support for evaluating TFloat16"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cc9b9cde27e28afe16accfc785380e8ff39929d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/0cc9b9cde27e28afe16accfc785380e8ff39929d", "committedDate": "2020-10-01T18:34:59Z", "message": "Add Activations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca77a0b78e419447a994e3c919c39689575bb8f4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ca77a0b78e419447a994e3c919c39689575bb8f4", "committedDate": "2020-10-01T18:37:17Z", "message": "Remove no-arg CTORs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/73091becbb882d55295fa16256c1133ed9c87be4", "committedDate": "2020-10-01T18:39:38Z", "message": "Fix Unit Tests to include positive and negative numbers on input."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNjg0OTg1", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-500684985", "createdAt": "2020-10-01T20:14:59Z", "commit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoxNDo1OVrOHbZYrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDo0NToyNlrOHbaO1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4OTUxNg==", "bodyText": "How does this emit negative zero? Could we catch that and emit a positive zero?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498489516", "createdAt": "2020-10-01T20:14:59Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5MzE0MA==", "bodyText": "Does this work on a vector now the check has been removed?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498493140", "createdAt": "2020-10-01T20:22:44Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.reduce_sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();\n+    int numDimensions = shape.numDimensions();\n+    if (numDimensions == 2) {\n+      return tf.nn.softmax(input);\n+    } else {\n+      Operand<T> e =\n+          tf.math.exp(\n+              tf.math.sub(input, tf.reduceMax(input, tf.constant(axis), ReduceMax.keepDims(true))));\n+      Operand<T> s = tf.reduceSum(input, tf.constant(axis), ReduceSum.keepDims(true));\n+      return tf.math.div(e, s);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ5NDE5OQ==", "bodyText": "This should probably note that the linear activation function is the identity function.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498494199", "createdAt": "2020-10-01T20:25:09Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Linear.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Linear activation function.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMjU5Mw==", "bodyText": "This should test that a negative or near zero value behaves appropriately rather than just the linear region.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498502593", "createdAt": "2020-10-01T20:43:42Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftplusTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftplusTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftplusTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Int() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softplus<TInt32> instance = new Softplus<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Float() {\n+    float[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    float[] expected = {\n+      1.3132616F, 2.126928F, 3.0485873F, 4.01815F, 5.0067153F, 6.0024757F, 7.0009117F, 8.000336F\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softplus<TFloat32> instance = new Softplus<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(expected, result);\n+      }\n+  }\n+\n+  /** Test of Softplus call method */\n+  @Test\n+  public void testCall__Double() {\n+    double[] input = {1, 2, 3, 4, 5, 6, 7, 8};\n+    double[] expected = {\n+      1.3132616875182228, 2.1269280110429727, 3.048587351573742,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUwMzM4Mg==", "bodyText": "It would be nice to have a test for the 3-tensor and a vector input as they have different code paths.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498503382", "createdAt": "2020-10-01T20:45:26Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SoftmaxTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SoftmaxTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testIntThrowsIAE() {\n+    int[][] input = {{1, -2, 3, -4}, {-1, 2, -3, 4}};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Softmax<TInt32> instance = new Softmax<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Float() {\n+    float[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};\n+    float[][] expected = {\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f},\n+      {0.032059f, 0.087144f, 0.236883f, 0.643914f}\n+    };\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession session = TestSession.createTestSession(tfMode)) {\n+        Ops tf = session.getTF();\n+        Softmax<TFloat32> instance = new Softmax<>(tf);\n+        Operand<TFloat32> result = instance.call(tf.constant(input));\n+        session.evaluate(tf.constant(expected), result);\n+      }\n+  }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_Ops_Operand_Double() {\n+    double[][] input = {{1, 2, 3, 4}, {5, 6, 7, 8}};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73091becbb882d55295fa16256c1133ed9c87be4"}, "originalPosition": 82}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "946d1d5251c3f9b6b9219e68cea6134f7985224f", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/946d1d5251c3f9b6b9219e68cea6134f7985224f", "committedDate": "2020-10-02T14:42:34Z", "message": "Modify JavaDoc indicating Linear activation is also known as Identity activation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c5cc4add70e70847e6852c6a3fce424b70d8925", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/7c5cc4add70e70847e6852c6a3fce424b70d8925", "committedDate": "2020-10-02T14:43:16Z", "message": "Changed DEFAULT values from private to public"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e32fe44207a95105f8d3babbb157197df59bd5ef", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/e32fe44207a95105f8d3babbb157197df59bd5ef", "committedDate": "2020-10-02T14:44:03Z", "message": "Fixed last sum to be over 'e' instead of 'input'"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "013091422585ffab264f80f0ff4036d5a714f2a6", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/013091422585ffab264f80f0ff4036d5a714f2a6", "committedDate": "2020-10-02T14:45:12Z", "message": "Added tests for various parameter constructs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c7d04774ea5e6a337f8f79c254deefb46d6270a6", "committedDate": "2020-10-02T14:45:36Z", "message": "added tests for 1D and 3D input"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMzM4OTU5", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-501338959", "createdAt": "2020-10-02T18:00:04Z", "commit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODowMDowNFrOHb2qlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODowMDoyM1rOHb2rMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTIzNw==", "bodyText": "Still snake_case, looks like the find replace didn't get all of them.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498969237", "createdAt": "2020-10-02T18:00:04Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ReLUTest.java", "diffHunk": "@@ -82,7 +82,7 @@ public void testCall__Long() {\n \n   /** Test of ReLU call method */\n   @Test\n-  public void testCall__Float16() {\n+  public void testCall_Float16() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk2OTM5Mw==", "bodyText": "Swap the snake_case for camelCase.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r498969393", "createdAt": "2020-10-02T18:00:23Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SoftmaxTest.java", "diffHunk": "@@ -109,4 +111,38 @@ public void testSoftmax_Ops_Operand_Double_Negative() {\n         session.evaluate(tf.constant(expected), result);\n       }\n   }\n+\n+  /** Test of Softmax method, of class Activations. */\n+  @Test\n+  public void testSoftmax_1D() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7d04774ea5e6a337f8f79c254deefb46d6270a6"}, "originalPosition": 16}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/de0e610ad8a7d732da9a11c319e6d0495e66b9b4", "committedDate": "2020-10-02T20:55:39Z", "message": "Change snake case to camel case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTU2NTU3", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-501556557", "createdAt": "2020-10-03T16:11:59Z", "commit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoxMTo1OVrOHcCVtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzozODo1MlrOHcCvLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ==", "bodyText": "Is it ok for the caller to provide null in this constructor and then use setTf(Ops) later? How do we feel about using  nullability annotations?\nI'd argue that at minimum, for our public APIs, we should always document whether null is acceptable or not (unless utterly clear from the context), and that the annotations are an easier and more useful way of doing that. Here's someone who landed on using JSR 305 annotations and provides a careful explanation of why.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499160501", "createdAt": "2020-10-03T16:11:59Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg==", "bodyText": "Here, for example, is where it seems clear enough to me that input shouldn't be nullable, that I wouldn't care as much about documenting or annotating it. That said, my personal favorite would be to thoroughly annotate, which could be partly done through package-level defaults.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161306", "createdAt": "2020-10-03T16:21:53Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTQwMA==", "bodyText": "&gt -> &lt", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161400", "createdAt": "2020-10-03T16:23:29Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTY3Mw==", "bodyText": "ELU -> Operand", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499161673", "createdAt": "2020-10-03T16:27:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg==", "bodyText": "This will come up a lot! Define a helper somewhere?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499162906", "createdAt": "2020-10-03T16:43:21Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw==", "bodyText": "What would you think about moving both the declaration and the initialization of negativePart down to immediately before it is used? I realize that would require a change to keep input as the original input instead of reusing it for intermediate calculations, but personally I'd see that as a feature not a bug.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499164313", "createdAt": "2020-10-03T17:02:50Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTI2MQ==", "bodyText": "Could we adapt this part of the Keras documentation?\nalpha: A float that governs the slope for values lower than the threshold.\nmax_value: A float that sets the saturation threshold (the largest value the function will return).\nthreshold: A float giving the threshold value of the activation function below which values will be damped or set to zero.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499165261", "createdAt": "2020-10-03T17:14:52Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTU4MA==", "bodyText": "Perhaps rename negativePart -> amountBelowThreshold? For one thing, negativePart is vague to me. For another, it's always positive.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499165580", "createdAt": "2020-10-03T17:19:01Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NDMxMw=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2Njk3Ng==", "bodyText": "I'm finding it extremely difficult to think through the interactions of all these options. E.g. if we calculate negativePart before applying clipMax, and then we clip the input, and then we subtract a fraction of negativePart from the input, did we do the right thing? How about if there was also a threshold?\nPerhaps either there's some reasoning we could document up front, or somehow there's a way to order the computation and name the intermediate results so it's clearly correct?\nPerhaps the problem is that there are neither documentation, nor examples, nor tests that show how these parameters are supposed to interact.\nOr if all of this accurately mimics the Python implementation, perhaps we leave it for now while opening a ticket for some newbie like me to come back and tidy it up?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499166976", "createdAt": "2020-10-03T17:38:05Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);\n+    }\n+\n+    if (alpha != 0.) {\n+      input =\n+          tf.math.sub(\n+              input, tf.math.mul(tf.dtypes.cast(tf.constant(alpha), dataType), negativePart));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NzAyMA==", "bodyText": "Is zero still the right lower clip if there's a threshold?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499167020", "createdAt": "2020-10-03T17:38:52Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+    Operand<T> negativePart = null;\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));\n+    } else if (maxValue == 6) {\n+      // if no threshold, then can use nn.relu6 native TF op for performance\n+      input = tf.nn.relu6(input);\n+      clipMax = false;\n+    } else {\n+      input = tf.nn.relu(input);\n+    }\n+    if (clipMax) {\n+      Operand<T> lmaxValue = tf.dtypes.cast(tf.constant(maxValue), dataType);\n+      Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+      input = tf.clipByValue(input, zero, lmaxValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNjI0MTQ1", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-501624145", "createdAt": "2020-10-04T13:34:10Z", "commit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMzozNDoxMFrOHcHpRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNjowMDoxN1rOHcIgGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0NzQzMA==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499247430", "createdAt": "2020-10-04T13:34:10Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODA3Mg==", "bodyText": "> -> &gt;", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499248072", "createdAt": "2020-10-04T13:40:18Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0ODYwMA==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499248600", "createdAt": "2020-10-04T13:45:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Sigmoid.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Sigmoid activation. <code>sigmoid(x) = 1 / (1 + exp(-x))</code>.\n+ *\n+ * <p>Applies the sigmoid activation function. For small values (<-5), <code>sigmoid</code> returns\n+ * a value close to zero, and for large values (>5) the result of the function gets close to 1.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODI2OQ==", "bodyText": "Sadly, the trailing period freaks out Maven's javadoc generation.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258269", "createdAt": "2020-10-04T15:26:04Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &gt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     ELU&lt;TFloat32&gt; input = ...;\n+ *     ELU&lt;TFloat32&gt;elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODM3Nw==", "bodyText": "(>, <) -> (&gt;, &lt;)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258377", "createdAt": "2020-10-04T15:27:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/HardSigmoid.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hard sigmoid activation.\n+ *\n+ * <p>A faster approximation of the sigmoid activation.\n+ *\n+ * <p>Defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x < -2.5: return 0</code>\n+ *   <li><code>if x > 2.5: return 1</code>\n+ *   <li><code>if -2.5 <= x <= 2.5: return 0.2 * x + 0.5</code>\n+ * </ul>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1ODg2NQ==", "bodyText": "trailing period freaks out Maven's javadoc generation", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499258865", "createdAt": "2020-10-04T15:32:35Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTg1Nw==", "bodyText": "This is boilerplate for us. Although it could be greatly reduced with a helper for the isFloating check, I wonder whether it's worth creating a subclass FloatingActivation that does the isFloating check and invokes a protected Operand<T extends TNumber> callFloating(Operand<T extends TNumber> input)? (Where TNumber would be replaced by TFloating as we build out our type family support.)", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499259857", "createdAt": "2020-10-04T15:42:22Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/SELU.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Scaled Exponential Linear Unit (SELU).\n+ *\n+ * <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+ *\n+ * <ul>\n+ *   <li><code>if x > 0: return scale * x</code>\n+ *   <li><code>if x < 0: return scale * alpha * (exp(x) - 1)</code>\n+ * </ul>\n+ *\n+ * <p>where <code>alpha</code> and <code>scale</code> are pre-defined constants (<code>\n+ * alpha=1.67326324</code> and <code>scale=1.05070098</code>).\n+ *\n+ * <p>Basically, the SELU activation function multiplies <code>scale</code> (> 1) with the output of\n+ * the elu function to ensure a slope larger than one for positive inputs.\n+ *\n+ * <p>The values of <code>alpha</code> and <code>scale</code> are chosen so that the mean and\n+ * variance of the inputs are preserved between two consecutive layers as long as the weights are\n+ * initialized correctly (see {@link org.tensorflow.framework.initializers.LeCun} with Normal\n+ * Distribution) and the number of input units is \"large enough\"\n+ *\n+ * <p><b>Notes: </b> To be used together with the {@link\n+ * org.tensorflow.framework.initializers.LeCun} initializer with Normal Distribution.\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1706.02515\">Klambauer et al., 2017</a>.\n+ */\n+public class SELU<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Scaled Exponential Linear Unit (SELU) activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public SELU(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1OTk5MA==", "bodyText": "extra right angle bracket", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499259990", "createdAt": "2020-10-04T15:43:53Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MDk3Nw==", "bodyText": "Perhaps worth defining an Operand.shape()?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499260977", "createdAt": "2020-10-04T15:54:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>>argument sets which axis of the\n+ * input the function is applied along.\n+ *\n+ * <p>Softmax is often used as the activation for the last layer of a classification network because\n+ * the result could be interpreted as a probability distribution.\n+ *\n+ * <p>The softmax of each vector x is computed as: <code>exp(x) / tf.sum(exp(x))</code>.\n+ *\n+ * <p>The input values in are the log-odds of the resulting probability.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Softmax<T extends TNumber> extends Activation<T> {\n+\n+  private static final int AXIS_DEFAULT = -1;\n+\n+  private final int axis;\n+\n+  /**\n+   * Creates a softmax activation where the default axis is {@link #AXIS_DEFAULT} which indicates\n+   * the last dimension.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Softmax(Ops tf) {\n+    this(tf, AXIS_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Softmax activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param axis The dimension softmax would be performed on.\n+   */\n+  public Softmax(Ops tf, int axis) {\n+    super(tf);\n+    this.axis = axis;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Shape shape = input.asOutput().shape();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI2MTQ2Nw==", "bodyText": "trailing period freaks out Maven's javadoc generation", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499261467", "createdAt": "2020-10-04T16:00:17Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Swish.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Swish activation function. <code>swish(x) = x * sigmoid(x)</code>.\n+ *\n+ * <p>Swish activation function which returns <code>x*sigmoid(x)</code>. It is a smooth,\n+ * non-monotonic function that consistently matches or outperforms <code>ReLU</code> on deep\n+ * networks, it is unbounded above and bounded below.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-20, -1.0, 0.0, 1.0, 20});\n+ *     Swish&lt;TFloat32&gt; swish = new Swish&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = swish.call(input);\n+ *     // result = [-4.1223075e-08f, -2.6894143e-01f,  0.0000000e+00f,\n+ *     //          7.3105860e-01f,  2.0000000e+01f ]\n+ *\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1710.05941\">Ramachandran et al., 2017</a>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/63c1f006ca2e16677790d5d203636e071a6e20b4", "committedDate": "2020-10-04T20:07:40Z", "message": "JavaDoc fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxOTc1MDUy", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-501975052", "createdAt": "2020-10-05T11:48:57Z", "commit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo0ODo1N1rOHcZd9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMjowMDoyNVrOHcZ2cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUzOTQ0NA==", "bodyText": "I like how this eliminates a chunk of (comment) boilerplate! But it doesn't document the IllegalArgumentException on !isFloating. Might be additional motivation to add a FloatingActivation subclass?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499539444", "createdAt": "2020-10-05T11:48:57Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Tanh.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hyperbolic tangent activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-3.0f,-1.0f, 0.0f, 1.0f, 3.0f});\n+ *     Tanh&lt;TFloat32&gt; tanh = new Tanh&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = tanh.call(input);\n+ *     // result = [-0.9950547f, -0.7615942f,  0.f,  0.7615942f,  0.9950547f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Tanh<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Hyperbolic tangent activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Tanh(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /** {@inheritDoc} */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0MDk0MQ==", "bodyText": "This test will be boilerplate. Perhaps create a helper class or superclass for tests of floating activations?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499540941", "createdAt": "2020-10-05T11:51:40Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class ELUTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public ELUTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallInt() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NTcxNQ==", "bodyText": "Perhaps also consolidate this sort of test logic into a helper class? E.g.\nassertFloat32Output(tf -> new ELU<Float32>(tf),\n            {1f, -0.86466473f, 3f, -0.9816844f, -0.63212055f, 2f, -0.95021296f, 4f},\n            {1, -2, 3, -4, -1, 2, -3, 4})", "url": "https://github.com/tensorflow/java/pull/123#discussion_r499545715", "createdAt": "2020-10-05T12:00:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/ELUTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class ELUTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public ELUTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallInt() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Operand<TInt32> input = tf.constant(new int[] {1, 2, 3, 4, 5});\n+              ELU<TInt32> instance = new ELU<>(tf);\n+              Operand<TInt32> result = instance.call(input);\n+            }\n+          });\n+  }\n+\n+  /** Test of ELU call method */\n+  @Test\n+  public void testCallFloat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4ODg3NTg4", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-508887588", "createdAt": "2020-10-15T02:07:44Z", "commit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjowNzo0NVrOHhuK_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwMjoyOTowNFrOHhuhAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMTUzMg==", "bodyText": "We've already discussed about adding the @Nullable annotation but this is not part of the standard JDK and we need to bring additional dependencies, which we try to avoid as much as possible to avoid conflict with other version of that same dependency that might have been imported by the user.\nThat's being said, when we discussed about it, it was related to the Core API. I'm OK though to bring more dependencies in the framework if we need to.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505121532", "createdAt": "2020-10-15T02:07:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDUwMQ=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMTc4MQ==", "bodyText": "Probably a way to avoid these setTF/getTF would be to push created Ops instances on a stack somewhere... but this is out of scope of this PR for sure", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505121781", "createdAt": "2020-10-15T02:08:55Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMjYxOQ==", "bodyText": "I agree with annotating @Nullable fields in the framework, especially that other JVM languages like Kotlin handle nullable values in a very specific way. Should we do it in this PR or go through all our classes later?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505122619", "createdAt": "2020-10-15T02:11:57Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Activation.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Abstract base class for Activations\n+ *\n+ * <p><b>Note:</b> The {@link #tf} attribute must be set prior to invoking the call method. See\n+ * {@link #setTF(Ops)} and the constructor {@link #Activation(Ops)}.\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public abstract class Activation<T extends TNumber> {\n+\n+  /** The TensorFlow Ops */\n+  protected Ops tf;\n+\n+  /**\n+   * Creates the abstract class for an Activation\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Activation(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Sets the TensorFlow Ops\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected void setTF(Ops tf) {\n+    this.tf = tf;\n+  }\n+\n+  /**\n+   * Gets the TensorFlow Ops\n+   *\n+   * @return the TensorFlow Ops\n+   */\n+  protected Ops getTF() {\n+    return this.tf;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   */\n+  public abstract Operand<T> call(Operand<T> input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTMwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMzI1Nw==", "bodyText": "Nit: no need of a else block if you return unconditionally in the if one", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505123257", "createdAt": "2020-10-15T02:14:13Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ELU.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential linear unit.\n+ *\n+ * <p>The exponential linear unit (ELU) with <code>alpha &gt; 0</code> is:\n+ *\n+ * <p><code>x</code> if <code>x &gt; 0</code> and <code>alpha * (exp(x) -\n+ * 1)</code> if <code>x &lt; 0</code>.\n+ *\n+ * <p>The ELU hyperparameter <code>alpha</code> controls the value to which an ELU saturates for\n+ * negative net inputs. ELUs diminish the vanishing gradient effect.\n+ *\n+ * <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean\n+ * activations that are closer to zero enable faster learning as they bring the gradient closer to\n+ * the natural gradient. ELUs saturate to a negative value when the argument gets smaller.\n+ * Saturation means a small derivative which decreases the variation and the information that is\n+ * propagated to the next layer.\n+ *\n+ * <p>Example Usage:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = &#46;&#46;&#46;;\n+ *     ELU&lt;TFloat32&gt; elu = new ELU&lt;&gt;(tf, 2.0f);\n+ *     Operand&lt;TFloat32&gt; result = elu.call(input);\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ * @see <a href=\"https://arxiv.org/abs/1511.07289\">Clevert et al, 2016, Fast and Accurate Deep\n+ *     Network Learning by Exponential Linear Units (ELUs)</a>\n+ */\n+public class ELU<T extends TNumber> extends Activation<T> {\n+\n+  private static final double ALPHA_DEFAULT = 1.0;\n+\n+  /** A scalar, slope of negative section. */\n+  private final double alpha;\n+\n+  /**\n+   * Creates a new ELU with alpha={@link #ALPHA_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ELU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ELU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha A scalar, slope of negative section. It controls the value to which an ELU\n+   *     saturates for negative net inputs.\n+   */\n+  public ELU(Ops tf, double alpha) {\n+    super(tf);\n+    this.alpha = alpha;\n+  }\n+\n+  /**\n+   * Gets the calculation operation for the activation.\n+   *\n+   * @param input the input tensor\n+   * @return The operand for the activation\n+   * @throws IllegalArgumentException if the data type is not a floating data type.\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {\n+      throw new IllegalArgumentException(\n+          \"Must be a Floating Point DataType: \" + input.asOutput().dataType());\n+    }\n+    Operand<T> result = tf.nn.elu(input);\n+    if (alpha == 1.0) {\n+      return result;\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyMzY4MA==", "bodyText": "Ultimately, we would have a TFloating family that can the be bound to the T parameter of that method (this family is present in PR #92 ).", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505123680", "createdAt": "2020-10-15T02:15:41Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Exponential.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Exponential activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *   Operand&lt;TFloat32&gt; input = tf.constant(\n+ *          new float[] {-3.0f,-1.0f, 0.0f,1.0f,3.0f});\n+ *   Exponential&lt;TFloat32&gt; exp = new Exponential&lt;&gt;(tf);\n+ *   Operand&lt;TFloat32&gt; result = exp.call(input);\n+ *   // result is [0.04978707f,  0.36787945f,  1.f,  2.7182817f, 20.085537f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Exponential<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates an Exponential activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Exponential(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /**\n+   * Calculates the Exponential activation.\n+   *\n+   * @param input the input tensor\n+   * @return an Operand for the exponential activation: <code>exp(x)</code>.\n+   * @throws IllegalArgumentException if the input is not a floating type\n+   */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+    if (!input.asOutput().dataType().isFloating()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjkwNg=="}, "originalCommit": {"oid": "de0e610ad8a7d732da9a11c319e6d0495e66b9b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNjI0NQ==", "bodyText": "LeakyRelu is not add to tf.nn because it is set as not visible in the main API def proto. To add it to tf.nn, you can override its visibility by setting it to VISIBLE in our proto", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505126245", "createdAt": "2020-10-15T02:25:27Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEyNzE2OQ==", "bodyText": "I would try to avoid changing the value of a method parameter, better use a new variable.", "url": "https://github.com/tensorflow/java/pull/123#discussion_r505127169", "createdAt": "2020-10-15T02:29:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/ReLU.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.math.Greater;\n+import org.tensorflow.op.nn.LeakyRelu;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Rectified Linear Unit(ReLU) activation.\n+ *\n+ * <p>With default values, this returns the standard ReLU activation: <code>max(x, 0)</code>, the\n+ * element-wise maximum of 0 and the input tensor.\n+ *\n+ * <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of\n+ * the activation, and to use a non-zero multiple of the input for values below the threshold.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(\n+ *              new float[] {-10f, -5f, 0.0f, 5f, 10f});\n+ *\n+ *     // With default parameters\n+ *     ReLU&lt;TFloat32&gt; relu = new ReLU&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = relu.call(input);\n+ *     // result is [0.f,  0.f,  0.f,  5.f, 10.f]\n+ *\n+ *     // With alpha = 0.5\n+ *     relu = new ReLU&lt;&gt;(tf, 0.5f, ReLU.MAX_VALUE_DEFAULT, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [-5.f , -2.5f,  0.f ,  5.f , 10.f]\n+ *\n+ *     // With maxValue = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, 5f, ReLU.THRESHOLD_DEFAULT);\n+ *     result = relu.call(input);\n+ *     // result is [0.f, 0.f, 0.f, 5.f, 5.f]\n+ *\n+ *     // With threshold = 5\n+ *     relu = new ReLU&lt;&gt;(tf, ReLU.ALPHA_DEFAULT, ReLU.MAX_VALUE_DEFAULT, 5f);\n+ *     result = relu.call(input);\n+ *     // result is [-0.f, -0.f,  0.f,  0.f, 10.f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the result\n+ */\n+public class ReLU<T extends TNumber> extends Activation<T> {\n+\n+  public static final float ALPHA_DEFAULT = 0.0f;\n+  public static final float MAX_VALUE_DEFAULT = Float.NaN;\n+  public static final float THRESHOLD_DEFAULT = 0.0f;\n+\n+  private final float alpha;\n+  private final float maxValue;\n+  private final float threshold;\n+\n+  /**\n+   * Creates a new ReLU with alpha={@link #ALPHA_DEFAULT}, maxValue={@link #MAX_VALUE_DEFAULT},\n+   * threshold={@link #THRESHOLD_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public ReLU(Ops tf) {\n+    this(tf, ALPHA_DEFAULT, MAX_VALUE_DEFAULT, THRESHOLD_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a new ReLU\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param alpha governs the slope for values lower than the threshold.\n+   * @param maxValue sets the saturation threshold (the largest value the function will return).\n+   * @param threshold the threshold value of the activation function below which values will be\n+   *     damped or set to zero.\n+   */\n+  public ReLU(Ops tf, float alpha, float maxValue, float threshold) {\n+    super(tf);\n+    this.alpha = alpha;\n+    this.maxValue = maxValue;\n+    this.threshold = threshold;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Operand<T> call(Operand<T> input) {\n+\n+    DataType<T> dataType = input.asOutput().dataType();\n+\n+    boolean clipMax = !Float.isNaN(maxValue);\n+    Operand<T> negativePart = null;\n+    if (alpha != 0) {\n+      if (Float.isNaN(maxValue) && threshold == 0) {\n+        // TODO LeakyRelu is not in tf.nn ????\n+        return LeakyRelu.create(tf.scope(), input, LeakyRelu.alpha(alpha));\n+      }\n+      if (threshold != 0) {\n+        negativePart =\n+            tf.nn.relu(\n+                tf.math.add(tf.math.neg(input), tf.dtypes.cast(tf.constant(threshold), dataType)));\n+      } else {\n+        negativePart = tf.nn.relu(tf.math.neg(input));\n+      }\n+    }\n+\n+    if (threshold != 0) {\n+      // computes input for input > threshold else 0\n+      Greater greater = tf.math.greater(input, tf.dtypes.cast(tf.constant(threshold), dataType));\n+      input = tf.math.mul(input, tf.dtypes.cast(greater, dataType));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMTg4NTMy", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-512188532", "createdAt": "2020-10-19T21:56:18Z", "commit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMTo1NjoxOFrOHkjHMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjoyOFrOHkjYBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4NjA2NQ==", "bodyText": "missing space after </code>", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508086065", "createdAt": "2020-10-19T21:56:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Softmax.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Softmax converts a real vector to a vector of categorical probabilities.\n+ *\n+ * <p>The elements of the output vector are in range (0, 1) and sum to 1.\n+ *\n+ * <p>Each vector is handled independently. The <code>axis</code>argument sets which axis of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDM3Mg==", "bodyText": "Similar to my previous comment, when we will bind T to TFloating, no doc will be required as no exception will be thrown", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508090372", "createdAt": "2020-10-19T22:06:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/activations/Tanh.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Hyperbolic tangent activation function.\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>\n+ *     Operand&lt;TFloat32&gt; input = tf.constant(new float[]\n+ *                                        {-3.0f,-1.0f, 0.0f, 1.0f, 3.0f});\n+ *     Tanh&lt;TFloat32&gt; tanh = new Tanh&lt;&gt;(tf);\n+ *     Operand&lt;TFloat32&gt; result = tanh.call(input);\n+ *     // result = [-0.9950547f, -0.7615942f,  0.f,  0.7615942f,  0.9950547f]\n+ * </pre>\n+ *\n+ * @param <T> the data type of the activation\n+ */\n+public class Tanh<T extends TNumber> extends Activation<T> {\n+\n+  /**\n+   * Creates a Hyperbolic tangent activation.\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Tanh(Ops tf) {\n+    super(tf);\n+  }\n+\n+  /** {@inheritDoc} */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUzOTQ0NA=="}, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMjU2MzM1", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-512256335", "createdAt": "2020-10-20T00:52:26Z", "commit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "state": "DISMISSED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMDo1MjoyNlrOHkmq-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMDo1MjoyNlrOHkmq-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0NDM3OA==", "bodyText": "Where were the expected values computed? I would've thought we'd be using the examples provided in the Python API docs: https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish", "url": "https://github.com/tensorflow/java/pull/123#discussion_r508144378", "createdAt": "2020-10-20T00:52:26Z", "author": {"login": "KartikChugh"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/activations/SwishTest.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.framework.activations;\n+\n+import org.junit.jupiter.api.*;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** @author Jim Clarke */\n+public class SwishTest {\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  public SwishTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallInt() {\n+    int[] input = {1, -2, 3, -4, -1, 2, -3, 4};\n+    for (TestSession.Mode tfMode : tfModes)\n+      assertThrows(\n+          java.lang.IllegalArgumentException.class,\n+          () -> {\n+            try (TestSession session = TestSession.createTestSession(tfMode)) {\n+              Ops tf = session.getTF();\n+              Swish<TInt32> instance = new Swish<>(tf);\n+              Operand<TInt32> result = instance.call(tf.constant(input));\n+            }\n+          });\n+  }\n+\n+  /** Test of Swish call method */\n+  @Test\n+  public void testCallFloat() {\n+    float[] input = {1, -2, 3, -4, -5, 6, -7, 8};\n+    float[] expected = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63c1f006ca2e16677790d5d203636e071a6e20b4"}, "originalPosition": 65}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2302cc508ab3d9c30d056f42909c06821c323edd", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/2302cc508ab3d9c30d056f42909c06821c323edd", "committedDate": "2020-10-21T19:04:55Z", "message": "Add TFloating family"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c44c62ce34e6756221cbffb493bc4859ed13294", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/4c44c62ce34e6756221cbffb493bc4859ed13294", "committedDate": "2020-10-21T19:09:05Z", "message": "Add JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef29af9debf37f56c383453b65d0f41acfc5d006", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ef29af9debf37f56c383453b65d0f41acfc5d006", "committedDate": "2020-10-21T19:37:47Z", "message": "Changed to TFloating where appropriate.\nMisc fixes to JavaDoc.\nIn ReLU, change to assign to new variable 'lInput' rather than change the 'input' parameter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75194368c33d8bbabf166bb6bc7a4723660a1c99", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/75194368c33d8bbabf166bb6bc7a4723660a1c99", "committedDate": "2020-10-21T19:42:15Z", "message": "Remove the test of int arguments for those classes changed to TFloating type, as they would no longer compile."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/27c1126ad67c5bf83a87c690e5881460c5a0ae38", "committedDate": "2020-10-21T20:10:14Z", "message": "Remove the test of int arguments for those classes changed to TFloating type, as they would no longer compile."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0MjcxMDY2", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-514271066", "createdAt": "2020-10-22T00:01:48Z", "commit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMDowMTo0OFrOHmL4gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMDowMTo0OFrOHmL4gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMjYyNg==", "bodyText": "Provably that the TNumber import is not required anymore?", "url": "https://github.com/tensorflow/java/pull/123#discussion_r509802626", "createdAt": "2020-10-22T00:01:48Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TFloat64.java", "diffHunk": "@@ -29,10 +29,11 @@\n import org.tensorflow.ndarray.NdArray;\n import org.tensorflow.ndarray.StdArrays;\n import org.tensorflow.ndarray.impl.dense.DoubleDenseNdArray;\n+import org.tensorflow.types.family.TFloating;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27c1126ad67c5bf83a87c690e5881460c5a0ae38"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b83f94fcbe96f71e0dc006b42aeeb09c07a9ce72", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b83f94fcbe96f71e0dc006b42aeeb09c07a9ce72", "committedDate": "2020-10-22T13:56:59Z", "message": "Make LeakyRelu visible so that it is included in tf.nn."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c59e9058d0c152bce63c0c30e60f4c37472c7cb0", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c59e9058d0c152bce63c0c30e60f4c37472c7cb0", "committedDate": "2020-10-22T15:03:00Z", "message": "Remove TNumber import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebbcc4fd9ca08ea1693424c480520278b05daa02", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ebbcc4fd9ca08ea1693424c480520278b05daa02", "committedDate": "2020-10-22T19:27:33Z", "message": "Add tf.nn.leakyRelu operation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1NjkzMDY3", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-515693067", "createdAt": "2020-10-23T13:58:29Z", "commit": {"oid": "ebbcc4fd9ca08ea1693424c480520278b05daa02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mjc2Njcx", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-516276671", "createdAt": "2020-10-24T13:10:50Z", "commit": {"oid": "ebbcc4fd9ca08ea1693424c480520278b05daa02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mjg3NDU2", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-516287456", "createdAt": "2020-10-24T15:54:48Z", "commit": {"oid": "ebbcc4fd9ca08ea1693424c480520278b05daa02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mjg5MDUy", "url": "https://github.com/tensorflow/java/pull/123#pullrequestreview-516289052", "createdAt": "2020-10-24T16:18:50Z", "commit": {"oid": "ebbcc4fd9ca08ea1693424c480520278b05daa02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3459, "cost": 1, "resetAt": "2021-11-02T12:20:56Z"}}}