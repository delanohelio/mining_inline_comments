{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4MDUzNDQ4", "number": 91, "title": "Initial checkin of Keras Optimzers and helper classes.", "bodyText": "Added Keras Optimizers and test cases.\nThe general structure is to pass Ops tf in all the constructors. There is an assertGraph() method in org.tensorflow.keras.optimizers.OptimizerInterface class that ensures that the tf.scope().env() represents a Graph, and is not Eager.\nThe following Optimizers provide a facade to the org.tensorflow.framework.optimizers Optimizers.\nAdaDelta, AdaGrad, AdaGradDA, Adam,  RMSProp, and SGD (Momentum).\nThe remaining Optimizers were created from scratch, following the pattern of the org.tensorflow.framework.optimizers  Optimizers.\nAdamax, Ftrl, and Nadam\nInternal support methods are found in org.tensorflow.keras.backend. The K class is a rewrite of most of the Python TensorFlow file, keras/backend.py. Classes in org.tensor.keras.backend.tf probably should be incorporated into other modules of TensorFlow java. This is currently under consideration.  Internal utility classes are in org.tensor.keras.utils\nTest cases are in src/main/test/org/tensorflow/keras/optimizers.\nTest case support classes are in src/main/test/org/tensorflow/keras/utils.\nHere you will find a TestSession abstract base class, and implementations for Graph mode (GraphTestSession) and\nEager mode (EagerTestSession).  These TestSession classes help streamline the test case code when evaluating Tensor values.\nA factory method on TestSession, allows the desired mode to be created based on\nthe Enum TestSession.Mode. Of course, Optimizers are currently constrained to Graph mode.\nFixed project dependencies in pom.xml to include the other tensorflow-java required modules, as well as\norg.json:json and org.apache.commons:commons-csv that are used in callbacks to be installed later.", "createdAt": "2020-07-28T21:03:34Z", "url": "https://github.com/tensorflow/java/pull/91", "merged": true, "mergeCommit": {"oid": "2843138c435b922ae249f3ba63da4820c5f51c9f"}, "closed": true, "closedAt": "2020-09-15T18:48:17Z", "author": {"login": "JimClarke5"}, "timelineItems": {"totalCount": 43, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5ca-JAH2gAyNDU4MDUzNDQ4OmVmMGNlNjdhMmU3OTYyODM4MDcyOWNmMTZmYjBjZDRkOTljYmU2YTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdJMYL_AFqTQ4ODk2OTcyNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "committedDate": "2020-07-28T20:26:34Z", "message": "Initial checkin of Keras Optimzers and helper classes.\nFixed dependencies in pom.xml"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU5MTgwNDEw", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-459180410", "createdAt": "2020-07-31T13:38:45Z", "commit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxMzozODo0NVrOG6I4fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNToxNToyOVrOG6MQXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNjEyNQ==", "bodyText": "Can we type these Operands generically? Many of them seem to be missing types.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463616125", "createdAt": "2020-07-31T13:38:45Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA==", "bodyText": "Do we have the String op names anywhere? If not we should figure out how to add them to the generated ops so we don't have random string constants. cc @karllessard", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463617414", "createdAt": "2020-07-31T13:40:56Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxOTA0OQ==", "bodyText": "If they are in python TF, then yes. You can add them to the ops system by moving them to a helper class in core and annotating them appropriately.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463619049", "createdAt": "2020-07-31T13:43:57Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 339}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA==", "bodyText": "We've already got this one (https://github.com/tensorflow/java/blob/master/tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/nn/SoftmaxCrossEntropyWithLogits.java). Is the Keras one different in some way?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463620028", "createdAt": "2020-07-31T13:45:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg==", "bodyText": "What's this for? If it's used across threads should it be a ConcurrentHashMap?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463620812", "createdAt": "2020-07-31T13:47:07Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 497}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMTEwOQ==", "bodyText": "This and the one below should probably live on DataType.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463621109", "createdAt": "2020-07-31T13:47:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 540}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMjM0NQ==", "bodyText": "Given the output washes the type off anyway, can we just have a single long if statement that checks it's a valid type, and then returns the NdArraySequence? Also do we want it to return Optional<NdArraySequence>?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463622345", "createdAt": "2020-07-31T13:49:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {\n+    return a.byteSize() < b.byteSize() ? b : a;\n+  }\n+\n+  /**\n+   * returns the smaller DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the smaller DataType\n+   */\n+  public DataType narrower(DataType a, DataType b) {\n+    return a.byteSize() > b.byteSize() ? b : a;\n+  }\n+\n+  public <T extends TNumber> NdArraySequence getTensorValue(Ops tf, Operand<T> operand) {\n+    DataType dtype = operand.asOutput().dataType();\n+    if (tf.scope().env().isGraph()) {\n+      try (Session session = new Session((Graph) tf.scope().env())) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          try (Tensor<TInt32> result =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 560}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjM3OQ==", "bodyText": "No star imports please.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463626379", "createdAt": "2020-07-31T13:56:24Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ConfusionMatrix.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjgyMA==", "bodyText": "Too many single quotes here.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463626820", "createdAt": "2020-07-31T13:57:07Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ConfusionMatrix.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.keras.utils.ShapeUtils;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Stack;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** ConfusionMatrix operations */\n+public class ConfusionMatrix {\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static Tuple removeSqueezableDimensions(Ops tf, Operand labels, Operand predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static Tuple removeSqueezableDimensions(\n+      Ops tf, Operand labels, Operand predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1\n+          && ShapeUtils.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1\n+          && ShapeUtils.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+    Operand rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE\n+        && ShapeUtils.isCompatible(predictionsShape.size(-1), 1)) {\n+      /**\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && ShapeUtils.isCompatible(labelsShape.size(-1), 1)) {\n+      /**\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Arrays.asList(-1L)));\n+    }\n+    return new Tuple(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the confusion matrix from predictions and labels.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels 1-D `Tensor` of real labels for the classification task.\n+   * @param predictions 1-D `Tensor` of predictions for a given classification.\n+   * @param numClasses The possible number of labels the classification task can have.\n+   * @param weights optional weights to be applied to the confusion matris\n+   * @param dtype Data type of the confusion matrix.\n+   * @param <T> the type of Operands\n+   * @param <U> the data type.\n+   * @return A `Tensor` of type `dtype` with shape `[n, n]` representing the confusion matrix, where\n+   *     `n` is the number of possible labels in the classification task.\n+   * @throws IllegalArgumentException If both predictions and labels are not 1-D vectors and have\n+   *     mismatched shapes, or if `weights` is not `None` and its shape doesn't match `predictions`.\n+   */\n+  public static <T extends TType, U extends TNumber> Operand confusionMatrix(\n+      Ops tf,\n+      Operand<T> labels,\n+      Operand<T> predictions,\n+      Operand<TInt64> numClasses,\n+      Operand<T> weights,\n+      DataType<U> dtype) {\n+    tf = tf.withSubScope(\"confusion_matrix\");\n+    Tuple ops = K.squeezeOrExpandDimensions(tf, predictions, labels, null);\n+    predictions = tf.dtypes.cast(ops.getPredictions(), TInt64.DTYPE);\n+    labels = tf.dtypes.cast(ops.getLabels(), TInt64.DTYPE);\n+\n+    List<Op> labelControls = new ArrayList<>();\n+    List<Op> predictionControls = new ArrayList<>();\n+\n+    labelControls.add(\n+        tf.assertThat(\n+            tf.reduceAny(\n+                tf.math.greaterEqual((Operand<TInt64>) labels, tf.constant(0L)),\n+                K.allAxis(tf, labels)),\n+            Arrays.asList(tf.constant(\"`labels` contains negative values\"))));\n+\n+    predictionControls.add(\n+        tf.assertThat(\n+            tf.reduceAny(\n+                tf.math.greaterEqual((Operand<TInt64>) predictions, tf.constant(0L)),\n+                K.allAxis(tf, labels)),\n+            Arrays.asList(tf.constant(\"`predictions` contains negative values\"))));\n+    if (numClasses == null) {\n+      numClasses =\n+          tf.math.maximum(\n+              tf.reduceMax(predictions, K.allAxis(tf, predictions)),\n+              tf.reduceMax(labels, K.allAxis(tf, labels)));\n+    } else {\n+      labelControls.add(\n+          tf.assertThat(\n+              tf.reduceAny(\n+                  tf.math.less((Operand<TInt64>) labels, numClasses), K.allAxis(tf, labels)),\n+              Arrays.asList(tf.constant(\"``labels` out of bound\"))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNzAxOQ==", "bodyText": "No star imports.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463627019", "createdAt": "2020-07-31T13:57:26Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNzc5Mg==", "bodyText": "Where is createOperand from? Is it a class or a static?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463627792", "createdAt": "2020-07-31T13:58:44Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.function.Function;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** Container for ControlDepencies, so that the primary Operand is remembered. */\n+public class ControlDependencies {\n+\n+  /**\n+   * Create a control dependency for the operand;\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param createOperand a function that creates an operand with the control dependency context\n+   * @param name the scope name to use\n+   * @param dependencies a list of control ops.\n+   * @param <T> the type of Operand\n+   * @return the Operand with control dependency scope\n+   */\n+  public static <T extends TType> Operand<T> addControlDependencies(\n+      Ops tf, Function<Ops, Operand<T>> createOperand, String name, Op... dependencies) {\n+    return addControlDependencies(tf, createOperand, name, Arrays.asList(dependencies));\n+  }\n+\n+  /**\n+   * Create a control dependency for an operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param createOperand function that creates an operand with the control dependency context\n+   * @param name the scope name to use\n+   * @param dependencies a list of control ops.\n+   * @param <T> the type of Operand\n+   * @return the Operand with control dependency scope\n+   */\n+  public static <T extends TType> Operand<T> addControlDependencies(\n+      Ops tf, Function<Ops, Operand<T>> createOperand, String name, List<Op> dependencies) {\n+    tf = tf.withSubScope(name).withControlDependencies(dependencies);\n+    return createOperand.apply(tf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODM2NQ==", "bodyText": "Should we override the one without gradients with this one? It should be straightforward to turn the other off, and to add this as an op in the right place. Needs more types first though.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463628365", "createdAt": "2020-07-31T13:59:46Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/NN.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.nn.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+\n+/** NN Operations */\n+public class NN {\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param tf\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  public static Operand sparse_softmax_cross_entropy_with_logits(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODk1MA==", "bodyText": "Do we know if upstream plans to change SparseTensor into something the C API knows about? cc @karllessard.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463628950", "createdAt": "2020-07-31T14:00:51Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/SparseTensor.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Represents a sparse tensor.\n+ *\n+ * @param <T> the type of the SparseTensor\n+ */\n+public class SparseTensor<T extends TType> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYzMTEyNQ==", "bodyText": "I think I'd prefer this to be called OperandTuple as it's not sufficiently general to use the name Tuple, especially when that might well collide with user code.\nAlso should it have more type bounds? The sample weights are probably floats, and the labels could be floats or ints. Ditto the lossesOrPredictions.\nOr we could make a more general Pair and a Triple. I nearly added a Pair when we added optimizer support, and there are places that could benefit.\nOne further alternative is just making explicit classes for the use cases we have, as those could be converted into records when we eventually bump Java versions.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463631125", "createdAt": "2020-07-31T14:04:54Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/Tuple.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Returns labels, losses or predictions and sample weights as a Tuple\n+ *\n+ * @param <T> the type of Operand\n+ */\n+public class Tuple<T extends TType> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYzMjUxMA==", "bodyText": "I don't like two methods returning the same field here. These should be combined (or we should completely refactor this class if we want the names to be meaningful).", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463632510", "createdAt": "2020-07-31T14:07:17Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/Tuple.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Returns labels, losses or predictions and sample weights as a Tuple\n+ *\n+ * @param <T> the type of Operand\n+ */\n+public class Tuple<T extends TType> {\n+\n+  private final Operand<T> labels;\n+  private final Operand<T> lossesOrPredictions;\n+  private final Operand<T> sampleWeights;\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, predictions, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param lossesOrPredictions the losses or predictions\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> lossesOrPredictions) {\n+    this(labels, lossesOrPredictions, null);\n+  }\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, predictions, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param lossesOrPredictions the losses or predictions\n+   * @param sampleWeights the sample weights\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> lossesOrPredictions, Operand<T> sampleWeights) {\n+    this.labels = labels;\n+    this.lossesOrPredictions = lossesOrPredictions;\n+    this.sampleWeights = sampleWeights;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsLabels() {\n+    return this.labels != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsPredictions() {\n+    return this.lossesOrPredictions != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsLosses() {\n+    return this.lossesOrPredictions != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsSampleWeights() {\n+    return this.sampleWeights != null;\n+  }\n+\n+  /** @return the labels */\n+  public Operand<T> getLabels() {\n+    return labels;\n+  }\n+\n+  /** @return the predictions */\n+  public Operand<T> getPredictions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY0ODM5OA==", "bodyText": "Typo in commment.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463648398", "createdAt": "2020-07-31T14:35:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY0OTc2NQ==", "bodyText": "Asserts are usually disabled right? Maybe these should be ifs that throw?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463649765", "createdAt": "2020-07-31T14:37:58Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MDQ5NA==", "bodyText": "s/AdaGrad/AdaGradDA/", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463650494", "createdAt": "2020-07-31T14:39:13Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F\n+        : \"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue;\n+    assert l1Strength >= 0.0F : \"l1Strength must be non-negative: \" + l1Strength;\n+    assert l2Strength >= 0.0F : \"l2Strength must be non-negative: \" + l2Strength;\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be positive.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), name, learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F\n+        : \"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue;\n+    assert l1Strength >= 0.0F : \"l1Strength must be non-negative: \" + l1Strength;\n+    assert l2Strength >= 0.0F : \"l2Strength must be non-negative: \" + l2Strength;\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdaGrad Optimizer from a config object", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MTg0OQ==", "bodyText": "Should we make base optimizers for these that the Keras ones extend? I think some of these optimizers might be new, and so I should incorporate them into the tf optimizers.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463651849", "createdAt": "2020-07-31T14:41:27Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private Scope scope;\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private float learningRate;\n+  private final float betaOne;\n+  private final float betaTwo;\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   */\n+  public Adamax(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   */\n+  public Adamax(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, float learningRate) {\n+    this(tf, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(Ops tf, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf));\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(\n+      Ops tf, String name, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf), name);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize, the config object has keys for \"name\",\n+   *     \"learning_rate\", \"epsilon\", \"beta_1\", \"beta_2\". If a key is missing the default value is\n+   *     used.\n+   */\n+  public static Adamax fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize\n+   */\n+  public static Adamax create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    float betaOne = (float) config.getOrDefault(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+    float betaTwo = (float) config.getOrDefault(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+    if (name == null) {\n+      return new Adamax(tf, learningRate, betaOne, betaTwo, epsilon);\n+    } else {\n+      return new Adamax(tf, name, learningRate, betaOne, betaTwo, epsilon);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+\n+    return Optional.empty();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamaxSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf.assign(betaOnePower, tf.constant(betaOne));\n+    ((Graph) tf.scope().env()).addInitializer(betaOnePowerInit);\n+  }\n+\n+  /**\n+   * Create the first and second moment slots\n+   *\n+   * @param v the variable\n+   * @param <T> the datatype of the variable\n+   */\n+  private <T extends TType> void createAdamaxSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), SECOND_MOMENT, secondMomentInitializer);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected <T extends TType> Op applyDense(Output<T> gradient, Output<T> variable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 237}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MjI0Nw==", "bodyText": "Same comment as AdaMax.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463652247", "createdAt": "2020-07-31T14:42:05Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Ftrl.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyFtrl;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Ftrl Optimizer that implements the FTRL algorithm. */\n+public class Ftrl extends org.tensorflow.framework.optimizers.Optimizer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MjUyNw==", "bodyText": "Should we check it's not positive infinity?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463652527", "createdAt": "2020-07-31T14:42:35Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Ftrl.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyFtrl;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Ftrl Optimizer that implements the FTRL algorithm. */\n+public class Ftrl extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String LEARNING_RATE_POWER_KEY = \"learning_rate_power\";\n+  public static final String INITIAL_ACCUM_VALUE_KEY = \"initial_accumulator_value\";\n+  public static final String L1STRENGTH_KEY = \"l1_regularization_strength\";\n+  public static final String L2STRENGTH_KEY = \"l2_regularization_strength\";\n+  public static final String L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY =\n+      \"l2_shrinkage_regularization_strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float LEARNING_RATE_POWER_DEFAULT = -0.5F;\n+  public static final float INITIAL_ACCUM_VALUE_DEFAULT = 0.1F;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT = 0.0F;\n+\n+  public static final String ACCUMULATOR = \"gradient_accumulator\";\n+  public static final String LINEAR_ACCUMULATOR = \"linear_accumulator\";\n+\n+  private final String name;\n+  private float learningRate;\n+  private final float learningRatePower;\n+  private final float initialAccumulatorValue;\n+  private final float l1RegularizationStrength;\n+  private final float l2RegularizationStrength;\n+  private final float l2ShrinkageRegularizationStrength;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private boolean useLocking = true;\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Ftrl(Ops tf) {\n+    this(\n+        tf,\n+        LEARNING_RATE_DEFAULT,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   */\n+  public Ftrl(Ops tf, String name) {\n+    this(\n+        tf,\n+        name,\n+        LEARNING_RATE_DEFAULT,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param learningRate the learning rate\n+   */\n+  public Ftrl(Ops tf, float learningRate) {\n+    this(\n+        tf,\n+        learningRate,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   * @param learningRate the learning rate\n+   */\n+  public Ftrl(Ops tf, String name, float learningRate) {\n+    this(\n+        tf,\n+        name,\n+        learningRate,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param learningRate the learning rate\n+   * @param learningRatePower\n+   * @param initialAccumulatorValue\n+   * @param l1Strength\n+   * @param l2Strength\n+   * @param l2ShrinkageRegularizationStrength\n+   */\n+  public Ftrl(\n+      Ops tf,\n+      float learningRate,\n+      float learningRatePower,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength,\n+      float l2ShrinkageRegularizationStrength) {\n+    super(assertGraph(tf));\n+    this.name = getOptimizerName();\n+    this.learningRate = learningRate;\n+    this.learningRatePower = learningRatePower;\n+    this.initialAccumulatorValue = initialAccumulatorValue;\n+    this.l1RegularizationStrength = l1Strength;\n+    this.l2RegularizationStrength = l2Strength;\n+    this.l2ShrinkageRegularizationStrength = l2ShrinkageRegularizationStrength;\n+    validateParams();\n+    initConfig();\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   * @param learningRate the learning rate\n+   * @param learningRatePower\n+   * @param initialAccumulatorValue\n+   * @param l1Strength\n+   * @param l2Strength\n+   * @param l2ShrinkageRegularizationStrength\n+   */\n+  public Ftrl(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float learningRatePower,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength,\n+      float l2ShrinkageRegularizationStrength) {\n+    super(assertGraph(tf), name);\n+    this.name = name;\n+    this.learningRate = learningRate;\n+    this.learningRatePower = learningRatePower;\n+    this.initialAccumulatorValue = initialAccumulatorValue;\n+    this.l1RegularizationStrength = l1Strength;\n+    this.l2RegularizationStrength = l2Strength;\n+    this.l2ShrinkageRegularizationStrength = l2ShrinkageRegularizationStrength;\n+    validateParams();\n+    initConfig();\n+  }\n+\n+\n+\n+  /**\n+   * Create a Ftrl Optmizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param config a config object to initialize\n+   * @return a new Frtl Optimizer\n+   */\n+  public static Ftrl create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float learningRatePower =\n+        (float) config.getOrDefault(LEARNING_RATE_POWER_KEY, LEARNING_RATE_POWER_DEFAULT);\n+    float initialAccumulatorValue =\n+        (float) config.getOrDefault(INITIAL_ACCUM_VALUE_KEY, INITIAL_ACCUM_VALUE_DEFAULT);\n+    float l1RegularizationStrength =\n+        (float) config.getOrDefault(L1STRENGTH_KEY, L1STRENGTH_DEFAULT);\n+    float l2RegularizationStrength =\n+        (float) config.getOrDefault(L2STRENGTH_KEY, L2STRENGTH_DEFAULT);\n+    float l2ShrinkageRegularizationStrength =\n+        (float)\n+            config.getOrDefault(\n+                L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY,\n+                L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+\n+    if (name == null) {\n+      return new Ftrl(\n+          tf,\n+          learningRate,\n+          learningRatePower,\n+          initialAccumulatorValue,\n+          l1RegularizationStrength,\n+          l2RegularizationStrength,\n+          l2ShrinkageRegularizationStrength);\n+    } else {\n+      return new Ftrl(\n+          tf,\n+          name,\n+          learningRate,\n+          learningRatePower,\n+          initialAccumulatorValue,\n+          l1RegularizationStrength,\n+          l2RegularizationStrength,\n+          l2ShrinkageRegularizationStrength);\n+    }\n+  }\n+\n+  /** Initialize the Config object from the current settings */\n+  protected void initConfig() {\n+    config.put(NAME_KEY, this.name);\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(LEARNING_RATE_POWER_KEY, learningRatePower);\n+    config.put(INITIAL_ACCUM_VALUE_KEY, initialAccumulatorValue);\n+    config.put(L1STRENGTH_KEY, l1RegularizationStrength);\n+    config.put(L2STRENGTH_KEY, l2RegularizationStrength);\n+    config.put(L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY, l2ShrinkageRegularizationStrength);\n+  }\n+\n+  /** Validate all the settings of the Frtl Optmizer */\n+  private void validateParams() {\n+    if (this.initialAccumulatorValue < 0.0F) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NDcxOQ==", "bodyText": "Should this be final and immutable after construction? initConfig could return a Collections.unmodifiableMap(). Alternatively should the learning rate be updated in here?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463654719", "createdAt": "2020-07-31T14:46:25Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Adam.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Adam Optimizer that implements the Adam algorithm. */\n+public class Adam extends org.tensorflow.framework.optimizers.Adam implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private float learningRate;\n+  private Map<String, Object> config = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NTU1MQ==", "bodyText": "Can we type this strongly? It seems weird to mimic method overloading by having a series of instanceofs inside a method. I realise it will blow up this class by making many more methods, but it's far safer. Especially as the generic check for the functions and suppliers will be washed off and so the user might get odd error messages.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463655551", "createdAt": "2020-07-31T14:47:54Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Optimizers.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import org.tensorflow.op.Ops;\n+\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Functions to get an Optimizer based on String name, an Optimizer class, or lambda function.\n+ *\n+ * <p>Example:\n+ *\n+ * <pre>\n+ *     Adam instance = Optimizers.get(tf, \"adam\");\n+ *     Ftrl instance = Optimizers.get(tf, ltf -> new Ftrl(ltf, 0.1f);\n+ * </pre>\n+ */\n+public class Optimizers {\n+\n+  static Map<String, Function<Ops, Optimizer>> map =\n+      new HashMap<String, Function<Ops, Optimizer>>() {\n+        {\n+          put(\"adadelta\", tf -> new AdaDelta(tf));\n+          put(\"adagrad\", tf -> new AdaGrad(tf));\n+          put(\"adagrad-da\", tf -> new AdaGradDA(tf));\n+          put(\"adam\", tf -> new Adam(tf));\n+          put(\"adamax\", tf -> new Adamax(tf));\n+          put(\"ftrl\", tf -> new Ftrl(tf));\n+          put(\"nadam\", tf -> new Nadam(tf));\n+          put(\"rmsprop\", tf -> new RMSProp(tf));\n+          put(\"sgd\", tf -> new SGD(tf));\n+        }\n+      };\n+\n+  /**\n+   * Get an Optimizer\n+   *\n+   * @param optimizerFunction either a String that identifies the Optimizer, an Optimizer class, or\n+   *     an Optimizer object.\n+   * @return the Optimizer object or null if not found.\n+   */\n+  public static Optimizer get(Ops tf, Object optimizerFunction) {\n+    return get(tf, optimizerFunction, null);\n+  }\n+\n+  /**\n+   * Get an Initializer\n+   *\n+   * @param si a lamda function\n+   * @return the Intializer object\n+   */\n+  public static Optimizer get(Ops tf, Function<Ops, Optimizer> func) {\n+    return func.apply(tf);\n+  }\n+\n+  /**\n+   * Get an Initializer\n+   *\n+   * @param optimizerFunction\n+   * @param custom_functions a map of Initializer lambdas that will be queried if the Optimizer is\n+   *     not found in the standard keys\n+   * @return the Optimizer object\n+   */\n+  public static Optimizer get(\n+      Ops tf, Object optimizerFunction, Map<String, Function<Ops, Optimizer>> custom_functions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NzE2MA==", "bodyText": "If momentum is zero, doesn't this still allocate all the extra RAM necessary to store the momentum parameters?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463657160", "createdAt": "2020-07-31T14:50:31Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/SGD.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Stochastic Gradient Descent and momentum optimizer. */\n+public class SGD extends org.tensorflow.framework.optimizers.Momentum", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MDA3Ng==", "bodyText": "This assert should probably be an if check. Also shouldn't it validate that the shapes are the same, rather than just containing the same number of elements?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463660076", "createdAt": "2020-07-31T14:55:21Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/ND.java", "diffHunk": "@@ -0,0 +1,717 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.Arrays;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+\n+// TODO used in the Callbacks, this should be a part of NDArray?\n+/** NDArray math Utilities */\n+public class ND {\n+\n+  /**\n+   * Returns a string representation of the contents of the specified array.\n+   *\n+   * <p>The string representation consists of a list of the array's elements, enclosed in square\n+   * brackets (\"[]\"). Adjacent elements are separated by the characters \", \" (a comma followed by a\n+   * space). Elements are converted to strings as by String.valueOf(int). Returns \"null\" if a is\n+   * null.\n+   *\n+   * @param array the array to convert.\n+   * @return the String representaion of the contents of the specified array\n+   */\n+  public static String toString(NdArray<?> array) {\n+    if (array == null) return \"null\";\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"[\");\n+    AtomicBoolean first = new AtomicBoolean(true);\n+    array\n+        .elements(0)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (!first.get()) {\n+                sb.append(\", \");\n+              } else {\n+                first.set(false);\n+              }\n+              Object f = v.getObject();\n+              if (v.rank() == 0) {\n+                sb.append(f);\n+              } else {\n+                sb.append(toString(v));\n+              }\n+            });\n+    sb.append(\"]\");\n+    return sb.toString();\n+  }\n+\n+  /**\n+   * Transforms a flat index into coordinates based on shape.\n+   *\n+   * @param shape the shape\n+   * @param index the index\n+   * @return the coordinates\n+   */\n+  private static long[] getCoordinates(Shape shape, long index) {\n+    long[] coordinates = new long[shape.numDimensions()];\n+\n+    int numDims = shape.numDimensions();\n+    int i = numDims - 1;\n+    for (; i >= 0; i--) {\n+      long size = shape.size(i);\n+      long mod = index % size;\n+      coordinates[i] = mod;\n+      index -= mod;\n+    }\n+    return coordinates;\n+  }\n+\n+  /**\n+   * Gets the square root of an array.\n+   *\n+   * @param a the array\n+   * @return the square root of the array.\n+   */\n+  public static FloatNdArray sqrt(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.sqrt(v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Gets the square of an array.\n+   *\n+   * @param a the array\n+   * @return the square of the array.\n+   */\n+  public static FloatNdArray square(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() * v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds two arrays\n+   *\n+   * @param a the array\n+   * @param b the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(float scalar, FloatNdArray a) {\n+    return add(a, scalar);\n+  }\n+\n+  /**\n+   * subtracts one array from the other\n+   *\n+   * @param a the minuend array\n+   * @param b the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtracte scalar from an array\n+   *\n+   * @param a the minuend array\n+   * @param scalar the subtrahend value\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtract an array from a scalar\n+   *\n+   * @param scalar the minuend value\n+   * @param a the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(scalar - v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply 2 arrays\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the resulting array from the muliply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, FloatNdArray b) {\n+    assert a.shape().equals(b.shape())\n+        : String.format(\n+            \"ValueError: operands do not have same shapes %s %s \", a.shape(), b.shape());\n+    boolean sameSize = a.shape().size() == b.shape().size();\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (sameSize) {\n+                result.setFloat(v.getFloat() * b.getFloat(idx), idx);\n+              } else {\n+                float value = v.getFloat() * b.getFloat(idx[0], 0L);\n+                result.setFloat(value, idx);\n+              }\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    if (a.shape().isScalar()) {\n+      a.scalars().forEach(f -> result.setFloat(f.getFloat() * scalar));\n+    } else {\n+      a.scalars().forEachIndexed((idx, f) -> result.setFloat(f.getFloat() * scalar, idx));\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply a scalar value with an array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(float scalar, FloatNdArray a) {\n+    return mul(a, scalar);\n+  }\n+\n+  /**\n+   * Divide two arrays\n+   *\n+   * @param a the dividend array\n+   * @param b the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide an array by a scalar\n+   *\n+   * @param a the dividend array\n+   * @param scalar the scalar divisor\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide a scalar by an array\n+   *\n+   * @param scalar the scalar dividend\n+   * @param a the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              float value = v.getFloat() == 0.0F ? Float.NaN : scalar / v.getFloat();\n+              result.setFloat(value, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the second array\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the scalar value\n+   *\n+   * @param a the first array\n+   * @param scalar the scalar value\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the scalar value by the power of the array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the first array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(float scalar, FloatNdArray a) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(scalar, v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Flatten an array to 1D\n+   *\n+   * @param a the array to flatten\n+   * @return the flattened array\n+   */\n+  public static float[] flatten(FloatNdArray a) {\n+    float[] result = new float[(int) a.shape().size()];\n+    int nDims = a.shape().numDimensions();\n+    AtomicInteger counter = new AtomicInteger();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result[counter.getAndAdd(1)] = v.getFloat();\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of the array\n+   *\n+   * @param a the array\n+   * @return the maximum value of the array\n+   */\n+  public static float max(FloatNdArray a) {\n+    AtomicReference<Float> maximum = new AtomicReference<>(Float.MIN_VALUE);\n+    a.scalars().forEach(f -> maximum.set(Math.max(maximum.get(), f.getFloat())));\n+    return maximum.get();\n+  }\n+\n+  /**\n+   * Get the minimum value of the array\n+   *\n+   * @param a the array\n+   * @return the minimum value of the array\n+   */\n+  public static float min(FloatNdArray a) {\n+    AtomicReference<Float> minimum = new AtomicReference<>(Float.MAX_VALUE);\n+    a.scalars().forEach(f -> minimum.set(Math.min(minimum.get(), f.getFloat())));\n+    return minimum.get();\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the maximum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray max(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 447}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MDY5Mw==", "bodyText": "Do we want a method that accepts a logger of some kind? Or an output stream?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463660693", "createdAt": "2020-07-31T14:56:25Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/ND.java", "diffHunk": "@@ -0,0 +1,717 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.Arrays;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+\n+// TODO used in the Callbacks, this should be a part of NDArray?\n+/** NDArray math Utilities */\n+public class ND {\n+\n+  /**\n+   * Returns a string representation of the contents of the specified array.\n+   *\n+   * <p>The string representation consists of a list of the array's elements, enclosed in square\n+   * brackets (\"[]\"). Adjacent elements are separated by the characters \", \" (a comma followed by a\n+   * space). Elements are converted to strings as by String.valueOf(int). Returns \"null\" if a is\n+   * null.\n+   *\n+   * @param array the array to convert.\n+   * @return the String representaion of the contents of the specified array\n+   */\n+  public static String toString(NdArray<?> array) {\n+    if (array == null) return \"null\";\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"[\");\n+    AtomicBoolean first = new AtomicBoolean(true);\n+    array\n+        .elements(0)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (!first.get()) {\n+                sb.append(\", \");\n+              } else {\n+                first.set(false);\n+              }\n+              Object f = v.getObject();\n+              if (v.rank() == 0) {\n+                sb.append(f);\n+              } else {\n+                sb.append(toString(v));\n+              }\n+            });\n+    sb.append(\"]\");\n+    return sb.toString();\n+  }\n+\n+  /**\n+   * Transforms a flat index into coordinates based on shape.\n+   *\n+   * @param shape the shape\n+   * @param index the index\n+   * @return the coordinates\n+   */\n+  private static long[] getCoordinates(Shape shape, long index) {\n+    long[] coordinates = new long[shape.numDimensions()];\n+\n+    int numDims = shape.numDimensions();\n+    int i = numDims - 1;\n+    for (; i >= 0; i--) {\n+      long size = shape.size(i);\n+      long mod = index % size;\n+      coordinates[i] = mod;\n+      index -= mod;\n+    }\n+    return coordinates;\n+  }\n+\n+  /**\n+   * Gets the square root of an array.\n+   *\n+   * @param a the array\n+   * @return the square root of the array.\n+   */\n+  public static FloatNdArray sqrt(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.sqrt(v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Gets the square of an array.\n+   *\n+   * @param a the array\n+   * @return the square of the array.\n+   */\n+  public static FloatNdArray square(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() * v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds two arrays\n+   *\n+   * @param a the array\n+   * @param b the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(float scalar, FloatNdArray a) {\n+    return add(a, scalar);\n+  }\n+\n+  /**\n+   * subtracts one array from the other\n+   *\n+   * @param a the minuend array\n+   * @param b the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtracte scalar from an array\n+   *\n+   * @param a the minuend array\n+   * @param scalar the subtrahend value\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtract an array from a scalar\n+   *\n+   * @param scalar the minuend value\n+   * @param a the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(scalar - v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply 2 arrays\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the resulting array from the muliply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, FloatNdArray b) {\n+    assert a.shape().equals(b.shape())\n+        : String.format(\n+            \"ValueError: operands do not have same shapes %s %s \", a.shape(), b.shape());\n+    boolean sameSize = a.shape().size() == b.shape().size();\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (sameSize) {\n+                result.setFloat(v.getFloat() * b.getFloat(idx), idx);\n+              } else {\n+                float value = v.getFloat() * b.getFloat(idx[0], 0L);\n+                result.setFloat(value, idx);\n+              }\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    if (a.shape().isScalar()) {\n+      a.scalars().forEach(f -> result.setFloat(f.getFloat() * scalar));\n+    } else {\n+      a.scalars().forEachIndexed((idx, f) -> result.setFloat(f.getFloat() * scalar, idx));\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply a scalar value with an array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(float scalar, FloatNdArray a) {\n+    return mul(a, scalar);\n+  }\n+\n+  /**\n+   * Divide two arrays\n+   *\n+   * @param a the dividend array\n+   * @param b the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide an array by a scalar\n+   *\n+   * @param a the dividend array\n+   * @param scalar the scalar divisor\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide a scalar by an array\n+   *\n+   * @param scalar the scalar dividend\n+   * @param a the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              float value = v.getFloat() == 0.0F ? Float.NaN : scalar / v.getFloat();\n+              result.setFloat(value, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the second array\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the scalar value\n+   *\n+   * @param a the first array\n+   * @param scalar the scalar value\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the scalar value by the power of the array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the first array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(float scalar, FloatNdArray a) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(scalar, v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Flatten an array to 1D\n+   *\n+   * @param a the array to flatten\n+   * @return the flattened array\n+   */\n+  public static float[] flatten(FloatNdArray a) {\n+    float[] result = new float[(int) a.shape().size()];\n+    int nDims = a.shape().numDimensions();\n+    AtomicInteger counter = new AtomicInteger();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result[counter.getAndAdd(1)] = v.getFloat();\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of the array\n+   *\n+   * @param a the array\n+   * @return the maximum value of the array\n+   */\n+  public static float max(FloatNdArray a) {\n+    AtomicReference<Float> maximum = new AtomicReference<>(Float.MIN_VALUE);\n+    a.scalars().forEach(f -> maximum.set(Math.max(maximum.get(), f.getFloat())));\n+    return maximum.get();\n+  }\n+\n+  /**\n+   * Get the minimum value of the array\n+   *\n+   * @param a the array\n+   * @return the minimum value of the array\n+   */\n+  public static float min(FloatNdArray a) {\n+    AtomicReference<Float> minimum = new AtomicReference<>(Float.MAX_VALUE);\n+    a.scalars().forEach(f -> minimum.set(Math.min(minimum.get(), f.getFloat())));\n+    return minimum.get();\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the maximum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray max(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.max(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing each item of the array to scalar\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array with the maximum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray max(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.max(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing each item of the array to scalar\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array with the maximum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray max(float scalar, FloatNdArray a) {\n+    return max(a, scalar);\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the minimum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray min(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.min(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing each item of the array to scalar\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array with the minimum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray min(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.min(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing each item of the array to scalar\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array with the minimum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray min(float scalar, FloatNdArray a) {\n+    return min(a, scalar);\n+  }\n+\n+  /**\n+   * Get the absolute value of each member of the array\n+   *\n+   * @param a the array\n+   * @return the array with the absolute value of each item.\n+   */\n+  public static FloatNdArray abs(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    a.scalars().forEachIndexed((idx, f) -> result.setFloat((float) Math.abs(f.getFloat()), idx));\n+    return result;\n+  }\n+\n+  /**\n+   * Sum all elements of an array\n+   *\n+   * @param a the array\n+   * @return an a array with one element containing the sum.\n+   */\n+  public static FloatNdArray sum(FloatNdArray a) {\n+    AtomicReference<Float> sum = new AtomicReference<>(0.f);\n+    a.scalars().forEach(f -> sum.set(sum.get() + f.getFloat()));\n+    return NdArrays.scalarOf(sum.get());\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axis the axis to sum\n+   * @return an a array the sum over the axis less the diemsnion\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, int axis) {\n+    return sum(a, axis, false);\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axis the axis to sum\n+   * @param keepDims indicates whether the dimensions over the sum should be kept or not.\n+   * @return an a array the sum over the axis\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, int axis, boolean keepDims) {\n+    Shape shape = a.shape();\n+    int nDims = shape.numDimensions();\n+    int xis = nDims - 1 - axis;\n+    long totalSize = shape.size();\n+    long axisSize = shape.size(xis);\n+    final float[] sums = new float[(int) axisSize];\n+\n+    a.scalars()\n+        .forEachIndexed(\n+            (idx, f) -> {\n+              sums[(int) idx[xis]] += f.getFloat();\n+            });\n+\n+    if (keepDims) {\n+      long[] newDims = shape.asArray();\n+      newDims[axis] = 1;\n+      final AtomicInteger counter = new AtomicInteger();\n+      FloatNdArray arrayK = NdArrays.ofFloats(Shape.of(newDims));\n+      arrayK\n+          .elements(newDims.length - 1)\n+          .forEachIndexed(\n+              (idx, v) -> {\n+                v.setFloat(sums[counter.getAndAdd(1)]);\n+              });\n+      return arrayK;\n+    } else {\n+      return NdArrays.vectorOf(sums);\n+    }\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axes the axis to sum\n+   * @param keepDims indicates whether the dimensions over the sum should be kept or not.\n+   * @return an a array the sum over the axis\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, Integer[] axes, boolean keepDims) {\n+    Shape shape = a.shape();\n+    if (axes == null) {\n+      FloatNdArray result = sum(a);\n+      if (keepDims) {\n+        float scalar = result.getFloat(0);\n+        long[] dims = {1, 1};\n+        Shape bShape = Shape.of(dims);\n+        FloatNdArray resultK = NdArrays.ofFloats(bShape);\n+        resultK.setFloat(scalar, 0, 0);\n+        return resultK;\n+      }\n+      return result;\n+    } else if (axes.length == 1) {\n+      return sum(a, axes[0], keepDims);\n+    } else {\n+      // TODO\n+      throw new UnsupportedOperationException(\"Multi Axis Not implemented Yet\");\n+    }\n+  }\n+\n+  /**\n+   * Calculate the l2 norm of the array\n+   *\n+   * @param x the array\n+   * @return the l2 norm of the array\n+   */\n+  public static FloatNdArray l2_norm(FloatNdArray x) {\n+    return l2_norm(x, -1);\n+  }\n+\n+  /**\n+   * Calculate the l2 norm of the array\n+   *\n+   * @param x the array\n+   * @param axis the axis to calculate over\n+   * @return the l2 norm of the array\n+   */\n+  public static FloatNdArray l2_norm(FloatNdArray x, int axis) {\n+    float epsilon = 1e-12F;\n+    FloatNdArray square_sum = ND.sum(ND.square(x), axis, true);\n+    FloatNdArray x_inv_norm = ND.div(1, ND.sqrt(ND.max(square_sum, epsilon)));\n+    return ND.mul(x, x_inv_norm);\n+  }\n+\n+  /**\n+   * Print the array to System.out.\n+   *\n+   * @param a the array\n+   */\n+  public static void print(FloatNdArray a) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 672}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MjE4Mw==", "bodyText": "Typo: \"Tensot\"", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463662183", "createdAt": "2020-07-31T14:59:06Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/PrintUtils.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TString;\n+\n+/**\n+ *  Utility to print Tensor values\n+ */\n+public class PrintUtils {\n+\n+    /**\n+     * Print a tensor's values\n+     * @param tensor the tensor to print\n+     */\n+    public static void print(Tensor tensor) {\n+        switch(tensor.dataType().name()) {\n+            case \"BFLOAT16\":\n+                printTBfloat16(tensor);\n+                break;\n+            case \"FLOAT16\":\n+                 printTFloat16(tensor);\n+                break;\n+            case \"FLOAT\":\n+                 printTFloat32(tensor);\n+                break;\n+            case \"DOUBLE\":\n+                 printTFloat64(tensor);\n+                break;\n+            case \"INT32\":\n+                 printTInt32(tensor);\n+                break;\n+            case \"INT64\":\n+                 printTInt64(tensor);\n+                break;\n+            case \"UINT8\":\n+                 printTUint8(tensor);\n+                break;\n+            case \"BOOL\":\n+                 printTBool(tensor);\n+                break;\n+            case \"STRING\":\n+                 printTString(tensor);\n+                break;\n+            default:\n+                break;\n+        }\n+        \n+    }\n+\n+    /**\n+     * Print a boolean Tensot", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MjMxNw==", "bodyText": "Typo: \"Tensot\"", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463662317", "createdAt": "2020-07-31T14:59:22Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/PrintUtils.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TString;\n+\n+/**\n+ *  Utility to print Tensor values\n+ */\n+public class PrintUtils {\n+\n+    /**\n+     * Print a tensor's values\n+     * @param tensor the tensor to print\n+     */\n+    public static void print(Tensor tensor) {\n+        switch(tensor.dataType().name()) {\n+            case \"BFLOAT16\":\n+                printTBfloat16(tensor);\n+                break;\n+            case \"FLOAT16\":\n+                 printTFloat16(tensor);\n+                break;\n+            case \"FLOAT\":\n+                 printTFloat32(tensor);\n+                break;\n+            case \"DOUBLE\":\n+                 printTFloat64(tensor);\n+                break;\n+            case \"INT32\":\n+                 printTInt32(tensor);\n+                break;\n+            case \"INT64\":\n+                 printTInt64(tensor);\n+                break;\n+            case \"UINT8\":\n+                 printTUint8(tensor);\n+                break;\n+            case \"BOOL\":\n+                 printTBool(tensor);\n+                break;\n+            case \"STRING\":\n+                 printTString(tensor);\n+                break;\n+            default:\n+                break;\n+        }\n+        \n+    }\n+\n+    /**\n+     * Print a boolean Tensot\n+     * @param t the tensor to print\n+     */\n+    public static void printTBool(Tensor<TBool> t) {\n+        t.data().scalars().forEach(s -> System.out.print(s.getBoolean() + \", \"));\n+        System.out.println();\n+    }\n+\n+    /**\n+     * Print a String Tensot", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MzQ3OA==", "bodyText": "Swap asserts for ifs.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463663478", "createdAt": "2020-07-31T15:01:30Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SmartCond.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.function.Supplier;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+\n+/** Implements a select Operation using Lambdas */\n+public class SmartCond {\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate (boolean) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Boolean pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NDEzMg==", "bodyText": "typo", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463664132", "createdAt": "2020-07-31T15:02:42Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SmartCond.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.function.Supplier;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+\n+/** Implements a select Operation using Lambdas */\n+public class SmartCond {\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate (boolean) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Boolean pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return pred ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate ( true == 1) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Number pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return pred.intValue() == 1 ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate ( true if the string argument is not null and is equal, ignoring\n+   *     case, to the string \"true\") used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(String pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return Boolean.valueOf(pred) ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Create a Select operation\n+   *\n+   * @param tf the tensorFlow Ops\n+   * @param pred the operand that evaluates to true or false\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return a Select Operation if in graph mode, else return the result of the select\n+   */\n+  public static Operand select(\n+      Ops tf, Operand<TBool> pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    if (tf.scope().env().isEager()) {\n+      return pred.asOutput().data().getBoolean() ? then_fn.get() : else_fn.get();\n+    } else { // TODO, maybe some day handle Supplier in the c interface\n+      return tf.select(pred, then_fn.get(), else_fn.get());\n+    }\n+  }\n+\n+  /**\n+   * Creates a slect operation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NTUxNA==", "bodyText": "Typos.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463665514", "createdAt": "2020-07-31T15:05:12Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SymbolicShape.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+\n+/**\n+ * Utility class that handles sybmolic shapes so that the shapes can be resolved during runtime and\n+ * stay consistent across operands.\n+ *\n+ * <p>The same symbols accros variaous shapes should have the same dimension values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA==", "bodyText": "Do we want the ability to overwrite values that already exist? If so, should we also have a remove method?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463666070", "createdAt": "2020-07-31T15:06:13Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SymbolicShapeDict.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Utility class that contains a dictionary of Symbols. The map is updated with symbols and their\n+ * dimension values. Used in creating matching shapes that depend on other shapes' dimensions.\n+ */\n+public class SymbolicShapeDict {\n+\n+  /** Dictionary containing the sybmols and their associated dimensions sizes. */\n+  private final Map<String, Long> map = new HashMap<>();\n+\n+  /**\n+   * Add a symbol with its associated dimensions size.\n+   *\n+   * @param symbol the symbol name\n+   * @param size the shape dimension to associate with the symbol\n+   */\n+  public void put(String symbol, Long size) {\n+    this.map.put(symbol, size);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2Njg0OQ==", "bodyText": "Do we need this if there is no support for it?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463666849", "createdAt": "2020-07-31T15:07:43Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/TypeUtils.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.TUint8;\n+\n+\n+/**\n+ *\n+ * @author Jim Clarke\n+ */\n+public class TypeUtils {\n+    \n+    //TODO\n+    public static boolean isComplex(DataType dtype) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw==", "bodyText": "Should we move all of this onto DataType? It feels like these would be better as instance methods.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463667213", "createdAt": "2020-07-31T15:08:22Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/TypeUtils.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.TUint8;\n+\n+\n+/**\n+ *\n+ * @author Jim Clarke\n+ */\n+public class TypeUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY3MTM4OQ==", "bodyText": "Formatting", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463671389", "createdAt": "2020-07-31T15:15:29Z", "author": {"login": "Craigacp"}, "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/utils/EagerTestSession.java", "diffHunk": "@@ -0,0 +1,747 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.io.PrintWriter;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Predicate;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.fail;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Eaager Mode Test Session */\n+public class EagerTestSession extends TestSession {\n+\n+  private final EagerSession session;\n+  private final Ops tf;\n+\n+  /** Create an Eager mode test session. */\n+  public EagerTestSession() {\n+    this.session = EagerSession.create();\n+    this.tf = Ops.create(session).withName(\"test\");\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Ops getTF() {\n+    return tf;\n+  }\n+\n+    /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTY1OTkw", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-465565990", "createdAt": "2020-08-12T03:26:25Z", "commit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "state": "COMMENTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzoyNjoyNVrOG_Qdig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMzoxOTo0MFrOG_6jtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MzE3OA==", "bodyText": "This dependency should be received transitively from tensorflow-core-platform, you shouldn't need to depend directly on it", "url": "https://github.com/tensorflow/java/pull/91#discussion_r468983178", "createdAt": "2020-08-12T03:26:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/pom.xml", "diffHunk": "@@ -14,53 +14,109 @@\n   limitations under the License.\n   =======================================================================\n   -->\n-<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\"\n-    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\n+         xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n \n-  <modelVersion>4.0.0</modelVersion>\n+    <modelVersion>4.0.0</modelVersion>\n \n-  <parent>\n-    <groupId>org.tensorflow</groupId>\n-    <artifactId>tensorflow-java</artifactId>\n-    <version>0.2.0-SNAPSHOT</version>\n-  </parent>\n-  <artifactId>tensorflow-keras</artifactId>\n-  <packaging>jar</packaging>\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-java</artifactId>\n+        <version>0.2.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-keras</artifactId>\n+    <packaging>jar</packaging>\n \n-  <name>TensorFlow Keras Library</name>\n-  <description>Java implementation of the TensorFlow Keras API</description>\n+    <name>TensorFlow Keras Library</name>\n+    <properties>\n+        <javacpp.platform.extension></javacpp.platform.extension>\n+    </properties>\n+    <description>Java implementation of the TensorFlow Keras API</description>\n \n-  <dependencies>\n-    <dependency>\n-      <groupId>org.junit.jupiter</groupId>\n-      <artifactId>junit-jupiter-api</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.junit.jupiter</groupId>\n-      <artifactId>junit-jupiter-engine</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n-  </dependencies>\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>tensorflow-core-api</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>tensorflow-core-generator</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>tensorflow-framework</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>ndarray</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <!-- https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java -->\n+        <dependency>\n+            <groupId>com.google.protobuf</groupId>\n+            <artifactId>protobuf-java</artifactId>\n+            <version>3.11.4</version>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MzMxOA==", "bodyText": "Nit: can you move the properties block after the name and description? It is kind of unusual seeing it squeezed between both (looks like some autoformat was applied in the file, if that was not your intention, probably you'll be better reverting undesired changes).", "url": "https://github.com/tensorflow/java/pull/91#discussion_r468983318", "createdAt": "2020-08-12T03:26:57Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/pom.xml", "diffHunk": "@@ -14,53 +14,109 @@\n   limitations under the License.\n   =======================================================================\n   -->\n-<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\"\n-    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\n+         xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n \n-  <modelVersion>4.0.0</modelVersion>\n+    <modelVersion>4.0.0</modelVersion>\n \n-  <parent>\n-    <groupId>org.tensorflow</groupId>\n-    <artifactId>tensorflow-java</artifactId>\n-    <version>0.2.0-SNAPSHOT</version>\n-  </parent>\n-  <artifactId>tensorflow-keras</artifactId>\n-  <packaging>jar</packaging>\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-java</artifactId>\n+        <version>0.2.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-keras</artifactId>\n+    <packaging>jar</packaging>\n \n-  <name>TensorFlow Keras Library</name>\n-  <description>Java implementation of the TensorFlow Keras API</description>\n+    <name>TensorFlow Keras Library</name>\n+    <properties>\n+        <javacpp.platform.extension></javacpp.platform.extension>\n+    </properties>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MzYwMQ==", "bodyText": "Since you depend on tensorflow-core-platform below, you shouldn't depend on the other core artifacts directly", "url": "https://github.com/tensorflow/java/pull/91#discussion_r468983601", "createdAt": "2020-08-12T03:28:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/pom.xml", "diffHunk": "@@ -14,53 +14,109 @@\n   limitations under the License.\n   =======================================================================\n   -->\n-<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\"\n-    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\n+         xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n \n-  <modelVersion>4.0.0</modelVersion>\n+    <modelVersion>4.0.0</modelVersion>\n \n-  <parent>\n-    <groupId>org.tensorflow</groupId>\n-    <artifactId>tensorflow-java</artifactId>\n-    <version>0.2.0-SNAPSHOT</version>\n-  </parent>\n-  <artifactId>tensorflow-keras</artifactId>\n-  <packaging>jar</packaging>\n+    <parent>\n+        <groupId>org.tensorflow</groupId>\n+        <artifactId>tensorflow-java</artifactId>\n+        <version>0.2.0-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>tensorflow-keras</artifactId>\n+    <packaging>jar</packaging>\n \n-  <name>TensorFlow Keras Library</name>\n-  <description>Java implementation of the TensorFlow Keras API</description>\n+    <name>TensorFlow Keras Library</name>\n+    <properties>\n+        <javacpp.platform.extension></javacpp.platform.extension>\n+    </properties>\n+    <description>Java implementation of the TensorFlow Keras API</description>\n \n-  <dependencies>\n-    <dependency>\n-      <groupId>org.junit.jupiter</groupId>\n-      <artifactId>junit-jupiter-api</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.junit.jupiter</groupId>\n-      <artifactId>junit-jupiter-engine</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n-  </dependencies>\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>tensorflow-core-api</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.tensorflow</groupId>\n+            <artifactId>tensorflow-core-generator</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1Njg1Mw==", "bodyText": "According to Google Java Style guide, which we apply as much as possible, constants should be all uppercase.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469656853", "createdAt": "2020-08-13T02:16:31Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODMzNA==", "bodyText": "I think throwing a regular IllegalArgumentException would be more appropriate since that is what we do everywhere else in the code on similar validations.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469658334", "createdAt": "2020-08-13T02:22:09Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODk4MQ==", "bodyText": "Just curious, why casting the booleans only, it was causing an error?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469658981", "createdAt": "2020-08-13T02:24:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1OTU4NQ==", "bodyText": "Before casting, should we check that the datatypes are not the same? Frankly, I don't know which one is the faster. If we do this, then we should probably do it everywhere we are casting a tensor to reflect the type of another. Maybe we can then wrap the cast method with somelike like this?\npublic static <T extends TType> Operand<T> castLike(Ops tf, Operand<T> x, Operand<?> y) {\n    if (x.asOutput().dataType() != y.asOutput().dataType()) {\n        return tf.dtypes.cast(y, x.asOutput().dataType());\n    }\n    return (Operand)y;\n}", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469659585", "createdAt": "2020-08-13T02:26:41Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MDc5OQ==", "bodyText": "Again, maybe replace assertions by exceptions?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469660799", "createdAt": "2020-08-13T02:31:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MTIxMg==", "bodyText": "Typo: \"lables\" -> \"labels\"", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469661212", "createdAt": "2020-08-13T02:32:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MTUxMA==", "bodyText": "Just double-checking with you @JimClarke5  that we expose as much as possible only the same methods found in the Python's version of this class. The goal of tensorflow-keras is really only to replicate the Keras API and I think other useful high-level utilities should be made available to users via tensorflow-framework instead.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469661510", "createdAt": "2020-08-13T02:34:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MzAwMQ==", "bodyText": "The way you retrieve the op type (in comment below) is a proper way to do it. Though I'm not sure to understand clearly what is the ultimate goal of this method.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663001", "createdAt": "2020-08-13T02:39:39Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MzYzNg==", "bodyText": "maybe checking the output.op.type() instead of the instance is better again because if your operand comes from, let say, a saved model that you have loaded, you won't retrieve the original instance of the op wrappers, it will just be a graph of nodes (outputs).", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663636", "createdAt": "2020-08-13T02:41:58Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2Mzk3NQ==", "bodyText": "Congrats @JimClarke5 , that is the right way of doing it \ud83d\udc4d", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663975", "createdAt": "2020-08-13T02:43:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NDMyMA==", "bodyText": "For Java, let's stick to camel case method names, as found in the guide as well (also applies to the methods below)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469664320", "createdAt": "2020-08-13T02:44:58Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NTUyMw==", "bodyText": "\"oneMinusEpsilonConst\"", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469665523", "createdAt": "2020-08-13T02:49:42Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 246}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NzMyNQ==", "bodyText": "Shape already support equality check via .equals(), I don't think we need something else", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469667325", "createdAt": "2020-08-13T02:56:35Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 402}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2ODkwNg==", "bodyText": "I'm ok with moving raw ops that we front in a .internal subpackage (or maybe tf.nn.raw?) To do this @JimClarke5 , just have just updated the API def for that op, right?\nThe original initial idea though was to leave all ops in Ops as closest as possible to their raw implementation in the native library and to wrap them with high-level manipulations only in tensorflow-framework. I think what prevent us to do this efficiently is that the annotation processor right now only runs on top of tensorflow-core-api... I think we need to discuss more about this.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469668906", "createdAt": "2020-08-13T03:02:49Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2OTQ2Mw==", "bodyText": "We should be able to support this soon :)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469669463", "createdAt": "2020-08-13T03:05:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 487}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3MTA5OA==", "bodyText": "Also agree that we don't want to keep a strong reference to the ExecutionEnvironment here", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469671098", "createdAt": "2020-08-13T03:11:54Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 497}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3MjY2Mw==", "bodyText": "To make this usable, NdArraySequence must be typed. If we decide to go ahead with this PR, it could make things easier but I'll need to double-check before confirming this. Otherwise, we will need one method per type.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469672663", "createdAt": "2020-08-13T03:18:44Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {\n+    return a.byteSize() < b.byteSize() ? b : a;\n+  }\n+\n+  /**\n+   * returns the smaller DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the smaller DataType\n+   */\n+  public DataType narrower(DataType a, DataType b) {\n+    return a.byteSize() > b.byteSize() ? b : a;\n+  }\n+\n+  public <T extends TNumber> NdArraySequence getTensorValue(Ops tf, Operand<T> operand) {\n+    DataType dtype = operand.asOutput().dataType();\n+    if (tf.scope().env().isGraph()) {\n+      try (Session session = new Session((Graph) tf.scope().env())) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          try (Tensor<TInt32> result =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMjM0NQ=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 560}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3Mjg4Ng==", "bodyText": "At some point, we will need to document all these methods but I guess it is fine doing it later when the interface is more stable", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469672886", "createdAt": "2020-08-13T03:19:40Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {\n+    return a.byteSize() < b.byteSize() ? b : a;\n+  }\n+\n+  /**\n+   * returns the smaller DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the smaller DataType\n+   */\n+  public DataType narrower(DataType a, DataType b) {\n+    return a.byteSize() > b.byteSize() ? b : a;\n+  }\n+\n+  public <T extends TNumber> NdArraySequence getTensorValue(Ops tf, Operand<T> operand) {\n+    DataType dtype = operand.asOutput().dataType();\n+    if (tf.scope().env().isGraph()) {\n+      try (Session session = new Session((Graph) tf.scope().env())) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          try (Tensor<TInt32> result =\n+              session.runner().fetch(operand).run().get(0).expect(TInt32.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TInt64.DTYPE)) {\n+          try (Tensor<TInt64> result =\n+              session.runner().fetch(operand).run().get(0).expect(TInt64.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TUint8.DTYPE)) {\n+          try (Tensor<TUint8> result =\n+              session.runner().fetch(operand).run().get(0).expect(TUint8.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TBfloat16.DTYPE)) {\n+          try (Tensor<TBfloat16> result =\n+              session.runner().fetch(operand).run().get(0).expect(TBfloat16.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat16.DTYPE)) {\n+          try (Tensor<TFloat16> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat16.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat32.DTYPE)) {\n+          try (Tensor<TFloat32> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat32.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat64.DTYPE)) {\n+          try (Tensor<TFloat64> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat64.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else {\n+          return null;\n+        }\n+      }\n+    } else {\n+      try (EagerSession session = EagerSession.create()) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          return ((Operand<TInt32>) operand).data().scalars();\n+        } else if (dtype.equals(TInt64.DTYPE)) {\n+          return ((Operand<TInt64>) operand).data().scalars();\n+        } else if (dtype.equals(TUint8.DTYPE)) {\n+          return ((Operand<TUint8>) operand).data().scalars();\n+        } else if (dtype.equals(TBfloat16.DTYPE)) {\n+          return ((Operand<TBfloat16>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat16.DTYPE)) {\n+          return ((Operand<TFloat16>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat32.DTYPE)) {\n+          return ((Operand<TFloat32>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat64.DTYPE)) {\n+          return ((Operand<TFloat64>) operand).data().scalars();\n+        } else {\n+          return null;\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed. 1. Squeezes last dim of `y_pred` or `y_true` if\n+   * their rank differs by 1 (using `confusion_matrix.remove_squeezable_dimensions`). 2. Squeezes or\n+   * expands last dim of `sample_weight` if its rank differs by 1 from the new rank of `y_pred`. If\n+   * `sample_weight` is scalar, it is kept scalar.\n+   *\n+   * @param tf the TensorVlow Ops\n+   * @param yPred Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param yTrue Optional label `Tensor` whose dimensions match `y_pred`.\n+   * @return Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has the last\n+   *     dimension squeezed, `sample_weight` could be extended by one dimension. If `sample_weight`\n+   *     is null, (y_pred, y_true) is returned.\n+   */\n+  public static Tuple squeezeOrExpandDimensions(Ops tf, Operand yTrue, Operand yPred) {\n+    return squeezeOrExpandDimensions(tf, yTrue, yPred, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed. 1. Squeezes last dim of `y_pred` or `y_true` if\n+   * their rank differs by 1 (using `confusion_matrix.remove_squeezable_dimensions`). 2. Squeezes or\n+   * expands last dim of `sample_weight` if its rank differs by 1 from the new rank of `y_pred`. If\n+   * `sample_weight` is scalar, it is kept scalar.\n+   *\n+   * @param tf the TensorVlow Ops\n+   * @param yPred Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param yTrue Optional label `Tensor` whose dimensions match `y_pred`.\n+   * @param sampleWeight Optional weight scalar or `Tensor` whose dimensions match `y_pred`.\n+   * @return Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has the last\n+   *     dimension squeezed, `sample_weight` could be extended by one dimension. If `sample_weight`\n+   *     is null, (y_pred, y_true) is returned.\n+   */\n+  public static Tuple squeezeOrExpandDimensions(\n+      Ops tf, Operand yTrue, Operand yPred, Operand sampleWeight) {\n+    Tuple tuple = new Tuple(yTrue, yPred);\n+    Shape ypredShape = yPred.asOutput().shape();\n+    long ypredRank = ypredShape.numDimensions();\n+\n+    if (yTrue != null) {\n+      Shape ytrueShape = yTrue.asOutput().shape();\n+      long ytrueRank = ytrueShape.numDimensions();\n+      if (ytrueRank != Shape.UNKNOWN_SIZE && ypredRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `y_true` and `y_pred`.\n+        if (ypredRank - ytrueRank != 1 || ypredShape.size(-1) == 1) {\n+          // y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(y_true, y_pred)\n+          tuple = ConfusionMatrix.removeSqueezableDimensions(tf, yTrue, yPred);\n+        }\n+      } else { // use dynamic rank\n+        tuple = ConfusionMatrix.removeSqueezableDimensions(tf, yTrue, yPred);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple(yTrue, yPred, sampleWeight);\n+    }\n+\n+    if (ypredRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - ypredRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (ypredRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple(yTrue, yPred, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand weightsRankTensor = tf.rank(sampleWeight);\n+    Operand rankDiff = tf.math.sub(weightsRankTensor, tf.rank(yPred));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple(yTrue, yPred, sampleWeight);\n+  }\n+\n+  private static Operand maybeAdjustWeights(Ops tf, Operand sampleWeight, Operand rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Arrays.asList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  private static Operand maybeExpandWeights(Ops tf, Operand sampleWeight, Operand rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 713}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcxMDA3MDgy", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-471007082", "createdAt": "2020-08-19T22:26:24Z", "commit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjoyNjoyNVrOHDdOKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMjo0NDo0OVrOHDeCtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM4NjUzNw==", "bodyText": "If this not belong to Keras in Python, then we should move either to tensorflow-framework or tensorflow-core-api. The way to decide if it is the one or the other is if we consider the operation as being something very close to a what could have been a core operation (e.g. sparseSoftmaxCrossEntropyWithLogits) or if it defines a higher-level concept. If that can help you to decide where this one (and the others you are adding) could fit....", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473386537", "createdAt": "2020-08-19T22:26:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ConfusionMatrix.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjM3OQ=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ==", "bodyText": "I don't think we need this class, as the user can very easily add its control dependencies using directly tf.withControlDependencies(...), unless you have another use case in mind?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473395889", "createdAt": "2020-08-19T22:39:11Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.function.Function;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** Container for ControlDepencies, so that the primary Operand is remembered. */\n+public class ControlDependencies {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NzI4OA==", "bodyText": "We should add sparseSoftmaxCrossEntropyWithLogits as an operator accessible via the Ops API, probably under tf.sparse.*", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473397288", "createdAt": "2020-08-19T22:41:14Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/NN.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.nn.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+\n+/** NN Operations */\n+public class NN {\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param tf\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  public static Operand sparse_softmax_cross_entropy_with_logits(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODM2NQ=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTMzMQ==", "bodyText": "This might get out of date if we add more float types in the future. We should add a type family called TFloating that is used to tagged all our floating point tensor types (that is what I did in https://github.com/karllessard/tensorflow-java/tree/tensor-as-ndarrays-3)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473399331", "createdAt": "2020-08-19T22:43:57Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/NN.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.nn.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+\n+/** NN Operations */\n+public class NN {\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param tf\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  public static Operand sparse_softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    tf = tf.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTk4OA==", "bodyText": "I really think SparseTensor should be a core concept of the library. I still don't know where and how it will fit exactly. Do you need this one to be checked-in immediatly @JimClarke5 ?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473399988", "createdAt": "2020-08-19T22:44:49Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/SparseTensor.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Represents a sparse tensor.\n+ *\n+ * @param <T> the type of the SparseTensor\n+ */\n+public class SparseTensor<T extends TType> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODk1MA=="}, "originalCommit": {"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c113a7e6dfcc98c0e57bede8dcb46de56125320", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/9c113a7e6dfcc98c0e57bede8dcb46de56125320", "committedDate": "2020-08-20T12:12:27Z", "message": "Added static final NAME to replace hardcoded String in the create method. This allows the NAME to be used elsewhere instead of hardcoding the string."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "824d4872257a5a614a53c7dff579748b819c800e", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/824d4872257a5a614a53c7dff579748b819c800e", "committedDate": "2020-08-20T12:14:26Z", "message": "Changed of method to use the DataType NAME attribute rather than hardcoding the string.\nadded methods isFloating(), isInteger(), isNUmeric(), isBoolean() and isString()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07a83a5aef5a25f385aa762e3bb929e22c8052e3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/07a83a5aef5a25f385aa762e3bb929e22c8052e3", "committedDate": "2020-08-20T12:17:40Z", "message": "Added method WriteFieldWithInitializer to output a \"final static String OP_NAME\" to each generated operation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d26831bab64cc4fdd2340403cafa229f0fb7099", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/3d26831bab64cc4fdd2340403cafa229f0fb7099", "committedDate": "2020-08-20T13:52:13Z", "message": "Added tf.nn.softmaxCrossEntropyWitLogits() and tf.nn.raw.softmaxCrossEntropyWitLogits()\nAdded tf.nn.sparesSoftmaxCrossEntropyWithLogits() and\ntf.nn.raw.sparesSoftmaxCrossEntropyWithLogits()\n\nAdded tf.nn.sigmoidCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11cda5fde99a6bec21fb04ed1465934ec1889485", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/11cda5fde99a6bec21fb04ed1465934ec1889485", "committedDate": "2020-08-20T13:58:05Z", "message": "Moved SoftmaxCrossEntropyWithLogits and  SparseSoftmaxCrossEntropyWithLogits to org.tensorflow.op.nn.raw"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "committedDate": "2020-08-20T14:00:25Z", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84f49db3fb6c085befabfcb8356ab77589facf5d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/84f49db3fb6c085befabfcb8356ab77589facf5d", "committedDate": "2020-08-20T14:03:38Z", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "208b84a1a09d38ab9821d66572c1328326685c5a", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/208b84a1a09d38ab9821d66572c1328326685c5a", "committedDate": "2020-08-20T14:42:22Z", "message": "fix dependencies for other Tensorflow Java modules"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "committedDate": "2020-08-20T14:46:20Z", "message": "formatting fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "committedDate": "2020-08-20T14:48:23Z", "message": "Fix ctors with name to properly pass the name to the the super ctor."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcba0a525d4712fca5210db71c141bc1bce87307", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/fcba0a525d4712fca5210db71c141bc1bce87307", "committedDate": "2020-08-20T17:04:27Z", "message": "change asserts to IllegalArgumentException\nfix javadoc, fix casts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "committedDate": "2020-08-20T17:05:22Z", "message": "change asserts to IllegalArgumentException"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d37298a6b9dcb206cdff985b96e340f7f93a075c", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d37298a6b9dcb206cdff985b96e340f7f93a075c", "committedDate": "2020-08-20T17:06:58Z", "message": "Moved back to tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c68812cc9c8815b0ea186f273748078c4907fbaa", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c68812cc9c8815b0ea186f273748078c4907fbaa", "committedDate": "2020-08-20T17:08:44Z", "message": "Moved SoftmaxCrossEntropyWithLogits.java and SparseSoftmaxCrossEntropyWithLogits.java to nn.raw,\nadded new versions of these to NnOps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "committedDate": "2020-08-20T17:50:01Z", "message": "Deleted files that are not necessary yet"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6515c248ddd8f6911f267bee0972d44ca20f0038", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/6515c248ddd8f6911f267bee0972d44ca20f0038", "committedDate": "2020-08-20T17:51:04Z", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76d0fe553559176a33040b4bdb8e07d8e033b08e", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/76d0fe553559176a33040b4bdb8e07d8e033b08e", "committedDate": "2020-08-20T18:30:16Z", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d2201df3a78e82c79b142fc7d7c3a461afa63444", "committedDate": "2020-08-20T19:43:35Z", "message": "Merge branch 'master' into master"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzYyNzU5", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-479362759", "createdAt": "2020-09-01T02:42:44Z", "commit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjo0Mjo0NVrOHKX1Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjo0Mjo0NVrOHKX1Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzODI3MA==", "bodyText": "Can we rename this class to something else?\nActually it should be located under org.tensorflow.op.nn. The packages in src/main and src/gen overlaps each other \"by design\", like it is done with org.tensorflow.op.core. So the idea is to put the wrapper directly into the right directory, org.tensorflow.op.nn in this case.\nThe class can either be split in 3 classes (one per endpoint, i.e. SoftmaxCrossEntropyWithLogits, SparseSoftmaxCrossEntropyWithLogits or SigmoidCrossEntropyWithLogits) or remain in a single class (CrossEntropy maybe? or just Helpers to reflect what the one already present in the core package?)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480638270", "createdAt": "2020-09-01T02:42:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzY0MjAz", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-479364203", "createdAt": "2020-09-01T02:47:58Z", "commit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjo0Nzo1OFrOHKYRVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMzoyMjoyNlrOHKbF6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0NTQ2Mw==", "bodyText": "Since this documentation is not generated, let's transcript all the Python code in it to Java, and replace markdown by Javadoc tags.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480645463", "createdAt": "2020-09-01T02:47:58Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MDI2Nw==", "bodyText": "Please use camelCase for variable names. The line wrapping is a bit weird here as well.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480650267", "createdAt": "2020-09-01T02:51:31Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng==", "bodyText": "If generic parameter cannot be carried, then we should carried the wildcard so IDEs or lint checks will not complain about it: DataType<?> dtype = ...", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480653396", "createdAt": "2020-09-01T02:53:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1NTc5Mw==", "bodyText": "\".... if axis is not the last dimension\"?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480655793", "createdAt": "2020-09-01T02:55:39Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1ODEyMQ==", "bodyText": "Idem: Operand<?>", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480658121", "createdAt": "2020-09-01T02:57:23Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NDQ3Mw==", "bodyText": "Shortcut: you can Constant.tensorOf(scope, inputShape) directly for the first, and Constant.arrayOf(scope, num) for the others", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480664473", "createdAt": "2020-09-01T03:02:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NzY3OQ==", "bodyText": "no need to convert newArray to a Shape if you pass it back as an array in Constant.vectorOf.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480667679", "createdAt": "2020-09-01T03:04:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2OTUyNA==", "bodyText": "Maybe you should create a subscope like you do in sparseSoftmaxCrossEntropyWithLogits?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480669524", "createdAt": "2020-09-01T03:05:46Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NDc4NQ==", "bodyText": "Nit: when there are no fields to write after this, it creates one extra empty line. We can avoid it by adding EndLine() only if op.outputs() is not empty:\nif (!op.outputs().empty()) {\n    writer.EndLine();\n    for (const ArgumentSpec& output : op.outputs()) {\n        writer.WriteField(output.var(), PRIVATE);\n    }\n}", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480674785", "createdAt": "2020-09-01T03:09:39Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/bazel/op_generator/op_generator.cc", "diffHunk": "@@ -509,6 +509,13 @@ void GenerateOp(const OpSpec& op, const EndpointSpec& endpoint,\n     RenderInterfaceImpl(op, mode, &writer);\n   }\n   writer.EndLine();\n+\n+  Variable nameVariable = Variable::Create(\"OP_NAME\", Type::Class(\"String\")); \n+  Javadoc name_javadoc = Javadoc::Create(\"The name of this op, as known by TensorFlow core engine\");\n+  string quoted_string = \"\\\"\" + op.graph_op_name() + \"\\\"\";\n+  writer.WriteFieldWithInitializer(nameVariable, PUBLIC|STATIC|FINAL, &name_javadoc, quoted_string );\n+  writer.EndLine();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NzQyMA==", "bodyText": "Typpppo :)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480677420", "createdAt": "2020-09-01T03:11:43Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3Nzk4OQ==", "bodyText": "camelCase for variable names", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480677989", "createdAt": "2020-09-01T03:12:08Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3ODk4MQ==", "bodyText": "...or Constant.arrayOf(scope, -1L, numClasses)", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480678981", "createdAt": "2020-09-01T03:12:52Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MDgxNg==", "bodyText": "I think you are missing TBfloat16 here?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480680816", "createdAt": "2020-09-01T03:14:10Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4Mjk2Nw==", "bodyText": "Missing brackets for if block", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480682967", "createdAt": "2020-09-01T03:15:46Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MzY5OQ==", "bodyText": "camelCase variable names", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480683699", "createdAt": "2020-09-01T03:16:19Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NDM4NQ==", "bodyText": "you shouldn't need the full canonical name for Shape in this method as you've already imported it", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480684385", "createdAt": "2020-09-01T03:16:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 313}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjQyOA==", "bodyText": "just Constant.tensorOf(scope, shape.size(ndims - 1)) will do the job here", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480686428", "createdAt": "2020-09-01T03:18:25Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 328}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjgwNw==", "bodyText": "camelCase variable names", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480686807", "createdAt": "2020-09-01T03:18:44Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NzQzNA==", "bodyText": "...Constant.arrayOf(scope, -1L)...", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480687434", "createdAt": "2020-09-01T03:19:13Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =\n+        Slice.create(\n+            scope,\n+            org.tensorflow.op.core.Shape.create(scope, logits, TInt64.DTYPE),\n+            rankMinusOne,\n+            one);\n+    Operand<TInt64> concat =\n+        Concat.create(\n+            scope,\n+            Arrays.asList(Constant.vectorOf(scope, new long[] {-1}), last_dim_size),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 344}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4ODEzNw==", "bodyText": "dimIndex", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480688137", "createdAt": "2020-09-01T03:19:46Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =\n+        Slice.create(\n+            scope,\n+            org.tensorflow.op.core.Shape.create(scope, logits, TInt64.DTYPE),\n+            rankMinusOne,\n+            one);\n+    Operand<TInt64> concat =\n+        Concat.create(\n+            scope,\n+            Arrays.asList(Constant.vectorOf(scope, new long[] {-1}), last_dim_size),\n+            Constant.scalarOf(scope, 0));\n+    return Reshape.create(scope, logits, concat);\n+  }\n+\n+  /**\n+   * Move the dim to the end if dim is not the last dimension.\n+   *\n+   * @param scope The TensorFlow Scope\n+   * @param input the input to reshape\n+   * @param dim_index the index to move\n+   * @param rank the number of Dimensions in the tensor\n+   * @param <T> the data type of the tensor.\n+   * @param <U> the data type of the rank\n+   * @return the reshaped input\n+   */\n+  private static <T extends TNumber, U extends TNumber> Operand<T> moveDimToEnd(\n+      Scope scope, Operand<T> input, int dim_index, Operand<U> rank) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY5MTY5MQ==", "bodyText": "I get a feeling that auto-formatter was a bit aggressive here, can you please just double-checked that the Google one was used when reformatting all the types classes?", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480691691", "createdAt": "2020-09-01T03:22:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TString.java", "diffHunk": "@@ -235,23 +232,26 @@ public TString using(Charset charset) {\n \n   static <T> Tensor<TString> createTensor(NdArray<T> src, Function<T, byte[]> getBytes) {\n     long size = StringTensorBuffer.computeSize(src, getBytes);\n-    return Tensor.of(TString.DTYPE, src.shape(), size, data ->\n-        ((TStringImpl)data).tensorBuffer.init(src, getBytes)\n-    );\n+    return Tensor.of(\n+        TString.DTYPE,\n+        src.shape(),\n+        size,\n+        data -> ((TStringImpl) data).tensorBuffer.init(src, getBytes));\n   }\n \n   static TString mapTensor(TF_Tensor nativeTensor, Shape shape) {\n     StringTensorBuffer buffer = TensorBuffers.toStrings(nativeTensor, shape.size());\n     return new TStringImpl(buffer, UTF_8_LAYOUT, shape);\n   }\n \n-  private static DataLayout<DataBuffer<byte[]>, String> UTF_8_LAYOUT = DataLayouts.ofStrings(StandardCharsets.UTF_8);\n+  private static DataLayout<DataBuffer<byte[]>, String> UTF_8_LAYOUT =\n+      DataLayouts.ofStrings(StandardCharsets.UTF_8);\n \n   private final StringTensorBuffer tensorBuffer;\n \n-  private TStringImpl(StringTensorBuffer buffer, DataLayout<DataBuffer<byte[]>, String> layout, Shape shape) {\n+  private TStringImpl(\n+      StringTensorBuffer buffer, DataLayout<DataBuffer<byte[]>, String> layout, Shape shape) {\n     super(layout.applyTo(buffer), shape);\n     tensorBuffer = buffer;\n   }\n }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "committedDate": "2020-09-03T14:21:27Z", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "889d67e11ec7154605a0cb235097e30c53a5704a", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/889d67e11ec7154605a0cb235097e30c53a5704a", "committedDate": "2020-09-03T14:21:33Z", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "515b799bf793fbb38a47b5aa92d948dad052187b", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/515b799bf793fbb38a47b5aa92d948dad052187b", "committedDate": "2020-09-03T14:34:57Z", "message": "Reformatted code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "committedDate": "2020-09-03T14:35:19Z", "message": "Added sub scope"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d21dd7266e48a4df36813ed3fb9dc1adee58915", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/8d21dd7266e48a4df36813ed3fb9dc1adee58915", "committedDate": "2020-09-03T16:30:10Z", "message": "Miscellaneous fixes based on review comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyMTg0MDkz", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-482184093", "createdAt": "2020-09-03T20:35:06Z", "commit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMDozNTowN1rOHM2mWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMDo1NjoyOFrOHM3NiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIzOTUxNQ==", "bodyText": "extra space before tf", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483239515", "createdAt": "2020-09-03T20:35:07Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw==", "bodyText": "Can we create a concrete type for the config instead of having a map of untyped values? e.g. could it be something like an AdaDelta.Config subclass following a building pattern similar to the ops options classes like this one? I probably don't understand the use of this config format though.\nThe same comment applies for all other optimizers", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483240743", "createdAt": "2020-09-03T20:37:51Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MTA5OQ==", "bodyText": "unrequired new line before bracket", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483241099", "createdAt": "2020-09-03T20:38:39Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MjQ3Ng==", "bodyText": "extra lines", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483242476", "createdAt": "2020-09-03T20:41:39Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {\n+      return new AdaDelta(tf, learningRate, rho, epsilon);\n+    } else {\n+      return new AdaDelta(tf, name, learningRate, rho, epsilon);\n+    }\n+  }\n+\n+  /**\n+   * Initialize the configuration based on which constructor is called.\n+   *\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  private void initConfig(float learningRate, float rho, float epsilon) {\n+    this.learningRate = learningRate;\n+    config.put(NAME_KEY, this.getOptimizerName());\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(RHO_RATE_KEY, rho);\n+    config.put(EPSILON_KEY, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mjk0MQ==", "bodyText": "double underscores.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483242941", "createdAt": "2020-09-03T20:42:41Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaGrad.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaGrad Optimizer that implements the AdaGrad algorithm. Adagrad is an optimizer with\n+ * parameter-specific learning rates, which are adapted relative to how frequently a parameter gets\n+ * updated during training. The more updates a parameter receives, the smaller the updates.\n+ */\n+public class AdaGrad extends org.tensorflow.framework.optimizers.AdaGrad\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mzg0Mg==", "bodyText": "please reformat these if blocks", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483243842", "createdAt": "2020-09-03T20:44:32Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    if( initialAccumulatorValue < 0.0F)\n+        throw new IllegalArgumentException(\"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue);\n+    if(l1Strength < 0)\n+      throw new IllegalArgumentException(\"l1Strength must be non-negative: \" + l1Strength);\n+    if(l2Strength < 0)\n+      throw new IllegalArgumentException(\"l2Strength must be non-negative: \" + l2Strength);\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be positive.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), name, learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    if( initialAccumulatorValue < 0.0F)\n+      throw new IllegalArgumentException(\"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue);\n+    if(l1Strength < 0)\n+      throw new IllegalArgumentException(\"l1Strength must be non-negative: \" + l1Strength);\n+    if(l2Strength < 0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NTI4Mw==", "bodyText": "Can we create a Adamax implementation in the framework first and then having its Keras wrapper extend from it like other optimizers? The same applies for all other Keras optimizers that are not present in the framework.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483245283", "createdAt": "2020-09-03T20:47:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NjQ4Ng==", "bodyText": "Again are you sure we need these explicit casting to Operand? If so, something is wrong with our generic signatures and we should address that.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483246486", "createdAt": "2020-09-03T20:49:59Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private Scope scope;\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private float learningRate;\n+  private final float betaOne;\n+  private final float betaTwo;\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   */\n+  public Adamax(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   */\n+  public Adamax(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, float learningRate) {\n+    this(tf, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(Ops tf, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf));\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(\n+      Ops tf, String name, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf), name);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize, the config object has keys for \"name\",\n+   *     \"learning_rate\", \"epsilon\", \"beta_1\", \"beta_2\". If a key is missing the default value is\n+   *     used.\n+   */\n+  public static Adamax fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize\n+   */\n+  public static Adamax create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    float betaOne = (float) config.getOrDefault(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+    float betaTwo = (float) config.getOrDefault(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+    if (name == null) {\n+      return new Adamax(tf, learningRate, betaOne, betaTwo, epsilon);\n+    } else {\n+      return new Adamax(tf, name, learningRate, betaOne, betaTwo, epsilon);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+\n+    return Optional.empty();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamaxSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf.assign(betaOnePower, tf.constant(betaOne));\n+    ((Graph) tf.scope().env()).addInitializer(betaOnePowerInit);\n+  }\n+\n+  /**\n+   * Create the first and second moment slots\n+   *\n+   * @param v the variable\n+   * @param <T> the datatype of the variable\n+   */\n+  private <T extends TType> void createAdamaxSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), SECOND_MOMENT, secondMomentInitializer);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected <T extends TType> Op applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> firstMomentSlot = getSlot(variable, FIRST_MOMENT).get();\n+    Variable<T> secondMomentSlot = getSlot(variable, SECOND_MOMENT).get();\n+    return ApplyAdaMax.create(\n+        scope,\n+        (Operand) variable,\n+        (Operand) firstMomentSlot,\n+        (Operand) secondMomentSlot,\n+        (Operand) tf.dtypes.cast(betaOnePower, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(learningRateConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(betaOneConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(betaTwoConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(epsilonConst, gradient.dataType()),\n+        (Operand) gradient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzY1MQ==", "bodyText": "If possible, I would prefer we avoid to suffix this class name with Interface, since none of our other interfaces have this suffix.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483247651", "createdAt": "2020-09-03T20:52:24Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/OptimizerInterface.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.Map;\n+import org.tensorflow.Graph;\n+import org.tensorflow.op.Ops;\n+\n+/** The main Interface for Keras Optimizers */\n+public interface OptimizerInterface {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ==", "bodyText": "missing space after if", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483247815", "createdAt": "2020-09-03T20:52:45Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/OptimizerInterface.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.Map;\n+import org.tensorflow.Graph;\n+import org.tensorflow.op.Ops;\n+\n+/** The main Interface for Keras Optimizers */\n+public interface OptimizerInterface {\n+\n+  /** The value for the name key in the Config object */\n+  String NAME_KEY = \"name\";\n+\n+  /**\n+   * Get a TensorFlow Graph from the Ops.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @return the graph\n+   * @throws java.lang.IllegalArgumentException if the TensorFlow Ops does not represent Graph mode\n+   */\n+  static Graph assertGraph(Ops tf) {\n+    if(!tf.scope().env().isGraph()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0OTU0NA==", "bodyText": "If we have a concrete type for our config, then either our Keras optimizers classes should be parametized to return the right type of config, or either it should be returned as an Object itself. I can't see in this PR what is the use of getConfig() so I can't give a clear answer.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483249544", "createdAt": "2020-09-03T20:56:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {\n+      return new AdaDelta(tf, learningRate, rho, epsilon);\n+    } else {\n+      return new AdaDelta(tf, name, learningRate, rho, epsilon);\n+    }\n+  }\n+\n+  /**\n+   * Initialize the configuration based on which constructor is called.\n+   *\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  private void initConfig(float learningRate, float rho, float epsilon) {\n+    this.learningRate = learningRate;\n+    config.put(NAME_KEY, this.getOptimizerName());\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(RHO_RATE_KEY, rho);\n+    config.put(EPSILON_KEY, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444"}, "originalPosition": 184}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c3cc78a999505240be35040b8865d86b955e1af", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/4c3cc78a999505240be35040b8865d86b955e1af", "committedDate": "2020-09-03T22:52:11Z", "message": "Fixed op_generator.cc to remove a spurious new line in the generated Java files for some Ops. This also  resulted in new generated  source that are also committed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44f530f292fdba34164de696fb454b30108064d3", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/44f530f292fdba34164de696fb454b30108064d3", "committedDate": "2020-09-03T22:53:44Z", "message": "Changed back to non-generic Operand until we resolve how to handle generics."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b8d3ac2d001251c95bd55ed9cd902431108468bd", "committedDate": "2020-09-03T22:55:55Z", "message": "Regenerated due to creation of SoftmaxCrossEntropyWithLogits.java,  SigmoidCrossEntropyWithLogits.java, and SparseSoftmaxCrossEntropyWithLogits.java under package org.tensorflow.op.nn in"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNDA4ODc0", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-483408874", "createdAt": "2020-09-07T09:38:00Z", "commit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwOTozODowMVrOHN4RhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMDo0MToyM1rOHN6WuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMxNTUyNA==", "bodyText": "AdaDelta -> Adam", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484315524", "createdAt": "2020-09-07T09:38:01Z", "author": {"login": "deansher"}, "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMyNTIwMw==", "bodyText": "shape1 -> shape0 on this line and the next.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484325203", "createdAt": "2020-09-07T09:54:22Z", "author": {"login": "deansher"}, "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {\n+    float m0 = 0.0F;\n+    float v0 = 0.0F;\n+    float m1 = 0.0F;\n+    float v1 = 0.0F;\n+    float[] var0_init = {1.0F, 2.0F};\n+    float[] var1_init = {3.0F, 4.0F};\n+    float[] grads0_init = {0.1F, 0.1F};\n+    float[] grads1_init = {0.01F, 0.01F};\n+    FloatNdArray var0_np = NdArrays.vectorOf(var0_init);\n+    FloatNdArray var1_np = NdArrays.vectorOf(var1_init);\n+    FloatNdArray grads0_np = NdArrays.vectorOf(grads0_init);\n+    FloatNdArray grads1_np = NdArrays.vectorOf(grads1_init);\n+\n+    float epsilon1 = 1e-3F;\n+\n+    try (TestSession session = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = session.getTF();\n+\n+      session.setEpsilon(epsilon1);\n+\n+      Shape shape0 = Shape.of(var0_init.length);\n+      Shape shape1 = Shape.of(var1_init.length);\n+      Variable<TFloat32> var0 = tf.withName(\"var0\").variable(shape0, TFloat32.DTYPE);\n+      Variable<TFloat32> var1 = tf.withName(\"var1\").variable(shape1, TFloat32.DTYPE);\n+\n+      Assign<TFloat32> var0Initializer = tf.assign(var0, tf.constant(var0_init));\n+      Assign<TFloat32> var1Initializer = tf.assign(var1, tf.constant(var1_init));\n+\n+      Constant<TFloat32> grads0 = tf.constant(grads0_init);\n+      Constant<TFloat32> grads1 = tf.constant(grads1_init);\n+\n+      /* initialize the local variables */\n+      session.run(var0Initializer);\n+      session.run(var1Initializer);\n+\n+      float learningRate = 0.001F;\n+      float beta1 = 0.9F;\n+      float beta2 = 0.999F;\n+      float epsilon = 1e-8F;\n+\n+      /* build the GradsAnvVars */\n+      List gradsAndVars = new ArrayList<>();\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads0.asOutput(), var0.asOutput()));\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads1.asOutput(), var1.asOutput()));\n+\n+      Adam instance = new Adam(tf, learningRate);\n+\n+      Op update = instance.applyGradients(gradsAndVars, \"AdamTest\");\n+\n+      /* Create and validae the shapes of the slota */\n+      Variable<TFloat32>[] firstMomentSlots = new Variable[2];\n+      Variable<TFloat32>[] secondMomentSlots = new Variable[2];\n+\n+      firstMomentSlots[0] = instance.getSlot(var0.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      secondMomentSlots[0] = instance.getSlot(var0.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      firstMomentSlots[1] = instance.getSlot(var1.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      secondMomentSlots[1] = instance.getSlot(var1.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      /** initialize the accumulators */\n+      session.run(tf.init());\n+\n+      session.evaluate(var0_init, var0);\n+      session.evaluate(var1_init, var1);\n+\n+      FloatNdArray m0_np = NdArrays.ofFloats(shape1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM0OTYyNQ==", "bodyText": "1e-7F -> epsilon here and a few lines down for var1_np.\nThe fact that the test doesn't notice this difference suggests using a substantially larger epsilon, but consistency with the Python test may be more important at the moment.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484349625", "createdAt": "2020-09-07T10:41:23Z", "author": {"login": "deansher"}, "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {\n+    float m0 = 0.0F;\n+    float v0 = 0.0F;\n+    float m1 = 0.0F;\n+    float v1 = 0.0F;\n+    float[] var0_init = {1.0F, 2.0F};\n+    float[] var1_init = {3.0F, 4.0F};\n+    float[] grads0_init = {0.1F, 0.1F};\n+    float[] grads1_init = {0.01F, 0.01F};\n+    FloatNdArray var0_np = NdArrays.vectorOf(var0_init);\n+    FloatNdArray var1_np = NdArrays.vectorOf(var1_init);\n+    FloatNdArray grads0_np = NdArrays.vectorOf(grads0_init);\n+    FloatNdArray grads1_np = NdArrays.vectorOf(grads1_init);\n+\n+    float epsilon1 = 1e-3F;\n+\n+    try (TestSession session = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = session.getTF();\n+\n+      session.setEpsilon(epsilon1);\n+\n+      Shape shape0 = Shape.of(var0_init.length);\n+      Shape shape1 = Shape.of(var1_init.length);\n+      Variable<TFloat32> var0 = tf.withName(\"var0\").variable(shape0, TFloat32.DTYPE);\n+      Variable<TFloat32> var1 = tf.withName(\"var1\").variable(shape1, TFloat32.DTYPE);\n+\n+      Assign<TFloat32> var0Initializer = tf.assign(var0, tf.constant(var0_init));\n+      Assign<TFloat32> var1Initializer = tf.assign(var1, tf.constant(var1_init));\n+\n+      Constant<TFloat32> grads0 = tf.constant(grads0_init);\n+      Constant<TFloat32> grads1 = tf.constant(grads1_init);\n+\n+      /* initialize the local variables */\n+      session.run(var0Initializer);\n+      session.run(var1Initializer);\n+\n+      float learningRate = 0.001F;\n+      float beta1 = 0.9F;\n+      float beta2 = 0.999F;\n+      float epsilon = 1e-8F;\n+\n+      /* build the GradsAnvVars */\n+      List gradsAndVars = new ArrayList<>();\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads0.asOutput(), var0.asOutput()));\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads1.asOutput(), var1.asOutput()));\n+\n+      Adam instance = new Adam(tf, learningRate);\n+\n+      Op update = instance.applyGradients(gradsAndVars, \"AdamTest\");\n+\n+      /* Create and validae the shapes of the slota */\n+      Variable<TFloat32>[] firstMomentSlots = new Variable[2];\n+      Variable<TFloat32>[] secondMomentSlots = new Variable[2];\n+\n+      firstMomentSlots[0] = instance.getSlot(var0.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      secondMomentSlots[0] = instance.getSlot(var0.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      firstMomentSlots[1] = instance.getSlot(var1.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      secondMomentSlots[1] = instance.getSlot(var1.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      /** initialize the accumulators */\n+      session.run(tf.init());\n+\n+      session.evaluate(var0_init, var0);\n+      session.evaluate(var1_init, var1);\n+\n+      FloatNdArray m0_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray v0_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray m1_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray v1_np = NdArrays.ofFloats(shape1);\n+\n+      for (int step = 0; step < 3; step++) {\n+\n+        // Test powers\n+        final float[] powers = {\n+          (float) Math.pow(beta1, step + 1), (float) Math.pow(beta2, step + 1)\n+        };\n+\n+        try (Tensor<TFloat32> result =\n+            session\n+                .getGraphSession()\n+                .runner()\n+                .fetch(\"beta1_power\")\n+                .run()\n+                .get(0)\n+                .expect(TFloat32.DTYPE)) {\n+          result\n+              .data()\n+              .scalars()\n+              .forEach(\n+                  f -> {\n+                    assertEquals(powers[0], f.getFloat(), epsilon1);\n+                  });\n+        }\n+        try (Tensor<TFloat32> result =\n+            session\n+                .getGraphSession()\n+                .runner()\n+                .fetch(\"beta2_power\")\n+                .run()\n+                .get(0)\n+                .expect(TFloat32.DTYPE)) {\n+          result\n+              .data()\n+              .scalars()\n+              .forEach(\n+                  f -> {\n+                    assertEquals(powers[1], f.getFloat(), epsilon1);\n+                  });\n+        }\n+        session.run(update);\n+\n+        float lr_t =\n+            learningRate\n+                * (float) Math.sqrt(1 - (float) Math.pow(beta2, (step + 1)))\n+                / (1 - (float) Math.pow(beta1, (step + 1)));\n+\n+        m0_np = calculateM(m0_np, grads0_np, beta1);\n+        v0_np = calculateV(v0_np, grads0_np, beta2);\n+        var0_np = calculateParam(var0_np, lr_t, m0_np, v0_np, 1e-7F);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "originalPosition": 215}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNTM5MTM2", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-483539136", "createdAt": "2020-09-07T13:02:35Z", "commit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMzowMjozNVrOHN-nxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMzowMjozNVrOHN-nxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQxOTUyNA==", "bodyText": "(I wouldn't propose holding up this PR for this issue.)\nThis test raises interesting questions about both our API layers and our testing strategy. It purports to be a test of keras.optimizers.Adam, but in fact it tests the actual underlying Adam logic that is exposed by framework.optimizers.Adam -- which is not, however, yet tested in framework.optimizers.\nIf we stick with the current layering strategy, I'd propose that in a future PR we move this test into framework.optimizers.", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484419524", "createdAt": "2020-09-07T13:02:35Z", "author": {"login": "deansher"}, "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd"}, "originalPosition": 90}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c32fc5be951166ffde8d8763dcc99dd7e5879e86", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c32fc5be951166ffde8d8763dcc99dd7e5879e86", "committedDate": "2020-09-07T18:15:30Z", "message": "change snake case to camel case. format code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "committedDate": "2020-09-07T18:38:12Z", "message": "clean upd warning,  format code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9c3134742e9155e457be157515d67b5c0bb7ac4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/e9c3134742e9155e457be157515d67b5c0bb7ac4", "committedDate": "2020-09-09T20:14:20Z", "message": "Added Adamax, Ftrl, and Nadam Optimizers. Added Optimizers enum for easy inclusion of a default optimizer. Cleaned up JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c30a72fa335f338727358b4299e4796a211403d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/5c30a72fa335f338727358b4299e4796a211403d", "committedDate": "2020-09-09T20:17:16Z", "message": "Removed optimize classes from tensorflow-keras, moved optimizer test cases to framework. Created Tests for GradientDescent and Momentum"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "committedDate": "2020-09-09T20:17:37Z", "message": "Fixed generics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7915e6309e9db7a536cda24eac8264578cdbfe31", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/7915e6309e9db7a536cda24eac8264578cdbfe31", "committedDate": "2020-09-09T23:03:09Z", "message": "Fixed from Unit test results"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec4f6790ff666a4c23f60e1b4764874d6167d392", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ec4f6790ff666a4c23f60e1b4764874d6167d392", "committedDate": "2020-09-09T23:08:41Z", "message": "added @SuppressWarnings(\"unchecked\") on Variable array"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4OTY5NzI0", "url": "https://github.com/tensorflow/java/pull/91#pullrequestreview-488969724", "createdAt": "2020-09-15T18:47:50Z", "commit": {"oid": "ec4f6790ff666a4c23f60e1b4764874d6167d392"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3451, "cost": 1, "resetAt": "2021-11-02T12:20:56Z"}}}