{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwMjM5Mzcx", "number": 129, "title": "Add Losses", "bodyText": "This PR adds losses to framework.\nAll the loss sub-classes inherit from Loss.\nThe Losses class, has methods that can be called directly to get raw loss values. These are utilized by the Loss subclasses before applying a Reduction to the loss. The Losses class will also  be used by some of the Metric classes when that feature is submitted.\nThe impl package has some helper methods and classes utilized by the loss classes, and are not expected to be exposed outside the framework module,  when we do modules.", "createdAt": "2020-10-08T22:57:20Z", "url": "https://github.com/tensorflow/java/pull/129", "merged": true, "mergeCommit": {"oid": "0a1a868dda9f89730cef6c77b2577bd68a67afcc"}, "closed": true, "closedAt": "2020-11-17T12:07:44Z", "author": {"login": "JimClarke5"}, "timelineItems": {"totalCount": 41, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdQk5sCgH2gAyNTAwMjM5MzcxOmM1N2EyZTc0MWIyMzU2OWQ0YjFhZDMzZTE4NDA0ZGJlMGRjODE0ZGM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddSy_3AFqTUzMjAzMzcxNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "committedDate": "2020-10-08T17:19:37Z", "message": "Merge pull request #3 from tensorflow/master\n\nSync with master tensorflow on upstream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cc26757f102688789b58d32f18d6fd7e4941fc2", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/9cc26757f102688789b58d32f18d6fd7e4941fc2", "committedDate": "2020-10-08T18:07:11Z", "message": "Initial checkin to rebase to Initialziers to pick up changes to ndarry Shape"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2508f5e58b59e18d3537d845491ce1e3f7afbd85", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/2508f5e58b59e18d3537d845491ce1e3f7afbd85", "committedDate": "2020-10-08T18:07:11Z", "message": "Initial Checkin for losses"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "committedDate": "2020-10-08T22:25:50Z", "message": "Fix reshape in sparseCategoricalCrossentropy()"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDA4Mjgx", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-506008281", "createdAt": "2020-10-09T21:54:41Z", "commit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMTo1NDo0MVrOHfZh9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMjoxMTowN1rOHfZvNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NjE5Ng==", "bodyText": "perhaps \"of the predictions and result\"?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502686196", "createdAt": "2020-10-09T21:54:41Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NzIwMw==", "bodyText": "The Tuple class name is uncomfortably vanilla for me. Perhaps LossTuple?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502687203", "createdAt": "2020-10-09T21:59:37Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODAxOA==", "bodyText": "The Javadocs in this file are still partly in markdown.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688018", "createdAt": "2020-10-09T22:03:44Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng==", "bodyText": "For this method, the returned sampleWeight is always null.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688796", "createdAt": "2020-10-09T22:07:20Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA==", "bodyText": "Is \"match\" the right way to describe the precondition relationship between predictions and labels?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689064", "createdAt": "2020-10-09T22:08:36Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTU4OQ==", "bodyText": "For consistency, labelsRank.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689589", "createdAt": "2020-10-09T22:11:07Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDg3NjM1", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-506087635", "createdAt": "2020-10-10T10:40:51Z", "commit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQxMDo0MDo1MVrOHffBxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQxODo0MzowNVrOHfhrdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjI2MQ==", "bodyText": "Extraneous *", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502776261", "createdAt": "2020-10-10T10:40:51Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NjcwMQ==", "bodyText": "Extraneous *", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502786701", "createdAt": "2020-10-10T12:42:15Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg==", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787006", "createdAt": "2020-10-10T12:45:30Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAzNQ==", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787035", "createdAt": "2020-10-10T12:45:46Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzA4Ng==", "bodyText": "Actually, there's a separate <U> for the labels.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787086", "createdAt": "2020-10-10T12:46:27Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg==", "bodyText": "How would you feel about mnemonic/indicative type names like L for labels? Or even LabelsT?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808082", "createdAt": "2020-10-10T16:35:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODY5OA==", "bodyText": "Inconsistency between accessing the superclass's tf directly and accessing its reduction via getReduction.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808698", "createdAt": "2020-10-10T16:42:33Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}\n+   *\n+   *\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link #REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+          Ops tf,  boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+      this(tf, null, fromLogits, labelSmoothing, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+      Ops tf, String name, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+      Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.binaryCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODgwNw==", "bodyText": "tf versus getReduction (but I'll stop mentioning these)", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808807", "createdAt": "2020-10-10T16:43:56Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link * Reduction#AUTO},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   * @param axis The channels axis. <code>axis=-1</code> corresponds to data format `Channels Last'\n+   *     and <code>axis=1</code> corresponds to data format 'Channels First'.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf,\n+      String name,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      Reduction reduction,\n+      int axis) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+    this.axis = axis;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+          Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.categoricalCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing, axis);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg==", "bodyText": "How would you feel about a line break after the whole tf.math.abs(...), to make it easier to scan the parameters of tf.math.mean?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502809972", "createdAt": "2020-10-10T16:56:11Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMTM4Ng==", "bodyText": "In current invocations of these constructors, the target argument always comes from a variable called predictions.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502811386", "createdAt": "2020-10-10T17:12:45Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {\n+  private final Operand<T> labels;\n+  private final Operand<T> target;\n+  private final Operand<T> sampleWeights;\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param target the losses or target\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> target) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA==", "bodyText": "This is the first of these methods where we used (target, output) instead of (labels, predictions).", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812240", "createdAt": "2020-10-10T17:22:42Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw==", "bodyText": "sub will do broadcasting if needed. Do we feel good about applying squeezeOrExpandDimensions and then subsequent broadcasting? If so, is there a succinct description we could provide for overall treatment of dimensions?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812533", "createdAt": "2020-10-10T17:25:45Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw==", "bodyText": "Seems like we should document the required relationships between labels and predictions and the resulting transformations? (Given our use of squeezeOrExpandDimensions followed by broadcasting.)", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813233", "createdAt": "2020-10-10T17:34:02Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg==", "bodyText": "This is the first case in Losses where we haven't followed squeezeOrExpandDimensions with broadcasting. Do we want to add broadcasting here for consistency, or document the difference?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813406", "createdAt": "2020-10-10T17:35:54Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw==", "bodyText": "Although the documentation of softmaxCrossEntropyWithLogits doesn't specify, I imagine it doesn't do broadcasting. So this would be another method in Losses that does squeezeOrExpandDimensions but does not broadcast.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815067", "createdAt": "2020-10-10T17:53:59Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ==", "bodyText": "Although in this internal case of this method, we do broadcast. I'll stop commenting on this issue.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815385", "createdAt": "2020-10-10T17:57:21Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng==", "bodyText": "Do we want to avoid this cast in the case where labels already has the same data type?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815866", "createdAt": "2020-10-10T18:03:36Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjE5Nw==", "bodyText": "Can just use dataType.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502816197", "createdAt": "2020-10-10T18:06:57Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg==", "bodyText": "I tripped over this private method having the usual naming of a loss method, since I didn't notice that it was private and so expected it to follow the conventions of public loss methods, such as invoking squeezeOrExpandDimensions.  Also (if I'm navigating accurately through unfamiliar territory), this method doesn't compute a binaryCrossentropy since it depends on its caller to compute the mean at the end.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502818616", "createdAt": "2020-10-10T18:31:54Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA==", "bodyText": "Should we follow the Python in documenting that labels are expected to be 0 or 1?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502819700", "createdAt": "2020-10-10T18:43:05Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee1c48a443810260be7319caab94bde8a3dae529", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ee1c48a443810260be7319caab94bde8a3dae529", "committedDate": "2020-10-11T16:05:56Z", "message": "Apply various fixes to JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "287c96e34eea177303716e6a2b72509c2c749333", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/287c96e34eea177303716e6a2b72509c2c749333", "committedDate": "2020-10-11T16:46:18Z", "message": "Change Tuple to LossTuple"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTgxMzIw", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-506181320", "createdAt": "2020-10-11T13:50:39Z", "commit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQxMzo1MDozOVrOHfnslA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQxNzozNzoyNVrOHfpSFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODI5Mg==", "bodyText": "Could just use dataType. I'll stop mentioning this.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918292", "createdAt": "2020-10-11T13:50:39Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODQ1Mg==", "bodyText": "I think \"percentage\" is extraneous here.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918452", "createdAt": "2020-10-11T13:52:06Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyMDQ2Nw==", "bodyText": "That said, it just occurred to me that we have another gap, and that filling that gap might help this issue.\nWe don't specify the behavior of these methods when labels and predictions don't have a permitted shape relationship. Nor do we make sure our behavior is consistent in that case.\nPerhaps we should\n\nspell out that there's an IllegalArgumentException for that in the statically-known-dimensions case,\nrename squeezeOrExpandDimensions into something like validateAndAdjustLossDimensions,\nhave that method throw IllegalArgumentException when appropriate,\nand then link to a fuller explanation in the documentation of the IllegalArgumentException?\n\nAlthough I have never been in the habit of subclassing IllegalArgumentException, I see Oracle does that sometimes. That could be an alternative way of pointing people to the fuller explanation.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502920467", "createdAt": "2020-10-11T14:09:55Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNDUyNQ==", "bodyText": "If the rank is unknown, then the size of the last dimension is guaranteed to be unknown, so isCompatible is guaranteed true. (But there may be some idiomatic reason for writing it this way, of which I am blissfully unaware.)", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502924525", "createdAt": "2020-10-11T14:42:56Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjYwMQ==", "bodyText": "I'm pretty sure this logic is wrong. Perhaps either\n\ndocument preconditions of removeSqueezableDimensions and check exactly those,\nor (my leaning) just invoke removeSqueezableDimensions and make it however smart it needs to be.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502926601", "createdAt": "2020-10-11T14:59:53Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzEwOQ==", "bodyText": "Specify dimension -1L. Also, I'd advocate doing our own check that the last dimension of sampleWeight has size 1. The Python documentation for tf.squeeze says that, if axes are specified, then \"it is an error to squeeze a dimension that is not 1.\"", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927109", "createdAt": "2020-10-11T15:03:28Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzgzMA==", "bodyText": "Do we also have to go dynamic in the case where the ranks are both known but the size of the last weight dimension is unknown?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927830", "createdAt": "2020-10-11T15:09:18Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODE4NA==", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of sampleWeight may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928184", "createdAt": "2020-10-11T15:11:55Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODUzNQ==", "bodyText": "Need to specify squeezed axis -1L. But in that case:\nWhat, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928535", "createdAt": "2020-10-11T15:15:02Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODYwOQ==", "bodyText": "Same comments as for predictions above.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928609", "createdAt": "2020-10-11T15:15:26Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODg5OQ==", "bodyText": "Do we need to also go fully dynamic in the case where the size of a last dimension is unknown?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928899", "createdAt": "2020-10-11T15:17:17Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTE5Ng==", "bodyText": "We do have to verify tf.math.equal(tf.constant(expectedRankDiff+1), rankDiff), right?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929196", "createdAt": "2020-10-11T15:19:54Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTM4MQ==", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929381", "createdAt": "2020-10-11T15:21:01Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTQxOA==", "bodyText": "Same comments as above for predictions.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929418", "createdAt": "2020-10-11T15:21:24Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzODIwNA==", "bodyText": "To use squeezeOrExpandDimensions here, we might want to generalize that method so it doesn't think in terms of predictions when here we instead pass loss.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502938204", "createdAt": "2020-10-11T16:40:34Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MDM0Ng==", "bodyText": "This is another case where the combination of squeezeOrExpandDimensions and broadcasting yields a complex relationship between shapes of sampleWeight, loss, and the return value. In particular, due to broadcasting in mul, in the case of reduction == NONE and a surprising shape of sampleWeight, the return value may have a very different shape than loss.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502940346", "createdAt": "2020-10-11T17:01:15Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU2OQ==", "bodyText": "rank could be -1 at this point.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941569", "createdAt": "2020-10-11T17:12:44Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] axes = new int[rank];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 298}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTcyMQ==", "bodyText": "allAxes?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941721", "createdAt": "2020-10-11T17:14:06Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ==", "bodyText": "What should happen if weightsRank is UNKNOWN?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502943791", "createdAt": "2020-10-11T17:32:41Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg==", "bodyText": "Here we are working with the original predictionsRank, when we wanted to be working with the new rank.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944132", "createdAt": "2020-10-11T17:35:45Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng==", "bodyText": "This method has a myriad of complex cases, so I think it deserves its own direct unit test.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944276", "createdAt": "2020-10-11T17:37:25Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "642069c34d9e6b6c3df92cab4672c315029555de", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/642069c34d9e6b6c3df92cab4672c315029555de", "committedDate": "2020-10-11T19:29:37Z", "message": "Repair JavaDOx"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "249b65194bb055decf02d61f56378e7771e6d05f", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/249b65194bb055decf02d61f56378e7771e6d05f", "committedDate": "2020-10-11T19:30:19Z", "message": "Fixed AllAxis to hanlde dynamic shape when static shape rank is unknown."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "794cfdca096223e521c8c45138fc872cc2a3ec75", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/794cfdca096223e521c8c45138fc872cc2a3ec75", "committedDate": "2020-10-11T19:36:18Z", "message": "change method name allAxis to allAxes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "committedDate": "2020-10-13T17:12:40Z", "message": "change private method binaryCrossentropy to binaryCrossentropyHelper"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/928ef066f8d250b4ae41799eea40ab03fe3ecd23", "committedDate": "2020-10-13T19:25:17Z", "message": "Fixed squeezeOrExpandDimensions to make sure the updated labels, predictions and weights are returned in LossTuple"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MjkwODQ0", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-516290844", "createdAt": "2020-10-24T16:45:18Z", "commit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQxNjo0NToxOFrOHny7dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNFQxODozMjo1NVrOHnzh-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDkzNA==", "bodyText": "This javadoc can get out of date as it links to Reduction#AUTO rather than REDUCTION_DEFAULT. If REDUCTION_DEFAULT changed we'd need to update all the docs.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511490934", "createdAt": "2020-10-24T16:45:18Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTMxNg==", "bodyText": "Does labelSmoothing = 1.0 mean the true label distribution is set to 1/n? I'm not sure what \"squeezing the values towards 0.5\" means, because it would only be 0.5 in a binary problem.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491316", "createdAt": "2020-10-24T16:49:17Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTU4MQ==", "bodyText": "What happens if multiple classes are set to 1.0? Does it throw some exception, or compute a different function? If it's the former we should document that.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491581", "createdAt": "2020-10-24T16:51:56Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,93 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.\n+ *\n+ * <p><code>loss = maximum(neg - pos + 1, 0)</code> where <code>neg=maximum((1-labels)*predictions)\n+ * </code> and <code>pos=sum(labels*predictions)</code>\n+ *\n+ * <p><code>labels</code> values are expected to be 0 or 1.</p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjE0MA==", "bodyText": "Doc says KL when it should say logcosh.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492140", "createdAt": "2020-10-24T16:58:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes Computes the logarithm of the hyperbolic cosine of the prediction error.\n+ *\n+ * <p><code>logcosh = log((exp(x) + exp(-x))/2)</code>, where <code>x</code> is the error <code>\n+ * predictions - y_true</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{1.f, 1.f}, {0.f, 0.f}});\n+ *    LogCosh logcosh = new LogCosh(tf);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.108\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.8f, 0.2f});\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions, sampleWeight);\n+ *    // produces 0.087f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.217f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces [0.217f, 0f]\n+ * </pre>\n+ */\n+public class LogCosh extends Loss {\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name and a Loss\n+   * Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public LogCosh(Ops tf, Reduction reduction) {\n+    this(tf, null, reduction);\n+  }\n+\n+  /**\n+   * Creates a Kullback Leibler Divergence Loss", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjIxNg==", "bodyText": "I think it's worth documenting it in case users build their own losses.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492216", "createdAt": "2020-10-24T16:59:23Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjUxOQ==", "bodyText": "My vote would be to stick to the Java conventions Jim described.\nI particularly dislike the Google style form where the type name is a word that isn't all caps, but I tend to find type variables that are longer than a single character tricky to read anyway.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492519", "createdAt": "2020-10-24T17:02:34Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Mjc2Ng==", "bodyText": "In graph construction mode the overhead is probably irrelevant because it's only called once during construction. In eager mode it could be faster as it could sidestep a JNI call in each step, but I suspect we've got other issues to get speed in eager mode.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492766", "createdAt": "2020-10-24T17:05:09Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDIxOQ==", "bodyText": "This mentions smoothing the labels towards 0.5, but I think it's really towards 1/n.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494219", "createdAt": "2020-10-24T17:20:56Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw==", "bodyText": "Is this properly backtracking? It looks like it's going to pull out the softmax output not the input.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494723", "createdAt": "2020-10-24T17:26:37Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg==", "bodyText": "The CosineSimilarity doesn't mention that the value can be positive, and doesn't seem to restrict the output of this function so it is non-positive.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511496862", "createdAt": "2020-10-24T17:49:26Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 298}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw==", "bodyText": "But it didn't extract the logits, so won't this perform the wrong calculation?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499143", "createdAt": "2020-10-24T18:14:31Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 507}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTQxMw==", "bodyText": "The doc is wrong here, it scales towards 1/n.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499413", "createdAt": "2020-10-24T18:17:34Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = tf.dtypes.cast(labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank-1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = tf.dtypes.cast(tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = tf.dtypes.cast(tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 609}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTk1NA==", "bodyText": "This links to Reduction#AUTO instead of REDUCTION_DEFAULT.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499954", "createdAt": "2020-10-24T18:23:49Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. The labels are\n+ * expected to be provided as integers. If you want to provide labels using <code>one-hot</code>\n+ * representation, please use {@link CategoricalCrossentropy} loss. There should be <code># classes\n+ * </code> floating point values per feature for <code>predictions</code> and a single floating\n+ * point value per feature for <code>label</code>.\n+ *\n+ * <p>In the snippet below, there is a single floating point value per example for <code>labels\n+ * </code> and <code># classes</code> floating pointing values per example for <code>predictions\n+ * </code>. The shape of <code>labels</code> is <code>[batch_size]</code> and the shape of <code>\n+ * predictions</code> is <code>[batch_size, num_classes]</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[] {1, 2});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 1.177f\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class SparseCategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final int AXIS_DEFAULT = -1;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final int axis;\n+\n+  /**\n+   * Creates a SparseCategoricalCrossentropy loss using {@link Class#getSimpleName()} as the loss\n+   * name, a Loss Reduction of {@link Reduction#AUTO}, and fromLogits={@link #FROM_LOGITS_DEFAULT}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDUxMg==", "bodyText": "We should probably note in the javadoc for the class that this is an internal implementation class and subject to change (and being locked off under the module system).", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500512", "createdAt": "2020-10-24T18:29:49Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDc5Mw==", "bodyText": "The last f is in caps. Is that intentional? It's not consistent throughout the file if so.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500793", "createdAt": "2020-10-24T18:32:55Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.junit.jupiter.api.Test;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+\n+public class CategoricalCrossentropyTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  /** Test of call method, of class CategoricalCrossentropy. */\n+  @Test\n+  public void testAllCorrectUnweighted() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession testSession = TestSession.createTestSession(tfMode)) {\n+        Ops tf = testSession.getTF();\n+\n+        long[] trueArray = {\n+          1L, 0L, 0L,\n+          0L, 1L, 0L,\n+          0L, 0L, 1L\n+        };\n+        float[] predArray = {\n+          1.f, 0.f, 0.f,\n+          0.f, 1.f, 0.f,\n+          0.f, 0.f, 1.F", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bc54dd821b01c368914efdae87e503c3a61d989", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/2bc54dd821b01c368914efdae87e503c3a61d989", "committedDate": "2020-10-27T16:24:22Z", "message": "Fix JavaDoc,\nAdd in rangeCheck and valueCheck\nMisc fixes based on review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/951443b6cba9e42911ca2cfae05bee920d5ff229", "committedDate": "2020-10-27T16:31:14Z", "message": "Fix unused imports and add @SuppressWarnings(\"unchecked\") for casts."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE5MjczMjY4", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-519273268", "createdAt": "2020-10-29T01:44:02Z", "commit": {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQwMTo0NDowMlrOHqEBIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQwMTo1MToyMlrOHqEMYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw==", "bodyText": "What is the reason for having a seperate Losses class that implements all individual loss algorithms instead of implementing them directly in their respective class?\nI guess is to offer the choice to the user to choose an object-oriented API or a functional one? Then if we offer this, should we do the same for all other concepts in the framework?\nI wondering if we shouldn't take that decision for our users and only present a single API to accomplish a given task, for simplicity and consistency. I personally have a preference for the OO approach.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513868067", "createdAt": "2020-10-29T01:44:02Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,711 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng==", "bodyText": "For me, a *Impl should be the implementation of an interface, this one looks more like a LossesHelper with all its static methods (and the class should probably be final).\nI did not went through the whole thing but it looks like these helpers could also be moved directly to Loss as protected methods?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513870946", "createdAt": "2020-10-29T01:51:22Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,402 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.AssertThat;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.SetDiff1d;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * These are helper methods for Losses and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the Loss package.\n+ */\n+public class LossesImpl {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "committedDate": "2020-10-29T17:54:49Z", "message": "Add copyright"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "committedDate": "2020-10-29T18:04:39Z", "message": "Add CastHelper and used that for all casts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02573b594ca552371b8f42fa9e53c019143e6931", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/02573b594ca552371b8f42fa9e53c019143e6931", "committedDate": "2020-11-09T18:18:15Z", "message": "Fix JavaDoc, change snake case to camel case."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3ODUxNDIz", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-527851423", "createdAt": "2020-11-11T04:20:21Z", "commit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwNDoyMDoyMVrOHw9mKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwNDozMjowMlrOHw9xxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwMjg4OA==", "bodyText": "Personally, I'd lean toward using some of our own single-letter conventions for situations that are common in our own code, including L as the labels type.\n\nThis may be hard to follow consistently once several letters have been used e.g. 'L' might be needed for something other than label type. Seems a tad more confusing than the standard type names", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521102888", "createdAt": "2020-11-11T04:20:21Z", "author": {"login": "KartikChugh"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTcxMQ==", "bodyText": "Is there a potential use case justifying exposing these to the public? Seeing as they are utilities needed to implement Losses/Metrics.\nAgree with a rename to LossesHelper or LossesUtility to differentiate from interface implementation, however.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105711", "createdAt": "2020-11-11T04:31:31Z", "author": {"login": "KartikChugh"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,402 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.AssertThat;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.SetDiff1d;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * These are helper methods for Losses and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the Loss package.\n+ */\n+public class LossesImpl {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, "originalCommit": {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA==", "bodyText": "Class should have Javadoc description, no?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105860", "createdAt": "2020-11-11T04:32:02Z", "author": {"login": "KartikChugh"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,303 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 14}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/0bf49fe3203eb5f810ea09e0322fd36b6945856c", "committedDate": "2020-11-11T17:01:22Z", "message": "Change class LossesImpl to LossesHelper"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4NzIyNzEz", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-528722713", "createdAt": "2020-11-12T03:20:01Z", "commit": {"oid": "0bf49fe3203eb5f810ea09e0322fd36b6945856c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwMzoyMDowMVrOHxnzdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwMzoyMDowMVrOHxnzdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc5NDQyMw==", "bodyText": "Can we remove this commented-out documentation?", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521794423", "createdAt": "2020-11-12T03:20:01Z", "author": {"login": "karllessard"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,728 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods/**\n+  //   * Calculates the loss\n+  //   *\n+  //   * @param labels the truth values or labels\n+  //   * @param predictions the predictions\n+  //   * @param sampleWeights Optional SampleWeights acts as a coefficient for the loss. If a scalar\n+  // is\n+  //   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a\n+  // tensor\n+  //   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by\n+  // the\n+  //   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n+  //   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element\n+  // of\n+  //   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all\n+  // loss\n+  //   *     functions reduce by 1 dimension, usually axis=-1.)\n+  //   * @param <T> The data type of the predictions, sampleWeights and loss.\n+  //   * @param <U> The data type of the labels.\n+  //   * @return the loss\n+  //   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0bf49fe3203eb5f810ea09e0322fd36b6945856c"}, "originalPosition": 624}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "committedDate": "2020-11-12T15:09:38Z", "message": "Remove commented out JavaDoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5MjE0MTk0", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-529214194", "createdAt": "2020-11-12T15:39:11Z", "commit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "state": "DISMISSED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNTozOToxMVrOHyAqCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNTo0NjozMFrOHyBBCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwMTYxMA==", "bodyText": "Cool -- Resolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522201610", "createdAt": "2020-11-12T15:39:11Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNDIyMg==", "bodyText": "Ok, sticking with the original plan! Resolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522204222", "createdAt": "2020-11-12T15:42:26Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNTk5OQ==", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522205999", "createdAt": "2020-11-12T15:44:43Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjM0OA==", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206348", "createdAt": "2020-11-12T15:45:05Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjc1Mw==", "bodyText": "minor -- we'll call it Resolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206753", "createdAt": "2020-11-12T15:45:35Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjk3Ng==", "bodyText": "minor -- we'll call it Resolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206976", "createdAt": "2020-11-12T15:45:52Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzE0Ng==", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207146", "createdAt": "2020-11-12T15:46:04Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzMyNw==", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207327", "createdAt": "2020-11-12T15:46:18Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzQ5Ng==", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207496", "createdAt": "2020-11-12T15:46:30Z", "author": {"login": "deansher"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ=="}, "originalCommit": {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882"}, "originalPosition": 255}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5MzEwNTgz", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-529310583", "createdAt": "2020-11-12T17:17:04Z", "commit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjY3OTIx", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-529667921", "createdAt": "2020-11-13T02:55:11Z", "commit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMjo1NToxMlrOHyXs9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMjo1NToxMlrOHyXs9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA==", "bodyText": "We should open an issue to track inserting these cast checks into the optimizers for uniformity.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522579190", "createdAt": "2020-11-13T02:55:12Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/utils/CastHelper.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** A helper class for casting an Operand */\n+public class CastHelper {\n+\n+  /**\n+   * Casts an operand to the desired type.\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param value the value to be cast\n+   * @param requiredType the required data type\n+   * @param <T> the required data type\n+   * @param <U> the original data type of the value\n+   * @return the value cast to the required data type.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T extends TType, U extends TType> Operand<T> cast(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NjY5OTE2", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-529669916", "createdAt": "2020-11-13T03:01:28Z", "commit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzowMToyOFrOHyXzxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMzoxMToyNlrOHyYKFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MDkzNA==", "bodyText": "label_smoothing -> labelSmoothing, here and elsewhere in this file.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522580934", "createdAt": "2020-11-13T03:01:28Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link Loss#REDUCTION_DEFAULT}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link\n+   * Loss#REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQwMA==", "bodyText": "This one's still got the doc from BinaryCrossEntropy wrt label_smoothing. And it's snake_case.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581400", "createdAt": "2020-11-13T03:03:20Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQyOA==", "bodyText": "Incorrect doc.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581428", "createdAt": "2020-11-13T03:03:30Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQ3MA==", "bodyText": "Incorrect doc.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581470", "createdAt": "2020-11-13T03:03:39Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NTEyOQ==", "bodyText": "This javadoc is better, but I think it should mention that this function is inverted from the regular cosine similarity, as that's 1 when the values are most similar and -1 when they point in opposite directions. It makes sense that it is inverted because then you can minimise it sensibly, but it is confusing if you're just browsing through.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522585129", "createdAt": "2020-11-13T03:08:00Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, "originalCommit": {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23"}, "originalPosition": 298}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjU1OQ==", "bodyText": "I think this would be better called smoothBinaryLabels as it's not specific to the binary cross entropy as far as I can tell. But it's a private method so it's not too much of an issue.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586559", "createdAt": "2020-11-13T03:11:08Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 617}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjY0Nw==", "bodyText": "Similar comment to above, but smoothCategoricalLabels. Also I think the doc should explicitly state that it's smoothing the labels towards 1/n where n is the number of classes.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586647", "createdAt": "2020-11-13T03:11:26Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = cast(tf,  tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param <T> the data type of the labels\n+   * @return the smoothed categorical labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsCatX(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6"}, "originalPosition": 636}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b211937c946a67c6f3830e70bdccf97a54cd8051", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b211937c946a67c6f3830e70bdccf97a54cd8051", "committedDate": "2020-11-13T14:56:15Z", "message": "Changed method name from smoothLabelsBinaryX to smoothBinaryLabels,\nsmoothLabelsCatX to smoothCategoricalLabels.\n\nAdded clarification oin JavaDoc for cosineSimilarity to describe the difference between the mathematical definition for cosine similarity and the loss definition."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "committedDate": "2020-11-13T14:56:54Z", "message": "Fixed JavaDoc for labelSmoothing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "914f16f4473512c8b5ef9df8ca43074b82d3edd0", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/914f16f4473512c8b5ef9df8ca43074b82d3edd0", "committedDate": "2020-11-13T14:57:43Z", "message": "Fixed JavaDoc to change label_smoothing to labelSmoothing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/7eefbb7f197c731a7d304d055fd242d1acd9835f", "committedDate": "2020-11-13T14:58:19Z", "message": "Fix formatting"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMTg5ODA2", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-530189806", "createdAt": "2020-11-13T16:05:51Z", "commit": {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNjowNTo1MlrOHy0SVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNjowNjoyNFrOHy0Tng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzUxMA==", "bodyText": "alue -> value", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047510", "createdAt": "2020-11-13T16:05:52Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -184,10 +184,9 @@ public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float la\n    *\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzgzOA==", "bodyText": "label_smoothing -> labelSmoothing, and in the docs below.", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047838", "createdAt": "2020-11-13T16:06:24Z", "author": {"login": "Craigacp"}, "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -169,10 +170,9 @@ public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing)\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f"}, "originalPosition": 28}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b87ad16118442643b845bb4e24a0145eea0056fb", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/b87ad16118442643b845bb4e24a0145eea0056fb", "committedDate": "2020-11-13T18:11:21Z", "message": "replace label_smoothing with labelSmoothing.\nfix typo error in JavaDoc comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "committedDate": "2020-11-16T23:17:39Z", "message": "Add copyright to test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d9fd24b809fd4141e61ca504b32b251a993cf8c", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/4d9fd24b809fd4141e61ca504b32b251a993cf8c", "committedDate": "2020-11-16T23:36:54Z", "message": "Fix copyright to attribute TensorFlow Authors."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "committedDate": "2020-11-16T23:45:07Z", "message": "Fix typo on broadcast in JavaDoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "744e32463c4aa8def4456fac4bcec53536a04fa4", "author": {"user": {"login": "JimClarke5", "name": "Jim Clarke"}}, "url": "https://github.com/tensorflow/java/commit/744e32463c4aa8def4456fac4bcec53536a04fa4", "committedDate": "2020-11-16T23:46:12Z", "message": "Fix typo on broadcast in JavaDoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMDAyMjgw", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-532002280", "createdAt": "2020-11-17T03:48:52Z", "commit": {"oid": "744e32463c4aa8def4456fac4bcec53536a04fa4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMDMzNzE1", "url": "https://github.com/tensorflow/java/pull/129#pullrequestreview-532033715", "createdAt": "2020-11-17T05:35:02Z", "commit": {"oid": "744e32463c4aa8def4456fac4bcec53536a04fa4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3461, "cost": 1, "resetAt": "2021-11-02T12:20:56Z"}}}