{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNjczMjg3", "number": 28, "title": "Optimizer package for Graph mode", "bodyText": "This adds a new subproject tensorflow-training which currently contains org.tensorflow.training.optimizers a package of gradient optimizers which apply the underlying gradient update ops to a TF Graph.\nIn addition to the optimizers it makes a few small changes in the tensorflow-core-api package: it adds a variables initialiser list to Graph, a method which constructs an initialiser node which initialises all the variables in the graph, plus a variableWithInit method which accepts an Operand which is used to provide the shape, type and initial value for the variable.\nThere's also an MNIST CNN test in there, which we can move out to wherever it needs to go, and be combined with the other ones.\nBefore this gets merged it needs better Javadoc in the Optimizer class, and the global variables used by some of the optimizers need wiring into the globals field in the base class. I'll fix those things next week.", "createdAt": "2020-02-08T03:11:17Z", "url": "https://github.com/tensorflow/java/pull/28", "merged": true, "mergeCommit": {"oid": "9107991a9bb1f915bb27c110395e4bb5159244b8"}, "closed": true, "closedAt": "2020-03-02T03:31:04Z", "author": {"login": "Craigacp"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcC0Q_QgFqTM1NTY0NjM4NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcJlKiiAFqTM2NjkzMzczNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1NjQ2Mzg0", "url": "https://github.com/tensorflow/java/pull/28#pullrequestreview-355646384", "createdAt": "2020-02-10T00:02:44Z", "commit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwMDowMjo0NFrOFnX1ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwMzowMzo1MFrOFnZNKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgyOTI4NA==", "bodyText": "yeah, those changes should go after you rebased one more time.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376829284", "createdAt": "2020-02-10T00:02:44Z", "author": {"login": "karllessard"}, "path": ".github/workflows/ci.yml", "diffHunk": "@@ -1,5 +1,11 @@\n name: CI jobs\n-on: [push, pull_request]\n+on:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgyOTg3OA==", "bodyText": "Ok now I understand what you meant last time by \"not placing the custom operators as the same spot as the generated ones...\"\nIn fact, you shouldn't, this file should be moved src/main/java/, not under src/gen/java/, without the \"DO NOT EDIT!\" notice. All sources are scanned for the @Operator annotation.\nCheck for example the other similar operators in here.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376829878", "createdAt": "2020-02-10T00:09:20Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzMDA3NQ==", "bodyText": "Google Java Style suggests to add a space after each commas.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376830075", "createdAt": "2020-02-10T00:11:23Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {\n+    Output<T> initOutput = init.asOutput();\n+    Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0NzcwNA==", "bodyText": "This pending PR will allow you to rename that method to whatever sounds better than create, which is a bit awkward in this case since it does not return an instance of VariableWithInit but of Variable.\nIf you agree, I suggest that we merge that PR first and that you rebase your code on it, taking advantage of the new @Endpoint annotation.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376847704", "createdAt": "2020-02-10T02:36:11Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0Nzg0MQ==", "bodyText": "Do we know how Assign reacts in eager mode? Does it really initialize the variable?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376847841", "createdAt": "2020-02-10T02:37:05Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {\n+    Output<T> initOutput = init.asOutput();\n+    Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);\n+    Assign<T> assignOp = Assign.create(scope,newVar,init);\n+    ExecutionEnvironment exEnv = scope.env();\n+    if (exEnv instanceof Graph) {\n+      Graph graph = (Graph) exEnv;\n+      graph.addInitializer(assignOp);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODk4OQ==", "bodyText": "I was thinking that we could move all examples to our other repository, tensorflow/java-models.\nI also have written MNIST example in its simplest nature, should we merge them together, just keep one of them or keep both (e.g. SimpleMnist vs DeepMnist?) I'm pretty sure @dhruvrajan also have one.\nAnyway, by moving it to the other repository, we won't have to take that decision before merging this PR.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376848989", "createdAt": "2020-02-10T02:45:17Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTExMQ==", "bodyText": "Again, this other pending PR would simply the signature of such operation with tf.nn.maxPool(relu1, tf.vector(1, 2, 2, 1), tf.vector(1, 2, 2, 1), PADDING_TYPE);, which I find more convenient. Should we merge it before as well? (if you agree with the proposed solution, of course)", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849111", "createdAt": "2020-02-10T02:46:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTMzNA==", "bodyText": "To show good practices, graph should be enclosed by a try-with-resource block.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849334", "createdAt": "2020-02-10T02:47:23Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTcxOA==", "bodyText": "what do you do with init here?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849718", "createdAt": "2020-02-10T02:50:00Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDA2OA==", "bodyText": "Tensors and sessions should also be enclosed in try-with-resources blocks", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850068", "createdAt": "2020-02-10T02:52:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();\n+\n+    return graph;\n+  }\n+\n+  public static void train(Session session, int epochs, int minibatchSize, float[][][][] data, int[] labels) {\n+    // Initialises the parameters.\n+    session.runner().addTarget(INIT).run();\n+    logger.info(\"Initialised the model parameters\");\n+\n+    float[][][][] featureBatch = new float[minibatchSize][][][];\n+    int[] labelBatch = new int[minibatchSize];\n+\n+    int interval = 0;\n+    for (int i = 0; i < epochs; i++) {\n+      logger.log(Level.INFO, \"Starting epoch \" + i);\n+      //Tensor<?> epoch = Tensor.create(i);\n+      for (int j = 0; j < data.length; j += minibatchSize) {\n+        for (int k = j, m = 0; k < (j + minibatchSize) && k < data.length; k++, m++) {\n+          featureBatch[m] = data[k];\n+          labelBatch[m] = labels[k];\n+        }\n+        //logger.info(\"Batch = \" + batch.size());\n+        Tensor<?> input = Tensor.create(featureBatch);\n+        Tensor<?> target = Tensor.create(labelBatch);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDMyMw==", "bodyText": "All those beautiful multidimensional arrays should be replaced TFTools NdArray, I can help you out with this, I really think writing our first MNIST example should be collaborative work to start with.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850323", "createdAt": "2020-02-10T02:54:03Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();\n+\n+    return graph;\n+  }\n+\n+  public static void train(Session session, int epochs, int minibatchSize, float[][][][] data, int[] labels) {\n+    // Initialises the parameters.\n+    session.runner().addTarget(INIT).run();\n+    logger.info(\"Initialised the model parameters\");\n+\n+    float[][][][] featureBatch = new float[minibatchSize][][][];\n+    int[] labelBatch = new int[minibatchSize];\n+\n+    int interval = 0;\n+    for (int i = 0; i < epochs; i++) {\n+      logger.log(Level.INFO, \"Starting epoch \" + i);\n+      //Tensor<?> epoch = Tensor.create(i);\n+      for (int j = 0; j < data.length; j += minibatchSize) {\n+        for (int k = j, m = 0; k < (j + minibatchSize) && k < data.length; k++, m++) {\n+          featureBatch[m] = data[k];\n+          labelBatch[m] = labels[k];\n+        }\n+        //logger.info(\"Batch = \" + batch.size());\n+        Tensor<?> input = Tensor.create(featureBatch);\n+        Tensor<?> target = Tensor.create(labelBatch);\n+        Tensor<?> loss = session.runner()\n+            .feed(INPUT_NAME, input)\n+            .feed(TARGET, target)\n+            .addTarget(TRAIN)\n+            .fetch(TRAINING_LOSS)\n+            .run().get(0);\n+        if (interval % 100 == 0) {\n+          logger.log(Level.INFO, \"Iteration = \" + interval + \", training loss = \" + loss.floatValue());\n+        }\n+        input.close();\n+        target.close();\n+        loss.close();\n+        interval++;\n+      }\n+      //epoch.close();\n+    }\n+  }\n+\n+  /**\n+   * Find the maximum probability and return it's index.\n+   *\n+   * @param probabilities The probabilites.\n+   * @return The index of the max.\n+   */\n+  public static int pred(float[] probabilities) {\n+    float maxVal = Float.NEGATIVE_INFINITY;\n+    int idx = 0;\n+    for (int i = 0; i < probabilities.length; i++) {\n+      if (probabilities[i] > maxVal) {\n+        maxVal = probabilities[i];\n+        idx = i;\n+      }\n+    }\n+    return idx;\n+  }\n+\n+  public static DataTuple loadData(String path) throws IOException, ClassNotFoundException {\n+    try (ObjectInputStream ois = new ObjectInputStream(new BufferedInputStream(new FileInputStream(path)))) {\n+      float[][][][] data = (float[][][][]) ois.readObject();\n+      int[] labels = (int[]) ois.readObject();\n+      return new DataTuple(data, labels);\n+    }\n+  }\n+\n+  private static class DataTuple {\n+    public final float[][][][] features;\n+    public final int[] labels;\n+\n+    public DataTuple(float[][][][] features, int[] labels) {\n+      this.features = features;\n+      this.labels = labels;\n+    }\n+  }\n+\n+  public static void main(String[] args) throws IOException, ClassNotFoundException {\n+    logger.info(\"Usage: MNISTTest <num-epochs> <minibatch-size> <optimizer-name> <train-data-path> <test-data-path>\");\n+\n+    logger.info(\"Loading training data\");\n+    DataTuple train = loadData(args[3]);\n+    logger.info(\"Loading testing data\");\n+    DataTuple test = loadData(args[4]);\n+\n+    logger.info(\"Loaded data.\");\n+\n+    float[][][][] trainData = train.features;\n+    int[] trainLabels = train.labels;\n+\n+    float[][][][] testData = test.features;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDU4MA==", "bodyText": "2020, also shouldn't it be copyright to The Tensorflow Authors like all other files in the project?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850580", "createdAt": "2020-02-10T02:56:08Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTE0OA==", "bodyText": "I don't think we need to enforce it but just FYI, pretty much all TF classes declare their members in this order, including both fields and methods: public, protected, default, private", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376851148", "createdAt": "2020-02-10T03:00:04Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ *\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg==", "bodyText": "would it be easy to allow the user override the name of the scope for the optimizer ops (instead of getOptimizerName())? Also, maybe a user would like to pass its own instance of Ops (which might already be a subscope of another block or contains control dependencies, etc.)?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376851752", "createdAt": "2020-02-10T03:03:50Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 67}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "195deff5a27bc8a56c91828020b085a3e35d5885", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/195deff5a27bc8a56c91828020b085a3e35d5885", "committedDate": "2020-02-12T16:09:06Z", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest."}, "afterCommit": {"oid": "7e3459b02ca2ca005de9f7725ea68b20ef19c4b2", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/7e3459b02ca2ca005de9f7725ea68b20ef19c4b2", "committedDate": "2020-02-13T02:08:59Z", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4MjM3OTgw", "url": "https://github.com/tensorflow/java/pull/28#pullrequestreview-358237980", "createdAt": "2020-02-13T14:02:04Z", "commit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxNDowMjowNVrOFpU3DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxNDozMDoxNVrOFp4XRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg3NzcwOQ==", "bodyText": "Now that we can rename the endpoint so that they don't necessarily match their operator class, would it make sense to simply name this one variable?\nIt won't conflict with the other generated variable endpoint, that takes a Shape and DataType in input.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378877709", "createdAt": "2020-02-13T14:02:05Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/gen/annotations/org/tensorflow/op/Ops.java", "diffHunk": "@@ -7117,6 +7118,19 @@ public VarIsInitializedOp varIsInitializedOp(Operand<?> resource) {\n     return VariableShape.create(scope, input, outType);\n   }\n \n+  /**\n+   * Factory method to create a new Variable with it's initializer.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public <T extends TType> Variable<T> variableWithInit(Operand<T> init,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4MzQ3Mg==", "bodyText": "This class name is a bit misleading, as all other *Ops classes are generated and exposes tf.* endpoints. Here, we are dealing with the endpoint implementations.\nI've never been a huge fan of the *Ops name for the generated classes neither (even if that was my idea if I recall correctly...) and we could rename them instead. Like TensorFlowApi, TensorFlowLinearApi, TensorFlowSparseApi, etc. would be better picks.\nBut if we don't want to do this breaking change then I think we need to come up with something else for this new class.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378883472", "createdAt": "2020-02-13T14:11:55Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/CoreOps.java", "diffHunk": "@@ -0,0 +1,45 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Container class for core methods which add or perform several operations\n+ * and return one of them.\n+ */\n+@Operator\n+public abstract class CoreOps {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4NDAxOQ==", "bodyText": "Just checking again with you if you tested it out in eager mode. If it fails, then we might prefer to throw explicitly an exception if the environment execution is not a Graph.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378884019", "createdAt": "2020-02-13T14:12:54Z", "author": {"login": "karllessard"}, "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/CoreOps.java", "diffHunk": "@@ -0,0 +1,45 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Container class for core methods which add or perform several operations\n+ * and return one of them.\n+ */\n+@Operator\n+public abstract class CoreOps {\n+\n+    /**\n+     * This class contains static factories.\n+     */\n+    private CoreOps() {}\n+\n+    /**\n+     * Factory method to create a new Variable with it's initializer.\n+     *\n+     * @param scope current scope\n+     * @param init The op to use to initialise this variable.\n+     * @param options carries optional attributes values\n+     * @return a new instance of Variable\n+     */\n+    @Endpoint(name=\"variableWithInit\")\n+    public static <T extends TType> Variable<T> createVariableWithInit(Scope scope, Operand<T> init, Variable.Options... options) {\n+        Output<T> initOutput = init.asOutput();\n+        Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);\n+        Assign<T> assignOp = Assign.create(scope,newVar,init);\n+        ExecutionEnvironment exEnv = scope.env();\n+        if (exEnv instanceof Graph) {\n+            Graph graph = (Graph) exEnv;\n+            graph.addInitializer(assignOp);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4NTAxMg==", "bodyText": "Ok, let's remove this one for now from this PR as they will go to the other repo. We'll sync up together to see how we want to merge both examples.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378885012", "createdAt": "2020-02-13T14:14:28Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODk4OQ=="}, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3MjUyNg==", "bodyText": "Well in this format, things are grouped per scope, meaning:\n\npublic fields\npublic methods\nprotected fields\nprotected methods\n... and so on\n\nAgain, I don't necessarily want to continue enforcing this in our new repo but maybe we can apply it at least in the core, so that all classes of a single artifact follows the same pattern?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379172526", "createdAt": "2020-02-13T23:16:36Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ *\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTE0OA=="}, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3Mzc3NQ==", "bodyText": "Would it be simpler to use Zeros for initializing this tensor? (I'm really asking, I'm not too sure if this Zeros operator is useful at all)", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379173775", "createdAt": "2020-02-13T23:20:24Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;\n+\n+  public AdaDelta(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.95f, 1e-8f);\n+  }\n+\n+  public AdaDelta(Graph graph, float learningRate, float rho, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.rho = rho;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdaDeltaSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createAdaDeltaSlot(Output<T> v) {\n+    Operand<T> accumulatorInitializer = tf", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3NjczNg==", "bodyText": "this tf.constant(Object, DataType) won't convert automatically your float values to the gradient type, so it will probably fail if you test with variable other that TFloat32. Plus, I've removed it in this PR, e.g. only tf.val(rho) is available now.\nIf you really need to convert your constants to the gradient datatype, then you'll need to do an explicit tf.cast(tf.val(rho)).\nThe same comment applies for other optimizers.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379176736", "createdAt": "2020-02-13T23:29:52Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;\n+\n+  public AdaDelta(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.95f, 1e-8f);\n+  }\n+\n+  public AdaDelta(Graph graph, float learningRate, float rho, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.rho = rho;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdaDeltaSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createAdaDeltaSlot(Output<T> v) {\n+    Operand<T> accumulatorInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), ACCUMULATOR, accumulatorInitializer);\n+    Operand<T> updateInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), ACCUMULATOR_UPDATE, updateInitializer);\n+  }\n+\n+  @Override\n+  protected <T extends TType> Operand<T> applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> accumSlot = getSlot(variable, ACCUMULATOR).get();\n+    Variable<T> accumUpdateSlot = getSlot(variable, ACCUMULATOR_UPDATE).get();\n+    return tf.train.applyAdadelta(variable, accumSlot, accumUpdateSlot,\n+        tf.constant(learningRate, gradient.dataType()),\n+        tf.constant(rho, gradient.dataType()),\n+        tf.constant(epsilon, gradient.dataType()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3ODcwNQ==", "bodyText": "Nit: spaces after commas", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379178705", "createdAt": "2020-02-13T23:35:12Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Adam.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * Optimizer that implements the Adam algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1412.6980\">paper</a>.\n+ */\n+@Operator\n+public class Adam extends Optimizer {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  private final float learningRate;\n+\n+  private final float betaOne;\n+\n+  private final float betaTwo;\n+\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+  private Variable<TFloat32> betaTwoPower;\n+\n+  public Adam(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.999f, 1e-8f);\n+  }\n+\n+  public Adam(Graph graph, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Endpoint(name=\"adam_minimize\")\n+  public static <T extends TType> Op createAdamMinimize(Scope scope, Operand<T> loss, float learningRate, float betaOne, float betaTwo, float epsilon, Optimizer.Options... options) {\n+    if (!(scope.env() instanceof Graph)) {\n+      throw new IllegalArgumentException(\"Optimizers are only supported on Graphs\");\n+    }\n+    Adam adam = new Adam((Graph)scope.env(),learningRate,betaOne,betaTwo,epsilon);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3OTA5MA==", "bodyText": "Again, Zeros maybe?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379179090", "createdAt": "2020-02-13T23:36:18Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Adam.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * Optimizer that implements the Adam algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1412.6980\">paper</a>.\n+ */\n+@Operator\n+public class Adam extends Optimizer {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  private final float learningRate;\n+\n+  private final float betaOne;\n+\n+  private final float betaTwo;\n+\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+  private Variable<TFloat32> betaTwoPower;\n+\n+  public Adam(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.999f, 1e-8f);\n+  }\n+\n+  public Adam(Graph graph, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Endpoint(name=\"adam_minimize\")\n+  public static <T extends TType> Op createAdamMinimize(Scope scope, Operand<T> loss, float learningRate, float betaOne, float betaTwo, float epsilon, Optimizer.Options... options) {\n+    if (!(scope.env() instanceof Graph)) {\n+      throw new IllegalArgumentException(\"Optimizers are only supported on Graphs\");\n+    }\n+    Adam adam = new Adam((Graph)scope.env(),learningRate,betaOne,betaTwo,epsilon);\n+    String name = null;\n+    for (Options o : options) {\n+      if (o.sharedName != null) {\n+        name = o.sharedName;\n+      }\n+    }\n+    if (name == null) {\n+      return adam.minimize(loss);\n+    } else {\n+      return adam.minimize(loss,name);\n+    }\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf\n+        .assign(betaOnePower, tf.constant(betaOne, TFloat32.DTYPE));\n+    graph.addInitializer(betaOnePowerInit);\n+    betaTwoPower = tf.withName(\"beta2_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaTwoPowerInit = tf\n+        .assign(betaTwoPower, tf.constant(betaTwo, TFloat32.DTYPE));\n+    graph.addInitializer(betaTwoPowerInit);\n+  }\n+\n+  @Override\n+  protected Optional<Operand<?>> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+    return Optional.empty();\n+  }\n+\n+  private <T extends TType> void createAdamSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3OTU3OQ==", "bodyText": "Missing doc", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379179579", "createdAt": "2020-02-13T23:37:54Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ0OTA2Ng==", "bodyText": "Yes I think it is ok that to have an entry point that accept a Graph but maybe add another that accept an Ops as well, so the user has full control on the name and control dependencies of the optmizers? Or maybe just a String for the name\nFor instance, does it make sense that a user would want to create two different optimizers of the same type (but with different parameters) for handling different variables? If so, then he'll need to give them different names or he will endup with a name conflict.\nI think you understand my questioning here, I'll let you decide what would be the best approach to handle those corner cases.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379449066", "createdAt": "2020-02-14T14:10:36Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg=="}, "originalCommit": {"oid": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1MDc5OA==", "bodyText": "graph.operations().forEachRemaining maybe?\nAlso, is it OK that an optimizer is always applied to all variables in the graph? Is this true for all kind of graphs?", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379450798", "createdAt": "2020-02-14T14:13:55Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Optional attributes for {@link org.tensorflow.training.optimizers.Optimizer}\n+   */\n+  public static class Options {\n+\n+    /**\n+     * @param sharedName If non-empty, this variable is named in the given bucket\n+     * with this shared_name. Otherwise, the node name is used instead.\n+     */\n+    public Optimizer.Options sharedName(String sharedName) {\n+      this.sharedName = sharedName;\n+      return this;\n+    }\n+\n+    protected String sharedName;\n+\n+    private Options() {\n+    }\n+  }\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());\n+    this.slots = new HashMap<>();\n+    this.globals = new ArrayList<>();\n+  }\n+\n+  public Op minimize(Operand<?> loss) {\n+    return minimize(loss, getOptimizerName() + \"-minimize\");\n+  }\n+\n+  public Op minimize(Operand<?> loss, String name) {\n+    List<GradAndVar<?>> gradsAndVars = computeGradients(loss);\n+\n+    return applyGradients(gradsAndVars, name);\n+  }\n+\n+  public <T extends TType> List<GradAndVar<?>> computeGradients(Operand<?> loss) {\n+    List<Operation> variables = new ArrayList<>();\n+    Iterator<Operation> opItr = graph.operations();\n+    while (opItr.hasNext()) {\n+      Operation op = opItr.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1Nzk0Mg==", "bodyText": "Nit: either it is GitHub or there is a wrong left margin to this line. Also, no need of else blocks below since you return in previous blocks.", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379457942", "createdAt": "2020-02-14T14:27:26Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Optional attributes for {@link org.tensorflow.training.optimizers.Optimizer}\n+   */\n+  public static class Options {\n+\n+    /**\n+     * @param sharedName If non-empty, this variable is named in the given bucket\n+     * with this shared_name. Otherwise, the node name is used instead.\n+     */\n+    public Optimizer.Options sharedName(String sharedName) {\n+      this.sharedName = sharedName;\n+      return this;\n+    }\n+\n+    protected String sharedName;\n+\n+    private Options() {\n+    }\n+  }\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());\n+    this.slots = new HashMap<>();\n+    this.globals = new ArrayList<>();\n+  }\n+\n+  public Op minimize(Operand<?> loss) {\n+    return minimize(loss, getOptimizerName() + \"-minimize\");\n+  }\n+\n+  public Op minimize(Operand<?> loss, String name) {\n+    List<GradAndVar<?>> gradsAndVars = computeGradients(loss);\n+\n+    return applyGradients(gradsAndVars, name);\n+  }\n+\n+  public <T extends TType> List<GradAndVar<?>> computeGradients(Operand<?> loss) {\n+    List<Operation> variables = new ArrayList<>();\n+    Iterator<Operation> opItr = graph.operations();\n+    while (opItr.hasNext()) {\n+      Operation op = opItr.next();\n+      if (op.type().equals(VARIABLE_V2)) {\n+        variables.add(op);\n+      }\n+    }\n+\n+    Output<?>[] variableOutputArray = new Output[variables.size()];\n+    for (int i = 0; i < variables.size(); i++) {\n+      // First output of a variable is it's output.\n+      variableOutputArray[i] = variables.get(i).output(0);\n+    }\n+\n+    Output<?>[] gradients = graph.addGradients(loss.asOutput(), variableOutputArray);\n+    List<GradAndVar<? extends TType>> gradVarPairs = new ArrayList<>();\n+\n+    for (int i = 0; i < variableOutputArray.length; i++) {\n+      @SuppressWarnings(\"unchecked\")\n+      Output<T> typedGrad = (Output<T>) gradients[i];\n+      @SuppressWarnings(\"unchecked\")\n+      Output<T> typedVar = (Output<T>) variableOutputArray[i];\n+      gradVarPairs.add(new GradAndVar<>(typedGrad, typedVar));\n+    }\n+\n+    return gradVarPairs;\n+  }\n+\n+  public Op applyGradients(List<GradAndVar<? extends TType>> gradsAndVars, String name) {\n+    List<Output<? extends TType>> variables = gradsAndVars.stream().map(GradAndVar::getVariable)\n+        .collect(Collectors.toList());\n+\n+    createSlots(variables);\n+\n+    Optional<Operand<? extends TType>> prepOp = prepare(name + \"/prepare\");\n+\n+    List<Operand<? extends TType>> updateOps = new ArrayList<>();\n+    prepOp.ifPresent(updateOps::add);\n+    for (GradAndVar<? extends TType> pair : gradsAndVars) {\n+      updateOps.add(applyDense(pair));\n+    }\n+\n+    return finish(updateOps, name);\n+  }\n+\n+  /**\n+   * Gets the slot associated with the specified variable and slot name.\n+   *\n+   * @param var      The variable to lookup.\n+   * @param slotName The slot name.\n+   * @return The slot or {@link Optional#empty}.\n+   */\n+  public <T extends TType> Optional<Variable<T>> getSlot(Output<T> var, String slotName) {\n+    return getSlot(var.op().name(), slotName);\n+  }\n+\n+  /**\n+   * Gets the slot associated with the specified variable and slot name.\n+   *\n+   * @param varName  The variable to lookup.\n+   * @param slotName The slot name.\n+   * @return The slot or {@link Optional#empty}.\n+   */\n+  private <T extends TType> Optional<Variable<T>> getSlot(String varName, String slotName) {\n+    Map<String, Variable<? extends TType>> variables = slots.get(slotName);\n+    if (variables != null) {\n+      Variable<? extends TType> slot = variables.get(varName);\n+      if (slot != null) {\n+        @SuppressWarnings(\"unchecked\") // This method should only be called when the type is known.\n+            Optional<Variable<T>> opt = Optional.of((Variable<T>) slot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1OTM5OA==", "bodyText": "Nit: no need of else block", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379459398", "createdAt": "2020-02-14T14:30:15Z", "author": {"login": "karllessard"}, "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/RMSProp.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the RMSProp algorithm.\n+ * <p>\n+ * See the <a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\">lecture\n+ * notes</a> that is inexplicably the canonical reference.\n+ */\n+public class RMSProp extends Optimizer {\n+\n+  public static final String RMS = \"rms\";\n+  public static final String MG = \"mg\"; // mean gradient?\n+  public static final String MOMENTUM = \"momentum\";\n+\n+  private final float learningRate;\n+  private final float decay;\n+  private final float momentum;\n+  private final float epsilon;\n+  private final boolean centered;\n+\n+  public RMSProp(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.0f, 1e-10f, false);\n+  }\n+\n+  public RMSProp(Graph graph, float learningRate, float decay, float momentum, float epsilon,\n+      boolean centered) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.decay = decay;\n+    this.momentum = momentum;\n+    this.epsilon = epsilon;\n+    this.centered = centered;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createRMSPropSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createRMSPropSlot(Output<T> v) {\n+    Operand<T> rmsInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(1.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), RMS, rmsInitializer);\n+    Operand<T> momentumInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), MOMENTUM, momentumInitializer);\n+    if (centered) {\n+      Operand<T> mgInitializer = tf\n+          .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+      createSlot(v.asOutput(), MG, mgInitializer);\n+    }\n+  }\n+\n+  @Override\n+  protected <T extends TType> Operand<T> applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> rmsSlot = getSlot(variable, RMS).get();\n+    Variable<T> momentumSlot = getSlot(variable, MOMENTUM).get();\n+    if (centered) {\n+      Variable<T> mgSlot = getSlot(variable, MG).get();\n+      return tf.train.applyCenteredRmsProp(variable, mgSlot, rmsSlot, momentumSlot,\n+          tf.constant(learningRate, gradient.dataType()),\n+          tf.constant(decay, gradient.dataType()),\n+          tf.constant(momentum, gradient.dataType()),\n+          tf.constant(epsilon, gradient.dataType()),\n+          gradient);\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1"}, "originalPosition": 92}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e9268791cddd0360cc646039994f20cb0bac808", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/9e9268791cddd0360cc646039994f20cb0bac808", "committedDate": "2020-02-25T13:34:18Z", "message": "Initial commit of gradient descent optimizers."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68a353c1fb555ac8a4e39f0ed65ac75de77864f4", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/68a353c1fb555ac8a4e39f0ed65ac75de77864f4", "committedDate": "2020-02-25T13:34:18Z", "message": "Adding Apache 2.0 license header to all optimizer files."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3f4be8ee32caeeb503ff625fcd8c95e4d0f33cd", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/b3f4be8ee32caeeb503ff625fcd8c95e4d0f33cd", "committedDate": "2020-02-25T13:34:18Z", "message": "Bug fix for the MNISTTest."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d1868ea31b7d0ca9b01cc0ad01bafb6f22c5536f", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/d1868ea31b7d0ca9b01cc0ad01bafb6f22c5536f", "committedDate": "2020-02-25T13:34:18Z", "message": "Refactor to uptake latest tensorflow-core changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d189cc79d58d968bd004c7c0c425c4dccfe802b", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/6d189cc79d58d968bd004c7c0c425c4dccfe802b", "committedDate": "2020-02-25T13:34:18Z", "message": "Added type safety and updates for new api."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53e438a76ea768341ae8bc4bd5aa8c1db4cca77a", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/53e438a76ea768341ae8bc4bd5aa8c1db4cca77a", "committedDate": "2020-02-25T13:35:00Z", "message": "Small changes, plus a fix for DataTypes to include references to the type."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83140b46bc1d4a712a81c4ac7c7b13776770eb0c", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/83140b46bc1d4a712a81c4ac7c7b13776770eb0c", "committedDate": "2020-02-25T13:35:48Z", "message": "Repackaging the optimizers into tensorflow-training, org.tensorflow.training."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2ac923b5c61b00f04bac75b0735ebfc3177d4cc", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/b2ac923b5c61b00f04bac75b0735ebfc3177d4cc", "committedDate": "2020-02-25T13:37:32Z", "message": "Initial commit of gradient descent optimizers."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7eb2e8c4e3597f4e05184af9ae70181f346e744", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/e7eb2e8c4e3597f4e05184af9ae70181f346e744", "committedDate": "2020-02-25T13:37:32Z", "message": "Adding Apache 2.0 license header to all optimizer files."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d63564458081e5c5bca6405aed0952ef346d383", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/3d63564458081e5c5bca6405aed0952ef346d383", "committedDate": "2020-02-25T13:37:32Z", "message": "Bug fix for the MNISTTest."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed71dc55ca5cad1d365f548166e0e853b62ba8e2", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/ed71dc55ca5cad1d365f548166e0e853b62ba8e2", "committedDate": "2020-02-25T13:37:32Z", "message": "Refactor to uptake latest tensorflow-core changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0544494775ede7f856f454cfc2f3f8c20303f24", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/b0544494775ede7f856f454cfc2f3f8c20303f24", "committedDate": "2020-02-25T13:37:32Z", "message": "Added type safety and updates for new api."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b29be50c4a61bcd43afea1a163fd669f6246562a", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/b29be50c4a61bcd43afea1a163fd669f6246562a", "committedDate": "2020-02-25T13:37:32Z", "message": "Repackaging the optimizers into tensorflow-training, org.tensorflow.training."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6cdb55c16b4437e5396f8959ccb1348212b0ec1f", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/6cdb55c16b4437e5396f8959ccb1348212b0ec1f", "committedDate": "2020-02-25T13:39:13Z", "message": "Delete pom.xml"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9d64c5885e169d5202f335b70992e9afb2af124", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/b9d64c5885e169d5202f335b70992e9afb2af124", "committedDate": "2020-02-25T13:40:26Z", "message": "Googlify with IntelliJ's Google Java Style Guide formatter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ae5ace082234c9c78e8d42efe550be42032edad", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/6ae5ace082234c9c78e8d42efe550be42032edad", "committedDate": "2020-02-25T13:40:26Z", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "committedDate": "2020-02-13T02:24:30Z", "message": "Updating to use @Endpoint, prototyping out @Endpoint on the Adam optimizer."}, "afterCommit": {"oid": "6ae5ace082234c9c78e8d42efe550be42032edad", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/6ae5ace082234c9c78e8d42efe550be42032edad", "committedDate": "2020-02-25T13:40:26Z", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56429e80522c205b2327a5c838dfe98894bcfe23", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/56429e80522c205b2327a5c838dfe98894bcfe23", "committedDate": "2020-02-25T15:08:23Z", "message": "Updating variableWithInit to use @Endpoint."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d8cb690974d9af71331dafa25891cd06fa8d123", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/5d8cb690974d9af71331dafa25891cd06fa8d123", "committedDate": "2020-02-25T15:40:56Z", "message": "Refactorings after code review."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51f5d47e87de878c16cf252a1e8e2578717ecbab", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/51f5d47e87de878c16cf252a1e8e2578717ecbab", "committedDate": "2020-02-25T15:41:20Z", "message": "Adding a couple of lines to the gitignore."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66876eddb2403d5ea60a96b42f071545b45802bd", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/66876eddb2403d5ea60a96b42f071545b45802bd", "committedDate": "2020-02-25T17:11:57Z", "message": "Adding a bit of documentation, threading the named operations through the constructors, removing the MNISTTtest."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a2fd256ff60786165f6f14772b9af05a0d3c9f2", "author": {"user": {"login": "Craigacp", "name": "Adam Pocock"}}, "url": "https://github.com/tensorflow/java/commit/7a2fd256ff60786165f6f14772b9af05a0d3c9f2", "committedDate": "2020-02-25T17:47:49Z", "message": "Adding a guard to prevent variableWithInit being called on an EagerSession."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b98f5227169bd09abd1390039ea6c41cf09075c", "author": {"user": {"login": "karllessard", "name": "Karl Lessard"}}, "url": "https://github.com/tensorflow/java/commit/1b98f5227169bd09abd1390039ea6c41cf09075c", "committedDate": "2020-03-02T03:29:09Z", "message": "Update Ops.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2OTMzNzM1", "url": "https://github.com/tensorflow/java/pull/28#pullrequestreview-366933735", "createdAt": "2020-03-02T03:29:24Z", "commit": {"oid": "1b98f5227169bd09abd1390039ea6c41cf09075c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3434, "cost": 1, "resetAt": "2021-11-02T12:20:56Z"}}}