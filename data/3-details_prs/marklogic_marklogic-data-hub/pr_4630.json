{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0MzA3NjU4", "number": 4630, "title": "DHFPROD-5989: Configure workUnit and endpointState in Spark connector", "bodyText": "Description\nChecklist:\n- Note: do not change the below\n\n\nOwner:\n\n\n JIRA_ID included in all the commit messages\n\n\n PR title is in the format JIRA_ID:Title\n\n\n Rebase the branch with upstream\n\n\n Squashed all commits into a single commit\n\n\n Added Tests\n\n\nReviewer:\n\n\n Reviewed Tests", "createdAt": "2020-09-28T17:51:37Z", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630", "merged": true, "mergeCommit": {"oid": "c9607468ba1a499b8c7f120802bfe717af72894c"}, "closed": true, "closedAt": "2020-10-01T19:15:54Z", "author": {"login": "anu3990"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNYOGBAFqTQ5NzgwMjMwMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOUFNbAFqTUwMDUyODA5MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3ODAyMzAw", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-497802300", "createdAt": "2020-09-28T18:45:22Z", "commit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODo0NToyMlrOHZLMQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODo0NToyMlrOHZLMQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE1OTgxMA==", "bodyText": "Please remove Sysout", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496159810", "createdAt": "2020-09-28T18:45:22Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+            if(jsonObject.get(\"apiPath\") == null || jsonObject.get(\"apiPath\").toString().length()==0) {\n+                if((jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) ||\n+                    (jsonObject.get(\"workUnit\")!=null && jsonObject.get(\"workUnit\").toString().length()>0))\n+                {\n+                    throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+                }\n+            }\n+            final String apiModulePath = (jsonObject.get(\"apiPath\")!=null && jsonObject.get(\"apiPath\").toString().length()>0) ?\n+                jsonObject.get(\"apiPath\").toString() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+            logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+            try {\n+                this.loader = InputEndpoint.on(\n+                    hubClient.getStagingClient(),\n+                    hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+                ).bulkCaller();\n+            } catch (ResourceNotFoundException ex) {\n+                throw new RuntimeException(\"Endpoint not found.\");\n+            }\n+\n+            if (jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) {\n+                loader.setEndpointState(new ByteArrayInputStream((jsonObject.get(\"endpointState\").toString()).getBytes()));\n+            }\n+            // TODO : remove the below else block after java-client-api 5.3 release\n+            else {\n+                loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \"}\").getBytes()));\n+            }\n+\n+            String uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";\n+\n+            if (jsonObject.get(\"workUnit\") != null && jsonObject.get(\"workUnit\").toString().length()>0) {\n+                JSONObject workUnitJson = new JSONObject(jsonObject.getString(\"workUnit\"));\n+                workUnitJson.putOpt(\"uriprefix\", uriPrefix);\n+                loader.setWorkUnit(new ByteArrayInputStream((workUnitJson.toString()).getBytes()));\n+                System.out.println(workUnitJson.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3ODA4NzA1", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-497808705", "createdAt": "2020-09-28T18:54:20Z", "commit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODo1NDoyMFrOHZLffw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxOTowNDo0M1rOHZL1Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2NDczNQ==", "bodyText": "We use ObjectMapper/JsonNode from Jackson, so please switch to that instead of this library.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496164735", "createdAt": "2020-09-28T18:54:20Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2ODIzMA==", "bodyText": "There's a lot going on here, and generally, I like to use private/protected methods to break up long methods so that they're easier to read and maintain. I prefer private, though I'll use protected when I want to write unit tests against the logic.\nI see the following steps in this code, where each can become a new private method:\n\nUse the user params to determine the apiPath, the workUnit, and endpointState (and throw a validation error as needed)\nUse the output of #1 to create a BulkInputCaller\n\nI think the signature of the first private method would be \"private JsonNode determineIngestionEndpointParams(Map)\". If the user has defined \"ingestendpointparams\", then use ObjectMapper to parse that into a JsonNode, and then validate it. Else, use objectMapper.createObjectNode() and populate apiPath based on the default API path, and then populate workUnit based on uriPrefix (and then @SameeraPriyathamTadikonda  would add more logic to this for collections/permissions/etc).\nThat first method would likely be protected so that you could easily write unit tests against it without having to involve everything else in this class.\nThe second private method would have a signature of \"private BulkInputCaller buildBulkInputCaller(JsonNode ingestionParams)\".\nEach private method can then have a try/catch as needed to ensure that if an error is thrown, the appropriate context is provided in the error message.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496168230", "createdAt": "2020-09-28T19:00:46Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+            if(jsonObject.get(\"apiPath\") == null || jsonObject.get(\"apiPath\").toString().length()==0) {\n+                if((jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) ||\n+                    (jsonObject.get(\"workUnit\")!=null && jsonObject.get(\"workUnit\").toString().length()>0))\n+                {\n+                    throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+                }\n+            }\n+            final String apiModulePath = (jsonObject.get(\"apiPath\")!=null && jsonObject.get(\"apiPath\").toString().length()>0) ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2ODkxMA==", "bodyText": "I think that since taskId doesn't matter, let's remove the debug logging above it. It would be meaningless to see it.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496168910", "createdAt": "2020-09-28T19:02:05Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java", "diffHunk": "@@ -53,7 +53,7 @@ public HubDataWriterFactory(Map<String, String> params, StructType schema) {\n         if (logger.isDebugEnabled()) {\n             logger.debug(\"Creating DataWriter with taskId: \" + taskId);\n         }\n-        return new HubDataWriter(hubClient, taskId, schema, params);\n+        return new HubDataWriter(hubClient, schema, params);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE3MDI1MQ==", "bodyText": "When there are a lot of params in a method, that's generally a sign that a class should be created.\nWhile we don't need this class in the application code, an \"Options\" class would be very helpful here for making it easier to write tests. It would have properties of batchSize, uriPrefix, apiPath, etc. And then it would have a \"Map toMap()\" method that builds a Map of params based on what's been configured.\nThen, each test will just build an Options class based on the scenario that it's testing. That will make it easier to write future tests - i.e. when @SameeraPriyathamTadikonda  needs to test for collections/permissions/etc, we don't have to add those to this method and fix every method that calls it - we just add new properties to our Options class, and the existing tests won't be affected.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496170251", "createdAt": "2020-09-28T19:04:43Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -77,18 +96,16 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n      * @param uriPrefix\n      * @return\n      */\n-    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix) {\n+    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix, String endpointState, String workUnit, String apiPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962"}, "originalPosition": 56}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/398a0ec14aa6e208e55b552e599b47ecf32d7962", "committedDate": "2020-09-28T17:49:09Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "5d9639acaa23af7f891bcd460aa35e69bce914ca", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/5d9639acaa23af7f891bcd460aa35e69bce914ca", "committedDate": "2020-09-28T23:59:02Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5d9639acaa23af7f891bcd460aa35e69bce914ca", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/5d9639acaa23af7f891bcd460aa35e69bce914ca", "committedDate": "2020-09-28T23:59:02Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/7c4ebc32159bc86bedd9f02a73436380038cdff1", "committedDate": "2020-09-29T00:01:49Z", "message": "Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4NDM2OTIw", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-498436920", "createdAt": "2020-09-29T12:55:43Z", "commit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjo1NTo0NFrOHZrvnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMzowNjozMlrOHZsMEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5MzE0OQ==", "bodyText": "We always want to provide the original exception here, and it's useful to include the message in the rethrown error's message - e.g. :\nthrow new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n\nAlso, I believe this will fail if ingestendpointparams isn't set, which Ernie doesn't need to set. So we need a test to verify that if ingestendpointparams isn't set, then no error is thrown and the default API is used.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496693149", "createdAt": "2020-09-29T12:55:44Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5MzY1Nw==", "bodyText": "This code should go into determineIngestionEndpointParams. That method should return a JsonNode with all 3 fields populated. Unit tests can then verify that those fields are correct based on a variety of inputs.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496693657", "createdAt": "2020-09-29T12:56:24Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -41,28 +44,26 @@\n     private InputEndpoint.BulkInputCaller loader;\n     private StructType schema;\n     private int batchSize;\n+    private String uriPrefix;\n \n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n+        this.uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        JsonNode endpointParamsJsonNode = determineIngestionEndpointParams(params);\n+        final String apiModulePath = (endpointParamsJsonNode.get(\"apiPath\")!=null &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NDIxMw==", "bodyText": "Use node.has(\"apiPath\"), that's a cleaner way to determine if the value is set.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496694213", "createdAt": "2020-09-29T12:57:12Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NTE2Mg==", "bodyText": "This code can be a lot simpler if the determineParams method always returns a value for endpointState - i.e. it should default to \"{}\".", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496695162", "createdAt": "2020-09-29T12:58:41Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {\n+            if((endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length()>0) ||\n+                (endpointParamsJsonNode.get(\"workUnit\")!=null && endpointParamsJsonNode.get(\"workUnit\").asText().length()>0))\n+            {\n+                throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+            }\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiModulePath, JsonNode endpointParamsJsonNode) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");\n+        }\n+        return configureBulkInputCaller(bulkInputCaller, endpointParamsJsonNode);\n+    }\n+\n+    private InputEndpoint.BulkInputCaller configureBulkInputCaller(InputEndpoint.BulkInputCaller loader, JsonNode endpointParamsJsonNode) {\n+\n+        try {\n+            if (endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NTkyNQ==", "bodyText": "The determineParams method should handle building the workUnit, unless Ernie provides its own. That should be the only method that has knowledge of how to build the workUnit; this method for building the BulkInputCaller then only has to worry about ensuring that the API exists and then building the caller object.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496695925", "createdAt": "2020-09-29T12:59:49Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {\n+            if((endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length()>0) ||\n+                (endpointParamsJsonNode.get(\"workUnit\")!=null && endpointParamsJsonNode.get(\"workUnit\").asText().length()>0))\n+            {\n+                throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+            }\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiModulePath, JsonNode endpointParamsJsonNode) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");\n+        }\n+        return configureBulkInputCaller(bulkInputCaller, endpointParamsJsonNode);\n+    }\n+\n+    private InputEndpoint.BulkInputCaller configureBulkInputCaller(InputEndpoint.BulkInputCaller loader, JsonNode endpointParamsJsonNode) {\n+\n+        try {\n+            if (endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+                loader.setEndpointState(new ByteArrayInputStream((endpointParamsJsonNode.get(\"endpointState\").toString()).getBytes()));\n+            }\n+            // TODO : remove the below else block after java-client-api 5.3 release\n+            else {\n+                loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \"}\").getBytes()));\n+            }\n+\n+            if (endpointParamsJsonNode.get(\"workUnit\") != null && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+                ObjectMapper objectMapper = new ObjectMapper();\n+\n+                JsonNode workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+                ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5OTY0MQ==", "bodyText": "Pinging @SameeraPriyathamTadikonda  about this - for each workUnit input that the endpoint supports, we'll want to verify that it works correctly in the BulkIngestTest class, as that class has no knowledge of Spark.\nThe tests for our Spark connector should then focus on converting the params Map into the proper workUnit object. Those tests don't need to connect to ML - they can be very fast unit tests that live in a separate class that doesn't extend AbstractHubCoreTest. I'll put together a PR to demonstrate this and submit it against this branch.\nWe'll still want one test in this project that writes data with every workUnit field populated, just to make sure that we're setting the correct workUnit fields.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496699641", "createdAt": "2020-09-29T13:05:24Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -30,7 +34,9 @@\n \n     @Test\n     void ingestThreeFruitsWithBatchSizeOfTwo() throws IOException {\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(\"2\", \"/testFruit\");\n+        Map<String, String> params = getHubPropertiesAsMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjcwMDQzNQ==", "bodyText": "Can this just be \"{}\", since we don't care about \"next\"? It's supposed to go away once we shift to the newer Java client, but I think it'd be better to make it \"{}\" in the meantime.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496700435", "createdAt": "2020-09-29T13:06:32Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub/src/test/java/com/marklogic/hub/dataservices/ingestion/BulkIngestTest.java", "diffHunk": "@@ -32,8 +32,8 @@ public void setupTest() {\n     public void testBulkIngest() {\n \n         String prefix = \"/bulkIngesterTest\";\n-        String endpointState = \"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";\n-        String workUnit      = \"{\\\"taskId\\\":\"+1+\"}\";\n+        String endpointState =  \"{\\\"next\\\":\"+0+\"}\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1"}, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "60358e2db444401a4c8e5827dac8167ddd119f8b", "author": {"user": {"login": "rjrudin", "name": null}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/60358e2db444401a4c8e5827dac8167ddd119f8b", "committedDate": "2020-09-29T16:35:59Z", "message": "DHFPROD-5989: Added Options class to simplify tests\n\nAlso made a couple fixes in HubDataWriter to support empty ingestendpointparams."}, "afterCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/6e0ccb36e5926380cee293321a4b62ab2472e4b4", "committedDate": "2020-09-29T20:19:49Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4ODczMTk1", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-498873195", "createdAt": "2020-09-29T20:37:13Z", "commit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozNzoxM1rOHaAWIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0Mjo0NVrOHaAsZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMDY4OA==", "bodyText": "I am wondering if the ResourceNotFoundException needs to be caught here. If we do throw a new exception, we definitely need to include the original exception so that Ernie doesn't lose any context.\nI did a quick test, and this is what the exception shows:\ncom.marklogic.client.ResourceNotFoundException: Local message: Could not read non-existent document. Server Message: RESTAPI-NODOCUMENT: (err:FOER0000) Resource or document does not exist:  category: content message: /data-hub/5/data-services/ingestion/bulkIngesterrrr.api\n\nThat is likely helpful enough for Ernie - it shows the module he referenced, and the Java Client is giving a good indication of the error. So I think this try/catch can be removed.\nAnd because of that, we don't really need this method anymore, since it's just a few lines of code now. If we do need some special error handling here, I think it's worth the separate method. But that doesn't appear to be the case.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497030688", "createdAt": "2020-09-29T20:37:13Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\").asText());\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", workUnitNode.toString());\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + uriPrefix + \"\\\"}\");\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiPath, String endpointState,\n+                                                               String workUnit) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiPath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzMwMA==", "bodyText": "Exception messages are crucial here to explain to Ernie what to do if he made a mistake. We want to be more precise about how to fix the problem - e.g.\nCannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497033300", "createdAt": "2020-09-29T20:39:45Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNDQ3MQ==", "bodyText": "No need to capture this here - batchSize needs to be captured, but not this. Just need to read this in the determineIngestionEndpointParams method.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497034471", "createdAt": "2020-09-29T20:40:52Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -41,28 +44,24 @@\n     private InputEndpoint.BulkInputCaller loader;\n     private StructType schema;\n     private int batchSize;\n+    private String uriPrefix;\n \n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n+        this.uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNjM5MQ==", "bodyText": "Use set(\"workUnit\", workUnitNode) instead. That returns the JSON structure instead of converting it into a string. You can then do setWorkUnit(new JacksonHandle(theNode)) and don't have to fiddle with converting things into strings. Same goes for endpointState.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497036391", "createdAt": "2020-09-29T20:42:45Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\").asText());\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", workUnitNode.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4"}, "originalPosition": 104}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/6e0ccb36e5926380cee293321a4b62ab2472e4b4", "committedDate": "2020-09-29T20:19:49Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/660d87d7616098274a5b97ff3d2005b0a07461fb", "committedDate": "2020-09-29T21:34:51Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4OTMwNDg0", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-498930484", "createdAt": "2020-09-29T22:11:55Z", "commit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjoxMTo1NVrOHaEANQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjoxNjoxOVrOHaEHRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MDYxMw==", "bodyText": "I think this needs to be a JsonNode as well instead of a string for consistency. You can do mapper.createObjectNode() and then node.put(\"uriprefix\", params.get(\"uriprefix\")) .\nI think it's good to handle these as JsonNode's instead of as strings. When Ernie provides us with a custom workUnit and/or endpointState, we should convert those ASAP to JsonNode. That way, if Ernie's JSON is malformed, it'll fail as quickly as possible - i.e. in the connector code and not in ML.\nAlso this is where @SameeraPriyathamTadikonda will add his code for collections/permissions/etc.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497090613", "createdAt": "2020-09-29T22:11:55Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +116,52 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).set(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\"));\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", params.get(\"uriprefix\"));\n+            ((ObjectNode)endpointParamsJsonNode).set(\"workUnit\", workUnitNode);\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MjQyMw==", "bodyText": "I'll test this locally, but I didn't think \"asText\" does what we want here - it's just getting the text of this node, it's not the same as toString. I think we want to use JacksonHandle here instead, but I'm not sure.\nWhich makes me realize - we really need a test that uses a custom API path. That way, we know for sure that our custom workUnit and endpointState are passed to our custom endpoint correctly.\nLet me handle that test - I want to think a bit about where the modules for this should go. I think they can go under ./marklogic-data-hub/src/test/ml-modules, I just need to confirm that. I'll try to submit that tonight or first thing tomorrow morning.\nIn the meantime though, this can go forward so that @SameeraPriyathamTadikonda  can make use of it.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497092423", "createdAt": "2020-09-29T22:16:19Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,23 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        JsonNode endpointParamsJsonNode = determineIngestionEndpointParams(params);\n+\n         this.loader = InputEndpoint.on(\n             hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n+            hubClient.getModulesClient().newJSONDocumentManager().read(endpointParamsJsonNode.get(\"apiPath\").asText(), new StringHandle())\n         ).bulkCaller();\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+        this.loader.setEndpointState(new ByteArrayInputStream((endpointParamsJsonNode.get(\"endpointState\").asText()).getBytes()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4OTc5OTYy", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-498979962", "createdAt": "2020-09-29T22:52:00Z", "commit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo1MjowMFrOHaFQEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo1MjowMFrOHaFQEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw==", "bodyText": "what's the purpose of userDefinedValue?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497111057", "createdAt": "2020-09-29T22:52:00Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub/src/test/java/com/marklogic/hub/dataservices/ingestion/BulkIngestTest.java", "diffHunk": "@@ -32,8 +32,8 @@ public void setupTest() {\n     public void testBulkIngest() {\n \n         String prefix = \"/bulkIngesterTest\";\n-        String endpointState = \"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";\n-        String workUnit      = \"{\\\"taskId\\\":\"+1+\"}\";\n+        String endpointState =  \"{}\";\n+        String workUnit      = \"{\\\"userDefinedValue\\\":\" + 1 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4OTgwNzM3", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-498980737", "createdAt": "2020-09-29T22:54:05Z", "commit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo1NDowNVrOHaFV5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo1NDowNVrOHaFV5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ==", "bodyText": "Do we need to add this even if user did not add the option URIPrefix?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497112549", "createdAt": "2020-09-29T22:54:05Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +116,52 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).set(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\"));\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", params.get(\"uriprefix\"));\n+            ((ObjectNode)endpointParamsJsonNode).set(\"workUnit\", workUnitNode);\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5NTA4NDEx", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-499508411", "createdAt": "2020-09-30T14:43:07Z", "commit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/660d87d7616098274a5b97ff3d2005b0a07461fb", "committedDate": "2020-09-29T21:34:51Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "d26929ecc037f084836d4c103d8415ba558ce840", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/d26929ecc037f084836d4c103d8415ba558ce840", "committedDate": "2020-09-30T16:39:14Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d26929ecc037f084836d4c103d8415ba558ce840", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/d26929ecc037f084836d4c103d8415ba558ce840", "committedDate": "2020-09-30T16:39:14Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "author": {"user": {"login": "wooldridge", "name": "Mike Wooldridge"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "committedDate": "2020-09-30T18:58:38Z", "message": "DHFPROD-6027: Add intro text to the top of each tile\n\nIncludes tests of intro text display."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "author": {"user": {"login": "wooldridge", "name": "Mike Wooldridge"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "committedDate": "2020-09-30T18:58:38Z", "message": "DHFPROD-6027: Add intro text to the top of each tile\n\nIncludes tests of intro text display."}, "afterCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/a444f2452ba4d9ef24707d03ddf575cf91f84935", "committedDate": "2020-09-30T22:37:55Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMDE5MjQ3", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500019247", "createdAt": "2020-10-01T05:34:12Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNDoxM1rOHa7FAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNDoxM1rOHa7FAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mjk2Mw==", "bodyText": "endpointParams.get(\"apiPath\").asText() will throw a NPE", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497992963", "createdAt": "2020-10-01T05:34:13Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMDIwMDIx", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500020021", "createdAt": "2020-10-01T05:36:36Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNjozNlrOHa7HwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNjozNlrOHa7HwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5MzY2NQ==", "bodyText": "we can use the variable \"doesNotHaveApiPath\" instead of writing the logic again", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497993665", "createdAt": "2020-10-01T05:36:36Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasWorkUnitOrEndpointState = endpointParams.has(\"workUnit\") || endpointParams.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        if (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMDIwMzAz", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500020303", "createdAt": "2020-10-01T05:37:23Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNzoyM1rOHa7InQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTozNzoyM1rOHa7InQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mzg4NQ==", "bodyText": "else block missing", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497993885", "createdAt": "2020-10-01T05:37:23Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasWorkUnitOrEndpointState = endpointParams.has(\"workUnit\") || endpointParams.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        if (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText())) {\n+            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+        }\n+\n+        // TODO : remove the below else block after java-client-api 5.3 release", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMDI0MDA1", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500024005", "createdAt": "2020-10-01T05:48:59Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTo0ODo1OVrOHa7U4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTo0ODo1OVrOHa7U4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NzAyNw==", "bodyText": "provide a meaningful name for this test", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497997027", "createdAt": "2020-10-01T05:48:59Z", "author": {"login": "SameeraPriyathamTadikonda"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataViaCustomEndpointTest.java", "diffHunk": "@@ -0,0 +1,67 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.FileHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.jupiter.api.Test;\n+import org.springframework.core.io.ClassPathResource;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataViaCustomEndpointTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void test() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMjU3Njg5", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500257689", "createdAt": "2020-10-01T11:38:31Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "DISMISSED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMTozODozMVrOHbGWNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMTo0NjoxN1rOHbGlZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3NzU4OA==", "bodyText": "An NPE won't occur because if apiPath were null or did not exist, then the first part of the conditional would be true and then Java won't evaluate the second part.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498177588", "createdAt": "2020-10-01T11:38:31Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mjk2Mw=="}, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3Nzk5MQ==", "bodyText": "There's no else needed - this is just saying \"If Ernie didn't provide a non-null endpointState, then toss in an empty object node\".", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498177991", "createdAt": "2020-10-01T11:39:17Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasWorkUnitOrEndpointState = endpointParams.has(\"workUnit\") || endpointParams.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        if (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText())) {\n+            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+        }\n+\n+        // TODO : remove the below else block after java-client-api 5.3 release", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mzg4NQ=="}, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4MTI2NQ==", "bodyText": "This is actually a common pattern we have in existing tests - since the class name already identifies the scope of the test, the method is just \"test\". If a second test method is added, then this would need to be updated.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498181265", "createdAt": "2020-10-01T11:45:51Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataViaCustomEndpointTest.java", "diffHunk": "@@ -0,0 +1,67 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.FileHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.jupiter.api.Test;\n+import org.springframework.core.io.ClassPathResource;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataViaCustomEndpointTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void test() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NzAyNw=="}, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4MTQ3Ng==", "bodyText": "I think you can drop this test because WriteDataViaCustomEndpointTest already covers this with an actual custom endpoint, as opposed to using the default endpoint.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498181476", "createdAt": "2020-10-01T11:46:17Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -68,45 +52,68 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n         assertFalse(uriQueryResult.hasNext());\n     }\n \n-    /**\n-     * Spark will do all of this in the real world - i.e. a user will specify the entry class and the set of options.\n-     * But in a test, we need to do that ourselves. So we create the DataSource class, build up the params, and then\n-     * call the factory/writer methods ourselves.\n-     *\n-     * @param batchSize\n-     * @param uriPrefix\n-     * @return\n-     */\n-    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix) {\n-        HubDataSource dataSource = new HubDataSource();\n-        final String writeUUID = \"doesntMatter\";\n-        final SaveMode saveModeDoesntMatter = SaveMode.Overwrite;\n-\n-        // Get the set of DHF properties used to connect to ML as a map, and then add connector-specific params\n-        Map<String, String> params = getHubPropertiesAsMap();\n-        params.put(\"batchsize\", batchSize);\n-\n-        if(uriPrefix!=null && uriPrefix.length()!=0) {\n-            params.put(\"uriprefix\", uriPrefix);\n-        }\n+    @Test\n+    public void ingestWithoutCustomApiWithCustomWorkunit(){\n+        ObjectNode customWorkUnit = objectMapper.createObjectNode();\n+        customWorkUnit.put(\"userDefinedValue\", 0);\n+\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestWorkUnit(customWorkUnit)),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        assertEquals(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\", ex.getMessage());\n+    }\n+\n+    @Test\n+    public void ingestWithIncorrectApi(){\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestApiPath(\"/incorrect.api\")),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        System.out.println(ex.getMessage());\n+        assertTrue( ex.getMessage().contains(\"Could not read non-existent document.\"));\n+    }\n+\n+    @Test\n+    public void ingestWithCustomApiWithCustomWorkunit() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNDk5Mzky", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500499392", "createdAt": "2020-10-01T16:01:41Z", "commit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "committedDate": "2020-10-01T16:16:32Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/a444f2452ba4d9ef24707d03ddf575cf91f84935", "committedDate": "2020-09-30T22:37:55Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}, "afterCommit": {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "committedDate": "2020-10-01T16:16:32Z", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNTI1NTE4", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500525518", "createdAt": "2020-10-01T16:32:50Z", "commit": {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNTI4MDkw", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#pullrequestreview-500528090", "createdAt": "2020-10-01T16:35:58Z", "commit": {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1884, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}