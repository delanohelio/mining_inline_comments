{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0NzQ3MTcy", "number": 4816, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMjo0NDowOVrOE1DWNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNDozNjowMFrOE1r2yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MDY0ODIxOnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMjo0NDowOVrOHtHTKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMjo0NDowOVrOHtHTKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA2NzU2Mw==", "bodyText": "This is only supporting what's need for the Customer view in the reference-entity-model project. Will be expanded via a separate story.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4816#discussion_r517067563", "createdAt": "2020-11-04T02:44:09Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java", "diffHunk": "@@ -0,0 +1,174 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.client.dataservices.OutputCaller;\n+import com.marklogic.client.ext.helper.LoggingObject;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Util;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.Map;\n+\n+/**\n+ * Does all the work of using a BulkOutputCaller to repeatedly fetch rows for a given partition.\n+ */\n+public class HubInputPartitionReader extends LoggingObject implements InputPartitionReader<InternalRow> {\n+\n+    private final HubClient hubClient;\n+    private final ObjectMapper objectMapper;\n+    private final OutputCaller.BulkOutputCaller<InputStream> bulkOutputCaller;\n+    private final StructType schema;\n+\n+    private InputStream[] rows;\n+    private int rowIndex;\n+    private JsonNode currentRow;\n+    private long numberOfRowsRead;\n+\n+    /**\n+     * @param options                options provided by the Spark user; needed to both connect to ML and to query for data\n+     * @param initializationResponse\n+     * @param partitionNumber\n+     */\n+    public HubInputPartitionReader(Map<String, String> options, JsonNode initializationResponse, int partitionNumber) {\n+        this.hubClient = HubClient.withHubClientConfig(Util.buildHubClientConfig(options));\n+        this.schema = (StructType) StructType.fromJson(initializationResponse.get(\"schema\").toString());\n+        this.objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointConstants = buildEndpointConstants(options, initializationResponse, partitionNumber);\n+        ObjectNode endpointState = objectMapper.createObjectNode();\n+        endpointState.put(\"batchNumber\", 1);\n+        this.bulkOutputCaller = buildOutputCaller(endpointConstants, endpointState);\n+    }\n+\n+    @Override\n+    public boolean next() throws IOException {\n+        if (rows == null || rowIndex >= rows.length) {\n+            readNextBatchOfRows();\n+        }\n+\n+        if (rows == null || rows.length == 0) {\n+            logger.info(\"Finished reading rows\");\n+            return false;\n+        }\n+\n+        this.currentRow = objectMapper.readTree(rows[rowIndex++]);\n+        return true;\n+    }\n+\n+    /**\n+     * Convert the most recently read JSON object into an InternalRow.\n+     *\n+     * @return\n+     */\n+    @Override\n+    public InternalRow get() {\n+        Object[] values = Arrays.stream(schema.fields()).map(field -> {\n+            String fieldName = field.name();\n+            if (currentRow.has(fieldName) && !\"null\".equals(currentRow.get(fieldName).asText())) {\n+                return readValue(field);\n+            }\n+            return null;\n+        }).toArray();\n+\n+        Row row = RowFactory.create(values);\n+        return RowEncoder.apply(this.schema).toRow(row);\n+    }\n+\n+    @Override\n+    public void close() {\n+        logger.debug(\"Closing\");\n+    }\n+\n+    private ObjectNode buildEndpointConstants(Map<String, String> options, JsonNode initializationResponse, int partitionNumber) {\n+        ObjectNode endpointConstants = objectMapper.createObjectNode();\n+        endpointConstants.set(\"initializationResponse\", initializationResponse);\n+        endpointConstants.put(\"partitionNumber\", partitionNumber);\n+        try {\n+            endpointConstants.set(\"sparkSchema\", objectMapper.readTree(schema.json()));\n+        } catch (Exception ex) {\n+            throw new RuntimeException(\"Unable to write StructType as JSON; cause: \" + ex.getMessage(), ex);\n+        }\n+        return endpointConstants;\n+    }\n+\n+    private OutputCaller.BulkOutputCaller<InputStream> buildOutputCaller(ObjectNode endpointConstants, ObjectNode endpointState) {\n+        InputStreamHandle defaultApi = hubClient.getModulesClient().newJSONDocumentManager()\n+            .read(\"/marklogic-data-hub-spark-connector/readRows.api\", new InputStreamHandle());\n+\n+        OutputCaller<InputStream> outputCaller = OutputCaller.on(hubClient.getFinalClient(), defaultApi, new InputStreamHandle());\n+\n+        return outputCaller.bulkCaller(outputCaller.newCallContext()\n+            .withEndpointConstants(new JacksonHandle(endpointConstants))\n+            .withEndpointState(new JacksonHandle(endpointState)));\n+    }\n+\n+    private void readNextBatchOfRows() {\n+        this.rows = bulkOutputCaller.next();\n+        if (rows.length > 0) {\n+            numberOfRowsRead += rows.length;\n+            if (logger.isDebugEnabled()) {\n+                logger.debug(\"Rows read so far: \" + numberOfRowsRead);\n+            }\n+            rowIndex = 0;\n+        }\n+    }\n+\n+    /**\n+     * Just doing the bare minimum here for now, will have lots more to support soon.\n+     *\n+     * @param field\n+     * @return\n+     */\n+    private Object readValue(StructField field) {\n+        final String fieldName = field.name();\n+        Object value;\n+        switch (field.dataType().typeName()) {\n+            case \"integer\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e84038b95dc0e710b44e214657d4ec031034eb5c"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NzI4NTIxOnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNDozNjowMFrOHuGMqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNDozNjowMFrOHuGMqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5ODA4OA==", "bodyText": "We'll have to keep an eye on this. Creating multiple HubDataSourceReader objects isn't good, but I'm worried there's some downside to caching this instance that I haven't found yet.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4816#discussion_r518098088", "createdAt": "2020-11-05T14:36:00Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java", "diffHunk": "@@ -16,28 +16,51 @@\n package com.marklogic.hub.spark.sql.sources.v2;\n \n import com.marklogic.client.ext.helper.LoggingObject;\n+import com.marklogic.hub.spark.sql.sources.v2.reader.HubDataSourceReader;\n import com.marklogic.hub.spark.sql.sources.v2.writer.HubDataSourceWriter;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.sources.v2.DataSourceOptions;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n import org.apache.spark.sql.sources.v2.StreamWriteSupport;\n import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader;\n import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter;\n import org.apache.spark.sql.streaming.OutputMode;\n import org.apache.spark.sql.types.StructType;\n \n import java.util.Optional;\n \n-public class DefaultSource extends LoggingObject implements WriteSupport, StreamWriteSupport {\n+public class DefaultSource extends LoggingObject implements WriteSupport, StreamWriteSupport, ReadSupport {\n+\n+    private HubDataSourceReader hubDataSourceReader;\n+\n+    public DefaultSource() {\n+        logger.debug(\"Created: \" + toString());\n+    }\n \n     @Override\n     public Optional<DataSourceWriter> createWriter(String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options) {\n-        return Optional.of(new HubDataSourceWriter(options.asMap(), schema, false) {\n-        });\n+        return Optional.of(new HubDataSourceWriter(options.asMap(), schema, false));\n     }\n \n     @Override\n     public StreamWriter createStreamWriter(String queryId, StructType schema, OutputMode mode, DataSourceOptions options) {\n         return new HubDataSourceWriter(options.asMap(), schema, true);\n     }\n+\n+    @Override\n+    public DataSourceReader createReader(DataSourceOptions options) {\n+        // Logging of the HubDataSourceReader's constructor indicates that in a simple Spark test program, this method\n+        // is called multiple times. On the first occasion, the getSchema method is called, and then instance is seemingly\n+        // discarded. On the second occasion, the planInputPartitions method is called, which allows for partition\n+        // readers to then be created. And then this is called on a third occasion for unknown reasons. For performance\n+        // reasons then, this class will only create one HubDataSourceReader. Testing has shown that if the Spark\n+        // program then calls \"format\" again on a SQLContext, a new instance of this class - DefaultSource - will be\n+        // created, thus ensuring that a new data source reader is created.\n+        if (hubDataSourceReader == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc273ad5cf4f4e9f2609b151ca74f68af089d578"}, "originalPosition": 49}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3068, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}