{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4NzY0NTEw", "number": 4755, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzozNVrOExMAsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0NDoyMVrOEx4jGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDEyNDY1OnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/SparkTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzozNVrOHnMBVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzozNVrOHnMBVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzQ2MA==", "bodyText": "Nice - I was wondering how to do this!", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853460", "createdAt": "2020-10-23T12:37:35Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/SparkTest.java", "diffHunk": "@@ -52,7 +52,7 @@ private static void setConnectionProperties(String[] args) {\n         SQLContext sqlContext = new SQLContext(javaSparkContext);\n         final String filePath = getTestFilePath();\n         logger.info(\"Loading from file: \" + filePath);\n-        Dataset<org.apache.spark.sql.Row> rows = sqlContext.load(filePath, \"csv\");\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.read().option(\"header\", true).csv(getTestFilePath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDEyNjgzOnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozODoxNVrOHnMCuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQyMjozNToyNlrOHn9IwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ==", "bodyText": "Do you know if this line is needed? I added it here when I was hacking around, but I don't know if it's needed. We want this to be as slim as possible, we want the bare minimum amount of code to enable streaming.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853819", "createdAt": "2020-10-23T12:38:15Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2MDg3OQ==", "bodyText": "Not sure about that Rob. However i did notice that SQLContext(inside loadRowsFromTestFile) is deprecated, so we may want to shift towards SparkSession.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510960879", "createdAt": "2020-10-23T15:24:02Z", "author": {"login": "anu3990"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ=="}, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1ODE3Nw==", "bodyText": "Changed the code to use SparkSession.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511658177", "createdAt": "2020-10-25T22:35:26Z", "author": {"login": "anu3990"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ=="}, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDEyODM3OnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozODo0NFrOHnMDyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozODo0NFrOHnMDyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDA4OA==", "bodyText": "Just a note, this duplication is totally fine right now. I'll submit a PR later to refactor it.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854088", "createdAt": "2020-10-23T12:38:44Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDEzMDc3OnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozOToyMFrOHnMFLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNToyNToxNlrOHnSoKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDQ0Ng==", "bodyText": "Nice - so the key is to read the header first and determine the schema from it, and then pass the schema in when calling readStream?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854446", "createdAt": "2020-10-23T12:39:20Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(JavaSparkContext javaSparkContext) {\n+        SQLContext sqlContext = new SQLContext(javaSparkContext);\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sqlContext.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2MTcwNg==", "bodyText": "Yes i believe so.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510961706", "createdAt": "2020-10-23T15:25:16Z", "author": {"login": "anu3990"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(JavaSparkContext javaSparkContext) {\n+        SQLContext sqlContext = new SQLContext(javaSparkContext);\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sqlContext.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDQ0Ng=="}, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDg1NjQwOnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTozMzo1NlrOHnTD-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTozMzo1NlrOHnTD-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2ODgyNw==", "bodyText": "Is there any reason why  \"test-data-hub-operator\" isn't used here (like SparkTest) ?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510968827", "createdAt": "2020-10-23T15:33:56Z", "author": {"login": "srinathgit"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzQyMDM0OnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0Mzo1N1rOHoNyEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0Mzo1N1rOHoNyEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMDg5Ng==", "bodyText": "At least for this test program, I think we need to pass in e.g. 3000l here to force a timeout. Our connector doesn't otherwise know when to stop.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511930896", "createdAt": "2020-10-26T12:43:57Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing SparkSession\");\n+            sparkSession.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(SparkSession sparkSession) {\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sparkSession.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sparkSession.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");\n+\n+        return rows;\n+    }\n+\n+    /**\n+     * Depending on how this program is run - e.g. via Gradle or an IDE - the path will resolve to either this project\n+     * directory or the root DHF project directory. So gotta support both.\n+     *\n+     * @return\n+     */\n+    private static String getTestFilePath() {\n+        String filePath = \"src/main/resources/data/databook.csv\";\n+        if (new File(filePath).exists()) {\n+            return new File(filePath).getAbsolutePath();\n+        }\n+        return new File(\"marklogic-data-hub-spark-connector/spark-test-project/\" + filePath).getAbsolutePath();\n+    }\n+\n+    /**\n+     * Customize the options in here as needed for ad hoc testing.\n+     *\n+     * @param rows\n+     */\n+    private static void writeRowsToDataHub(Dataset<Row> rows) throws StreamingQueryException {\n+        rows.writeStream()\n+            .format(\"com.marklogic.hub.spark.sql.sources.v2\")\n+            .option(\"mlHost\", host)\n+            .option(\"mlUsername\", username)\n+            .option(\"mlPassword\", password)\n+            .option(\"collections\", \"sparkTestOne,sparkTestTwo\")\n+            .option(\"hubDhs\", \"false\")\n+            .option(\"hubSsl\", \"false\")\n+            .option(\"batchSize\", \"3\")\n+            .option(\"checkpointLocation\", \"src/main/resources/logs\")\n+            .start()\n+            .awaitTermination();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzQyMTY5OnYy", "diffSide": "RIGHT", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0NDoyMVrOHoNy8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0NDoyMVrOHoNy8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMTEyMw==", "bodyText": "Can you remove this line from both test programs? I think I included it via copy/paste, but it doesn't seem necessary, and we want as few lines of code here as possible.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511931123", "createdAt": "2020-10-26T12:44:21Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d"}, "originalPosition": 33}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3170, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}