{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4NzY0NTEw", "number": 4755, "title": "DHFPROD-6142:Add test program for streaming data into ML", "bodyText": "Description\nChecklist:\n- Note: do not change the below\n\n\nOwner:\n\n\n JIRA_ID included in all the commit messages\n\n\n PR title is in the format JIRA_ID:Title\n\n\n Rebase the branch with upstream\n\n\n Squashed all commits into a single commit\n\n\n Added Tests\n\n\nReviewer:\n\n\n Reviewed Tests", "createdAt": "2020-10-23T07:01:30Z", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755", "merged": true, "mergeCommit": {"oid": "dd088c1018647afd90b33c78b343b430ace35e46"}, "closed": true, "closedAt": "2020-10-27T21:35:49Z", "author": {"login": "anu3990"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdVV5DTAFqTUxNTYyNTcwMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdWvU7bAFqTUxODEzMzk2Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1NjI1NzAx", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#pullrequestreview-515625701", "createdAt": "2020-10-23T12:37:35Z", "commit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozNzozNVrOHnMBVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxMjozOToyMFrOHnMFLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzQ2MA==", "bodyText": "Nice - I was wondering how to do this!", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853460", "createdAt": "2020-10-23T12:37:35Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/SparkTest.java", "diffHunk": "@@ -52,7 +52,7 @@ private static void setConnectionProperties(String[] args) {\n         SQLContext sqlContext = new SQLContext(javaSparkContext);\n         final String filePath = getTestFilePath();\n         logger.info(\"Loading from file: \" + filePath);\n-        Dataset<org.apache.spark.sql.Row> rows = sqlContext.load(filePath, \"csv\");\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.read().option(\"header\", true).csv(getTestFilePath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ==", "bodyText": "Do you know if this line is needed? I added it here when I was hacking around, but I don't know if it's needed. We want this to be as slim as possible, we want the bare minimum amount of code to enable streaming.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853819", "createdAt": "2020-10-23T12:38:15Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDA4OA==", "bodyText": "Just a note, this duplication is totally fine right now. I'll submit a PR later to refactor it.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854088", "createdAt": "2020-10-23T12:38:44Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDQ0Ng==", "bodyText": "Nice - so the key is to read the header first and determine the schema from it, and then pass the schema in when calling readStream?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854446", "createdAt": "2020-10-23T12:39:20Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(JavaSparkContext javaSparkContext) {\n+        SQLContext sqlContext = new SQLContext(javaSparkContext);\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sqlContext.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1NzgzMjcw", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#pullrequestreview-515783270", "createdAt": "2020-10-23T15:33:56Z", "commit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTozMzo1NlrOHnTD-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTozMzo1NlrOHnTD-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2ODgyNw==", "bodyText": "Is there any reason why  \"test-data-hub-operator\" isn't used here (like SparkTest) ?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510968827", "createdAt": "2020-10-23T15:33:56Z", "author": {"login": "srinathgit"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8"}, "originalPosition": 22}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9cfb1618188ca3edf15849610a64236119702bc8", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/9cfb1618188ca3edf15849610a64236119702bc8", "committedDate": "2020-10-23T06:58:29Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}, "afterCommit": {"oid": "751a217e26213c385857370ba445b1ba638af48f", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/751a217e26213c385857370ba445b1ba638af48f", "committedDate": "2020-10-25T22:33:59Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "751a217e26213c385857370ba445b1ba638af48f", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/751a217e26213c385857370ba445b1ba638af48f", "committedDate": "2020-10-25T22:33:59Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}, "afterCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/3482fa0b427d7aace4f2052ea8e386726e783c0d", "committedDate": "2020-10-25T22:37:23Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2NzM3NzY2", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#pullrequestreview-516737766", "createdAt": "2020-10-26T12:43:57Z", "commit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0Mzo1N1rOHoNyEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMjo0NDoyMVrOHoNy8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMDg5Ng==", "bodyText": "At least for this test program, I think we need to pass in e.g. 3000l here to force a timeout. Our connector doesn't otherwise know when to stop.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511930896", "createdAt": "2020-10-26T12:43:57Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing SparkSession\");\n+            sparkSession.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(SparkSession sparkSession) {\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sparkSession.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sparkSession.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");\n+\n+        return rows;\n+    }\n+\n+    /**\n+     * Depending on how this program is run - e.g. via Gradle or an IDE - the path will resolve to either this project\n+     * directory or the root DHF project directory. So gotta support both.\n+     *\n+     * @return\n+     */\n+    private static String getTestFilePath() {\n+        String filePath = \"src/main/resources/data/databook.csv\";\n+        if (new File(filePath).exists()) {\n+            return new File(filePath).getAbsolutePath();\n+        }\n+        return new File(\"marklogic-data-hub-spark-connector/spark-test-project/\" + filePath).getAbsolutePath();\n+    }\n+\n+    /**\n+     * Customize the options in here as needed for ad hoc testing.\n+     *\n+     * @param rows\n+     */\n+    private static void writeRowsToDataHub(Dataset<Row> rows) throws StreamingQueryException {\n+        rows.writeStream()\n+            .format(\"com.marklogic.hub.spark.sql.sources.v2\")\n+            .option(\"mlHost\", host)\n+            .option(\"mlUsername\", username)\n+            .option(\"mlPassword\", password)\n+            .option(\"collections\", \"sparkTestOne,sparkTestTwo\")\n+            .option(\"hubDhs\", \"false\")\n+            .option(\"hubSsl\", \"false\")\n+            .option(\"batchSize\", \"3\")\n+            .option(\"checkpointLocation\", \"src/main/resources/logs\")\n+            .start()\n+            .awaitTermination();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMTEyMw==", "bodyText": "Can you remove this line from both test programs? I think I included it via copy/paste, but it doesn't seem necessary, and we want as few lines of code here as possible.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511931123", "createdAt": "2020-10-26T12:44:21Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d"}, "originalPosition": 33}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/3482fa0b427d7aace4f2052ea8e386726e783c0d", "committedDate": "2020-10-25T22:37:23Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}, "afterCommit": {"oid": "69759a8823244ddb82c2996ed81dbb1a4d603809", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/69759a8823244ddb82c2996ed81dbb1a4d603809", "committedDate": "2020-10-27T17:48:43Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "committedDate": "2020-10-27T19:24:16Z", "message": "DHFPROD-6142:Add test program for streaming data into ML."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "69759a8823244ddb82c2996ed81dbb1a4d603809", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/69759a8823244ddb82c2996ed81dbb1a4d603809", "committedDate": "2020-10-27T17:48:43Z", "message": "DHFPROD-6142:Add test program for streaming data into ML"}, "afterCommit": {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "author": {"user": {"login": "anu3990", "name": "Anushree Sinha"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "committedDate": "2020-10-27T19:24:16Z", "message": "DHFPROD-6142:Add test program for streaming data into ML."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MTA4NzA0", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#pullrequestreview-518108704", "createdAt": "2020-10-27T20:19:47Z", "commit": {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MTMzOTY3", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#pullrequestreview-518133967", "createdAt": "2020-10-27T20:51:58Z", "commit": {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1787, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}