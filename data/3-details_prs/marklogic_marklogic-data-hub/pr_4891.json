{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0MjIxMzE4", "number": 4891, "title": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "bodyText": "Description\nExport rows from TDE view that contains every type of TDE column\nChecklist:\n- Note: do not change the below\n\n\nOwner:\n\n\n JIRA_ID included in all the commit messages\n\n\n PR title is in the format JIRA_ID:Title\n\n\n Rebase the branch with upstream\n\n\n Squashed all commits into a single commit\n\n\n Added Tests\n\n\nReviewer:\n\n\n Reviewed Tests", "createdAt": "2020-11-19T20:44:29Z", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891", "merged": true, "mergeCommit": {"oid": "460cafffd7e70099672da42e1004f68fd8f15320"}, "closed": true, "closedAt": "2020-11-24T21:08:07Z", "author": {"login": "srinathgit"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdenNrWABqjQwMjM1ODQ3Njk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdfvXlPgFqTUzNzg2MDA3MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "baaad61af2fd2ca8f5e7c1ba365af4adbbcd232f", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/baaad61af2fd2ca8f5e7c1ba365af4adbbcd232f", "committedDate": "2020-11-19T20:42:41Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}, "afterCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "committedDate": "2020-11-21T07:55:19Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "committedDate": "2020-11-21T07:55:19Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}, "afterCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "committedDate": "2020-11-23T18:57:54Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NzU5MjEz", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#pullrequestreview-536759213", "createdAt": "2020-11-23T18:55:48Z", "commit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxODo1NTo0OFrOH4bHjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowNTo1NlrOH4beHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNjYwNQ==", "bodyText": "Instead of renaming this, which then changes all the current clients, just introduce this as a new method, and then modify loadSimpleCustomerTDE to call loadTDE(\"Customer\"). That retains a self-documenting method name and avoids changing all the clients.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528926605", "createdAt": "2020-11-23T18:55:48Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java", "diffHunk": "@@ -35,15 +35,15 @@ void verifyMarkLogicSupportsRowID() {\n     }\n \n     // TODO Will soon have a nice convenience method for doing this\n-    protected void loadSimpleCustomerTDE() {\n+    protected void loadTDE(String name) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNjkyMA==", "bodyText": "Is this meant to be here?", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528926920", "createdAt": "2020-11-23T18:56:27Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java", "diffHunk": "@@ -27,6 +27,7 @@ void test() {\n     }\n \n     private void verifyTwoColumnsSelected() {\n+        readRows(\"customerId,customerSince\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNzQxMg==", "bodyText": "I believe you can remove tde:vars and tde:path-namespaces, which simplifies the template.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528927412", "createdAt": "2020-11-23T18:57:13Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/resources/tde-views/User.tdex", "diffHunk": "@@ -0,0 +1,322 @@\n+<tde:template xml:lang=\"zxx\" xmlns:tde=\"http://marklogic.com/xdmp/tde\">\n+    <tde:description>\n+        Simplified version of the TDE template that's generated in the reference-entity-model project\n+    </tde:description>\n+    <tde:context>/*:envelope/*:instance[*:info/*:version = \"0.0.1\"][*:User]</tde:context>\n+    <tde:vars>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNzU2MQ==", "bodyText": "Can also remove this template, as we don't need these triples for testing.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528927561", "createdAt": "2020-11-23T18:57:26Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/resources/tde-views/User.tdex", "diffHunk": "@@ -0,0 +1,322 @@\n+<tde:template xml:lang=\"zxx\" xmlns:tde=\"http://marklogic.com/xdmp/tde\">\n+    <tde:description>\n+        Simplified version of the TDE template that's generated in the reference-entity-model project\n+    </tde:description>\n+    <tde:context>/*:envelope/*:instance[*:info/*:version = \"0.0.1\"][*:User]</tde:context>\n+    <tde:vars>\n+        <tde:var>\n+            <tde:name>RDF</tde:name>\n+            <tde:val>\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"</tde:val>\n+        </tde:var>\n+        <tde:var>\n+            <tde:name>RDF_TYPE</tde:name>\n+            <tde:val>sem:iri(concat($RDF, \"type\"))</tde:val>\n+        </tde:var>\n+    </tde:vars>\n+    <tde:path-namespaces>\n+        <tde:path-namespace>\n+            <tde:prefix>es</tde:prefix>\n+            <tde:namespace-uri>http://marklogic.com/entity-services</tde:namespace-uri>\n+        </tde:path-namespace>\n+    </tde:path-namespaces>\n+    <tde:templates>\n+        <tde:template>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDQ2NA==", "bodyText": "To make this future-proof (in case TDE introduces a new column type), let's say that if the type value is not in the map, we'll just use string.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528930464", "createdAt": "2020-11-23T19:02:43Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub/src/main/resources/ml-modules/root/marklogic-data-hub-spark-connector/readLib.sjs", "diffHunk": "@@ -170,7 +207,7 @@ function buildSchemaFieldsBasedOnTdeColumns(schemaName, viewName, selectedColumn\n     if (selectedColumns == null || selectedColumns.includes(columnName)) {\n       schemaFields.push({\n         name: columnName,\n-        type: column[\"sys.sys_columns.type\"],\n+        type: tdeToSparkDataTypeMap.get(column[\"sys.sys_columns.type\"]),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwMw==", "bodyText": "For clarity, just do \"InternalRow row : rows.get(0)\" since we know there's only one.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528930803", "createdAt": "2020-11-23T19:03:26Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMTQ2NQ==", "bodyText": "Both this and the date row should have assertion messages explaining why these are the expected values, as it's not clear to the casual reader", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528931465", "createdAt": "2020-11-23T19:04:38Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {\n+            //The comment in the assertions will have the datatype of the property in TDE template\n+            assertEquals(1, row.getInt(0), \"integer\");\n+            assertEquals(1, row.getInt(1), \"int\");\n+            assertEquals(Byte.decode(\"1\"), row.getByte(2),\"byte\");\n+            assertEquals(1, row.getInt(3),\"positiveInteger\");\n+            assertEquals(1, row.getInt(4), \"nonNegativeInteger\");\n+            assertEquals(1L, row.getLong(5), \"long\");\n+            assertEquals(1L, row.getLong(6), \"unsignedLong\");\n+            assertEquals(1L, row.getLong(7), \"unsignedInt\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(8), \"short\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(9), \"unsignedByte\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(10), \"unsignedShort\");\n+            assertEquals(\"John\", row.getString(11), \"string\");\n+            assertEquals(6218, row.get(12, DataTypes.DateType), \"date\");\n+            assertEquals(537336758000000L, row.get(13, DataTypes.TimestampType), \"dateTime\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjEyMg==", "bodyText": "For tests, just do throw new RuntimeException(e). \"fail\" is really for when you expect an exception to occur, but one doesn't - in which case, it's better to use assertThrows.", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528932122", "createdAt": "2020-11-23T19:05:36Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {\n+            //The comment in the assertions will have the datatype of the property in TDE template\n+            assertEquals(1, row.getInt(0), \"integer\");\n+            assertEquals(1, row.getInt(1), \"int\");\n+            assertEquals(Byte.decode(\"1\"), row.getByte(2),\"byte\");\n+            assertEquals(1, row.getInt(3),\"positiveInteger\");\n+            assertEquals(1, row.getInt(4), \"nonNegativeInteger\");\n+            assertEquals(1L, row.getLong(5), \"long\");\n+            assertEquals(1L, row.getLong(6), \"unsignedLong\");\n+            assertEquals(1L, row.getLong(7), \"unsignedInt\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(8), \"short\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(9), \"unsignedByte\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(10), \"unsignedShort\");\n+            assertEquals(\"John\", row.getString(11), \"string\");\n+            assertEquals(6218, row.get(12, DataTypes.DateType), \"date\");\n+            assertEquals(537336758000000L, row.get(13, DataTypes.TimestampType), \"dateTime\");\n+            assertEquals(\"20:12:38\", row.getString(14), \"time\");\n+            assertEquals(\"/John/IRI\", row.getString(15), \"IRI\");\n+            assertEquals(\"/John/IRI\", row.getString(16), \"anyURI\");\n+            assertEquals(true, row.getBoolean(17), \"boolean\");\n+            assertEquals(-10, row.getInt(18), \"negativeInteger\");\n+            assertEquals(-10, row.getInt(19), \"nonPositiveInteger\");\n+            assertEquals(30.5f, row.getFloat(20), \"float\");\n+            assertEquals(30.5d, row.getDouble(21), \"double\");\n+            assertEquals(30.5d, row.getDouble(22), \"decimal\");\n+            assertEquals(\"1987-01\", row.getString(23), \"gYearMonth\");\n+            assertEquals(\"1987+02:00\", row.getString(24), \"gYear\");\n+            assertEquals(\"--01+02:00\", row.getString(25), \"gMonth\");\n+            assertEquals(\"---10\", row.getString(26), \"gDay\");\n+            assertEquals(\"--01-10\", row.getString(27), \"gMonthDay\");\n+            assertEquals(\"P1M\", row.getString(28), \"duration\");\n+            assertEquals(\"P1M\", row.getString(29), \"yearMonthDuration\");\n+            assertEquals(\"P30DT1H\", row.getString(30), \"dayTimeDuration\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(31), \"point\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(32), \"longLatPoint\");\n+\n+            try {\n+                String base64BinaryString = new String(row.getBinary(33), \"UTF-8\");\n+                String hexBinaryString = new String(row.getBinary(34), \"UTF-8\");\n+                assertEquals(\"RHVtIHNwaXJvIHNwZXJv\", base64BinaryString, \"base64Binary\");\n+                assertEquals(\"44756D20737069726F20737065726F\", hexBinaryString, \"hexBinary\");\n+                String s1 = new String(Base64.decode(base64BinaryString), \"UTF-8\");\n+                String s2 = new String(Hex.decodeHex(hexBinaryString.toCharArray()), \"UTF-8\");\n+                assertEquals(\"Dum spiro spero\", s1);\n+                assertEquals(s1, s2);\n+\n+            } catch (Exception e) {\n+                logger.error(\"Failed to decode binary string: cause; \" + e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjM4Mw==", "bodyText": "Better yet - don't catch anything, just add \"throws Exception\" to the test method", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528932383", "createdAt": "2020-11-23T19:05:56Z", "author": {"login": "rjrudin"}, "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {\n+            //The comment in the assertions will have the datatype of the property in TDE template\n+            assertEquals(1, row.getInt(0), \"integer\");\n+            assertEquals(1, row.getInt(1), \"int\");\n+            assertEquals(Byte.decode(\"1\"), row.getByte(2),\"byte\");\n+            assertEquals(1, row.getInt(3),\"positiveInteger\");\n+            assertEquals(1, row.getInt(4), \"nonNegativeInteger\");\n+            assertEquals(1L, row.getLong(5), \"long\");\n+            assertEquals(1L, row.getLong(6), \"unsignedLong\");\n+            assertEquals(1L, row.getLong(7), \"unsignedInt\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(8), \"short\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(9), \"unsignedByte\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(10), \"unsignedShort\");\n+            assertEquals(\"John\", row.getString(11), \"string\");\n+            assertEquals(6218, row.get(12, DataTypes.DateType), \"date\");\n+            assertEquals(537336758000000L, row.get(13, DataTypes.TimestampType), \"dateTime\");\n+            assertEquals(\"20:12:38\", row.getString(14), \"time\");\n+            assertEquals(\"/John/IRI\", row.getString(15), \"IRI\");\n+            assertEquals(\"/John/IRI\", row.getString(16), \"anyURI\");\n+            assertEquals(true, row.getBoolean(17), \"boolean\");\n+            assertEquals(-10, row.getInt(18), \"negativeInteger\");\n+            assertEquals(-10, row.getInt(19), \"nonPositiveInteger\");\n+            assertEquals(30.5f, row.getFloat(20), \"float\");\n+            assertEquals(30.5d, row.getDouble(21), \"double\");\n+            assertEquals(30.5d, row.getDouble(22), \"decimal\");\n+            assertEquals(\"1987-01\", row.getString(23), \"gYearMonth\");\n+            assertEquals(\"1987+02:00\", row.getString(24), \"gYear\");\n+            assertEquals(\"--01+02:00\", row.getString(25), \"gMonth\");\n+            assertEquals(\"---10\", row.getString(26), \"gDay\");\n+            assertEquals(\"--01-10\", row.getString(27), \"gMonthDay\");\n+            assertEquals(\"P1M\", row.getString(28), \"duration\");\n+            assertEquals(\"P1M\", row.getString(29), \"yearMonthDuration\");\n+            assertEquals(\"P30DT1H\", row.getString(30), \"dayTimeDuration\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(31), \"point\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(32), \"longLatPoint\");\n+\n+            try {\n+                String base64BinaryString = new String(row.getBinary(33), \"UTF-8\");\n+                String hexBinaryString = new String(row.getBinary(34), \"UTF-8\");\n+                assertEquals(\"RHVtIHNwaXJvIHNwZXJv\", base64BinaryString, \"base64Binary\");\n+                assertEquals(\"44756D20737069726F20737065726F\", hexBinaryString, \"hexBinary\");\n+                String s1 = new String(Base64.decode(base64BinaryString), \"UTF-8\");\n+                String s2 = new String(Hex.decodeHex(hexBinaryString.toCharArray()), \"UTF-8\");\n+                assertEquals(\"Dum spiro spero\", s1);\n+                assertEquals(s1, s2);\n+\n+            } catch (Exception e) {\n+                logger.error(\"Failed to decode binary string: cause; \" + e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjEyMg=="}, "originalCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/e776e5d2209fbe40fe2fb8721dfb471850c713cd", "committedDate": "2020-11-23T21:22:33Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "committedDate": "2020-11-23T18:57:54Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}, "afterCommit": {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "author": {"user": {"login": "srinathgit", "name": "Srinath S"}}, "url": "https://github.com/marklogic/marklogic-data-hub/commit/e776e5d2209fbe40fe2fb8721dfb471850c713cd", "committedDate": "2020-11-23T21:22:33Z", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3NTA1MzQ5", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#pullrequestreview-537505349", "createdAt": "2020-11-24T13:46:27Z", "commit": {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3ODYwMDcx", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#pullrequestreview-537860071", "createdAt": "2020-11-24T20:00:11Z", "commit": {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1707, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}