{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzNDg1NjIy", "number": 1332, "title": "Support handling planned maintenance events submitted via a topic", "bodyText": "This PR resolves #1248.\nKnown issues:\n\nIf a detected maintenance event cannot be handled due to AnomalyDetectionStatus being not READY, it will be dropped. The support for stronger handling semantics will be added on later PRs.\nIdempotency support (handling duplicate maintenance events) will be added on later PRs.", "createdAt": "2020-09-26T04:38:07Z", "url": "https://github.com/linkedin/cruise-control/pull/1332", "merged": true, "mergeCommit": {"oid": "29fbbfa10f7be52a10df21cbba4f8b3d40f76992"}, "closed": true, "closedAt": "2020-10-01T20:18:16Z", "author": {"login": "efeg"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdMiwU2gH2gAyNDkzNDg1NjIyOmFhZjA2MTQwNWM0NDM4NjRmMzZkMmY4MDRhMDVlZTFhMDJhNmRkZTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOHzPqgH2gAyNDkzNDg1NjIyOmQwMzE1YWYwYzhiMTliNmYyM2Q2NzI3MDQwZjdjZWQ1MmE0MTVlYjk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/aaf061405c443864f36d2f804a05ee1a02a6dde5", "committedDate": "2020-09-26T04:33:53Z", "message": "Support handling planned maintenance events submitted via a topic"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MDI0NTU0", "url": "https://github.com/linkedin/cruise-control/pull/1332#pullrequestreview-498024554", "createdAt": "2020-09-29T02:39:38Z", "commit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMjozOTozOFrOHZWdsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMzo0MDo0OFrOHZXYPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NDQ5Ng==", "bodyText": "KMF aka XinfraMonitor has an interface which abstracts away topic creation.  This is sometimes useful if you need to stub out a noop topic creation or specific environments require additional steps for topic creation.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496344496", "createdAt": "2020-09-29T02:39:38Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/KafkaCruiseControlUtils.java", "diffHunk": "@@ -168,6 +195,194 @@ public static String getRequiredConfig(Map<String, ?> configs, String configName\n     return value;\n   }\n \n+  /**\n+   * Creates the given topic if it does not exist.\n+   *\n+   * @param adminClient The adminClient to send createTopics request.\n+   * @param topicToBeCreated A wrapper around the topic to be created.\n+   * @return {@code false} if the topic to be created already exists, {@code true} otherwise.\n+   */\n+  public static boolean createTopic(AdminClient adminClient, NewTopic topicToBeCreated) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0ODAyMw==", "bodyText": "What happens when the partition is empty, not containing any offsets?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496348023", "createdAt": "2020-09-29T02:53:45Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/KafkaCruiseControlUtils.java", "diffHunk": "@@ -168,6 +195,194 @@ public static String getRequiredConfig(Map<String, ?> configs, String configName\n     return value;\n   }\n \n+  /**\n+   * Creates the given topic if it does not exist.\n+   *\n+   * @param adminClient The adminClient to send createTopics request.\n+   * @param topicToBeCreated A wrapper around the topic to be created.\n+   * @return {@code false} if the topic to be created already exists, {@code true} otherwise.\n+   */\n+  public static boolean createTopic(AdminClient adminClient, NewTopic topicToBeCreated) {\n+    try {\n+      CreateTopicsResult createTopicsResult = adminClient.createTopics(Collections.singletonList(topicToBeCreated));\n+      createTopicsResult.values().get(topicToBeCreated.name()).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+      LOG.info(\"Topic {} has been created.\", topicToBeCreated.name());\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      if (e.getCause() instanceof TopicExistsException) {\n+        return false;\n+      }\n+      throw new IllegalStateException(String.format(\"Unable to create topic %s.\", topicToBeCreated.name()), e);\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   *\n+   * @param topic The name of the topic.\n+   * @param partitionCount Desired partition count.\n+   * @param replicationFactor Desired replication factor.\n+   * @param retentionMs Desired retention in milliseconds.\n+   * @return A wrapper around the topic with the given desired properties.\n+   */\n+  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n+    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n+      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n+                                                       + \" must be positive for the topic (%s).\", partitionCount,\n+                                                       replicationFactor, retentionMs, topic));\n+    }\n+\n+    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n+    Map<String, String> config = new HashMap<>(2);\n+    config.put(RetentionMsProp(), Long.toString(retentionMs));\n+    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n+    newTopic.configs(config);\n+\n+    return newTopic;\n+  }\n+\n+  /**\n+   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n+   *\n+   * @param configsToAlter A set of config altering operations to be populated.\n+   * @param desiredConfig Desired config value by name.\n+   * @param currentConfig Current config.\n+   */\n+  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n+    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n+      String configName = entry.getKey();\n+      String targetConfigValue = entry.getValue();\n+      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n+      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n+        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update topic configurations with the desired configs specified in the given topicToUpdateConfigs.\n+   *\n+   * @param adminClient The adminClient to send describeConfigs and incrementalAlterConfigs requests.\n+   * @param topicToUpdateConfigs Existing topic to update selected configs if needed -- cannot be {@code null}.\n+   * @return {@code true} if the request is completed successfully, {@code false} if there are any exceptions.\n+   */\n+  public static boolean maybeUpdateTopicConfig(AdminClient adminClient, NewTopic topicToUpdateConfigs) {\n+    String topicName = topicToUpdateConfigs.name();\n+    // Retrieve topic config to check if it needs an update.\n+    ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topicName);\n+    DescribeConfigsResult describeConfigsResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+    Config topicConfig;\n+    try {\n+      topicConfig = describeConfigsResult.values().get(topicResource).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      LOG.warn(\"Config check for topic {} failed due to failure to describe its configs.\", topicName, e);\n+      return false;\n+    }\n+\n+    // Update configs if needed.\n+    Map<String, String> desiredConfig = topicToUpdateConfigs.configs();\n+    if (desiredConfig != null) {\n+      Set<AlterConfigOp> alterConfigOps = new HashSet<>(desiredConfig.size());\n+      maybeUpdateConfig(alterConfigOps, desiredConfig, topicConfig);\n+      if (!alterConfigOps.isEmpty()) {\n+        AlterConfigsResult alterConfigsResult\n+            = adminClient.incrementalAlterConfigs(Collections.singletonMap(topicResource, alterConfigOps));\n+        try {\n+          alterConfigsResult.values().get(topicResource).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+        } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+          LOG.warn(\"Config change for topic {} failed.\", topicName, e);\n+          return false;\n+        }\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Increase the partition count of the given existing topic to the desired partition count (if needed).\n+   *\n+   * @param adminClient The adminClient to send describeTopics and createPartitions requests.\n+   * @param topicToAddPartitions Existing topic to add more partitions if needed -- cannot be {@code null}.\n+   * @return {@code true} if the request is completed successfully, {@code false} if there are any exceptions.\n+   */\n+  public static boolean maybeIncreasePartitionCount(AdminClient adminClient, NewTopic topicToAddPartitions) {\n+    String topicName = topicToAddPartitions.name();\n+\n+    // Retrieve partition count of topic to check if it needs a partition count update.\n+    TopicDescription topicDescription;\n+    try {\n+      topicDescription = adminClient.describeTopics(Collections.singletonList(topicName)).values()\n+                                    .get(topicName).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      LOG.warn(\"Partition count increase check for topic {} failed due to failure to describe cluster.\", topicName, e);\n+      return false;\n+    }\n+\n+    // Update partition count of topic if needed.\n+    if (topicDescription.partitions().size() < topicToAddPartitions.numPartitions()) {\n+      CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(\n+          Collections.singletonMap(topicName, NewPartitions.increaseTo(topicToAddPartitions.numPartitions())));\n+\n+      try {\n+        createPartitionsResult.values().get(topicName).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+      } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+        LOG.warn(\"Partition count increase to {} for topic {} failed{}.\", topicToAddPartitions.numPartitions(), topicName,\n+                 (e.getCause() instanceof ReassignmentInProgressException) ? \" due to ongoing reassignment\" : \"\", e);\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Sanity check whether there are failures in partition offsets fetched by a consumer. This typically happens due to\n+   * transient network failures (e.g. Error sending fetch request XXX to node XXX: org.apache.kafka.common.errors.DisconnectException.)\n+   * that prevents the consumer from getting offset from some brokers for as long as reconnect.backoff.ms.\n+   *\n+   * @param endOffsets End offsets retrieved by consumer.\n+   * @param offsetsForTimes Offsets for times retrieved by consumer.\n+   */\n+  public static void sanityCheckOffsetFetch(Map<TopicPartition, Long> endOffsets,\n+                                            Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes)\n+      throws SamplingException {\n+    Set<TopicPartition> failedToFetchOffsets = new HashSet<>();\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      if (entry.getValue() == null && endOffsets.get(entry.getKey()) == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MjI5MA==", "bodyText": "Do you want to add a magic number and/or CRC to detect corruption?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496352290", "createdAt": "2020-09-29T03:10:22Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AddBrokerPlan.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+\n+public class AddBrokerPlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+  private final Set<Integer> _brokers;\n+\n+  public AddBrokerPlan(long timeMs, int brokerId, Set<Integer> brokers) {\n+    super(MaintenanceEventType.ADD_BROKER, timeMs, brokerId);\n+    if (brokers == null || brokers.isEmpty()) {\n+      throw new IllegalArgumentException(\"Missing brokers for the plan.\");\n+    }\n+    if (brokers.size() > Short.MAX_VALUE) {\n+      throw new IllegalArgumentException(String.format(\"Cannot add more than %d brokers (attempt: %d).\",\n+                                                       Short.MAX_VALUE, brokers.size()));\n+    }\n+    _brokers = brokers;\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  public Set<Integer> brokers() {\n+    return _brokers;\n+  }\n+\n+  @Override\n+  public ByteBuffer toBuffer(int headerSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA==", "bodyText": "Randomly generating the group id makes it impossible to track offset commits using lagmontor.  Randomly generating the client id breaks quotas based on client ids.  Also each time there is a new client id a new set of metrics are generated on both the broker and the client itself.  Since the brokers are probably longer lived than the clients this results is accumulated garbage.  Open source kafka does periodically clean it out but for some reason we've opted to keep them around in memory forever.  Recommend never having random numbers be part of the group or client ids.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496353768", "createdAt": "2020-09-29T03:16:34Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AnomalyDetectorUtils.java", "diffHunk": "@@ -28,10 +38,42 @@\n   public static final String ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG = \"anomaly.detection.time.ms.object\";\n   public static final long MAX_METADATA_WAIT_MS = 60000L;\n   public static final Anomaly SHUTDOWN_ANOMALY = new BrokerFailures();\n+  public static final Random RANDOM = new Random();\n \n   private AnomalyDetectorUtils() {\n   }\n \n+  /**\n+   * Create a Kafka consumer for retrieving reported Maintenance plans.\n+   * The consumer uses {@link String} for keys and {@link MaintenancePlan} for values.\n+   *\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n+   */\n+  public static Consumer<String, MaintenancePlan> createMaintenanceEventConsumer(Map<String, ?> configs, String groupIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.\n+    if (bootstrapServers.length() > 2) {\n+      bootstrapServers = bootstrapServers.substring(1, bootstrapServers.length() - 1);\n+    }\n+\n+    // Create consumer\n+    long randomToken = RANDOM.nextLong();\n+    Properties consumerProps = new Properties();\n+    consumerProps.putAll(configs);\n+    consumerProps.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+    consumerProps.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupIdPrefix + randomToken);\n+    consumerProps.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, groupIdPrefix + \"-consumer-\" + randomToken);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA==", "bodyText": "The output of this method is not guaranteed to be stable across Java versions.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496354558", "createdAt": "2020-09-29T03:19:43Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AnomalyDetectorUtils.java", "diffHunk": "@@ -28,10 +38,42 @@\n   public static final String ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG = \"anomaly.detection.time.ms.object\";\n   public static final long MAX_METADATA_WAIT_MS = 60000L;\n   public static final Anomaly SHUTDOWN_ANOMALY = new BrokerFailures();\n+  public static final Random RANDOM = new Random();\n \n   private AnomalyDetectorUtils() {\n   }\n \n+  /**\n+   * Create a Kafka consumer for retrieving reported Maintenance plans.\n+   * The consumer uses {@link String} for keys and {@link MaintenancePlan} for values.\n+   *\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n+   */\n+  public static Consumer<String, MaintenancePlan> createMaintenanceEventConsumer(Map<String, ?> configs, String groupIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Njk2MQ==", "bodyText": "I've always thought this was strange as this can be implemented as\n  byte id() {\n    return (byte) ordinal();\n  }", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496356961", "createdAt": "2020-09-29T03:30:01Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventType.java", "diffHunk": "@@ -23,7 +23,38 @@\n  * </ul>\n  */\n public enum MaintenanceEventType {\n-  ADD_BROKER, REMOVE_BROKER, FIX_OFFLINE_REPLICAS, REBALANCE, DEMOTE_BROKER, TOPIC_REPLICATION_FACTOR;\n+  // Do not change the order of enums. Append new ones to the end.\n+  ADD_BROKER((byte) 0),\n+  REMOVE_BROKER((byte) 1),\n+  FIX_OFFLINE_REPLICAS((byte) 2),\n+  REBALANCE((byte) 3),\n+  DEMOTE_BROKER((byte) 4),\n+  TOPIC_REPLICATION_FACTOR((byte) 5);\n+\n+  // This id helps with serialization and deserialization of event types\n+  private final byte _id;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzM5NQ==", "bodyText": "Seems like this protocol is going to have many different types of messages, but low QPS.  Seems like a good use case for some kind of serialization library.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496357395", "createdAt": "2020-09-29T03:31:46Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenancePlan.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import java.nio.ByteBuffer;\n+\n+\n+/**\n+ * An abstract class for all maintenance plans.\n+ */\n+public abstract class MaintenancePlan {\n+  protected final MaintenanceEventType _maintenanceEventType;\n+  protected final long _timeMs;\n+  protected final int _brokerId;\n+\n+  public MaintenancePlan(MaintenanceEventType maintenanceEventType, long timeMs, int brokerId) {\n+    _maintenanceEventType = maintenanceEventType;\n+    _timeMs = timeMs;\n+    _brokerId = brokerId;\n+  }\n+\n+  /**\n+   * Retrieve the maintenance event type of this maintenance plan.\n+   *\n+   * @return the maintenance event type of this maintenance plan, which is stored in the serialized metrics so\n+   * that the deserializer will know which class should be used to deserialize the data.\n+   */\n+  public MaintenanceEventType maintenanceEventType() {\n+    return _maintenanceEventType;\n+  }\n+\n+  /**\n+   * @return The timestamp in ms that corresponds to the generation of this plan.\n+   */\n+  public long timeMs() {\n+    return _timeMs;\n+  }\n+\n+  /**\n+   * @return The id of the broker that reported this maintenance event.\n+   */\n+  public int brokerId() {\n+    return _brokerId;\n+  }\n+\n+  /**\n+   * @return The plan version for serialization/deserialization.\n+   */\n+  public abstract byte planVersion();\n+\n+  /**\n+   * Serialize the maintenance plan to a byte buffer with the header size reserved.\n+   *\n+   * @param headerSize the header size to reserve.\n+   * @return A {@link ByteBuffer} with header size reserved at the beginning.\n+   */\n+  public ByteBuffer toBuffer(int headerSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTE2MA==", "bodyText": "See comment about group id", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496359160", "createdAt": "2020-09-29T03:39:29Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/SamplingUtils.java", "diffHunk": "@@ -346,124 +315,70 @@ private static boolean skipBuildingBrokerMetricSample(BrokerLoad brokerLoad, int\n   }\n \n   /**\n-   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   * Create a Kafka consumer for retrieving reported Cruise Control metrics.\n+   * The consumer uses {@link String} for keys and {@link CruiseControlMetric} for values.\n    *\n-   * @param topic The name of the topic.\n-   * @param partitionCount Desired partition count.\n-   * @param replicationFactor Desired replication factor.\n-   * @param retentionMs Desired retention in milliseconds.\n-   * @return A wrapper around the topic with the given desired properties.\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n    */\n-  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n-    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n-      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n-                                                       + \" must be positive for the topic (%s).\", partitionCount,\n-                                                       replicationFactor, retentionMs, topic));\n+  public static Consumer<String, CruiseControlMetric> createMetricConsumer(Map<String, ?> configs) {\n+    // Get bootstrap servers\n+    String bootstrapServers = (String) configs.get(METRIC_REPORTER_SAMPLER_BOOTSTRAP_SERVERS);\n+    if (bootstrapServers == null) {\n+      bootstrapServers = bootstrapServers(configs);\n     }\n-\n-    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n-    Map<String, String> config = new HashMap<>(2);\n-    config.put(RetentionMsProp(), Long.toString(retentionMs));\n-    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n-    newTopic.configs(config);\n-\n-    return newTopic;\n-  }\n-\n-  /**\n-   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n-   *\n-   * @param configsToAlter A set of config altering operations to be populated.\n-   * @param desiredConfig Desired config value by name.\n-   * @param currentConfig Current config.\n-   */\n-  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n-    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n-      String configName = entry.getKey();\n-      String targetConfigValue = entry.getValue();\n-      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n-      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n-        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n-      }\n+    // Get group id\n+    long randomToken = RANDOM.nextLong();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTQ4Ng==", "bodyText": "This can probably be a much smaller topic.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496359486", "createdAt": "2020-09-29T03:40:48Z", "author": {"login": "smccauliff"}, "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -245,6 +246,14 @@ We are still trying to improve cruise control. And following are some configurat\n | min.broker.sample.store.topic.retention.time.ms       | Integer | N         | 3600000       | The config for the minimal retention time for Kafka broker sample store topic                                                                                                                           |\n | skip.sample.store.topic.rack.awareness.check          | Boolean | N         | false         | The config to skip rack awareness sanity check for sample store topics                                                                                                                                  |\n \n+### MaintenanceEventTopicReader configurations\n+| Name                                          | Type    | Required? | Default Value           | Description                                                       |\n+|-----------------------------------------------|---------|-----------|-------------------------|-------------------------------------------------------------------|\n+| maintenance.event.topic                       | String  | N         | __MaintenanceEvent      | The name of the Kafka topic to consume maintenance events from    |\n+| maintenance.event.topic.replication.factor    | Short   | N         | min(2, #alive-brokers)  | The replication factor of the maintenance event topic             |\n+| maintenance.event.topic.partition.count       | Integer | N         | 32                      | The partition count of the maintenance event topic                |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/174414248d750a9b9bd7ebe449348efb0ae7921d", "committedDate": "2020-09-30T03:01:21Z", "message": "Address the feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5MDc2MTc2", "url": "https://github.com/linkedin/cruise-control/pull/1332#pullrequestreview-499076176", "createdAt": "2020-09-30T04:28:47Z", "commit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNDoyODo0N1rOHaM5Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNToxNjozOFrOHaNmzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzNjMxOQ==", "bodyText": "The magic number is there just incase someone produces garbage into your topic.  This dosen't happen often but it does happen.  For example our internal message format has a magic byte '0' there have been times when 1/256 of the invalid messages that where incorrectly produced into a topic would pass this check.  This can sometimes lead to OOM when the bytes are deserialized as the length field is bad.  For example this happens when client is not configured with TLS then connects to the TLS port on the broker.  It interprets the TLS handshake as some length field and the client OOMs. Having a magic number (preferably more than one byte) helps prevent this.  CRC should prevent this as well.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497236319", "createdAt": "2020-09-30T04:28:47Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AddBrokerPlan.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+\n+public class AddBrokerPlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+  private final Set<Integer> _brokers;\n+\n+  public AddBrokerPlan(long timeMs, int brokerId, Set<Integer> brokers) {\n+    super(MaintenanceEventType.ADD_BROKER, timeMs, brokerId);\n+    if (brokers == null || brokers.isEmpty()) {\n+      throw new IllegalArgumentException(\"Missing brokers for the plan.\");\n+    }\n+    if (brokers.size() > Short.MAX_VALUE) {\n+      throw new IllegalArgumentException(String.format(\"Cannot add more than %d brokers (attempt: %d).\",\n+                                                       Short.MAX_VALUE, brokers.size()));\n+    }\n+    _brokers = brokers;\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  public Set<Integer> brokers() {\n+    return _brokers;\n+  }\n+\n+  @Override\n+  public ByteBuffer toBuffer(int headerSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MjI5MA=="}, "originalCommit": {"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzOTEzNg==", "bodyText": "What's the advantage of doing offset management this way?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497239136", "createdAt": "2020-09-30T04:41:10Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;\n+  // A maintenance event has a certain validity period after which it expires and becomes invalid. A delay in handling\n+  // could be introduced by the Kafka producer that generates the plan, the consumer that retrieves the plan, or the\n+  // network that connects these clients to the Kafka cluster. Maintenance event topic reader discards the expired events.\n+  protected long _maintenancePlanExpirationMs;\n+\n+  public static final String MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG = \"maintenance.plan.expiration.ms\";\n+  public static final long DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS = Duration.ofMinutes(15).toMillis();\n+  public static final String MAINTENANCE_EVENT_TOPIC_CONFIG = \"maintenance.event.topic\";\n+  public static final String DEFAULT_MAINTENANCE_EVENT_TOPIC = \"__MaintenanceEvent\";\n+  public static final String MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG = \"maintenance.event.topic.replication.factor\";\n+  public static final short DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR = 2;\n+  public static final String MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG = \"maintenance.event.topic.partition.count\";\n+  public static final int DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT = 8;\n+  public static final String MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG = \"maintenance.event.topic.retention.ms\";\n+  public static final long DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS = Duration.ofHours(6).toMillis();\n+  public static final Duration CONSUMER_CLOSE_TIMEOUT = Duration.ofSeconds(2);\n+  public static final String CONSUMER_CLIENT_ID_PREFIX = MaintenanceEventTopicReader.class.getSimpleName();\n+  // How far should topic reader initially (i.e. upon startup) look back in the history for maintenance events.\n+  public static final long INIT_MAINTENANCE_HISTORY_MS = 60000L;\n+\n+  /**\n+   * Seek to the relevant offsets (i.e. either (1) end time of the last event read period or (2) end offset) that the\n+   * consumer will use on the next poll from each partition.\n+   *\n+   * @return End offsets by the partitions to be consumed.\n+   */\n+  protected Map<TopicPartition, Long> seekToRelevantOffsets() throws SamplingException {\n+    Map<TopicPartition, Long> timestampToSeek = new HashMap<>(_currentPartitionAssignment.size());\n+    for (TopicPartition tp : _currentPartitionAssignment) {\n+      timestampToSeek.put(tp, _lastEventReadPeriodEndTimeMs);\n+    }\n+    Set<TopicPartition> assignment = new HashSet<>(_currentPartitionAssignment);\n+    Map<TopicPartition, Long> endOffsets = _consumer.endOffsets(assignment);\n+    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = _consumer.offsetsForTimes(timestampToSeek);\n+    sanityCheckOffsetFetch(endOffsets, offsetsForTimes);\n+\n+    // If offsets for times are provided for a partition, seek to the returned offset. Otherwise, i.e. for partitions\n+    // without a record timestamp greater than or equal to the target timestamp, seek to the end offset.\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      TopicPartition tp = entry.getKey();\n+      OffsetAndTimestamp offsetAndTimestamp = entry.getValue();\n+      _consumer.seek(tp, offsetAndTimestamp != null ? offsetAndTimestamp.offset() : endOffsets.get(tp));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0MTAyNA==", "bodyText": "This creates some kind of strange circular dependency.  Consider replacing this with relevant interfaces.  For example time can be abstracted with https://docs.oracle.com/javase/8/docs/api/java/time/Clock.html presumably the config object related to the KCC object also has some interface or an object of that class can just be referenced directly.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497241024", "createdAt": "2020-09-30T04:49:04Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjQyMA==", "bodyText": "Not all the producers will have the same clock and so time is not monotonically increasing.  Does this still work if that is the case?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497246420", "createdAt": "2020-09-30T05:10:30Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;\n+  // A maintenance event has a certain validity period after which it expires and becomes invalid. A delay in handling\n+  // could be introduced by the Kafka producer that generates the plan, the consumer that retrieves the plan, or the\n+  // network that connects these clients to the Kafka cluster. Maintenance event topic reader discards the expired events.\n+  protected long _maintenancePlanExpirationMs;\n+\n+  public static final String MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG = \"maintenance.plan.expiration.ms\";\n+  public static final long DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS = Duration.ofMinutes(15).toMillis();\n+  public static final String MAINTENANCE_EVENT_TOPIC_CONFIG = \"maintenance.event.topic\";\n+  public static final String DEFAULT_MAINTENANCE_EVENT_TOPIC = \"__MaintenanceEvent\";\n+  public static final String MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG = \"maintenance.event.topic.replication.factor\";\n+  public static final short DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR = 2;\n+  public static final String MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG = \"maintenance.event.topic.partition.count\";\n+  public static final int DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT = 8;\n+  public static final String MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG = \"maintenance.event.topic.retention.ms\";\n+  public static final long DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS = Duration.ofHours(6).toMillis();\n+  public static final Duration CONSUMER_CLOSE_TIMEOUT = Duration.ofSeconds(2);\n+  public static final String CONSUMER_CLIENT_ID_PREFIX = MaintenanceEventTopicReader.class.getSimpleName();\n+  // How far should topic reader initially (i.e. upon startup) look back in the history for maintenance events.\n+  public static final long INIT_MAINTENANCE_HISTORY_MS = 60000L;\n+\n+  /**\n+   * Seek to the relevant offsets (i.e. either (1) end time of the last event read period or (2) end offset) that the\n+   * consumer will use on the next poll from each partition.\n+   *\n+   * @return End offsets by the partitions to be consumed.\n+   */\n+  protected Map<TopicPartition, Long> seekToRelevantOffsets() throws SamplingException {\n+    Map<TopicPartition, Long> timestampToSeek = new HashMap<>(_currentPartitionAssignment.size());\n+    for (TopicPartition tp : _currentPartitionAssignment) {\n+      timestampToSeek.put(tp, _lastEventReadPeriodEndTimeMs);\n+    }\n+    Set<TopicPartition> assignment = new HashSet<>(_currentPartitionAssignment);\n+    Map<TopicPartition, Long> endOffsets = _consumer.endOffsets(assignment);\n+    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = _consumer.offsetsForTimes(timestampToSeek);\n+    sanityCheckOffsetFetch(endOffsets, offsetsForTimes);\n+\n+    // If offsets for times are provided for a partition, seek to the returned offset. Otherwise, i.e. for partitions\n+    // without a record timestamp greater than or equal to the target timestamp, seek to the end offset.\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      TopicPartition tp = entry.getKey();\n+      OffsetAndTimestamp offsetAndTimestamp = entry.getValue();\n+      _consumer.seek(tp, offsetAndTimestamp != null ? offsetAndTimestamp.offset() : endOffsets.get(tp));\n+    }\n+\n+    return endOffsets;\n+  }\n+\n+  protected void addMaintenancePlan(MaintenancePlan maintenancePlan, Set<MaintenanceEvent> maintenanceEvents) {\n+    LOG.debug(\"Retrieved maintenance plan {}.\", maintenancePlan);\n+    Map<String, Object> parameterConfigOverrides = new HashMap<>(4);\n+    parameterConfigOverrides.put(KAFKA_CRUISE_CONTROL_OBJECT_CONFIG, _kafkaCruiseControl);\n+    parameterConfigOverrides.put(ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG, _kafkaCruiseControl.timeMs());\n+    parameterConfigOverrides.put(MAINTENANCE_EVENT_TYPE_CONFIG, maintenancePlan.maintenanceEventType());\n+    switch (maintenancePlan.maintenanceEventType()) {\n+      case ADD_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((AddBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case REMOVE_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((RemoveBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case FIX_OFFLINE_REPLICAS:\n+      case REBALANCE:\n+        break;\n+      case DEMOTE_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((DemoteBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case TOPIC_REPLICATION_FACTOR:\n+        parameterConfigOverrides.put(TOPICS_WITH_RF_UPDATE_CONFIG, ((TopicReplicationFactorPlan) maintenancePlan).topicRegexWithRFUpdate());\n+        break;\n+      default:\n+        throw new IllegalStateException(String.format(\"Unrecognized event type %s\", maintenancePlan.maintenanceEventType()));\n+    }\n+    maintenanceEvents.add(_kafkaCruiseControl.config().getConfiguredInstance(AnomalyDetectorConfig.MAINTENANCE_EVENT_CLASS_CONFIG,\n+                                                                             MaintenanceEvent.class,\n+                                                                             parameterConfigOverrides));\n+  }\n+\n+  /**\n+   * See {@link MaintenanceEventReader#readEvents(Duration)}\n+   * This method may block beyond the timeout in order to execute additional logic to create maintenance events from the\n+   * maintenance plans retrieved from the relevant topic.\n+   *\n+   * @param timeout The maximum time to block for retrieving records from the relevant topic.\n+   * @return Set of maintenance events, or empty set if none is available after the given timeout expires.\n+   */\n+  @Override\n+  public Set<MaintenanceEvent> readEvents(Duration timeout) throws SamplingException {\n+    LOG.debug(\"Reading maintenance events.\");\n+    long eventReadPeriodEndMs = _kafkaCruiseControl.timeMs();\n+    if (refreshPartitionAssignment()) {\n+      _lastEventReadPeriodEndTimeMs = eventReadPeriodEndMs;\n+      return Collections.emptySet();\n+    }\n+\n+    long timeoutEndMs = eventReadPeriodEndMs + timeout.toMillis();\n+    Set<MaintenanceEvent> maintenanceEvents = new HashSet<>();\n+    try {\n+      Map<TopicPartition, Long> endOffsets = seekToRelevantOffsets();\n+      LOG.debug(\"Started to consume from maintenance event topic partitions {}.\", _currentPartitionAssignment);\n+      _consumer.resume(_consumer.paused());\n+      Set<TopicPartition> partitionsToPause = new HashSet<>();\n+\n+      do {\n+        ConsumerRecords<String, MaintenancePlan> records = _consumer.poll(timeout);\n+        for (ConsumerRecord<String, MaintenancePlan> record : records) {\n+          if (record == null) {\n+            // This means that the record cannot be parsed because the maintenance event type is not recognized.\n+            // It might happen when newer type of maintenance events have been added and the current code is still old.\n+            // We simply ignore that metric in this case (see MaintenancePlanSerde#fromBytes).\n+            LOG.warn(\"Cannot parse record, please update your Cruise Control version.\");\n+            continue;\n+          }\n+\n+          long planGenerationTimeMs = record.value().timeMs();\n+          if (planGenerationTimeMs + _maintenancePlanExpirationMs < eventReadPeriodEndMs) {\n+            LOG.warn(\"Discarding the expired plan {}. (Expired: {} Evaluated: {}).\", record.value(),\n+                     planGenerationTimeMs + _maintenancePlanExpirationMs, eventReadPeriodEndMs);\n+          } else if (planGenerationTimeMs >= eventReadPeriodEndMs) {\n+            TopicPartition tp = new TopicPartition(record.topic(), record.partition());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjkzMQ==", "bodyText": "What does it mean to rebalance a particular broker?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497246931", "createdAt": "2020-09-30T05:12:39Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/RebalancePlan.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+\n+\n+public class RebalancePlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+\n+  public RebalancePlan(long timeMs, int brokerId) {\n+    super(MaintenanceEventType.REBALANCE, timeMs, brokerId);\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  /**\n+   * Deserialize given byte buffer to a {@link RebalancePlan}.\n+   *\n+   * @param headerSize The header size of the buffer.\n+   * @param buffer buffer to deserialize.\n+   * @return The {@link RebalancePlan} corresponding to the deserialized buffer.\n+   */\n+  public static RebalancePlan fromBuffer(int headerSize, ByteBuffer buffer) throws UnknownVersionException {\n+    verifyCrc(headerSize, buffer);\n+    byte version = buffer.get();\n+    if (version > PLAN_VERSION) {\n+      throw new UnknownVersionException(\"Cannot deserialize the plan for version \" + version + \". Current version: \" + PLAN_VERSION);\n+    }\n+\n+    long timeMs = buffer.getLong();\n+    int brokerId = buffer.getInt();\n+\n+    return new RebalancePlan(timeMs, brokerId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NzU5Nw==", "bodyText": "Do you want to validate the regex here?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497247597", "createdAt": "2020-09-30T05:15:12Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/TopicReplicationFactorPlan.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+public class TopicReplicationFactorPlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+  // A map containing the regex of topics by the corresponding desired replication factor\n+  private final Map<Short, String> _topicRegexWithRFUpdate;\n+\n+  public TopicReplicationFactorPlan(long timeMs, int brokerId, Map<Short, String> topicRegexWithRFUpdate) {\n+    super(MaintenanceEventType.TOPIC_REPLICATION_FACTOR, timeMs, brokerId);\n+    if (topicRegexWithRFUpdate == null || topicRegexWithRFUpdate.isEmpty()) {\n+      throw new IllegalArgumentException(\"Missing replication factor updates for the plan.\");\n+    }\n+    // Sanity check: the number of bulk updates for replication factor of topics.\n+    if (topicRegexWithRFUpdate.size() > Byte.MAX_VALUE) {\n+      throw new IllegalArgumentException(String.format(\"Cannot update more than %d different replication factor (attempt: %d).\",\n+                                                       Byte.MAX_VALUE, topicRegexWithRFUpdate.size()));\n+    }\n+    // Sanity check: Each regex must have some value.\n+    for (String regex : topicRegexWithRFUpdate.values()) {\n+      if (regex == null || regex.isEmpty()) {\n+        throw new IllegalArgumentException(\"Missing topics of the replication factor update for the plan.\");\n+      }\n+    }\n+    _topicRegexWithRFUpdate = topicRegexWithRFUpdate;\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  public Map<Short, String> topicRegexWithRFUpdate() {\n+    return _topicRegexWithRFUpdate;\n+  }\n+\n+  @Override\n+  public ByteBuffer toBuffer(int headerSize) {\n+    byte numRFUpdateEntries = (byte) _topicRegexWithRFUpdate.size();\n+    int requiredCapacityForRFEntries = 0;\n+    for (Map.Entry<Short, String> entry : _topicRegexWithRFUpdate.entrySet()) {\n+      requiredCapacityForRFEntries += (Short.BYTES /* replication factor */ + Integer.BYTES /* regex length */\n+                                       + entry.getValue().getBytes(StandardCharsets.UTF_8).length /* regex */);\n+    }\n+    int contentSize = (Byte.BYTES /* plan version */\n+                       + Long.BYTES /* timeMs */\n+                       + Integer.BYTES /* broker id */\n+                       + Byte.BYTES /* number of replication factor update entries */\n+                       + requiredCapacityForRFEntries /* total capacity for all entries */);\n+    ByteBuffer buffer = ByteBuffer.allocate(headerSize + Long.BYTES /* crc */ + contentSize);\n+    buffer.position(headerSize + Long.BYTES);\n+    buffer.put(planVersion());\n+    buffer.putLong(timeMs());\n+    buffer.putInt(brokerId());\n+    buffer.put(numRFUpdateEntries);\n+    for (Map.Entry<Short, String> entry : _topicRegexWithRFUpdate.entrySet()) {\n+      buffer.putShort(entry.getKey());\n+      byte[] regex = entry.getValue().getBytes(StandardCharsets.UTF_8);\n+      buffer.putInt(regex.length);\n+      buffer.put(regex);\n+    }\n+    putCrc(headerSize, buffer, contentSize);\n+    return buffer;\n+  }\n+\n+  /**\n+   * Deserialize given byte buffer to an {@link TopicReplicationFactorPlan}.\n+   *\n+   * @param headerSize The header size of the buffer.\n+   * @param buffer buffer to deserialize.\n+   * @return The {@link TopicReplicationFactorPlan} corresponding to the deserialized buffer.\n+   */\n+  public static TopicReplicationFactorPlan fromBuffer(int headerSize, ByteBuffer buffer) throws UnknownVersionException {\n+    verifyCrc(headerSize, buffer);\n+    byte version = buffer.get();\n+    if (version > PLAN_VERSION) {\n+      throw new UnknownVersionException(\"Cannot deserialize the plan for version \" + version + \". Current version: \" + PLAN_VERSION);\n+    }\n+\n+    long timeMs = buffer.getLong();\n+    int brokerId = buffer.getInt();\n+    byte numRFUpdateEntries = buffer.get();\n+    Map<Short, String> topicRegexWithRFUpdate = new HashMap<>(numRFUpdateEntries);\n+\n+    for (int i = 0; i < numRFUpdateEntries; i++) {\n+      short replicationFactor = buffer.getShort();\n+      int regexLength = buffer.getInt();\n+      String regex = new String(buffer.array(), buffer.arrayOffset() + buffer.position(), regexLength, StandardCharsets.UTF_8);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0Nzk0OA==", "bodyText": "Is this duplicated elsewhere?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497247948", "createdAt": "2020-09-30T05:16:38Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/SamplingUtils.java", "diffHunk": "@@ -346,124 +313,70 @@ private static boolean skipBuildingBrokerMetricSample(BrokerLoad brokerLoad, int\n   }\n \n   /**\n-   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   * Create a Kafka consumer for retrieving reported Cruise Control metrics.\n+   * The consumer uses {@link String} for keys and {@link CruiseControlMetric} for values.\n+   *\n+   * This consumer is not intended to use (1) the group management functionality by using subscribe(topic) or (2) the Kafka-based\n+   * offset management strategy. Hence, the {@link ConsumerConfig#GROUP_ID_CONFIG} config is irrelevant to it.\n    *\n-   * @param topic The name of the topic.\n-   * @param partitionCount Desired partition count.\n-   * @param replicationFactor Desired replication factor.\n-   * @param retentionMs Desired retention in milliseconds.\n-   * @return A wrapper around the topic with the given desired properties.\n+   * @param configs The configurations for Cruise Control.\n+   * @param clientIdPrefix Client id prefix.\n+   * @return A new Kafka consumer\n    */\n-  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n-    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n-      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n-                                                       + \" must be positive for the topic (%s).\", partitionCount,\n-                                                       replicationFactor, retentionMs, topic));\n+  public static Consumer<String, CruiseControlMetric> createMetricConsumer(Map<String, ?> configs, String clientIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = (String) configs.get(METRIC_REPORTER_SAMPLER_BOOTSTRAP_SERVERS);\n+    if (bootstrapServers == null) {\n+      bootstrapServers = bootstrapServers(configs);\n     }\n \n-    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n-    Map<String, String> config = new HashMap<>(2);\n-    config.put(RetentionMsProp(), Long.toString(retentionMs));\n-    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n-    newTopic.configs(config);\n-\n-    return newTopic;\n+    // Create consumer\n+    long randomToken = RANDOM.nextLong();\n+    Properties consumerProps = new Properties();\n+    consumerProps.putAll(configs);\n+    consumerProps.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+    consumerProps.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, clientIdPrefix + \"-consumer-\" + randomToken);\n+    consumerProps.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\");\n+    consumerProps.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n+    consumerProps.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, Integer.toString(Integer.MAX_VALUE));\n+    consumerProps.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+    consumerProps.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, MetricSerde.class.getName());\n+    consumerProps.setProperty(ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG, configs.get(RECONNECT_BACKOFF_MS_CONFIG).toString());\n+    return new KafkaConsumer<>(consumerProps);\n   }\n \n-  /**\n-   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n-   *\n-   * @param configsToAlter A set of config altering operations to be populated.\n-   * @param desiredConfig Desired config value by name.\n-   * @param currentConfig Current config.\n-   */\n-  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n-    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n-      String configName = entry.getKey();\n-      String targetConfigValue = entry.getValue();\n-      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n-      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n-        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n-      }\n+  private static String bootstrapServers(Map<String, ?> configs) {\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.\n+    if (bootstrapServers.length() > 2) {\n+      bootstrapServers = bootstrapServers.substring(1, bootstrapServers.length() - 1);\n     }\n+    return bootstrapServers;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 174}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ffcf0ca2ef5f1e29a428047bac3f63b1a2c414e", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/4ffcf0ca2ef5f1e29a428047bac3f63b1a2c414e", "committedDate": "2020-09-30T19:51:23Z", "message": "Address the feedback."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5NzIzNTcz", "url": "https://github.com/linkedin/cruise-control/pull/1332#pullrequestreview-499723573", "createdAt": "2020-09-30T18:57:07Z", "commit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxODo1NzowN1rOHarKbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxOTowMDowMlrOHarQ7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMjIwNg==", "bodyText": "I don't understand what this plan is supposed to do.  Can there be some javadoc here to explain the purpose of this class.", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497732206", "createdAt": "2020-09-30T18:57:07Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/TopicReplicationFactorPlan.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+public class TopicReplicationFactorPlan extends MaintenancePlan {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMzg3MQ==", "bodyText": "Do admin clients actually use the broker specified in bootstrap servers or do they refresh metadata from bootstrap server and then choose one of the brokers specified in the metadata?", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497733871", "createdAt": "2020-09-30T19:00:02Z", "author": {"login": "smccauliff"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReaderTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.CruiseControlIntegrationTestHarness;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.easymock.EasyMock;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventType.REBALANCE;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+\n+\n+public class MaintenanceEventTopicReaderTest extends CruiseControlIntegrationTestHarness {\n+  public static final String TEST_TOPIC = \"__CloudMaintenanceEvent\";\n+  public static final String TEST_TOPIC_REPLICATION_FACTOR = \"1\";\n+  public static final String TEST_TOPIC_PARTITION_COUNT = \"8\";\n+  public static final String TEST_TOPIC_RETENTION_TIME_MS = \"3600000\";\n+  public static final String RETENTION_MS_CONFIG = \"retention.ms\";\n+  public static final long TEST_REBALANCE_PLAN_TIME = 1601089200000L;\n+  public static final long TEST_EXPIRED_PLAN_TIME = TEST_REBALANCE_PLAN_TIME - 1L;\n+  public static final int TEST_BROKER_ID = 42;\n+  public static final Duration TEST_TIMEOUT = Duration.ofSeconds(5);\n+  public static final Map<Short, String> TEST_TOPIC_REGEX_WITH_RF_UPDATE = Collections.singletonMap((short) 2, \"T1\");\n+  private TopicDescription _topicDescription;\n+  private Config _topicConfig;\n+\n+  /**\n+   * Setup the unit test and produce maintenance plans to the maintenance topic.\n+   */\n+  @Before\n+  public void setup() throws Exception {\n+    super.start();\n+    produceMaintenancePlans();\n+  }\n+\n+  /**\n+   * Produces plans to the {@link #TEST_TOPIC}.\n+   * All plans except {@link RebalancePlan} are expired.\n+   */\n+  private void produceMaintenancePlans() {\n+    Properties props = new Properties();\n+    props.setProperty(ProducerConfig.ACKS_CONFIG, \"-1\");\n+    try (Producer<String, MaintenancePlan> producer = createMaintenancePlanProducer(props)) {\n+      sendPlan(producer, new RebalancePlan(TEST_REBALANCE_PLAN_TIME, TEST_BROKER_ID));\n+      sendPlan(producer, new FixOfflineReplicasPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID));\n+      sendPlan(producer, new DemoteBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new AddBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new RemoveBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new TopicReplicationFactorPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, TEST_TOPIC_REGEX_WITH_RF_UPDATE));\n+    }\n+  }\n+\n+  @javax.annotation.Nonnull\n+  protected Producer<String, MaintenancePlan> createMaintenancePlanProducer(Properties overrides) {\n+    Properties props = new Properties();\n+    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());\n+    props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName());\n+    props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, MaintenancePlanSerde.class.getCanonicalName());\n+    //apply overrides\n+    if (overrides != null) {\n+      props.putAll(overrides);\n+    }\n+    return new KafkaProducer<>(props);\n+  }\n+\n+  private void sendPlan(Producer<String, MaintenancePlan> producer, MaintenancePlan maintenancePlan) {\n+    producer.send(new ProducerRecord<>(TEST_TOPIC, maintenancePlan), (recordMetadata, e) -> {\n+      if (e != null) {\n+        fail(\"Failed to produce maintenance plan\");\n+      }\n+    });\n+  }\n+\n+  @After\n+  public void teardown() {\n+    super.stop();\n+  }\n+\n+  @Override\n+  protected Map<String, Object> withConfigs() {\n+    Map<String, Object> configs = new HashMap<>(5);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_CONFIG, TEST_TOPIC);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG, TEST_TOPIC_REPLICATION_FACTOR);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG, TEST_TOPIC_PARTITION_COUNT);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG, TEST_TOPIC_RETENTION_TIME_MS);\n+    configs.put(AnomalyDetectorConfig.MAINTENANCE_EVENT_READER_CLASS_CONFIG, MaintenanceEventTopicReader.class.getName());\n+    return configs;\n+  }\n+\n+  /**\n+   * Retrieve the latest metadata for {@link #_topicDescription} and {@link #_topicConfig} topics.\n+   * To ensure the latest metadata update, admin clients retrieve the metadata from both brokers and ensure that they\n+   * have the same metadata.\n+   */\n+  private void retrieveLatestMetadata() throws InterruptedException, ExecutionException {\n+    TopicDescription description0;\n+    TopicDescription description1;\n+    Config topicConfig0;\n+    Config topicConfig1;\n+    ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, TEST_TOPIC);\n+    AdminClient adminClient0 = KafkaCruiseControlUtils.createAdminClient(Collections.singletonMap(\n+        AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, broker(0).plaintextAddr()));\n+    AdminClient adminClient1 = KafkaCruiseControlUtils.createAdminClient(Collections.singletonMap(\n+        AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, broker(1).plaintextAddr()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d"}, "originalPosition": 138}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b394034cd2064f253e70cfc74b4f699cffd6a68", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/5b394034cd2064f253e70cfc74b4f699cffd6a68", "committedDate": "2020-10-01T01:39:24Z", "message": "Use Gson for de/serialization of maintenance plans."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b1fcef05aa2698e1cfa84c006ee9ee6e496c815b", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/b1fcef05aa2698e1cfa84c006ee9ee6e496c815b", "committedDate": "2020-10-01T01:34:12Z", "message": "Use Gson for de/serialization of maintenance plans."}, "afterCommit": {"oid": "5b394034cd2064f253e70cfc74b4f699cffd6a68", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/5b394034cd2064f253e70cfc74b4f699cffd6a68", "committedDate": "2020-10-01T01:39:24Z", "message": "Use Gson for de/serialization of maintenance plans."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88605563058cd02e08d152a1b6798a960b979f96", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/88605563058cd02e08d152a1b6798a960b979f96", "committedDate": "2020-10-01T01:47:04Z", "message": "Resolve encoding issue."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0315af0c8b19b6f23d6727040f7ced52a415eb9", "author": {"user": null}, "url": "https://github.com/linkedin/cruise-control/commit/d0315af0c8b19b6f23d6727040f7ced52a415eb9", "committedDate": "2020-10-01T02:17:29Z", "message": "Add missing JavaDoc to plans."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 66, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}